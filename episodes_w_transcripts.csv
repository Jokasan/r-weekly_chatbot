"","ep_name","ep_date","ep_duration","ep_description_short","ep_transcript"
"1","issue_2026_w_03_highlights",2026-01-14,42M 57S,"It's a new year of R Weekly Highlights! In this episode we learn of the positive intellectual diversity impacting R and Python communities, a great starting point to create your first RAG-powered knowledge store, and a new package enters the Shiny ecosystem that is certainly not tiny in its ambitions. Episode Links This week's curator: Eric…","[00:00:03] Eric Nantz:

Hello, friends. Happy New Year twenty twenty six. The R Weekly Highlights Podcast is back for the New Year. We definitely had a little bit of a break to recharge our respective data, science, batteries, however you want to call it, time with family, but I hope you all are having a great start to 2026. But yeah, it feels great to finally be back behind the microphone and talking to you all about the latest highlights that have been shared in this week's Our Weekly Issue. My name is Eric Nance, and I'm delighted that you're joining us wherever you are around the world.

And, yes, thankfully, I'm not alone in 2026 as my awesome co host Mike Thomas is here virtually joining me as always. Mike, yeah, hard to believe 2026 already. It's amazing how Yep. It

[00:00:49] Mike Thomas:

Another year, more Mike and Eric, our weekly highlights.

[00:00:53] Eric Nantz:

Let's keep going strong. We'll keep going as long as this train doesn't stop moving. So we'll see how far we get. But speaking of how far we get, there's a little inside baseball here. Around the end of the year, usually there's a certain person on the highlight our weekly team that's in charge of making the schedules for the curation. Guess who forgot to do that until last weekend? Yes. That was me. So that meant that with a short notice of notifying everyone else about the schedule, I guess I better do the issue. So it was me curating this week, but we'll have the full rotation already programmed up, ready to go for the rest of our team.

But, yeah, had to dust off the holiday cobwebs a little bit, but as always, we had some terrific poll requests to our GitHub repository for R Weekly, which was a massive help. So thanks to all of you around the world and my fellow curators for helping with this week's issue. And we're going to lead off with kind of a recap of a very important and very highly talked about theme, stemming from the recent POSIT conference in 2025, and that is seeing the influence that goes kind of bidirectional, but definitely in ways that us as R users can appreciate in the world of not just the verses, so to speak, of R and Python, but really working together and benefiting from each other in terms of dev practices and community practices, and that is very succinctly talked about in our first highlight here coming to us from Emily Reederer, who is a senior manager of data science at Capital One, and she has been one of the top thought leaders in this space of going across different data science languages and using them to your best advantage across both ways, and her post here is entitled R in Python, from Polyglot to Pluralism, and her main thesis here of the post is about, you know what, the days of the verses of the language wars, yeah, they're kind of done.

This is now more about how can we get better data science accomplished, leveraging the best of each of these ecosystems and how can they learn from each other. So in her mind, you know, having the diversity from the intellectual dev standpoint and the community standpoint, it just literally helps make the tooling better. And this was highlighted quite a lot in many of the POSIT conference talks, and she talks about a few of them here, and of course my co host looking at me is no stranger to this topic, I'll touch on that in a little bit here, but she shouts out two of the talks that leads off this post here regarding how best practices with our package development in recent years have really begun to influence Python development.

And these talks are given by Rich Iona and Michael Chow over at POSIT, talking about their various Python packages and what they have learned from their days of building the R package equivalents. So on Rich's viewpoint, he has made the great tables package in Python, which is the analog to GT from the R side of things. So he has some really nice points in the post here that Emily highlights in this, in that it's about not just the code itself, but really understanding that different users are going to have different needs for their package and having that user experience, that onboarding about how to use the package being heavily featured and addressed in the package documentation, as well as built in data sets to help try these things out.

So Rich's talk was excellent, I didn't see a live, but I've seen the recording of it, talking about these attributes and how they've informed the GreatTables development. And I've been on record many times on this very podcast saying Rich authored some of the best documentation I've ever seen for packages, and he's carrying that same mindset to the Python side as well with gray tables. And then Michael Chow's talk about, called the cursive documentation, this was an interesting take on this as well, but it's also his experience with making sure that there isn't just all terse documentation from a quote unquote API like standpoint, which a lot of Python modules and packages do get very terse with their documentation, but having more of the long form vignette style documentation is very helpful for those that are getting to know the package for the first time.

And that is something that the Python community, I think, is slowly adopting. But R had it here first, as they say, thinking all the way back to Fritz Leisch, building what became Sweve that was built into R since the very beginning. Of course, we've had R Markdown since then, but being able to build literate programming literally in the language itself, and we're seeing a lot of tooling in Python and elsewhere adopt those kind of mindsets. And then she also mentions that it kind of goes both ways as well, so to speak, the engineering side of it.

We're seeing some advancements in the R ecosystem to take advantage of newer libraries to help with co development, such as a highly regarded air utility based in Rust to help lint our code, and it is blazing fast. I love using it. I use it every day in my positron editor. It just cleans up so much for me in that little control s keyboard shortcut. And then also a shout out to the rig utility, built by posit as well as a way to manage multiple r installations on a host system no matter Windows, Mac, or Linux.

That is something that Python has been trying to address, sometimes a mixed success depending on the framework you use, for managing Python libraries and installations. And we're seeing also how, say, the TidyModels team was heavily informed by what scikit learn has done in their machine learning exploits on the Python side. So there's a lot of interesting ways that both language communities are benefiting from each other in these practices, and it's really, really interesting to see how far this takes.

And Mike, you're no stranger to this, you deal with both languages and you've been a a highly, regarded voice in this matter too. What did you think of Emily's post here?

[00:07:58] Mike Thomas:

Yeah. Absolutely. And I've been very inspired by Emily in the past really on this topic. I know she gave a talk at posit comp. I think it was two years ago. It was called, cleverly titled Python Ergonomics. And I took a lot of inspiration from that in developing the talk that I gave at this this past year's positconf on building multilingual data science teams, which I believe was in the sort of the same session, as as Rich and Michael's talk, which was fantastic. And obviously, developers like Rich and Michael are are really doing a great job of bringing ours great focus on user experience to Python through their their table, packages like great tables and point blank. And we also have other packages like plot nine and Python that mirrors g g plot two and R. It's more great work from from posit to essentially make these capabilities language agnostic.

It's it's just pick whichever language you want, and the same tooling essentially will be there for you. Some of the key traits, you know, that I think are being adopted by Python, as Emily mentions, is making packages nicer, by including, you know, sample datasets. We take for granted that forever. You know, if you install a package, there is almost always a package dataset that enables you to get up to speed quicker and and run examples, that you see, I of dplyr. We always have the Star Wars dataset. Right? It's just built in for us to test out all of the different, you know, crazy things that dplyr can do for us without needing to load some CSV that we need to have on our laptop first. So it allows users to get up and running a lot quicker.

And, you know, I think that really bleeds into documentation in terms of, vignettes for Python and them trying to move beyond, you know, what they may have had in the past around package documentation. My guess is that package down, the the R package that allows us to build these these HTML websites that we can host with, you know, GitHub pages and things like that, probably really distance R from Python in terms of robustness of the documentation in the last few years. I'm assuming that the Python community caught wind of what package down could do and I I think created a fairly obvious gap.

And obviously, you know, think Quarto has also been a game changer here. And the more that we can enable developers to leverage Quarto in package documentation and wire those two things together, I think the more that we can provide clarity to end users, and LLMs, unfortunately, nowadays, on how to best use our R and Python packages. And as you mentioned, you know, Python like robustness in R, R is gaining, you know, high performance developer tooling that's very much inspired by Python. We think of Air, the the code formatter you mentioned. Rig is another one that Emily mentions that helps to mirror the the functionality and feel of, PY N for helping users manage multiple r versions.

If you are a docker user, there is a dev container feature, in the rocker project that, allows you to essentially leverage rig to install the exact version of r that you want to install in that container. So if you were trying to do reproducible data science as as we really have a focus on on our team, that may be helpful for you. It's something that we use in just about every project that we hand off to clients in particular because we know that it will work on their machine exactly as intended. So I I, you know, really think that this blog post is another fantastic, one from from Emily on this particular topic. Anything that she puts out is essentially required reading, in my view. Yeah. I think sort of this concept of intellectual diversity, you know, makes tools better across both R and Python. And and when developers from these different backgrounds

[00:11:52] Eric Nantz:

collide and come together, we get better engineering in R and we get better design in Python. Yeah. Yeah. And I I definitely agree with all those points. Another utility that we've been seeing slowly slowly get some attention on the Python side back to package management side of it, UV has been highly regarded as another way to manage your project specific dependencies, and we're now seeing an R sort of equivalent called RV that we talked about, I think, late last year. So, yeah, reproducible data science on the tooling side, as well as being able to leverage a package effectively.

We're in a good time here and with the advent of having to, you know, use the best tool for the job. The gaps have definitely tightened a bit, but there will be cases where you need to hop back and forth and that's where having the choices, but also, you know, what to look for is highly important. So I dare say this is a great start for stuffing my data science polygout wallet, if you will, with some with some great content. Yes. I've been watching too much ESPN lately. You're welcome.

[00:13:04] Mike Thomas:

I will second second the UV comment. If you are starting a new Python project or just getting into Python, start with UV instead of any of any of those other Python environment managers. Is so easy to use and it just works fast. And I did just see RV come across my desk as you mentioned, Eric. I have not tried it yet. I don't know if you have,

[00:13:28] Eric Nantz:

but I'm excited to take a look. It is really promising. So I look forward to playing with this even more, even as the veteran of PACRAD and RM, which have, we have a, mostly beneficial relationship. Sometimes things go haywire though. So I'm always looking to see what else is out there. So yeah, there's a couple mic drop moments right away in this podcast.

[00:13:52] Mike Thomas:

Choices are good.

[00:14:09] Eric Nantz:

And yes, 2026. You can't escape the use of LLMs across much of our dev workflow, and boy, when they work well, they really work well. But there's always that age old trade off, right, of if what I'm building now requires something that has some knowledge, so to speak, or resource that wasn't necessarily used to train these models, just what do you do? What if you have this very intricate source of information domain specific wise that you want to have available to you, but you know that off the shelf Gemini or quad model just doesn't have any exposure to it? That's where a really important technique that I've heard about for a long time, but admittedly was kind of intimidated by until recently, called Retrieval Augmented Generation, RAG for short, is a very important set of tooling, so to speak, that you can have in your next project that has a leverage, the use of AI.

And how the heck do you get started from this as an R user? Well, guess what? The tooling has definitely caught up speaking of, you know, tooling and R ecosystem getting up to speed much quicker these days. There is some great packages out there that you can use to build your own knowledge storage in R to be tapped into by various LLMs. And this tutorial blog post comes to us from Miles Mitchell over at Jumping Rivers, and this is a great practical post on how you can build a Rag knowledge store using just off the shelf packages in the R ecosystem.

So as we talked about, this is where RAG is important for a lot of reasons here, one of which, like I said, is if you have another additional context source, if you will, typically documentation, maybe reports or whatnot, and you want to make that available to your LLM session or model provider to be able to tap into as needed. So there are lots of different phases of this, but luckily in R, there is a particular package that can help you get started really quickly, and that comes to us from the POSIT team called Ragnar, and this is a very important package in this space, I'll link to it in show notes as always, but this is helping you implement Rag all in the confines itself. And it's got a few different steps in this process, but we'll walk through a little bit here verbally what that entails.

First of which, you got to have that knowledge source available, hopefully in a web kind of friendly text format. A lot of times say markdown documentation sites are a great source of this. So in this example, they're actually looking at the web version of a textbook called Efficient R Programming, co authored by one of the, you know, the founders of Jumping Rivers, Colin Jawispie, and that URL is available. You can find that in the show notes as well. So what Ragnar lets you do is read the contents of that website as Markdown with the appropriate name, read as Markdown function.

Once you have that, this is now split up into what are called kind of chunks in a nice tidy tibble with each chunk being a different row, and then the text actually containing the the actual, like, verbatim text in that particular chunk as defined by the starting and ending index of characters. Now that's just step one. You gotta you know, you can actually visualize this in your R session with a Ragnar chunks view function, which is neat to kinda see what's behind the scenes there, but this is not ready for the models models yet. There is something you need to do to create under the hood what's been known as like a vector database, a way to translate this text into tokens that then the LLMs can use, and that's where there is a Ragnar store create function that will help you take into this, or define, I should say, what this kind of internal vector database will look like, but the embeddings are model specific.

So depending on which model you're using, you might have to use a different embed function that comes from the package, such as one for Gemini, maybe one for OpenAI, whatever else, they have separate functions for each of these. Once you have that kind of store set up, then you feed in those chunks that you got from the website into that store, and then from there, you got it in the store, but imagine like a grocery store where all the products are not organized yet, you got to actually start organizing this and create an index, and that's where the Ragnar store build index function comes in to get it actually ready for the LLM usage. So all this is stepped up through in the blog post, but once you have that, then you can actually start to use this using the Ragnar retrieve function, such as in the example here, they take that store as the first input and then ask the question, who are the authors of EfficientR programming? And sure enough, you get a tipple back and it will give you text to actually tell you in the well, actually, not so much text yet, but the actual well, not just the text, it's a context, it's a distance calculations, all in the confines of this vector database investigation, and you will actually see then the text showing the authors there.

So there is a lot of interesting things you can do with this because if you want to feed in more than one source, you can feed multiple, you know, sets of knowledge chunks into these stores and just kind of rinse and repeat along the way, and then lo and behold, you can set up via the Elmer package a chat function, or chat object I should say, and then register this store as another means for that particular model in Elmer to get this knowledge, and sure enough, they do just that with asking us some questions about, say, good practice for parallel computing in R, and then it can even tell you where it got the answer from.

The one some things to watch out for is this, is as you're building these stores, these different chunks into these stores, this is going to add to your token count when you go back and forth between these model providers, so cost may or may not be an issue, depends on your organization, but this is a great way to get started with getting that internal resource ready for your models to run. And I think this is a great way to jumpstart your Rag journey into using packages like Elmer and the like to supercharge your next chat sessions.

So, Mike, have you done much of RAGs lately? This sounds like a pretty interesting technique here.

[00:21:49] Mike Thomas:

Yeah. We've done, you know, quite a bit with RAG and I think there's always been a trade off for AI engineers. You know, I I don't know if it's starting to change or if it it has changed, but the concept of leveraging Rag versus actually trying to, like, fine tune your own model, which I believe would be, you know, actually potentially training an LM or or doing some form of that would be the the fine tuning, which is, I think, much more difficult than leveraging rag. And if you can get good results out of your rag setup, it's probably a lot, easier to sort of stand that solution up. And right? The the real problem that everybody's dealing with is that LLMs, you know, tend to hallucinate a lot because they lack, you know, real real time information or they will hallucinate because they don't have access to the data that you are asking it about because it wasn't trained on that. And they have gotten better at this, I think, in, you know, more recent times where maybe they'll actually admit that they don't know the answer to your question. But I remember when LLMs were first coming out, if you ask them, you know, a a question, like, I had asked them who who Catchbrooke Analytics was, I think, at one point in time, and it it just made up a completely fake company, that said they were in pharmaceutical sales or something like that. You you couldn't find it on the Internet. I don't know. Maybe there is another catch, Brooke, analytics.

You know, it was it was interesting. They hallucinated, you know, all the time, unless you, you know, had that rag set up and you gave them specific information about the question that you were asking. And that rag workflow, you know, really helps to provide that model with sort of this external knowledge store, as we call it, which, you know, are often our our documentation, to look up the information before answering. And the first step of that process is parsing that documentation into something that the LLM can read. And as you mentioned, that's where the Ragnar package comes in to build that knowledge store and extract text, you know, from these, you know, PDF documents or other sources, chunking it into contextual pieces, and storing that in a vector database so that the model, can search on it efficiently.

And then Elmer is sort of the second piece of that equation that allows our users to interact with various LLM APIs, you know, such as OpenAI, Google Gemini, Claude, and leverage that connection to the rag store that they were able to create. And Miles does a great job, providing us with an example, you know, building that knowledge store using the efficient r programming textbook, registering it with Elmer, and then the you know, clearly accurately answers some of those technical questions. Think about parallel computing and even sites exactly where it found the information, which I know is an important sort of development and very important for, you know, production level workflows as well and auditability and and traceability in a lot of domain settings.

And I think sort of at a higher level, you know, Rag sort of makes LLMs production ready, for a lot of organizational use cases where they need specialized access to to knowledge that's really specific to your organization and might be outside of the corpus of data that the LLM was trained on. And or you you may potentially just wanna have the LLM focused on the information that you wanted to know about, and really not care about, you know, all of the other potential information that it was was trained on. Last year, you know, we had built out a simple radio app to demonstrate rag and gave it a single PDF. You know, this wasn't publicly available information in the PDF for its knowledge store. And we had a really cool component to that app that was a button that could toggle rag on or off.

And if rag was on, I would ask it some random question like, you know, give me a recipe for banana bread, which was totally different, you know, than the PDF context we were giving. And it would actually come back to me and say, hey. You know, I don't think that's appropriate right now because I'm sort of set up to answer questions about this particular domain. And I thought thought that that was awesome. Yeah. So that might be an interesting thing if you need to sort of demonstrate the capabilities of Rag, in your Shiny app or some sort of a web interface like Gradeo to consider doing something like that because it was really eye opening, for a lot of our audience.



[00:26:11] Eric Nantz:

Yeah. I'm jotting that down as I start to build more interfaces on top of these. But, yeah, it's it's a very interesting interesting technique and it does make me it makes important another thing to underscore is if you want to use this, you know, having that knowledge store or that source of that knowledge store hopefully easy to digest via these mechanisms. I know in a lot of organizations, lot of knowledge is locked in proprietary files and whatnot, so it can be a Herculean effort to get that ready for consumption into RAG. But boy, I think the benefits will definitely pay off if you can get support to translate some of this internal domain knowledge into ways that we can leverage more effectively.

I mean, basics like PDFs of these certain, in the pharma side of things, something called the schedule of events that determine when patients get certain, you know, treatment drug versus like lab assessments versus other assessments. A lot of that's locked away in some very bad formats. And we're trying to make more machine readable formats of all this so that in the future, you know, things like RAD could be built to help interrogate some past, you know, study information or whatnot. Still a very long ways to go in this space, but I'm excited to see just where you can take these techniques and building interfaces on top of it because these models can't do everything folks. You got to help it out along the way.

And rounding out our highlights today, you know, Mike and I are always fans of Shiny. So as I'm curating this issue and I get this list of the new packages that have come since our last issue as well as the updated packages, and there was one that particularly caught my eye as far as like, you know, what we might talk about or at least might be featured in the highlights section, and I saw this package called Tiny Shiny Server, and I thought, that's interesting. What is this all about? So I started reading up the it's just landed on GRAND recently, so I started looking up the GitHub repo and I saw things, Mike, that boy sure sounded kind of familiar, but this is all done in R itself. So let's let's talk about what this has been built for, at least according to the README.

This is appropriate for hosting multiple shiny apps behind a single server. Sounds familiar. Development environments of multiple projects. Yeah. Maybe. Small scale production deployments. Oh, you and I deal with production all the time. And hosting R Markdown and Cordial dashboards with the Shiny runtime. Okay. This is this is intriguing, so I actually gave it a spin on my, Nixify dev environment set up at my other computer, and for the most part, it did basically what was advertised, and what's interesting is once you define a directory of your app dashboard, if you're doing like portal dashboard or flex dashboard in the R Markdown side, you basically make a subdirectory and they have an example directory you can copy straight from the package itself, and within that you could have any number of Shiny apps as subdirectories.

So they have simple inventory one, simple reporting one, the sales one, and whatnot. And then there is a configuration file in JSON called config dot JSON, where you spell out all these different apps, each get their own, you know, sub entry, the name of it, where it's located on the file system, and then another interesting flag called resident. If resident is set to true, that means that the app is always running. This would be very similar to the classic, you know, Shiny server days, the app is just running no matter what, or I pause the connect internally.

Whereas if it's set to false, it'll only spin up when it's accessed. That part sounds really familiar to me. You probably will comment on that in a little bit, but I'm wondering just how does that work just from the R side of things? That usually is kind of like a container kind of thing to make sure that is only spun up on demand, much like services like shinyapps.io would do, or you'll probably mention another one a little, but I won't steal your thunder there. Now I will say I tried that feature. It did not work on my system, so there may be a little bit of gotchas in how that was all implemented, but it was intriguing nonetheless to see them trying to pull it off.

So this readme is quite comprehensive. I kept looking, it's like there's a lot of options here, a lot of configuration, even about if you're going to host this behind a proxy or SSL, some instructions for that. If you want to do like an NGINX kind of reverse proxy or a caddy file. So all this is very DevOps y kind of features here too, with a management interface as well if you want to look at a high level of all these apps. So I was, you know, admittedly very blown away just how much could be accomplished literally with just an R package to do all this.

And then I did a little, you know, I like to, you know, shout out whoever authors these utilities on our weekly, so I took a look at the root of the repository, lab seventeen oh two, I believe is the alias, and then I read the description, just some random hobby projects I made throughout the years, The more recent stuff is mostly AI created as a way to learn how to work with AI. And sure enough, if you go to the TinyServer GitHub repo, on GitHub you always see like a contributors link or set of contributors on the right side.

Sure enough, right under lab seventeen oh two is Claude. So I'm guessing this was a mostly AI generated package. I will say though, how it got there, I have no idea. I mean, it sounds like Claude somehow, but I must say pretty impressed with how far it went. I'm not going to pretend to say I'm going to use this in production anytime soon, but it's a great way to see just using some of the dependency packages that they reference here in the description file, such as like the web sockets kind of packages, but really it's HTTP UAV.

It's not a huge dependency footprint. Might be an interesting learning opportunity nonetheless. So yeah, impressed in a few different ways, but definitely caught my eye and it just shows you. If I hadn't started looking around that GitHub repo, I was almost completely fooled. Maybe it was because of lack of sleep too. I don't know. But this is this is probably going to be more common than we realize, Mike. So very interested in your takes on what Chinese Shiny server is trying to accomplish here.

[00:33:49] Mike Thomas:

Yeah. I mean, I don't want to make any assumptions on how much of it was AI generated versus how much of it was sort of human reviewed. I assume all of us, you know, are doing that to some extent these but I think, you know, as you mentioned, Eric, probably there is a a real thing going on in the ecosystem and data science in general, where fully AI generated stuff is is getting out there for better or for worse. This this concept is, I think, excellent. Having a sort of more lightweight, you know, WebSocket enabled, you know, proxy server designed to host maybe a couple shiny apps in our markdown documents and and Cordo dashboards behind a single server.

I think, you know, obviously, there's there's great need for making productionizing of your data science content easier, right, if possible, especially in smaller settings. I think WebR has done some of that for us, but I think for a lot of folks, it's probably still a little scary territory because there are some gotchas, from my experience on hosting, you know, Shiny apps, for example, with WebR that are very very niche to, that that WebR framework as opposed to if you were just, you know, running it, obviously, on your own laptop.

But, I think this is really, really interesting. As you mentioned, one of that that standout features that they market in the the repositories readme is the the resident versus on demand apps where, you know, resident apps are always running. I would I think this gets into R sessions. Right? You would have to have different apps run on different ports from the the same R session. I imagine that we're only spawning a single R session in this process or maybe not.

[00:35:39] Eric Nantz:

It seemed like one to me when I ran it last night. It was pretty bizarre.

[00:35:43] Mike Thomas:

Obviously, you couldn't get that on demand apps, the other side of it, up and running. And I would imagine that that would have to mean that you would have multiple R sessions running, in order to have one spin down. But, obviously, I had two apps, one was a resident and one was on demand, I would assume that I would need two hour sessions. I'm not I'm not sure. You know? And then sort of the questions then become, you know, what's the separation between users? Are they sharing sessions, not sharing sessions?

You know, a project that we talk a lot about and implement quite a bit for a lot of our clients and even ourselves is the ShinyProxy project, which this feels kind of like a lightweight ShinyProxy, environment. And that is very much Docker based such that every app that gets spun up for a particular user is in a separate container, sort of by default. You can do some things to to mess with that. But by default, every user that logs in and and wants to start an app essentially launches a docker container that creates a lot of separation between users.

But, you know, the other interesting thing here, you know, that I think a lot of these types of frameworks have, like, you know, shiny proxy and obviously, posit connect, is that tiny shiny server boasts a web based dashboard on port thirty eight thirty nine that shows you sort of admin level statistics on, you know, real time connection counts, IP addresses of users, app health status. Supposedly, again, I haven't played around with this. I don't know if you peaked at port thirty eight thirty nine while you were trying it out yourself, Eric. Yeah. I did. It was, yeah, some pretty interesting metrics. I'm, again, all impressed just from the R side of things. It's crazy to me.

Very interesting. And then, the the last sort of feature around management and monitoring is automatic memory management. It boasts, you know, such as, you know, cleaning up stale connections after thirty minutes, assuming that's the default, and then clearing, you know, dead process objects that obviously, may remain after, you know, users end their session. But very, very interesting, you know, particularly I think for those that find or don't need, you know, crazy full scale production deployments for, what they're trying to host. This might be sort of a quick start version, you know, simple sort of JSON configuration here, that may get you up and running in a smaller production ish setting or POC setting.



[00:38:20] Eric Nantz:

Yeah. I mean, again, impressive technically how all this was done, and I can definitely see value in exploring kind of under the hood from the friendly confines of R how all these different concepts work. So yep, this got my attention and yeah, I may play with it a bit more just to see just what are the internals of this package really doing. And my quick glance last night, it looked like some use of reference classes and whatnot. So it's definitely using some kind of object oriented programming, which again, interesting to see what, you know, what drives behind the choices of these different frameworks and also seeing how this compares with some of the great packages that Thomas Lynn Peterson at POSIT has created with his take on web technology and R with like his Fiery package and other packages that the Plumber two reinvention of a plumber is basing off of as its dependency footprint. So yeah, definitely something I'm going to ponder and like I always say every time we do this show, I learn something new and this was by far a new take on Shiny session management.

Again, straight from R itself. So who knows what you find out on these issues and you can find out a lot more. Guess what? You just go to rlk.org. This is the first issue of 2026 so we hope to have obviously many more after this and we have our curator team ready to help out with the rest of the issues. Before we go, I do want to acknowledge some interesting feedback not so much feedback from our contact page but we were recently, I recently got a nice note from a, listener, I believe, Yan Terman over on Blue Sky, who did an interesting analysis of scraping the transcripts of our very podcast and doing the episode length across time of what we've talked about.

And, you know, I put a link in our show notes, Mike, if you want to check it out, but there's an interesting spike apparently in 2022 somewhere where we had a long episode and it's been a general trend back up. You and I doing long episodes, who would have thought that, right? But it's an interesting graph and apparently he's been doing listening to us while biking across across America. So, hey. If we can make your bike rides more fun, we're we're here for you, buddy.

[00:40:47] Mike Thomas:

That is awesome. Thank you for that analysis. I can't wait to check it out. And we may ramble once in a while, but only only once in in a while. It's sporadic. Mean, the data shows it. Perfect. Who knows?

[00:41:01] Eric Nantz:

Yeah. Yeah. No. Anyway, you tune in for the R content. Maybe you stay for the rants, depending on how that flipped around. I have no idea. Nonetheless, if you want to find out more, obviously everything's at rok.org. This is a community project, obviously we value your input and how you can help out is sending those great resources via a poll request to the github repository of R Weekly. Everything's available at rweekly.org. You know, it's not hard to find and also we love hearing from you wherever you shout us out on social media or you get into our contact page on the episode show notes. We're all right there. This podcast has great transcripts as we just heard, as well as chapter markers, so you can always skip to whatever highlight you're interested in. Hopefully, you listen to the whole episode, but we won't judge.

Nonetheless, we're just happy you're listening at all. So with that, you can get in touch with me on social media in various ways. I am on Blue Sky with r podcast@bsky.social. I'm also on Mastodon or I'm rpodcast@podcastindex.social, and I'm on LinkedIn. Just search my name and you will find me causing trouble there. Mike, where can listeners find you?

[00:42:15] Mike Thomas:

You can find me on blue sky at mike dash thomas dot b s k y dot social, or you can find me on LinkedIn by searching Ketchbrooke Analytics, k e t c h b r o o k, and see what I'm up to lately.

[00:42:28] Eric Nantz:

Very nice stuff, and it's great to get back in 2026 with you. And, yeah, I'll try to keep my ranting to a a minimum sometimes, but I can't guarantee it. But nonetheless, I think we got relatively unscathed here. Any feedback's welcome no matter how far I take it. Alright. Well, that'll do it for episode 217 of highlights, and we will be back with another edition of RWKY highlights next week."
"2","issue_2025_w_51_highlights",2025-12-17,31M 38S,"A retrospective that showcases favourite data visualisation projects and insights from 2025, a practical guide offering R package maintainers methods to gauge how their package is being used, and an R package release introducing additional helper functions that extend dplyr::mutate() for generating columns with useful rowwise and list-column…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 216 of our weekly highlights. This is the podcast Mostly Weekly, where we talk about the latest highlights and other excellent resources that have been shared on this week's our weekly issue. My name is Eric Nance, and, yes, the the year is already wrapping up. It's amazing how quickly 2025 has gone in certain aspects. But, yeah, here we are in December. Right? So we're gonna close out strong here. This will very likely be our last episode of the year as obviously next week and I hear from many celebrating the Christmas holiday that will be right smack at our usual record times. And we'll likely take the rest of the year off, but we're gonna finish hopefully strong in this episode here.

And my co host, Mike, is joining me right now virtually, but we're gonna kinda get right into it here to save us some time and because yours truly forgot to hit the record button when we first started this. But this week's issue is curated by Batoa Almerzak. And as always, she had tremendous help from our fellow, our weekly team members, and contributors like all of you around the world with your poll requests and offer suggestions. And leading off of our highlights this week, and it's been, you know, as as this time of year rolls around, the time to reflect, time to think about our journeys, whether in data data science, data visualization, etcetera. But on the topic of data visualization, one of our most frequent contributors to our weekly in terms of the highlights is back with telling us about her journey throughout the year and her visualization adventures.

And that is none other than Nicola Rini, who I would also want to shout out for being very gracious of her time at the recent r pharma conference of helping us with a workshop and kicking off our second day, but nonetheless she is here with this blog post entitled my year in data visualization and she leads off with the fact that it's been a pretty busy year for her personally such as a small detail of changing a job midway through, but that change of job apparently has given her the ability to work on data visualization a lot more regularly now as part of her new role.

So that's been great to see a lot of her learning journey in this, but one of those various ways that she learned is to try new ways of visualization. So after her introduction to the post, she leads off with, it definitely doesn't look this way, but it is a built from scratch visualization of what's called a Tardis, which is kind of like a looks like a police station entry point or a box. I believe that's native to Great Britain or England, but she built this, not with r, entirely with d three. Every rectangle, every line, every feature of this, whether it's the roof, the windows, the door handles, the letter on the door itself, all built through code.

And you can actually visualize this online. She's got a and now that I see when it's online, it's referencing doctor who, another great show for all of you out there. I need to catch up on that, but this was, you know, dealing with, traveling through time and they used this, Tardis which was a ship disguised as a police box. So that's where it's inspired from. But when you click that link, you can actually play with this in real time and get tool tips over the different parts of the building. So it's not just the visuals, the interactivity of it to boot.

So amazing stuff, and there is a link to the dedicated blog post about this if you wanna learn more. Really good stuff here. And next from the data visualization side with actual data itself, she highlights in one of her favorite visualizations this year was looking at how passport strengths have changed over time throughout the different countries and continents around the world. And this is definitely, a really powerful hybrid of a line chart with the different lines for the different countries or the regions within them.

Very quick to the point, but nice use of facets, nice use of color, and it is really really inspiring to see you can do that all with ggplot two. And then she also talks about her visualizations around the fact that the tidy Tuesday project has been a huge focus for her. Ever since her beginning of working with that data in 2021. But it has been a great catalog that she's put together, which you can see in her actual graph gallery, visualization gallery, if you want to look at that in the blog post, but she puts in this post here, her first visualization for transit infrastructure costs across the different countries and for our first ever contribution to tidy Tuesday that looks fantastic as well.

So she had no idea back when she started how long that she would be doing this but she's keeping on going strong and speaking of strong she wants to keep going strong in 2026 on top of doing new workshops in the realm of visualization. She has a new book coming out folks called the art of data visualization with ggplot2. I am definitely putting that on my bookshelf. I cannot get enough of this kind of content and I will be keeping my eyes open she says to stay tuned for more information and that will be something I use to try and level up my visualization side of it which to be frank here I have not had nearly the growth of my visualization exploits that she's had this year but hey better late than never so I'm hoping in 2026 I get to train myself a bit better, the data vis side of things.

Mike, what did you think about Nicola's journey here?

[00:06:22] Unknown:

I thought this was a fantastic reflective post to to wrap the Europe in data visualization from Nicola's perspective, and great shout out for her new book. I've had the opportunity to take a look at the online version a little bit. It looks phenomenal, some great use cases, very applied. It seems to be a beautiful quarto style book, so you gotta love that as well. And once you have a chance to check it out online, buy a copy from CRC Press and support Nicola in all that she has given us in the data visualization community. She consistently does a phenomenal job, via her website and GitHub, making her materials, you know, not only for her 21 talks in 2025, but her many, many more talks and presentations in years prior, Making that all publicly available from for us to learn from and not just the slides, but also the underlying code within the GitHub repositories that that she consistently puts together. And I'm very grateful for that. I have have leveraged those repositories many times over, in some of my data visualization projects that I've had to do, that have been very helpful. So thank you, Nicola, for that. And as you mentioned, Eric, this this passport strength visualization that she highlights as one of her favorites from the year is so cool.

The craziest that I get is usually fascinating either my bar line or scatter plot. I've got lots of improvement, and this chart has so much information packed into it. But at the same time, it is is very accessible to understand the message that's trying to be conveyed, the highlighting of the lines that she wants to draw your eye to, and then sort of the graying out of the other lines that you can still see them if you want, but they're not sort of telling the story, and she's able to leave the story with the highlighted lines.

And she also highlighted, no pun intended, the ggplot two battles project, which was an effort by Mike Lattimore to create a competition. It looks like to recreate a chart using r, ggplot and WebR, and obviously doing all of this right within the browser, which is super cool. And, you know, she also highlights sort of at the end of this blog post that the TidyTuesday effort, I know she's been involved in. And my recommendation here is if you're looking to advance your career or stand out, you know, in the hiring process, which I know is really difficult these days, or even if you're looking to network with other data scientists, I know there's a fantastic community on Blue Sky around folks doing TidyTuesday.

It's such an easy thing to get involved in, and a great way to start learning. As well, you can join the DSLC Slack community, Slack Workspace, because I know there's a lot of folks folks on there doing TidyTuesday as well. And, that would be my recommendation for any career advancement into 2026 for the data vis folks out there.

[00:09:08] Eric Nantz:

Yeah. It's it's amazing, like you said, how open that Nicola has been and offers in this space to help everybody, you know, learn new techniques and and for her to learn new things too and not just keep it on their silos, but, but to share it as well from both herself and others in this in this space. So this will be some fun some fun watching and some fun, learning for me as I get some downtime this break to hopefully level up on some of my skills. Famous last words, of course. And up next in our highlights today, Mike and I are no strangers to building, whether it's applications or packages used by our clients or our customers and our respective organizations.

And sometimes we get asked quite a bit, how's it being used? What are the most popular uses for it? And sometimes it's hard to just think about what are the best ways to assess that. And it gets probably even more complicated, honestly, when you think about the world of open source as well, because you may be thinking you've you've done all you can, but maybe there are a lot of other areas that you're not quite that, you know, not quite thinking about. But it's not as hard as you might think to be able to find some interesting ways to assess just how your r package is being used in the world.

And this next post is coming to us from the rOpenSci blog, author by former curator of r weekly, Ma'al Salman. And she has this great, great post entitled how to assess usage of your package and going over some of the various ways that you can start measuring this and not in any creepy ways or anything, but some pretty novel ways to look at this. Right off the top, probably one of the most easiest ways if your package is on the the CRAN repository or Bioconductor, you can find data associated with the download of your package. And she's got links to both the CRAN logs project as well as the Bioconductor project and their package stats.

Now there are sometimes, you know, I have to take some of that with a grain of salt for sure. So you gotta watch that a little bit, but it can be a good, you know, first indicator of things. And then the other thing to note is that what if your package, has, is part of reverse dependencies? And you may wanna talk about or figure out a way to enumerate just what is the dependency footprint, so to speak, about where your package is being used or what your dependency footprint might be for your package. And she has an example here for counting the number of hard dependencies, which are the imports packages are declared in the description file of the cURL r package.

Pretty easy way to do that with some package, coding here. And there's also you can count the number of direct or what she calls nonrecursive dependencies with a package called p k g cache. I I never heard of this before, so I'll have to take a look at this when I get some time. But there's a nice snippet here to count the number of direct, dependencies with curl, and ends up being around 436 if my eyes are serving me correct. And then the total number of dependencies when you go through that kind of entangled diagram of network of sorts for curl is well, my gosh. My eyes just see me. 5,277.

My goodness. That's that's a that's a big number. So what's nice is that this these metrics are also accessible on the our universe project. So you can grab that information from a package website as well. But the package cache, package itself, easy for me to say, gives you another way to get to those dependency metrics. And then also if your package is distributed on GitHub, there are some neat ways you can use intelligent or advanced GitHub searches to find instances of where your package is being used. And she's got an example of, again, going back to the cURL package searching for the text of library cURL and to be able to see just how many, you know, repositories that shows up in.

But then also you can take advantage of badges as well, such as from our universe and others, that give you a way to quickly on a repo enumerate how many times your package has been, you know, downloaded from our universe and the like. Some cool ways to do that. From the research perspective, you can also look for citations of your packages. And once again the our universe project gives you a display on your packages page on your citations as well. So you might want to see maybe not so much the number but just what is the breadth of, you know, research out there that's leveraging your your particular package.

Other ways of doing this as well with repositories like GitHub, usually people will send a little like button, or like indicator for a repo, and that can be nice to kinda see some higher level metrics of just how much is being, you know, liked in the in the open, so to speak. But, also, what kind of contributions are getting to the repository? A lot of issues being reported, maybe code contributions. You can you can take a look at a lot of that too, and there is some experimental support to get this, from the rOpenSci side of things looking at, repo metrics and experimental dashboard that she has linked to in the show notes, to kind of take a look at a high level for the rOpenSci project, but maybe you have a suite of packages that it might fall under. Who knows?

There are some newer efforts to try to build in a non obtrusive form of telemetry, in the r ecosystem. This has been a touchy subject in years past where it did cause quite a social stir, if you will, for some, I believe some at the time posit engineers asking about something like this. But one of the newer packages out there that's gained a lot of good attention, it is using this but an ability to opt out of it is the duct plier package. They really want to use this to help inform their priority for development.

But like I said, there are transparent ways to turn that off if you're not comfortable. But if you think about it from the world of a maintainer, yeah, your package may be used by thousands of people potentially. But how is it being used and what are the gaps that are being seen? Sometimes people don't report those issues on GitHub or the mailing list or whatnot. So being able to find a way to let users in a non privacy violating way to send that information to developers I think is something that needs to be looked at and maybe duck player is the first you know success story in that space but I guess time will tell.

As you can see, there are varied approaches to assessing usage, whether it's number of downloads, citations, footprint and dependencies. But my I was giving us a lot to think about if we're gonna build some of these, you know, real time kind of tracking in our packages, but different ways of doing that. So definitely something for me to ponder about this break, about my new Shiny State package. Are there ways I can measure that usage in the open source? So we shall see.

[00:17:24] Unknown:

There you go. Yeah. For the the only way that I've ever measured usage of any of my packages is using the crayon or they call it medic crayon badges in my read me, that show me how many either lifetime downloads I've had, how many downloads per month or year. I think it has some different options, but it's a nice it's a nice feature that we've had for a long time. And I haven't done much more than that and like you, Eric, I had not been aware of the package cache, our package yet, but it seems like it does a lot of the heavy lifting for us, allowing us to explore, you know, not only the packages dependencies but also its reverse dependencies which can be pretty interesting as well. And it's nice to see that we can do this and when I say this, I mean, use that this package cache, our package for both crayon and bioconductor, and one really useful tip that Mel points out in this blog post is leveraging the advanced search sort of capabilities on GitHub, which is something that I have not done and I know, you know, sort of these search queries have so much power behind them if you take a few minutes to actually learn how they work. Right? And Mel put a great use case together about how to search for code mentions on GitHub.

Really just seems like sort of a get request. You're just appending on to this URL, of github. Github.com/search. And a great example there, she posts to try to take a look at how many times, the the curl package is used in GitHub repositories. We can also, in terms of citations, our OpenScI has a citations database. They manage our universe, has some citation statistics too, which I've I've noticed lately as well as Mel, also points out. And I think this has a lot to do with this, DOI, I guess, citation framework, which admittedly I clearly don't know a whole lot about, but I should because lately we have been publishing, you know, a few a few new R packages and I would love to be able to, you know, stand up some sort of scholarly framework around them as well and and, you know, see if they're getting cited or used by others in interesting ways. So, and the last thing that you mentioned, Eric, was was telemetry and, it seems like it's opt in for the Duckplier package, which is great. I think that's the way that it should be. And and one thing that I I wonder and I haven't researched yet, but I was curious as to if Python, you know, has anything like this in terms of just the language itself or the new Python packages that are published and kind of what the standard practice is around telemetry. It seems a little counter to the spirit of open source, but I certainly understand how it could be tremendously beneficial.



[00:20:12] Eric Nantz:

I yeah. And and like I said, it's a touchy subject. And within the R community itself, I wanna say it was either Jim Hester or someone else was was, thinking about how they might build that into a framework like dev tools. And it, yeah, it was not taken very well, and they put the brakes on that from what I understand. But I I think still, if you give a mechanism to or at least be transparent as possible about how you're gonna use that telemetry and, more importantly, ensure that no one's privacy is compromised.

I hate to say it, but there is a lot of people out there that are consuming these packages, these tools, even when I think about the general Linux community as well. A lot of times, these developers feel like they're in a vacuum a little bit. They they have their core group that is, you know, very, you know, communicative with them. But a lot of the regular users out there are using it using it. Maybe something goes wrong. They may just got tired of it and move on to something else and not report back to the to the project.

So I I see both sides of it, but you have to do it responsibly. So I like I said, I'm gonna watch how Doug Plyer handles this and seeing what kind of, you know, information they are getting from this. And if it does what it they are advertising it to do to help inform their future development, but hopefully in a respectful way. I I think, yeah, I would like to see how other languages are approaching this because you just as much as I would like everybody to immediately go file an issue if something goes wrong, if they know where the repository is for that particular package, A lot of times they're not doing that.

It can be hard as a developer to figure that out. But, hey, maybe in 2026 we'll see we'll see better ways to approach it. I have no idea. And rounding out our highlights today. Mike and I have also been very big, consumers, if you will, of the tidyverse suite of packages, especially dplyr, to do much of our data munging transformations. And, yes, every single script I write with dplyr often has one or more mutate calls. Some of them are quite simple. Some may be just recoding something with case when. Sometimes I'll do some basic math with it.

But then there may be times you do a lot of complicated logic with it. Many times I would throw that in a function, not to build it from scratch, then feed it in the mutate. Maybe it's becoming part of my per, you know, iteration pipelines. But there are some there is a newer, at least newer to me, package that is giving mutate a whole lot more than when it comes advertised with. And this package is called mutagen, authored by Gustavo Velasquez and this is version o dot five that we're talking about here. But this is again one of those cases where our wiki shows me something new that I had no idea existed.

And right off the tin, Gustavo says that the goal of mutagen is to provide extensions to the dplyr mutate function. And, apparently, this has been inspired by other statistical computing frameworks such as Stata because right on the page of the site or the the package down, site for the package, there is a nice table that shows the handful of different functions that mutagen gives you all prefixed by gen underscore, what it would be in r base r to do something like that from scratch, and then what from the Stata language that inspired it. I won't read them all one by one here, but some of these I've definitely used my own take on doing it, like calculating the column percentage for for a dataset.

There is now a gen underscore cal percent function inspired by the egen pc function in Stata, but there's much more than just that. We got functions like row median, row miss, row n t h, all sorts of these that may not even existed in Stata, but are are part of this mutagen pipeline. So again I'm gonna have to take a look at this and see how much this might alleviate some of my custom code, but very intriguing to see once again the community come through with building on top of the amazing dplyr package and this I think could be useful in many different ways.

So, yeah, new to me, Mike. What did you think about mutagen?

[00:25:25] Unknown:

I'm certainly very intrigued by it. It seems like a lot of this as you mentioned, Eric, may be inspired by Stata. 14 out of the 17 functions that are currently exported from mutagen are row wise operations as well, which I know is kind of antithetical to how a dplyr works in general, although, obviously, it does have a row wise, operator at this point in time, which is is fantastic. But these seem to be potential shortcuts to a lot of summary statistic operations, from what I can tell, you know, perhaps sort of akin to what we might get from functions in Janitor or Skimmer. And I can, I think, speak for all of us when I say that sometimes calculating counts or percentage of null value or or things like that can be a little more complicated in r than probably it should be and the code can be a little more verb verbose, than you might think and there's all sorts of different funky things that we can do to, you know, sum up Boolean values and and things like that that might be, a little counterintuitive to new users, whereas, you know, some of these functions may be a little little easier to deal with, a little more obvious? So definitely intrigued by by this new package and excited to see where it continues to go and perhaps, you know, give us some of these, you know, janitor like statistics that help us do our exploratory data data analysis a lot quicker than we might have had to in the past.



[00:26:56] Eric Nantz:

Yes. And this does look like a new package, always relatively new. I'm going through the the commit history and, yeah, this was well, actually, no, not new. It was started back in 2019, but it definitely got a lot more development in in the more recent years. So interesting how this works on those that open source. You have the idea, then maybe you have to step away for a bit for various reasons, and then get back to development. That's like the story of my shiny state development journey in a nutshell until I finally pulled the pulled the trigger on that this year. But, yeah, I'll be watching this space quite closely.

And it would be interesting, obviously, in the world of LLMs and the like, when you ask it to, you know, maybe update your dplyr code that derive these additional variables, how much of verbose code will it give you as opposed to using, like, mutagen? That might be a fun fun thing to try out depending on which model you look at. But there's a lot of things to try out. Right? And especially this year in 2025, and rweekly has been your source for inspiration, hopefully, to look at what the community is building, try out new workflows and data science, and learn from some of the best in our community.

Where can you find that? Of course. That's at rweekly.org. That's where everything is, both the latest issue right on the front page as well as the archive of all the other issues. And, yeah, we definitely weren't able to talk about all the issues, so you're definitely welcome to check those out as you get some downtime this holiday season. And I'm happy to say, Mike, that in between our last episode, we got some listener feedback. It's it's great to see this. So this came from our contact form from Willie Ray, and they write, hey, guys. Thanks for the our branding shout out. Oh, yes. I do remember shouting that out. I heard at the end that you've got some PowerPoint angst.

Yeah. We do. Something I was thinking about. Quarto can render reports to a variety of slideshow formats including PowerPoint. I haven't tested this. It's not in our project's roadmap. But I think brand YAML should be able to style those documents too without too terribly much hacking. The website template and our branding uses coral to render out a brand YAMLified website. Seems like you should be able to do the same with a slideshow. Thanks again for the kind words. Yes. Well, yeah, that may be something we should definitely try. I mean, I'm trying my best to avoid PowerPoint these days, but maybe our branding and and PowerPoint could work well together with Corto. What do you think about that? We've got a brand dot YAML file that we use internally at Catchbook, and I Willie, the next time that we need to generate some PowerPoint

[00:29:41] Unknown:

slides, I will absolutely give it a shot and let you know. But great idea.

[00:29:45] Eric Nantz:

Yeah. Excellent. So that's exactly what you can do to get in contact with us is just use that contact page directly in the episode show notes. We'd love to hear from you. It's always great to hear from our audience. And if you wanna find a a different way to contact us, we got a few additional ways. And I do promise in 2026, I'll make this even easier for people, but there are ways to either send us a fun little boost in your new podcast app of choice, or you can get in touch with us on social media.

I am, blue sky with at our podcast @bsky.social. And if you watch my feed as of today, I'm testing out some funny API stuff. So, there'd be dragons perhaps of that. But you can also find me on Mastodon or I'm at our podcast at podcastindex.social. And I am on LinkedIn. You can search my name and you'll find me there. Mike, where can a list of who's got a hold of you?

[00:30:41] Unknown:

You can find me on blue sky at mike dash thomas dot b s k y dot social. Or if you search Catch Brook Analytics on LinkedIn, k e t c h b r o o k, you can see what I'm up to.

[00:30:53] Eric Nantz:

Awesome stuff. And once again, the year is wrapping up. I do admit, Mike, the timing of next week is a little dicey. Right? We got the holidays right there. So Yes. I'm guessing we'll probably be taking the week off. We'll see if we get another one out there before the end of the year. But if you don't hear from us, it probably means that 2026 will be the next time you hear from us, you know, how things go when you got kids that are coming out of school in a couple days and you gotta frantically figure out what the heck you're gonna keep them busy with. That's just me personally, but I'm sure all of you out there have similar things you're you're wrestling with. But nonetheless, we appreciate you listening to us on episode 216 of our movie highlights. And, yes, we will be back with another episode fairly soon. Thanks a lot, everybody.

"
"3","issue_2025_w_50_highlights",2025-12-10,38M 3S,"How the recent frontier LLM model releases compare for successfully generating R code, our take on the new Test Set data science podcast, and a surprising entry in the world of languages equipped for data science. Episode Links This week's curator: Sam Parmar - @parmsam@fosstodon.org (Mastodon) & @parmsam_ (X/Twitter) 2025-12-05 AI…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 215 of the Our Weekly Highlights podcast. Sorry for missing last week, but, sometimes the the day job end of year stuff got me wrapped up in all sorts of fun stuff last week, but I'm back once again and really excited to close out the year strong here, so to speak. We got more awesome r content as was shared on this week's r weekly issue to talk to you all about. So my name is Eric Nance, and I'm delighted that you joined us from wherever you are around the world and wherever you're listening.

And keeping me on, this is always is my awesome cohost, Mike Thomas, who just had to endure yet more rants in our preshow here. And, Mike, how are you doing today? Doing great, Eric. Hoping to wrap up the year strong here with the our weekly highlights heading into the holidays. It is frigid out here on the East Coast in The US, but, we'll try to keep everybody warm. That's right. I was wearing my other jacket here downstairs until we got started with this because it is a bit chilly here in the good old basement here.

We don't exactly have a huge budget here for recording here. We'll get in more than that later. But nonetheless, we can talk about some awesome stuff that doesn't need a budget, so to speak, because it comes for free for all of you on this week's our weekly issue and has been curated by Sam Palmer this week, another one of our somewhat newer curator, but he's actually been on the team for over a year, and he's been tremendously helpful as always with this issue. And he had tremendous help from our fellow rweekly team members and contributors like all of you around the world of your poll request right at our rweekly.org site. So let's get right to it because the world of artificial intelligence development of large language models moves at a breakneck pace throughout the years.

And one of the nice things that we've seen come out in the data science community to try and cut through all the fluff of this, so to speak, because believe me, there's a lot of fluff out there. Hate to bring up LinkedIn from time to time, but there's a lot of slop on there with AI so you gotta watch it there. I I kid. I kid. Not really. But one of the things that has been helpful is the recent effort launched by Pazit called the AI newsletter, which is authored by Sarah Altman and Simon Couch, both have been really involved in developing LOM technology and packages over at posit, and they give it you the straight scoop so to speak, on a lot of the newer developments and a lot of the practical issues that you may hear about but maybe want, more, you know, robust take on especially from the data science perspective.

So their most recent issue that came out, earlier in the month at the December 5, and in this issue there are a couple cool things that caught my eye. First of which, as I mentioned, these models are rapidly evolving, and there are some new releases on the Frontier providers. And I guess somewhat not surprising, but yet it's still interesting to see that from Anthropic's side of things, the new Claude Opus four dot five model has been deemed by by Sarah and and Simon here to be the best coding model available according to their benchmarks, which does kind of track from some of the things we've heard in the various community members here that are in the world of LOMs.

Seems like Claude has been one that most software developers developers in general turn to, to generate their programming code of choice. And it's getting really nice with our code as I've been playing with it a bit here and there in positron assistant. So I'll certainly be eager to try cloud Opus four dot five. And it does sound like they are working on the pricing to be a little more competitive compared to maybe some initial releases. So that's something to keep in mind too, which does remind me there's another part of the post that kinda clears up, which can be a confusion for those new to the world of Frontier l o m providers.

There's often two ways to pay for these services. One of which is that you literally sign up for an account on these services and pay a monthly fee. You may pay it yearly, but it's a static fee. But then there's also the API token method where you pay for what you use in terms of API tokens. In general, especially if you're new to this, a lot of folks, myself included, recommend you go the API token route as you're just getting your feet wet with things. And it maybe turns to the more rigorous, account, you know, account subscription for times that you really need it. But there's lots of great information in the post on where you can learn more about those those, choices.

The other model that got talked about here is Google's recent update to Gemini, which I know a lot of people have been using because I believe Gemini is actually free to start with, to get your feet wet, but they have a new release called nano banana pro. Who knows what they use to name these things, but I'm sure that generates some fun images. But what was interesting in this post here is that they took a little bit of a challenge here and saw that it might actually be helpful in terms of being an image model, that's what this one is, that can generate interesting and at least semi clear technical diagrams of software.

So in the blog post, you'll see when you when you click on this after listening that they asked it to create an infographic explaining how the Elmer R package works, and it looks like a pretty solid graph. Certainly, there may be some questionable choices on where the arrows are going in terms of their layout, but I could see this being kinda neat. If you don't wanna code up something in those, open source kind of diagramming software or the other diagramming software offer that or software that's proprietary, maybe an LOM can ingest kinda your overall code base and help you do that. So I'll I'll take that note of that for future developments and the like.

But, yeah, there's definitely more highlights here in terms of other, interesting developments. The other one I'll highlight before I turn it over to Mike here is that they have been, deposit teams and regularly benchmarking how well the LMs are generating our code because there are some nice packages that they've been developing, Simon especially, such as the vitals package, and then using kind of a built in, you might call test case called r, a r e, to run on all these models and see how well it performs.

And now instead of just relying on the blog post to come every so often to summarize performance, they have a fun little shiny app that we'll link to in the show notes that compares the model performance, and you can choose as many as are offered here and there's there's quite a bit offered here. But when I looked at it earlier this morning preparing the show notes here, it definitely shows that Claude Opus is the leader in terms of getting the percent correct in cogeneration based on the truth, so to speak, and this are, you know, scenario followed by GPT five and then SONNET four dot five and the like. But you can you can add in, like, the older models, and you can see the evolution of how how well or maybe not so well some of these models are performing. So great little tool to bookmark, not just for performance, but also the ratio of cost to performance and getting the nitty gritty on pricing details, which if you're in an organization, might be really important or just important to you, so to speak, if you're paying out of pocket for this.

So really nice app here. Straight to the point. Nicely done by b s lib and the like. So definitely bookmark that if you're new to this space. Yeah. I always enjoy reading this when it comes out every few weeks or so, but, Mike, what did you think about the roundup here in this newsletter?

[00:08:31] Mike Thomas:

Yeah. The the end of last month, we saw a ton of activity in sort of the closed source, you know, frontier model updates that Gemini three pro update, I think, kinda rock the world. I think it sort of threw anthropic and and claw and, ChatGPT, maybe OpenAI for a minute, but I think, that their responses between GPT 5.1 pro and now CloudOpus 4.5 have been pretty strong as well. And I've also been experimenting with both a little bit and kinda concur that CloudOpus 4.5 is is the best for what we're doing day in and day out with our coding, and quarto type of stuff.

The some of the stuff that I was experimenting with Gemini three Pro was good. I think maybe, like, a little too verbose, a little too research y, and a little less targeted, but I think that there's probably some, fantastic applications as well. And the pricing model changes is is pretty interesting. It sounds like, Anthropic actually increased the price of, like, their smaller haiku model and decreased the price of of Opus, kind of causing the the cost to converge towards the the balanced SONNET, other sort of pricing model that they have. So it's it's interesting that the the the highest, performing, supposedly, model with Opus is now sort of one of the the cheaper sides, and it seems like that's gonna be a bait and switch, like, just to get you in. Right? And then then ramp up the cost eventually once you get hooked. But we're we're pretty hooked on on Opus, so they'll probably have us for a while.

And the one thing I I haven't explored, but glad the Paza team did in terms of Sam and and Sarah in this this article, or Simon and and Sarah, excuse me, is explore sort of these image models as well and get into multimodality here with Gemini's Nano Banana Pro. And one of the things I think that they found interesting compared to every other image model that I've seen thus far is it's not necessarily throwing out images that have, like, junk text in them and Wingdings and stuff that looks like it's from another language.

I saw something going around on LinkedIn where or Blue Sky maybe, where there was an image in a research paper that was published in the prolific science magazine Nature. I think. That was like a total AI junk slop, has since been retracted. I think it was a research paper on autism, pretty pretty serious stuff. And, been retracted because it was, I think, found to be just, you know, half AI generated or at least the the diagrams were. And if they use Nano Banana Pro, they're maybe their junk science wouldn't have been discovered or at least not as

[00:11:24] Eric Nantz:

quickly. So I don't know if it's It makes me both happy and sad at the same time. I don't know.

[00:11:29] Mike Thomas:

Yes. Exactly. But, Sarah and Simon did find, you know, an interesting use case for these nano for the nano Banana Pro, image generation models and their data science workflows in terms of generating, co what they're calling coherent, sometimes, technical diagrams, which in a lot of our repositories and a lot of our work and documentation that that we do at Catchbook, you know, we're almost always creating workflow diagrams, technical diagrams that are accompanied with the code and the software that we're creating. And, you know, there's great tools out there like, Mermaid JS and things like that, Excalidraw, that allow us to do those things, but it it does take some time. So if there's the potential for, you know, Nano Banana Pro to be able to speed that process up or maybe even just help us iterate, that's pretty interesting.

And the other part of, I think the article here that they shared in the the newsletter is some additional updates to our packages, in terms of the vitals package, now has a o dot 2.o release, so still very early in its life, but that's a really cool package for doing evals and evaluating LLM tools using Elmer. So o o dot two dot o is now on CRAN as well. And, yeah, as you mentioned to Eric, that Shiny app that they have developed to track performance, in terms of R code generation and the accuracy of R code generation across these new Frontier models is really, really cool.

I think it draws data, from using the the vitals package as well, and they I know they try to update it regularly. That's something that I'd seen Sharon Machlis do over on Blue Sky and not a lot of other folks trying to do, at least in terms of our benchmarking. So for the, those of us that are still developing in R and haven't switched everything over to Python, this is fantastic because most of the benchmarking that I've seen thus far in terms of code generation is just around Python. And we've been using or or I've been using the API model, pricing model for all of these services for a while, but I think it's it's probably time that I consider maybe the all in monthly cost because that may be beneficial because I have a we have caps on most of our APIs where, I get a notification every time we use $5.

And that that $5 notification is starting to to ding, like, every other day or every every three days. So probably time to consider that all in monthly cost, but it was a great way to or it has been a great way to dip our feet into trying to leverage these in our workflows.

[00:14:10] Eric Nantz:

Yeah. I think, especially when you get into more of the agentic side of things and you're not just doing simple code completions or simple answers to cryptic JavaScript or XML based parsing that I've done in the past, I know people are trying to build I heard a story from another colleague at a different company. Just as a challenge, try to build like a like an internal version of a Stack Overflow. And it worked pretty well, but I don't think just the API usage was kinda got it. It was very much an agentic thing, but that just shows you just how far people are pushing these things. And like I mentioned before, generating full blown Linux distribution configuration files for it in a couple hours versus, like, five weeks it might take somebody new to roll the nicks or whatnot. So that's it's this agentic flow is happening, and I'm hearing code and cursor being used quite a bit in those situations.

But I will say back on the API front, if you want something where you can kind of pick and choose the models, but in, like, one overall platform, OpenRouter has been really helpful to me. It helps you, you know, pick and choose between, say, you know, OpenAI's models, Anthropics, Google's, and others. And then you just basically give your service account credentials for these API keys into OpenRouter, and they'll take care of the rest. So some tools can benefit from that where you just wanna be you don't wanna have to hop back and forward to, like, five or six different services. You wanna put it all in one place. There's not this is not meant to be free advertising. I'm just saying it actually works pretty well. So you might wanna check that out. If you're using more than a handful of these at a time, it can be a lot to manage, especially someone new like me. So, yeah, lots of great developments here. And, again, really great to see this Elmer ecosystem just grow exponentially.

We learned a lot more about this in recent conferences I've been a part of, both the, Gen AI day for our pharma that we have the recordings on YouTube and some great content that will be on YouTube soon from our recent r pharma conference in respect to how we're leveraging Elmer and the like. So good times to be had. And speaking of good times, jeez, Mike. There's a little competition out there in the world of podcasting, apparently, but, no, it's all in good fun, of course, because our friends at Posit have been hard at work in developing some novel podcast content called the test set. Sounds like you've had a chance to listen to a few of these, and maybe you could take us through what your impressions are of of the test set.



[00:17:09] Mike Thomas:

I have. I think they have a higher budget than we have, Eric. They've got some really nice backgrounds, and it's very well produced. It's great. I think originally, it sort of started out with, Michael Chow as the host, and now they've brought Wes McKinney in as a co host on most of the most recent episodes, I should say. And they've been fantastic. You know, really great deep dives into the backgrounds of many sort of prominent figures in data science across both both R and Python. Folks like Julia Silgi, you would know, James Blair who works at Posit as as an engineer there, and, Kelly Baldwin most recently, who I believe is a a professor of statistics, in data science at Cal Poly, which is really interesting to get her perspective on how she teaches and, sort of different strategies that she has as well as her background coming up through data science. So I have really enjoyed it thus far. It sounds like they have some pretty exciting guests on the roadmap for 2026 as well, folks from DBT, Shopify, Astral, Mode, Meta, and and a bunch more. So I can't get enough of, which maybe is unhealthy listening to data science podcasts after I'm done working for the day and walking the dog and mowing the lawn on the weekends.

But I have long been bit by the data science bug and started out probably listening to the, original R podcast by my cohost here as well as Shiny developer series and and just ingesting as much as I possibly can. So I really enjoy these conversations. It's as a way, maybe for somebody who works kind of fully remote in my team. It's it's a way to connect with the greater, you know, data science ecosystem and and folks that are out there, Greater data science community, hear what everybody's thinking about, interested in, and and working on. So I appreciate posit making the investment to put this together and and produce it and share it with the world.



[00:19:16] Eric Nantz:

Yep. It is on my catalog, on my little phone here to catch up on the back catalog. I did listen to a little bit of, one of the episodes earlier this week with, Minh, Chitchennai, Rundell, on her experiences with teaching in the world of data science, especially in the advent of LOMs. And she has some interesting takes on how she's leveraging the novel technology to make running a course easier, but also kind of the real fundamental issues with students now growing up in this new age, which is almost like, you know, when old timers like me were growing up with the Internet just starting in high school and college and how how that revolutionized my my my workflow there. But it's a but, yeah, great perspectives, great, you know, lineup of guests.

So like I said, it must be nice to have a positive budget instead of a pseudo negative one like what we have here, but, hey, fair play, and then they got the right minds behind it. So I'll definitely have that in my podcaster of choice, feeder of choice because it is both on YouTube and in your favorite podcast provider. Certainly, I would say definitely audio friendly. I think like you said, Mike, I'm my consumption of content in the world of data science and open source software development, I am often doing something else while I listen. And then the challenge is when I hear this great insight of, like, being able to time it, hit pause, write it down, or at least jot down a link to follow-up on. But there are some creative things I'm sure in that space. But, yeah, test sets, going on strong and look forward to seeing what else they have in their in their, episode pipeline.



[00:20:57] Mike Thomas:

Good podcast name too.

[00:21:00] Eric Nantz:

Yes. Naming things is hard. I can but I it was easy back then because there was no R podcast before, so I got the easy one. Better than that. Yes. I'm gonna get better than that. I someday, I tell you, I'm gonna get that one back up and running again. I've got I've got plans, buddy. Got plans. So I know, Mike, you've been talking, especially in your recent conference talk about, you know, bilingual type of data science, especially from the R and Python side of things. And there was a post a few weeks ago by, a prominent member of the r community, Klaus Wilk.

Definitely caused a bit of a stir about his take on why Python was not such a great language for data science. Certainly a mix of, I would say, things I nodded my head on and some things I'm like, yeah. That sounds like just a bad experience and maybe with a better environment management or things like that, it might have gotten better. But it definitely got a lot of people talking, a lot of people thinking. And our last highlight here is definitely from one of the leading thinkers in the space of alternative ways to do data science, John Carroll.

He goes, he was inspired by Klaus' post to look at a language that you may, on the surface, not expect to be great with data science workflows. But in John's mind, it's actually coming a pretty long way. What language are we talking about here? Haskell. Haskell is, definitely kind of towing the line between a lower level language and and maybe some interpretable type of components to it. If my memory serves me correct, I believe Haskell is the basis for the Pandoc utility, which Pandoc is what we use quite a bit as the back end converter system going from, like, markdown over to HTML or to, you know, open office formats and whatnot.

I could be wrong on that. Someone have to fact may check on that. I'm sure they will. But that's where I first heard of Haskell. But I do know that there are a lot of different use cases for it. And up until recently, John has been using this Haskell in a lot of interesting kind of advent of code challenges, but he has seen some interesting traction in the world of Haskell with respect to data science, such as the data Haskell project, which is meant to be kind of a curated organization of various packages in the Haskell ecosystem and tutorials and in the future kind of learning environments to get your journey off the right foot on using Haskell for data science.

Now there are some key differences between Haskell and the language we're familiar with such as r or Python, that are definitely worth taking a note of, especially if you're new to the syntax that definitely would take some learning. One interesting one is that we're so used to using parentheses to feed in, like, the name of the function, and then put within the parentheses and name of arguments. No parentheses in Haskell. It's just spaces. So that takes a little getting used to when you're reading that the first time even for a simple sum summarization function or whatnot.

But there are some things that are kind of similar, such as it has the concept of list, but list and Haskell are kinda more like vectors and r where they kinda need to be the same type of each element inside. You can't mix and match like an integer and a character and a Haskell list. They must be a single type. But one interesting thing is, you know, ever since, you know, a couple years ago, our now has the native pipe operator, obviously inspired by Magruder and other packages. Haskell has the same pipe in its system too. So if you're used to pipe workflows, more power to you. You can use it.

There are some nuances, though. As you go from left to right, it'll pass the left side of the pipe, you know, side of the pipe chain over at the end of whatever's on the right side as opposed to the first argument or whatever you're doing on the right side. Again, you may not run into that unless you are really taking this first spin, but that is something that you might wanna look into. But going back to what Klaus had mentioned about, you know, why he felt Python wasn't a great language for data science and what attributes of r do make it a great language for data science, there are four key pillars that Klaus talked about.

First was non mutability or basically the idea of keeping things kind of static and not modifying without, you know, guardrails behind it with respect to objects and results of functions and whatnot, having built in concept for missing values as well as vectorization and also hooks for nonstandard evaluation. So in the remainder of John's post here, he talks about how Haskell actually also addresses these four pillars. First of which is the non mutability, and this is where Haskell is what we call a strongly typed language.

That may be a term not familiar to you if you've only done r or Python work, but, basically, what strong type kinda means in this context is that it is really difficult to change, if not impossible, the types of objects that you work with unless you use the kind of guardrails or functions that the language provides. This is where you can't really change, like, a character to a number easily or things like that. And Haskell does share those traits of being a strongly typed language with some recent advancements kind of at this intersection of r and strong type languages to enhance r itself, such as the the vapor framework from John Coon, type r, and our Lang checks, and even some other, you know, efforts like that. But Haskell has it all built in, so that's one nice feature.

And it also has a concept of missing values. You do have to, I think, tap into another package for it, but you can use built in constructs that have an interesting label such as just or nothing. So this one kind of made my head scratch a little bit, but there's an example here about a list of numbers one to four and then a list with, quote, just one, just two, nothing, and just four. And then there are checks to see which one has missing values and which one doesn't, and it does detect which one has a missing value.

And that you've got to do if you want to do a summarization of that where it just had one, two, and four, you have to exclude the nothing from that on that list before you can do it. There's no n a dot r m type of argument in Haskell that we're seeing here, but that's pretty interesting. But what the other interesting thing is is that while it doesn't have built in vectorization, it does have great support for iteration via the map kind of constructs that we use from per and the like, but they come built in to Haskell. So some nice examples of doing doing those kind of mapping operations, but then the part that John thinks really shows some great promise is a nonstandard evaluation where you can do a lot of interesting dplyr like constructs with Haskell. Again, combining that pipe operator to do things like filtering and doing things like changing variables or adding deriving variables as they call it. And you can print out the data frame in a nice text kind of display, not too dissimilar.

So we print out a tibble or something in the world of r and, of course, in Python with pandas and the like. Some pretty interesting things you can do there. Now, again, the hat the syntax is quite different. It's not like I'm going to Haskell anytime soon, but it is interesting to see that some languages that you may not expect do come of here some built in tricks or can tap into its own package ecosystem to do some data science operations. And it does sound like they're trying to beef up the resources on the use of Haskell for data science. I think John himself is contributing some small pull requests in different repositories.

And maybe the maybe the Haskell world of data science kinda takes off in 2026. Who knows? But as usual, open source, great to have choice and great to learn something new along the way. So if you got some downtime in December, maybe give Haskell a spin. Who knows? Yeah. I think Jonathan

[00:30:33] Mike Thomas:

has been one throughout the years of our weekly to explore different options in data science programming. I think that it may have been him that explored APL,

[00:30:46] Eric Nantz:

a programming language. That's right. We covered that. That was a fun one. I remember that correctly,

[00:30:52] Mike Thomas:

which, just kind of used, like, arbitrary random characters that didn't necessarily make a whole lot of logical sense, but but worked just fine. And I think maybe that was the point of APL. Haskell is definitely a little easier to read and consume for me. And it sounds like a lot of ground has been broken through this data Haskell project as well as this data frame package within Haskell that allows it to be quite a bit friendlier to data scientists as well. You know, I think that familiar pipe operator is is really interesting when you take a look at the documentation in terms of how it lines up with maybe your familiarity in r.

I think one of the big differences is that in, instead of using parentheses for function calls, you actually use a space. So in r, you would do some parenthesis x close paren, and then in Haskell, that would just be some space x as well. And, yeah, as you mentioned, Eric, the the whole immutability, concept as well, which Jonathan gave in blog post this example of if I have an an r vector with with three elements that I define using, you know, the lowercase c function, I can sort of overwrite the second element if I just assign some value to the vector with, you know, square brackets after it that have maybe number the number two in it if I'm trying to overwrite the the second element. And that's really difficult and kind of impossible to do. Not necessarily impossible, but, they make it difficult to do in Haskell. It's kind of off limits. You have to go through quite a few hoops to be able to to update, you know, a single element within a vector.

So that's, you know, one interesting thing, and I think that leads to the one interesting difference, and I think that leads to the concept of immutability and and strongly typed languages. And Haskell also has, you know, some really strong sort of support for handling, missing values as well, which can always sort of be this tricky thing in R where we have just NA and then we also have NA underscore for all these different types, character, you know, number, things like that that make it make it difficult.

And if you have to deal with missingness, Haskell may potentially do a better job. And as you mentioned in in terms of performance, it lacks, you know, this built in vectorization. It's it's not an array language, but since it's compiled, it it really compensates for this using some pretty nifty compiler tricks, it seems like, which is is pretty cool. And it also aligns with, you know, nonstandard evaluation type of features that we see in things like dplyr. And there is actually a great, article, I believe, in the data frame docs for this Haskell data frame library that does a comparison, between dplyr on how you would filter rows using the the dplyr filter function, and then compares that to what you do in Haskell, Haskell, which looks to me just kind of on the outset of sort of this combination between, combination between r and Python a little bit. You know, there's an at symbol that sort of specifies data types in the middle of this filter statement, but not that far off. So maybe an interesting thing to look at if you're one potentially doing Advent of Code or something like that where you're trying to explore, you know, different edges of the data science ecosystem and maybe trying different languages that you haven't picked up before, Haskell might be a good one to take a crack at.

Particularly now that this data Haskell ecosystem, is available and this data frame package, has been, you know, has had the tires kicked on it. It seems like for quite a while now and seems like a great option. So great blog post from Jonathan. Always always very interested to see how he's pushing the fringes of data science.

[00:34:49] Eric Nantz:

Yep. And speaking of pushing, there was a a great, write up in in the vectorization portion of his post where you can you can see kind of the benefits of Haskell, kinda have some good compilation tricks, and it can be actually quite fast for things that would be challenging r itself to compile or or to run such as doing of a huge length of of of integers. There's a great example of, you know, just reversing the sorting of that and taking literally almost zero seconds on on Haskell's side versus over four and a half seconds on the r side. So while, again, the vectorization isn't quite built in, it does have hooks and compilation, that does sound like it's good for performance too. So for those of the need for speed, sounds like Haskell can get you there as well. But speaking of speed, we better, speed right through to the end of this because we got everything to attend to, but we won't have time for additional fines. But, again, fantastic issue, put together by Sam here, and where you can find that is at rwahoo.org, of course. That's always where you find the latest issue as well as links to all the previous issues that, again, wonderful content over the year. And, yeah, 2025 has been a fantastic year of content, I would say.

And we love hearing from you as well. You can always fact check me on what I get wrong in my summaries here, but, you can find us or get in contact with us a few different ways. You can send us a little, contact submission form, which is in the episode show notes wherever you're listening to this humble little podcast. You can also get in touch with us on these social media outlets out there. I am on Blue Sky with @rpodcastatbsky.social. I'm also, Mastodon. We have @rpodcastatpodcastindex on social, and I'm on LinkedIn. I'm not causing a a I fluff. You can search for my name, and you'll find me there. And, Mike, where can the listeners find you? You can find me on blue sky at mike dash thomas dot b s k y dot social,

[00:36:55] Mike Thomas:

or you can find me on LinkedIn if you search Ketchbrook Analytics, k e t c h b r o o k. You can find out what we're up to. Awesome stuff. Yeah. I know you've recently completed some really fun projects. We hope to hear about those somewhat soon. But time's our enemy, of course, to write all that stuff up. And some new R packages on Crayon in the last month that we've published, which we're pretty excited about. Although one is, one is having some issues that we need to update. The joys of crayons We got the crayon email. Yep. We were good. And then the website,

[00:37:27] Eric Nantz:

that the package interacts with changed. Put up a firewall. Oh. Verify that you're a human.

[00:37:31] Mike Thomas:

Verify that you're a human type of page and it broke our download dot file function in our

[00:37:38] Eric Nantz:

just in case you've been there. If you know, you know. You know, you know. Yep. That's bit me and behind another places many, many times. But nonetheless, very exciting stuff to see contributing to open source. And with that, we will close-up episode 215 of our wiki highlights, and we hopefully will be back with another fresh episode to help wrap up the year next week."
"4","issue_2025_w_48_highlights",2025-11-26,38M 49S,"Prepare to deploy multiple LLM-powered agents for your next (secret) missions with mini007, a new contender to the high-performance linter tools with blazing performance that doesn't seem possible (but it is), and a usethis-like manager for your projects needing unified branding across deliverables. Episode Links This week's curator: Eric…","[00:00:03] Eric Nantz:

Hello, friends. Did you miss us? Oh, we missed you all. We are finally back of another episode of our weekly highlights. Yes. This is Brian. I actually don't know how many weeks it's been, but let's let's set that slide for a bit. In any event, when we are usually recording, this is the usually weekly show where we talk about the awesome happenings, the awesome resources, and much more that are all documented on this week's our weekly issue. My name is Eric Nance, and yes, I'm definitely rusty on this. So bear with me here. But thank goodness, I don't have to be rusty alone here. You know, we have the pro here joining at the virtual hip here. Mike Thomas. Mike, please keep me on. This has been a minute since I had this mic in front of me.



[00:00:45] Mike Thomas:

It's been a while. It's a short week with Thanksgiving here in The US. We're a little all over the place, but we're gonna get an episode out.

[00:00:52] Eric Nantz:

That's right. We're striking now because I know after today, the kids are off from school for the week, and, yeah, no recordings happening there even though I am trying desperately to resurrect my tech setup for some future, livestream of dev work. It's it's this close, Mike. It's this close. I think you're gonna like it when it's out there, but you know me. I overengineer all the things, and I'm proud of it.

[00:01:17] Mike Thomas:

Never. Never.

[00:01:18] Eric Nantz:

Never. No. Not at all. Well, speaking of hopefully, not overengineering. Although, sometimes it does happen from time to time. Let's check who curated this issue. Oh, yes. It's mister over engineer himself. Yes. It was me and my my turn. So how's that for a coincidence? First episode in a few weeks, and that's the issue I ended up talking or curating. But that's probably for the best since I got a a nice preview of what we're gonna talk about here. And we got some great selection here, but as always, this is a team effort, folks, and we have awesome an awesome team of curators that help each other out. So my tremendous thanks to the team as always for their tremendous help and for the wonderful poll requests from all of you in the community. I think I merged nine of them in this issue. So that's rocking.

Love the open source mindset here. So let's dive right into it, shall we? And, yes, it wouldn't be a twenty twenty five our weekly issue without our first visit into the world of LMS and AI. And, no, we're not gonna bore you with any LinkedIn clickbait stuff here. I have always been bullish, if you will, on the tooling that's we're seeing in the community, especially on the our side that's building upon some solid foundations in an open way. And, of course, we've covered the Elmer package quite a bit in many episodes this year and frankly even early last year as well, and that's really starting to mature.

We have a lot of great extensions building upon it. But what is interesting about this first highlight here is this extension, first of which, is not coming from POSIT directly. And I think it is encompassing an area that I know I've been wanting to learn how the best handle that I think a lot of other frameworks that are somewhat either proprietary or at least have an opinionated workflow do expose to you, but maybe not in a very transparent way. So what we're talking about here is the idea of not just a single, quote, unquote, agent in your LLM generation or utilization.

We're talking about multi agent workflows. And that is coming to us from a very fun name package, mini double o seven. This is a new r package authored by Mohammed El Fodo Ihadden. I know I didn't get that right, but he's been a part of the r community for many, many years. I've actually filed a lot of his work in the shiny space, but it looks like he's turning his attention to LLM, pipelines as well. And so this first highlight comes to us from a blog post on the art consortium blog as well as a link to a recent, presentation from Mohammed at the RAI conference that just happened about a month or so ago.

And there was actually I think it was a hybrid of a workshop session where Mohammed walked through the use cases and kind of getting started with mini double o seven. So we'll have a link in the show notes to the GitHub repository. I think that's one of the the best places to go for this, and it's a very comprehensive readme here of just what is possible with this package. So first, let's again, you know, frame this as this is building on top of Elmer, and you start off with building a chat object via Elmer using either the Frontier model provider or perhaps a self hosted provider.

That's all the same. But what mini double o seven brings to you is a new custom class called agent. And this is where you can add a single agent with its own custom instructions, its own usage guidelines, and, of course, tying it with that previous chat object that you create at the outset. And this is already giving you a nice idea here where with these agents, you could have one, you're gonna have two, you're gonna have however many you like, and they could each be tailored to a different aspect of your workflow.

So, one of the examples in the readme is maybe you want a custom agent tailored to translating, say, a foreign language. You might have another agent that's a little more needs to be a little more powerful. Maybe it's trying to help answer research questions or at least understand the content of what it's translating. And perhaps you have even another agent that's trying to do some, you know, maybe new ideas, maybe new data analysis, or new summarizations. And so there is an example here, like, I mentioned in this readme of this kind of three pronged approach to multi agents.

But again, within each of these, give it a name, you give it a custom instruction via the instruction parameter, and which object, like I said, that we that you created with Elmer at the outset. Now these are still kinda separate. Here's where the interesting stuff really comes into play. You now have a new way of orchestrating these together via the lead agent class. And this is where, again, it builds upon the existing Elmer object for the chat. But now you can say for this lead agent, use these other three agents as the way you delegate responsibilities or delegate how these different agents answer the questions.

This is where I know I have seen, at least from my limited experience of LN Technologies, different, you know, platforms such as, like, Claude or maybe others that let you do multi agent stuff, maybe even behind the scenes. Maybe they wrap that for you, but you don't necessarily get a lot of transparency in how that's actually built. But throughout the mini double o seven, kind of paradigm for running the functions, you can have what they literally have a function for a human in the loop to help kinda stagger and put some, like, either breakpoints or, like, checkpoints to make sure that this multi agent workflow is doing what you are expecting.

That's both for kinda, like, you know, a stepwise kind of approach to reviewing what's happening. And you can also review the plan that it, that it wants to do before you actually start executing things, which, again, in my industry is something you definitely wanna look at. It's just what exactly is gonna go on under its control before you actually kick off those, analyses. So that is really interesting. And then another interesting part is, again, due to this multi agent workflow, you could have it actually use different alternative models in some of these agents.

And either you yourself pick what you think is the best response, or you could even let the lead agent use these different, say, sub agents with different sub models, say, from OpenAI or or anthropic or the like, and then you can let the multi that lead agent do the decision for you and still have it explained to you why it chose the way it did. So I think there's a lot of potential here. I could see a lot of interesting use cases being built upon this as you think about either in the automation space or maybe a more sophisticated data pipeline where you've got different, you know, like what the tidy verse suite of packages is popularized or the r for data science workflow.

Gotta import your data, gotta maybe do some transformations, summarizations, and then reporting. Maybe this is a way to give you even more custom flexibility in those kind of pipelines and then feed that into some other ETL process or the like. This is just me literally spitballing here. But I think there's a lot of potential here and the fact that it's building upon Elmer, I feel like there's a lot of confidence, a lot of at least at least a robust approach to start this. As someone new to all this still, I I wanna try with maybe some basic projects, but it definitely got me thinking that this might be the ticket to some more sophisticated workflows in this space. But curious, Mike, I don't know if you have experience in multi agent workflows, and what do you think about mini double o seven here?



[00:10:08] Mike Thomas:

I don't have a lot of experience with multi agent workflows, to be honest. I'm I haven't barely scratched the surface with agents and tools and all of that. But I do know Hadley Wickham gave a a great presentation at Positconf that was for audiences like me that had no idea what that terminology was, and boiled it down to very simplistic terms terms that resonated with kind of my approach to r. But I like the the r six approach to this mini double o seven package because it feels like we're creating sort of building blocks that we're putting together and orchestrating, in a way that I think lines up with how the best practices currently are with these agentic workflows. They're kind of little building box little building blocks, that we put together and stitch together in particular ways to try to accomplish the the task at hand. And it so I think it's a very nice parallel in terms of the design, of the the API for this, mini double o seven package with a lot of really nice little features as well.

This this set budget, allows you method allows you to actually set a particular budget in terms of dollars at least as shown in the example here that essentially make it impossible for you to go over that budget, with your LLM, you know, frontier provider of choice, as well as, you know, provide you with some warnings as you approach that budget at thresholds that you set yourself. So very handy things I would imagine, especially when you're orchestrating these multi agent workflows that are are calling these LLM APIs, quite often. You know, maybe more so than what you would just, typically be used to as you interact with sort of the web based, you know, chat GPT or Claws of the world as well.

That idea, as you mentioned, Eric, that you can have multiple agents. And the example that Mohammad gives here is have a researcher agent, a summarizer agent, and a translator agent all sort of working together to answer this this one example question that says, you know, tell me about the economic situation in Algeria, summarize it into three bullet points. That's the summarizer after the researcher, then translate it into German, and that's the translator there at the end. So sort of three different agents all, executing on this particular task. You know, there's this downstream sort of domino effect workflow.

And there's this really nice, function in here called visualize underscore plan that actually shows this, sort of relationship as a a DAG, in a visual pane in your editor, which is really cool. I I wonder sort of what the, underlying packages that's being used to create this, sort of workflow diagram. I imagine maybe this network or something like that, but I haven't dug into the internals of the package enough. But, again, just a really nice example sort of along with that that set budget function, this visualize plan function is a really nice to have, you know, utility that this mini double o seven package provides as well as that human in the loop, set underscore h I t l, human in the loop, that allows you to decide which steps in the particular workflow you want to interject and and have that human be, sort of a decision maker in the process as well. So I really appreciate the design here. I really like the way that, Mohammed was able to articulate this package in the YouTube video, which I'd highly recommend you watch. There's also accompanying, PDF slides as well and, really interesting sort of lightweight but incredibly useful, framework that Mohammed has provided for us, our users, to work with these agentic workflows.



[00:14:03] Eric Nantz:

Yeah. And I was combing through the repo. It's actually very minimal dependencies over in Elmer itself, which you would expect. This is not a heavy footprint, which to me, something seems as, you know, ambitious to my eyes as this. You would think it's gonna import all sorts of stuff. And, no, it's a very maybe, like, six dependencies here. That's a very lightweight footprint, and I think there's a lot of potential. It's a very clean code base, and I'm definitely gonna be watching that that presentation and seeing those slides because I wanna I'm still starting small in my typical LLM usage, but there are a lot of things that we're hearing, especially in the life sciences side. I hear presentations about different, you know, statisticians and different companies building multi agents for automation, but we don't really know how they did it yet. This may be a case where I can at least prototype what they might be doing and translate that to what we're trying to do at our company and whatnot. So, yep, this is, going into the the December kinda hacking list of me to explore cool stuff when I get some time off.

Well, as many know, Mike, we often leverage what we talk about at the outset, large language models to help make things faster for us to get to that answer more quickly and whatnot. Well, there's been a lot of great traction in terms of fast linting of your code, the cleanup, but I often get wrong. Maybe a missing brace, maybe lines that I should have had a carriage return somewhere, or maybe, you know, parameters that I misspecified. We often look for ways that just speed up our correction of that instead of just poking through that manually and hoping for the best.

I admit since about six months ago or so, I've adopted the utility called air by posit by posit into my positron experience. Oh, no. That's been so helpful to me. Now that has been great for just kind of the basic formatting, although I haven't really used it to its full potential yet. But what if I told you, Mike, there is a new linting framework out there that when you point it to the entire source code of r itself, the r files of r itself, It takes just seven hundred milliseconds to lint the entire code base. Would you believe me?



[00:16:46] Mike Thomas:

No. I wouldn't believe you. That's what I thought.

[00:16:50] Eric Nantz:

But nonetheless, it is real, and it's fantastic. And what we're talking about here is the Jarl utility. This has been authored by Etienne Botcher. Hopefully, I'm saying that right. And boy, boy, oh, boy. This came out of nowhere. But, apparently, this has been in the works for a while because this is actually one of the projects that was helped funded by Doctor consortium grant program. Now just what is Jaro exactly? Well, I kind of said it at the at the outset. It is, quote unquote, just another R winner. But boy oh boy, I think that is selling itself short because one of the biggest reasons it has a huge speed, you know, promise and actually prove that speed is that this is raining in rust.

Rust is becoming a lot more common now in the r community especially in light of I think I saw a great presentation at at Pazikoff about the Rust ecosystem and r. We've seen a few utilities spin up, and this is just another example of piggybacking off of the promise and the and the enhancements of Rust to make something blazing fast and with good quality. So this is not as meant to be a strict replacement for what you may be familiar with and that is the winter package, which many of us have used for years and years within our various workflows, whether in our studio IDE or Versus Code or Positron.

Linter has been around for a while. I can say that it's probably getting a little bit longer in the tooth, so it might not have the best performance for larger code bases, but Jarl is definitely helping in a lot of those use cases. There is on the blog post of a link to as always, there is actually a screencast showing that it's not just frivolous pointing to this our code base. It is real. You can play it and it's only about sixteen seconds and Melissa knows it's just looking at the console. It is just as fast.

And the example that's in the blog post here, we see the formatting or linting, if you will, of an apply function or a function that uses apply under the hood. And this is where it's not just looking at the syntax, it's looking at, is there a better way to do it? And sure enough, it picks there is a better way to do it. So if you wanna do your own version of of a mean for rows, instead of just doing the apply function verbatim, you can use the row means function. But this lint this charl is intelligent enough to find these kind of areas where maybe there's a base r function that can help you out and maybe you just didn't know about it.

Trust me. That happens to me quite a bit. Every time I read blog posts, I often see, like, the nz char function in our base. And I'm like, what is that? And then it dawned on me, oh, that's non zero characters. Silly me. So I think Jaro is gonna be great at picking up things like this. It is a single binary. So this is one of those things where you can install it on multiple platforms out to see how well it works on Linux. Hopefully, it does, but it usually rest things work pretty well there. And then you can just use the custom extensions that are also available to utilize this in your IDE of choice such as Versus Code or Positron. So definitely look at the extension store for ways that you can plug in JARO for those workflows.

That that is acknowledged that this is a very early effort. However, this is a really, really promising effort. So I definitely wanna try this in my dev environment to see not just at picking up some of these base r, maybe kind of misses I might have. I'll be just gonna see what they can do with shiny code too. Maybe it can help me lint some of that as well and make things a bit more performant. So, again, credit credit where credit's due here. I I love seeing this tooling come out, and Atayna has really done an excellent job here at Jaro, and I'll be keeping an eye on this quite a bit. When I feel the need for speed, I'm going to Jarl for my linting needs.

Yes. I came up with that on the fly. You're welcome.

[00:21:13] Mike Thomas:

That was beautiful. To me, it feels like air just came out, and I I can't believe that I am still trying to pick up air, and now we have Jarl, which is sort of built on top of that. It sounds like that is potentially even better. And the fact that it can identify these inefficiencies in our code. Right? It's not even incorrect code, in the examples that Etienne shows us. It's it's inefficiencies in our code and actually fix those with this dash dash fix flag, in your JARL command it to the terminal is mind blowing, and it opens up the the door for just tremendous amount of possibilities, I imagine, to hopefully take that that junk inefficient code that your LLM spat back to you, and clean it up because that is I feel like what most of my day is is these days.



[00:22:08] Eric Nantz:

I don't know if it ever is necessary. Seen on that one too, my friend. Oh, goodness. Yes.

[00:22:13] Mike Thomas:

Yes. Too many for loops. Please Please, train on train on the per documentation for the love of God. Hot take incoming. I love it. Not a lot of tidy code, unfortunately, from what I've Yes. I've seen. But hopefully, it's getting better. Opus 4.5 just dropped. I have to to check it out in Gemini three. But, this is, you know, a fantastic utility. The fact that, you know, it it's just built on Rust and the the blazing speed. It's almost like my initial experiences with DuckDV where I would hit enter, and it would happen so fast that I would think something was wrong, that I think I screwed something up. So I think we're gonna see a similar experience here with this Jarl utility, especially when you're leveraging it on code bases that are smaller than the entire R source code, which I would imagine is, most of the code bases that you work with on a day to day basis. And, you know, one of the the things I struggle with with the error formatter, that Linter has sort of traditionally done a better job of is incorporating some particular custom, you know, or overrides, if you will, of the default linting behavior.

There's some situations where I like a little bit more, white a little bit more space, a little bit more carriage returns in my code, from line to line that I think, you know, we don't necessarily have the control over with air, as opposed to, I believe, with the the linter package in r. We've had the ability to override some of those defaults and and set up some custom formatting or linting options. And it sounds like maybe from this blog post that that is on the roadmap for Jarl. And I'm really excited to bring my very opinionated, linting decisions and code, styling and formatting decisions, into the fray here as I continue to to leverage these fantastic new, code linting utilities that are coming out for us.



[00:24:18] Eric Nantz:

I do say we know which which opinion thing you will put in there. Get those four loops out of here, baby. I I I can definitely concur with that side of it. And like I said, I'll be interested to see just out of the box what it does to shiny coat. Because there are sometimes things went to shiny code that I don't quite agree with in terms of spacing and maybe the way I I specify parameters that I'm like, you know, they're yeah. You know, it's hard to customize that. So I'm I'm I'm gonna I'm gonna give it a shot on one of my dev projects. So hopefully, I'll be able to install it in my, custom Nix, configs and and give it give it a whirl. But, Positron works just as great. As he as he mentions in the post, there are extensions for Positron, Versus Code, and the Zed editor, which I know is getting a lot more traction in the DevOps space as well. So maybe, hopefully, RStudio support soon. I don't know. But either way, I'm gonna give it a shot.

And rounding out our highlights today, we've been looking at a lot of different things this year in our weekly, and one of them has been a way that especially those of us working in an organization, no matter big or small, we often are given either rules or at least try to adhere to best practices with respect to how things are styled, so to speak. Maybe you have a company wide branding set of guidelines for your presentations, reports, maybe even web applications. And one of the things that has come out in in earlier part of this year, again, from Posit, has been the brand.yaml utility to help kinda give you that customization, especially for web based, you know, deliverables in one place. A single YAML that you could apply in many different situations, whereas Shiny apps, quarter reports, our markdown reports, and the like.

Well, I think, well, in this last high, we're gonna have, at our disposal, kind of a use this kind of wrapper on top of brand dot yammo in the early stages here. And this is a new r package from this year called r branding. This is authored by Willie Ray and Andrew Pulsipher. And this I believe I may have seen a presentation about this. I don't know if that was at the shiny comp. It does sound vaguely familiar. But what our branding does is it basically gives you some nice quality of life wrappers to help interact with the management and getting custom templates going built upon brand.yammer.

So what it lets you do is that it lets you initialize a new branding project, and then you can choose where it's stored remotely. It supports at this time, you're restoring up a public GitHub repo or if you're in an organization, a private GitHub repo of sorts. And it lets you easily manage grabbing the config information from that brand.yammer stored remotely into your local project. So you still have kind of a single source of truth for where you store your branding project. And then along the lines of this, as I mentioned, our branding will let you apply all these techniques, all these customizations across what brand.yam will supports.

That would be, of course, quartal reports, r markdown reports, Shiny applications. And one thing that really caught my eye with this package, even themed plots in the g g pot two ecosystem and on the kind of intersection of of Shiny with b s lib and thematic. So you can have that unified theme approach across not just your apps UI, but also the plots that you're producing as well. That's always a plus 100 to me because a lot of times I get get in get into kinda dicey situations where I got this great theme, and I realized, oh, jeez. My plot was too custom to figure that out. So I'm interested to see if our brand new is gonna help in that side of it too.

And again, one of the other things on the package site, which I'll link to in the show notes, is it's got a boatload of templates for you to see just what a finished product might look like, And you could build upon that or extend it across these different paradigms for a simple quartal site. They got you covered. I get Tempo for Quartal. Even a quartal report that implements Jujupilot two. Again, get that unified theming. You can get it right there. And there are actually three different shiny templates. This is really intriguing to me, such as the very basic histogram version or that k means app that we'd often see in the shiny gallery and a more complex app as well, which, again, has a dashboard kind of layout, but you could easily take this and run with it. And, lastly, a even more complex app with multiple tabs. Looks like some BS web action going there of a wastewater data visualization.

So this is a great way. If you heard about Brand Yammum, you're kinda thinking, well, it sounds good, but how do I really manage it in practice across multiple projects? I think our branding could be a very unique, utility to put in your toolbox for your branding adventures. And it looks like according to the package site here, both Lily and Andrew look to be supporting the Centers for Disease Control and Prevention Center for forecasting and outbreak analytics. So that tells me they've been using this quite extensively in production, which is always a great thing to see.

So great. Great. And last but certainly not least, on the package site, they have a great tutorial on accessibility, which is something we've been, you know, very big champions of here on our weekly across the different venues of of UI toolkits. I think they got some great checklist about what you should do with a dashboard design, avoiding common pitfalls, about lots of things that we may see talked about in spurious or I should say various presentations and documents. But they're kind of putting this in one place with links to how you can test your various website or shiny app for accessibility compliance.

Really great stuff, and you have cheat sheets about this too. So the documentation of our branding is top notch. And like I said, I'm gonna be putting this in my toolbox for my branding adventures.

[00:31:19] Mike Thomas:

Likewise, Eric. And I think this package really helps solve a problem that that we've had, maybe you've had, and and a lot of folks have had in terms of sort of managing or passing around that underscore brand dot YAML file from project to project because you wanna consistently utilize it so that all of your projects have the same sort of thematic elements and and look similar and match your your corporate, color palettes and things like that. But it's you also want a place where that can potentially be updated over time and we have a good version history of that. And there's a few different options, you know, that we've used in the past that haven't been great. You know, things like, GitHub template repositories for that, as well as, what was the other one that I was thinking of? Developing like a internal R package that essentially has a function in there that creates your brand dot yml file.

But this is a much more straightforward way to do that. So you can manage that brand dot yml file in its own, GitHub repository potentially, and then download that and leverage it in the project that you're working on at hand. So, excellent, excellent work done. It's it's nice to see kind of the the intersection of an organization like CDC being able to put out something, in the public for us that we can leverage. So that is awesome, contributing to open source. And the package down site is fantastic as well. As you mentioned, there's a whole entire article on accessibility guidelines, sort of beyond the the templates as well that they provide and and some of the additional articles that they have. But I really enjoyed sort sort of going through their general design principles for accessibility, which are are kind of these five principles on making content perceivable, making your interactions operable, which I am probably guilty of overusing, some interactions and and collapsible elements and things like that that probably reduce accessibility. So this was a a good way for me to sort of stop and think, about some of the projects that we've worked on and the Shiny apps we've done recently and some potential redesign, choices that we could make. The last three are make the structure understandable, make the data accessible, and design for consistent behavior, which kind of touches touches on modularity, code modularity, and things like that. So I really in enjoyed, reading through that article.

Specifically, it it talks about dashboard design, good practices, bad practices, a lot of pitfalls around text and alt text and things like that. So great great new package to have out there for those of us who are trying to maybe create Shiny apps from project to project that look similar and don't look completely different, like I may have been guilty of in the past.

[00:34:11] Eric Nantz:

I can I can relate to that, but also sometimes you may be dealing with, like, in your case, a client that does have very opinionated guidelines and maybe you hopefully get more than one project with them and then you can say, oh, guess what? We can use the same assets from this app to this other report and even in my company yeah. We're one company. Right? But even different teams have different needs for various things. Maybe a discovery group doesn't care as much about certain things as a communications group where yours truly literally had to make a shiny up that was a glorified PowerPoint maker of a custom theme.

Yeah. That wasn't pleasant. If they can just get with the times and do web based reports, then I can use our branding to do that and probably solve, like, 90% of that pain that I had to to deal with. But you're not here to listen to me rant about PowerPoint. You've heard enough about that over the years of this show. That's what happens when we've been off the mic for a few weeks. Yep. It's pent up, Mike. It just gotta get out somewhere. But luckily, I think I've got out some pretty good stuff, I would say, in these summaries. And speaking of good stuff, there's a lot of good stuff in this issue. It was actually pretty tough to determine what would be the 10 candidate restores for the highlights. There was a there was a heck of a lot to choose from. So it's a lot of fun to to curate this. Lots of great issue great packages that are being thrown out there. I'm gonna I could talk all day about these, but we do have our respective day jobs to get back to. So we'll kind of wrap things up here.

But as you probably guess, the end of the year has been pretty crazy for us. We hope to at least get one or two more episodes in before the end of the year. Well, we'll see what happens on a scheduling side and hopefully come back even refreshed for the next year as well. But what can refresh us? Well, hearing from all of you in the community is always one of the biggest ways that I get my my jam, so to speak. And there are lots of ways you can do that. You can, in your favorite podcast player, just hit that little contact page in the show notes, and you can send us a message right then and there. If you want those modern podcast apps like I've been using and many others in the Linux community have been using, you could send us a fun little boost there too if you like to send us a little fun message.

But, also, we are available on social media as well. You can find me on blue sky with at rpodcast@bsky.social. You can find me on Mastodon where I'm at rpodcast@podcastindex.social, and I'm on LinkedIn. When I'm not causing too much trouble, you can search my name and you'll find me there. And, Mike, where can the listeners find you?

[00:36:50] Mike Thomas:

You can find me on blue sky at mike dash thomas dot b s k y dot social, or you can find me on LinkedIn if you search Ketchbrooke Analytics, k e t c h b r o o k. You can see what I'm up to.

[00:37:04] Eric Nantz:

Awesome stuff. I know you, actually, you and I both had our recent, positconf talks now available on YouTube as well. So if you haven't seen my if you weren't there in person to see Mike's fantastic talk, go on YouTube and watch it. It is required viewing, especially if you're in the interoperability intersection of r and Python. You you framed it in a way that I could never do enough justice for. So require viewing, my friend. Well, for those in the shiny space,

[00:37:33] Mike Thomas:

looking to to improve their caching workflows, Eric's talk is is certainly

[00:37:39] Eric Nantz:

required viewing in my view. I'm getting the itch to get back to shiny state development, buddy. I've got some ideas. Some people are clamoring for that investigation into using golem or rhino for it. So I got I got some homework to do. So, hopefully, I'll be able to stream some of that as well. But in any event, this is audio, so we're gonna close-up shop here. Thank you so much for joining us for episode 214 of our weekly highlights. And, hopefully, within a week or sooner or later, and we don't know yet, we'll be back with another edition of our weekly highlights very soon. Thanks a lot, everybody.

Sandy for just another r winter, authored by I forgot your name.

[00:38:30] Mike Thomas:

Author by I don't believe you. We'll have to edit that one. Did I time that right?

[00:38:37] Eric Nantz:

A little bit. No, sirree bob. Nonetheless. Okay. You you corpse me there. Alright."
"5","issue_2025_w_42_highlights",2025-10-15,39M 33S,"Creating your own race to a programming finish line with a hidden gem in mirai, the lowdown on using Generative AI for data visualization with more than a few surprises, and going from start to finish with a Shiny app created under the BDD mindset. Episode Links This week's curator: Sam Parmar - @parmsam@fosstodon.org (Mastodon) & @parmsam_…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 213 of the Our Week Who Howlads podcast. And thanks for being patient with us. We were off last week. I'll get to why in a little bit. But if you're tuning in for the first time, this this is the usually weekly show where we talk about the excellent highlights and other resources that have been shared on this week's our weekly issue at ourweekly.0rg. My name is Eric Nance, and I'm glad you joined us from wherever you are around the world. And at least he's back at this time, hopefully, not as rusty as I am. My awesome cohost, Mike Thomas, is with me as always. Hi, Mike. How are you doing?



[00:00:38] Mike Thomas:

Doing pretty well. We had a a big nor'easter yesterday that wiped out power, but we are back up and running today and ready to record.

[00:00:46] Eric Nantz:

Well, happy to hear you are safe at least after that one. I had a good good little, time off. The kids had the fall break from school. We're not to put that back in my day hat on, but back in my day, we didn't get things like fall breaks. It was only spring break, and that was it. So these kids don't know how good they have it. So it was truly put on a lot of miles going up to the friendly confines of Michigan again and caught my first ever Detroit Red Wings game in person, which, didn't quite go so well that night, but them's the breaks. I guess that shouldn't go to the games anymore because they won the next two handily. So what what kind of stuff? Gonna try again. We'll try again. If they make the playoffs, I can't resist. I gotta go again.

Small sample size. Yeah. Yeah. We have an end of of one so far. So, anyway, playoffs, let's cross our fingers. We're still early, but we'll see what happens. But we can guarantee at least we got some awesome highlights to discuss today, and this week's issue was curated by Sam Parmer. And as always, he had tremendous help from our fellow Rwicky team members and contributors like all of you around the world with your poll requests and other wonderful suggestions. So we're gonna speed right into the first highlight and that's, I guess, an intentional pun here because we're gonna look at ways you can speed up assessing different results with one of the packages as getting a lot of attention in the world of asynchronous programming in the our ecosystem.

We are talking about the Mirai package in this first highlight, authored by Charlie Gao who recently joined Posit as an engineer and he has made tremendous progress, making Mirai a first class citizen for almost all things asynchronous across many different workflows in the R ecosystem. And sure enough, there are some hidden gems in there that aren't even as obvious right off the tin. But one of our fellow curators of our we definitely took notice and he's gonna discuss that in our first highlight here.

And so this is coming from our longtime curator Jonathan Caro and on his microblog he talks about a neat little exploration, on one of these hidden functions in MirrorEye. And in particular, what motivated this is he was looking at the latest post from, another, site called Thinking Machines Lab, and it had an interesting header image about how the LLM annotated what looked like a sentence, but in certain parts of the sentence, it had the scores associated by the LOM to put that various word to complete that sentence. And so if you look at the the link in your in your computer after this, you'll see that a few of the words in this sentence have like a threshold next to it of the probability of it being chosen.

So that got him thinking that, you know, there are some times where you might have multiple avenues to get to a solution. You don't necessarily know which one's going to pan out or which one might be quote unquote best. So as he was looking at Mirai, in some additional research, he noticed this little gem called race underscore Mirai in one of its functions, which lets you evaluate with one or more Mirai objects running them at the same time, and then the, the output of that function will return whenever the fastest one completes no matter which one it is.

So that's interesting. The the example in the documentation has some little sleep calls so you know which one's gonna finish first. But you thought that was kind of intriguing because there are a lot of times in the world of data science when you might have multiple op, multiple possibilities to accomplish a task and you don't necessarily know which one's gonna be better. So often you'll do benchmarking and do one at a time to kind of see how they go, but you know what? Why not run them all at once? So it is a classic computer science problem as well as he notes in the blog, and he thought a good way to test this might be to implement three different methods of sorting in r. And this is a little moment for me because I just usually take the default for the sort function, but there are different methods for sorting. We got the quick method, the radix method, and the shell method. Those are all documented in the help page for sort, but he thought it might be interesting to spin up some Mirai daemons for each of these. So he did three of them.

And then to be able to quickly have some helper functions or Mirai objects to run these different sort algorithms with a vector of numbers, in this case, a pretty small vector nonetheless. But he timed that out, and sure enough, the method of sorting called quick was the winner in this very first test. Again, not many ends in this case, but he wanted to prove it out after the mirror I call by just running a simple benchmark from the bench package. Then sure enough in those results, the quick method is the winner there.

So that was great and all but that was only for you know up to a thousand numbers there why not bump it up a little bit more let's do a little bigger here shall we so he does that to now about looks like 5,000,000 numbers instead so once you have that what's the fastest here again using Mirai's race Mirai function this time the winner is the Radix method I actually don't know radix does in this case, but that's where the help functions for and sure enough he thought okay sanity check time. Let's run this in benchmark once again and sure enough the fastest is radix but he did notice that and this is a common thing in a lot of things asynchronous processing or HPC computing.

When you use multiple cores or multiple systems there is a bit of overhead when you hand off different, like, data types, data structures, or other parameters in these processes. So sometimes that can be a big deal. Not so much here, but it is something worth as a caveat to note in case you're doing this with a larger set of information. So that means that this race mirror eye is indeed working. It's getting that fastest one out first and that return object so you can use your imagination like he did at the end of the post and do a simple wrapper function, calls a race sort, which does this entire process of spinning up the Mirai daemons, the three different Mirai objects, and then tack caching those results in an object and then be able to inspect what it did there. So there are there's pretty interesting ways you could do this.

You may have some caveats to deal with if you do, say, more than three of these. I don't know. We haven't he hasn't really exhausted. We looked at that yet, but I think for some quick benchmarking, this is a pretty neat, neat opportunity to leverage MirrorEye in a novel use case. So it looks like you did get some, a lot of comments on this on Mastodon after he posted this. So a lot of people are interested in trying this out, I think. And I think I'm gonna add this in my Mirai expirations down the road.

So great way to race to that finish line without really knowing which race car is gonna get into that finish line first. So pretty cool little hidden gem in Mirai, and I'm always curious to see more what I could do with that package.

[00:08:40] Mike Thomas:

I'd really appreciated sort of the simple approach to demonstrating this race mirror eye function, because I think he he did a great job sort of explaining the art of the possibilities. And as you mentioned, Eric, setting it up with, the the paper from the Thinking Machines Lab, with the LLM kind of thought process and how it determines what words it's going to include in a sentence. And then even the the reference to the Tuftee book as well, seeing with with fresh eyes where he has kind of this very interesting graphical sentence out there. And his approach here to demonstrating what RayseMirai does, you know, reminds me a lot of, like, the bench package can do with that benchmark function. And I'm not sure sort of what additional insights you get by leveraging MirrorEye to accomplish that where you are kind of benchmarking, you know, the same process that you're running three different ways, on different,

[00:09:41] Eric Nantz:

cores of your computer. Right? Essentially. Right. Right. As opposed to for that matter. It could be no Right. Cluster or anything. Yep.

[00:09:49] Mike Thomas:

That's a very good point. Right. As opposed to, I think, what the bench package tries to do or some of those other ones would be, you know, running it on the same, you know, either single thread or or multiple threads, that same process, multiple times one after the other. So I think that's there may be certainly some interesting use cases where you would want to kind of execute these things all simultaneously and compare the times and obviously get out whichever one ran quickest. I could think of a lot of opportunities here to to do this, to try to improve data pipelines or things like that, and, you know, run different, data wrangling pipelines.

There's a lot of discussion in the comments here that I think, you know, the audience may find interesting. And the most of the com most of the conversation here in the comments of the blog post are around particular use cases, you know, timeouts, cancellations, things like that. So that may help to frame this up a little bit more conceptually. But I think this this, race function is is really, really cool, and I'm excited to see how the MirrorEye package, you know, continues to grow and gain some steam in the ecosystem.



[00:10:58] Eric Nantz:

Yeah. I know Charlie's not slowing down anytime soon. I had a chance to catch up with him a bit of Positconf, and he's got a lot of a lot of big ideas to make this, you know, even more supercharged for for new use cases. So, yeah, if it's not in your async toolbox already, yeah, definitely have a look at Mirai for your async needs. And next, speaking of comparing different ways to get to a result, we, you know, it's twenty twenty five. There are always gonna be some form of an AI type of post and and a highlight, and we have another one, but it's, definitely interesting use case that is coming up a lot more these days on how you can use AI to help accomplish some data analysis needs. And in this particular highlight, we're gonna look at what are some of the common, you know, services that we can use for GenAI generation of LMS in the context of data visualization, which as always can have a lot of different avenues you can pursue here.

And so this post is coming from a long time contributor to our weekly, Nicole O'Rini, who has a great, blog post once again. And and by the way, Nicole Nicola, your quartal site is fantastic. It was hugely helpful for me when I was migrating my shiny dev series site to to quartal. So certainly thank you for sharing your your journey in that migration. But I digress. We're back to Jane AI here. She thought it'd be an interesting exercise to look at one of the things that she does routinely, which is visualization often with the tiny Tuesday data sets that are released every week. And she wondered, you know, I've been doing a lot of this on my own. Wonder how the different Gen AI, you know, model providers are stacking up to a realm of data visualization.

Now there are a lot of ways you can approach this and one might think, well, why don't we take like a classical data set like the MT cars or IRIS or even more recently the penguins data set. She thought, you know what? For this exercise, let's go a little under the radar here. Let's look at some tidy Tuesday datasets that don't necessarily get a lot of press or a lot of examples, if you will, in the online resources that these models are trained on. So she decided to use two different datasets for this, and we'll go through these each one by one here. But the first one is for weather forecasting data.

And instead of looking at the entire dataset, she tried to subset these a bit so that it would not break any, like, context limits when you upload these as part of your prompt for the AI models. So with this, she was she subsetted it to the city of Syracuse in 2021 to look at the predicted forecast for the different days of that year, and she asked the l o m to look at doing a visualization of the forecasted temperature versus the observed temperature. Pretty logical comparison and she even includes in the post what she would do in this situation if it was her hand generating the visualizations and as always they look fantastic. Great use of the best practices for visualization.

So what were the contenders for benchmarking in these, AI models here? The four contenders are chat g b t, Claude by anthropic, copilot, and lastly, Gemini, which has been getting a little more attention because it does have a free tier that I think a lot of people are leveraging at the moment. So she threw a prompt in here, which wasn't a a huge prompt by any means, but it it was to paraphrase here create the relationship or chart for the relationship between forecasted temperature and observed temperature.

She mentions that one of the columns explains over the forecast for high or low temperatures which is important when you think about how to organize this, and that she had a very generic sentence at the end. The generated image should follow data visualization best practices. There are a lot of best practices, so how did this stack up? Well, from the case of the weather forecasting, chat g p t did a halfway decent job and again you'll want to look at the post on your computer or phone when you after listening to this episode.

It's a basic scatter plot with the bet with the predictive line with different colors for each of the high or low temperature, you know, points on the graph. But it did some things that were kind of odd such as guessing this the units being Celsius instead of Fahrenheit. It even has a 100 degrees Celsius as for the limit on the axis. Yeah. That you might be melting at that point if I had my mess correct in my head. So the l and then quite get that nuance. Didn't quite do the best color choice for the high and low temperatures for accessibility reasons, and it was a static plot, which again in some cases might be good, but, you know, it's like interactivity in the age of web based formats and shiny apps.

Things get pretty interesting when you look at the different models. Claude basically built an entire React app out of this. Wow, so it was interactive but a little bit excessive probably, and it did like two things not just the plot but then kind of an analysis of the predictability. And so if you don't know react, good luck debugging that if someone went wrong. But, it did some interesting choices nonetheless. Copilot did okay. You know, it's got an interactive type of chart, with the scatterplot of, again, the two color choices for the different higher low temperature.

It did have some nice air checking in the code apparently, but it didn't actually generate the chart when it returned out. She had to expand the chart as part of the objects that were those, being returned. Not as obvious if you're new to this. So that's that was a little weird because you could get the Python code but it had to hit the button calling see my thinking. While the LMS do this when X tries to hide a lot behind the scenes, so she had to dig the dig the thing the thing out. Last but not least is Gemini, which had an interactive chart with, again, some suspect color choices for the high and low.

But being interactive, she could both see the data table and the plot. But, you know, not too bad. Not too bad at all. So I think in this case, Jim and I seem to do pretty well here. But again, each of these, in my opinion, have their benefits and trade offs. But that wasn't the only dataset she looked at, Mike. Looks like she's got another set here looking at CEO departures data. How did the Gen AI models do with this?

[00:18:29] Mike Thomas:

And I think it's involuntary, non health related CEO departures data, where you're looking at the relationship between fiscal year and CEO dismissal rate. So I think for anybody who's familiar with data visualization, when you hear that one of your axes or or variables is going to be a year or some sort of a date, right, that leads us to probably thinking about, line charts, right, for that particular trend analysis. So ChatGPT produced a line chart, and it produced, you know, some accompanying text sort of explaining what it did.

Fairly basic chart doesn't necessarily follow the best data visualization practices like, you know, rotated access labels. It did plot the rate rather than the absolute count given, which was probably, more appropriate for, the dataset that was being used and the, the visual that was produced. So not too bad. Claude unable to produce a visualization. It said that the air the air message that Claude returned was that the text was 2413% larger than allowed. So I think we exceeded the context window and the data was just too big to upload.

Copilot said pretty much the exact same thing. Gemini has a nice big context window and produced a small interactive chart, with some accompanying explanation about what it, produced. And again, Eric, similar to the prior dataset, it seems like there's a lot of consistency in the design choices that, these LLMs make when you ask them to produce data visualization. So the the the line chart sort of conceptually from a formatting standpoint looks very similar to the weather forecast data. And Gemini was also smart enough to plot, the rate instead of the absolute count in terms of CEO dismissals by year.

So I can, you know, summarize some of the final thoughts that Nicola had as well. I think she's kind of rank ordering these these AI tools with Gemini being at the top with, you know, sort of good overall comparable performance across the the two datasets and the prompts that she provided. Obviously, charts are are not perfect by any means, but they do allow interactive, data exploration and don't necessarily over complicate things. ChatGPT, I think was was kind of next in terms of overall utility. You know, very comparable across the two different datasets, very similar. Charts look kind of a little amateurish ish and not quite as good as what Gemini was able to produce.

Claude was good for for small datasets but over engineered things way too much. And I can definitely, assimilate with that. I asked a very simple question of a modeling question, financial modeling question the other day to Claude, and it just built me a React app.

[00:21:31] Eric Nantz:

Wow.

[00:21:32] Mike Thomas:

That I did not ask for necessarily need. It was awesome, but went way above and beyond. And Nicola says it it'll do that unless you prompt it not to. So that's something that I have to, think about and and consider is actually telling it not to go above and beyond, which I hadn't thought of. And then Copilot seemed to be kind of the the lowest end of the totem pole here, sort of poor overall. Initially, produced some code to create a chart, but not at the chart itself. And unless you actually click that see my thinking button and unable to work with larger data, I've had similar experiences with Copilot. I don't really understand Copilot because the Microsoft, you know, marketing posts I see lately say that, you know, sitting behind Copilot now is the latest version of ChatGPT or the latest version of of Claude. So I think that there's some integration there. I don't want anybody I feel like if I speak ill of Copilot, somebody from Microsoft is gonna knock on my door two seconds later. So I'm gonna end it there, but I really appreciate it. Yeah. Data visualization isn't something that I've thrown at LLMs too often, and I think Nicole did a great job of, sort of providing us with a couple of use cases across a couple of datasets and and taking a look at benchmarking the results that came back from the different tools that are out there. So very interesting and a reason for me to probably dig into Gemini more than I have to this point.



[00:23:02] Eric Nantz:

Yeah. You and me both. It's been I've heard a lot about it. Just haven't really dived into it. And another thing that'll be interesting to compare as you recall with us at at Positconf a month ago, we did see in one of the keynotes, Joe Chang, unveil the Databot plug in to Positron, which helps you leverage the LOMs to help analyze datasets, and it can produce visualization. So I'm interested to see what Databot's gonna do with a prompt similar to one Nikola has put in here. That might be, an interesting comparison because I know they've done a lot of tuning, as Joe has mentioned, to make sure it doesn't do anything too crazy and it sticks to the confines of the particular data you're looking at. But I think that would be an interesting comparison too.

I still do not do a lot of the agentic side of this other than my, as I mentioned a couple episodes ago, my 4AM escapade, the creative quick shiny after my presentation, which actually was helped because of the shiny context with with that agent encoding. Not like this. I think this if I just did like a well, apparently, if I did quad, I would have had to react that. That would have made no sense in my in my presentation. So good thing I didn't do that. But this is this is some interesting stuff, and the jury's still out to just how far you can take it with such a subjectively, you know, subjectively creative, you know, visualization because there are a lot of choices you have to make as a human that we take for granted that these elements are. Once they get good at this, yeah, then we might get into Skynet territory, but we're we're not quite there yet.

And last in our highlights today, we're gonna revisit a topic that's definitely gotten a lot of attention, especially in the shiny community. And I would say almost the past year, we see multiple, you know, top notch developers and practitioners speak about the benefits and the awesome ways you can supercharge your development with a paradigm called behavior driven development or BDD for short. In fact, I was rewatching some of the PasaConf recordings and there was a great presentation about BDD in the context of building a shiny dashboard.

But one of the things that we've been featuring in previous episodes of our weekly highlights is kind of the nuts and bolts of how this works in the context of a shiny application. And the contributor at the forefront of this has been Jacob Sobolewski, who I believe is an engineer at at Absalon, who has been doing multiple posts. And I do mean multiple on his blog about the different niches inside BDD and other test, you know, development strategies. And we've we've covered them. I know at least a handful of them in previous episodes.

But when he comes with us today is probably the best place to start for this. So it's kind of interesting that this didn't come first, but this was the culmination not too dissimilar to what Veera did in our last episode with the shiny side of l o m series. Now if you've heard about BDD and you're like, what's in it for me? But what does this actually look like in practice? This post is for you. Because Jacob in this post, in a very comprehensive way, takes you from literally nothing. You've got an idea for an app, but you have nothing yet.

And how BDD can help construct the inner workings of this application in ways that, at least I can speak for myself, I did not expect at all. And the overall framing of this is going from the outside in. And let's elaborate what that really means. Instead of whipping out your your favorite template snippet of like your app UI and app server functions, you're starting from scratch with writing specifications. Now not just specifications on like a plain text document, but actually specifications within test stat.

What the heck do I mean by this? Well, if you got your test stat structure all booted up in your application structure, you can then do within this a test script like he calls, and let me phrase the context of this. He is building an application that's a simple kind of user form with a few different fields. And then when that feed those fields are quote, unquote submitted, it's going to some storage system for another app or another process that consume. Quite common in the world of Shiny. You're assembling user inputs. You're taking those. You're sending them somewhere.

So he starts literally from the basics of this specification around the idea of the data submission process. And in this code that he has in the blog post, you have two different approaches to this. But one, within test that you have a function called describe, and this is where you can put, like, the overall subject of that specification, and then the function it about what quote unquote that specification is supposed to accomplish. This is where it gets interesting. He has three functions that look as if I'm writing them in plain language, given no content, pipe that to when I submit entry with all required fields, pipe that, then there are entries.

I really had to take a double take on this for a little bit. It was like, where did those come from? The test that, you know, just go crazy on me? No. These functions don't exist yet. That's the paradigm here that does take a little bit getting used to. Use plain language to create functions. Actually don't have anything in it yet, and we're starting from the specification. So of course that's not gonna work on its own. Right? It's not supposed to work on yet because we don't have flush anything out yet.

So that's where you start building the actual test infrastructure to fill in the blanks a little bit on these functions, like given no content. What does that actually mean? Well, now in this next part of the post, he does put in some ideas of what this means. So in the case of given no content, you're creating whatever the storage mechanism is going to be. You're going to create what's called a driver to put things into that storage and then returning the context object associated with it. Again, nothing's been built for that part yet either. Those innards are still not fleshed out yet, but he's doing this for each of those steps in the specification, fleshing out those functions kind of in a step by step fashion.

So again, that's the point. They don't exist yet, but it's framing your mind. Now again, this is why I'm interpreting it. Maybe I'm getting this totally wrong. It's framing your mind for what you need to complete in order to satisfy this specification. So you can kind of start doing this bit by bit. So then you think, okay. Well, I got those inner parts of that specification function not fleshed out yet. How do we actually do this? So this is where things get interesting and I learned something new just even in the in the weeds a little bit.

You can also extend certain objects from shiny test two, which he's gonna use in this example. The app driver r six object is what Shiny test two uses to, you know, do the manipulation in your app, like setting inputs or returning stuff. You can extend the existing class, and he's got a little utility function in there called fill required fields and has, you know, the placeholders for those different inputs, then he's able to put those in. And again, the key part is it's hiding implementation details still, even though he got the the bones of this driver function, there's still some things that need to be fleshed out.

And then it only then does he get to actually starting to write work on the app itself. So we went from like the very bare bones specification to the inner workings of those specifications and now to the app itself and things are still kind of abstract, but then it's like, okay, now we got the app scaffolding. How do we implement like the storage mechanism? Now he brings an r six class into the picture, but it didn't start this way. That's the key. It did not start this way. And then he goes through the different specifications fleshing out what the driver will do to satisfy these different specifications.

Absolutely fascinating. And, again, there we would need probably another hour to really dive in the nuts and bolts of this. But the nice thing here is that this has examples that you can draw upon for, again, a very basic app, and that's his his call to action at the end here. Try this out once. I think he realizes that when we look at this on the surface, it may be difficult to grok until you actually get into the weeds of it. But as you get into the weeds of this, he does have links to those previous posts that I mentioned about whether it's the cadence of BDD all the way into the nuts and bolts of unit testing versus acceptance testing and a lot of the things that he started with in this series.

But now this is kind of like that flowchart of where things fit in that big picture. I admit I'm in a case where I've started a big app about a year ago, so I can't, like, retrofit it per se. But when I get a new app in my in my shop, I'm definitely gonna try this out if it's basic enough because I don't know. Still, the jury's still out on huge apps and how this will work, but it definitely got me thinking a bit. So the the shiny developer in me thinks that this might be worth a test drive of a smaller app. But, Mike, what do you think about the BDD workflow here?



[00:33:40] Mike Thomas:

I like it. I feel very seen or called out by Jacob, in the try this yourself section where he goes, you don't have excuses anymore. Here's your playbook. He has given us a playbook multiple times. And this is probably, as you mentioned, Eric, the best end to end simple simplified use case, for leveraging this particular frame framework, this BDD framework. And there's a lot of great benefits to it. You you're really starting again from the outside in with these high level things that you're trying to accomplish and and writing those down. It almost feels Bayesian in a way.

And and you're starting with a very simple implementation that forces you or really encourages you to write your unit tests at the the same time, and put those out before you actually get to writing the Shiny code itself. Very, probably, object oriented programming comes in here in a lot of different ways, I imagine, not just in some of the examples, but I think it it sort of makes sense to use an OOP approach when you're using the speed ed, throw another few more acronyms out there, type of framework.

And it it certainly makes a lot of sense. I see the reason behind doing it. And, you know, I really like the approach that he's he's laid out here for us in this particular example. So Jacob really just has this four step process where he he recommends us to try to get started if you don't want to, you know, sort of copy exactly what he did in this blog post, you know, with this, again, you know, cucumber and and Gherkin methodology for writing sort of in plain English, things that you're trying to accomplish.

So these four steps are, you know, write one acceptance test describing user value, and it it has to fail. Build just enough infrastructure to run it, which also must fail, and then develop the and implement the simplest solution that passes to to make it pass, and then refactor when your tests provide safety, once those tests are in place to keep it passing, which makes a whole lot of sense. It's obviously sort of not the workflow that I'm sure most of us shiny developers started out in our career doing. Maybe you did.

But for me, it was, hey, how can I write my code to to make my app do cool stuff as quickly as possible as opposed to build the safety nets around it first, or think about it sort of from a high level first? And I think we all know in our gut that we should have gone the opposite way, and Jacob has provided us now with ample resources to help hold our hand when we decide to to go that opposite way in that sort of more safeguarded approach. So this is, again, not his first blog post in this series, and probably the one that lays it out as obviously for me as possible. So I'd really encourage anyone in the shiny community out there to to take a look at this. If this is new concepts to you, I I think this will really hit home, this particular blog

[00:36:52] Eric Nantz:

post. Yeah. This is going into my running list of different expirations to challenge myself in my dev workflow. And behind the scenes, I've been doing some things to eventually, you know, within hopefully a few months, resurrect the shiny dev series. And I think something like this is right to evaluate even if it's just myself initially. But, I I think it's like you said, there's no excuse now. So I I I definitely have some projects coming up that I think could be a good test case for this, and especially since I'm beefing up some of the other open source work I do, I think this could be a great companion. Maybe some of the shiny state stuff I do better down the road. So who knows how far I can take it. But, you and your learning journey of our can take it your your learning journey in all sorts of directions of our weekly. We've got not just these highlights. We've got so many of our great updates. There's great showcases in the world of data science, new packages, great tutorials, and whatnot.

Again, we could blab around for three more hours about the content in each of these issues, but we can't do that today. So we'll leave you with, of course, the link to get all this information is at rweekly.0rg, the the latest issue right on the front page, as well as the archive of issues too if you want to catch up on what you missed. And also, we love to hear from you and the audience as well. One of the best ways to help out the project is via a poll request, and you can do that with, like, a new blog post link, a new package link. Just send us a pull request on GitHub. It's all marked down all the time. Markdown is awesome. Markdown is king. I write our markdown, markdown, all the markdowns out there, different flavors. I love them all.

But we also love hearing from you directly too. You can get in touch with us via the contact page in the episode show notes. You can also get a hold of us on social media. I am at blue sky with@rpodcastatbsky.social. Also, I'm Massimo with @rpodcastatpodcastindexonsocial, and I'm on LinkedIn. Just search my name. You'll find me there causing all sorts of chaos. Mike, where can listeners find you? On blue sky at mike dash thomas dot b s k y dot social

[00:39:12] Mike Thomas:

or on LinkedIn in if you search Ketchbrooke Analytics, k e t c h b r o o k, you can see what I'm up to. Alright. Well, that will do it. I'll put a stamp on episode 213.

[00:39:22] Eric Nantz:

No more fall breaks at least until the holiday season, I guess. But we should be back hopefully with another edition of our weekly highlights next week."
"6","issue_2025_w_40_highlights",2025-10-01,44M 31S,"The ducks have made their presence felt in the world of databases, but now you can take a dip in a new Ducklake all within R! If you ever felt someone should write a book on the many ways you can slidecraft Quarto presentations, we have good news for you. Lastly, a spectacular series on harnessing the power of LLMs in your Shiny apps concludes with…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 212 of the R Weekly Holidays podcast. Boy, that number keeps going up and up, and sometimes it surprises even me. But I'm happy to have you join us wherever you are around the world. And if you're new to the show, this is the weekly podcast, usually weekly podcast, where we talk about the excellent highlights and other resources that are shared on this week's our weekly issue at ourweekly.0rg. My name is Eric Nance, and like I said, I'm happy to have you join us. It's already been a hectic morning for me personally where I was part of a R pharma diversity alliance hackathon with version control, and I'm happy to say no fires or get had to be put out. And we got three poll requests in. So I'm gonna call that a win. So already a a nice day going so far, but day's getting even better because who's joining me here on the virtual screen here? It is my awesome cohost, Mike Thomas.

Mike, do you have any git fires today? I hope everything's going well over there.

[00:01:01] Mike Thomas:

No git fires. Today's a good day because I am writing code. I'm doing targets quarto types intersection, and everything is working swimmingly for now.

[00:01:12] Eric Nantz:

I feel like I should say, oh my, afterwards, but that is a very gnarly combination. But, yeah, I'm really interested to see the fruits of those labors, and I've been knee deep in many workflow actions myself, whoever in targets or even in GitHub actions proper, and debugging those is always an adventure. But that's not the focus today. We got our weekly to talk about today. As always, let's check our notes here. Who was the the individual that curated this? Oh, it's a goof me. Okay. It was my turn in the rotation. So it's about every, what, eight weeks or so, I'm called upon again. So as usual, this is never an effort I do alone. We have, terrific help from our fellow contributors, your curators, I should say, as well as contributors like all of you around the world with your terrific poll requests, other suggestions, and even helping the site itself. So special shout out to good friend of mine from the shiny community, Dean Atelli, who I had a great chance to meet again at POSITCOMF.

He sent a PR to make our site looking even better, especially when you look at the all, tab of the r weekly and you get all those issues. I've loaded up once some nice CSS formatting there as always. Anything to improve the site, we are more than happy to get in there. But let's improve your our knowledge, shall we? So let's dive into our highlights today, and we lead off with one of the hot topics that definitely Mike and I saw at Pazacampa this year and the year prior. Man, there is a ton of momentum around duck DB folks. DuckDV is becoming, you might call the new SQL lite on steroids. It is just doing so much in the community for in memory data management, and it also has some awesome integrations with WebAssembly, other great things that we heard even at the conf with integrations with Shiny, so many integrations, even semantic searching. There's I could go on and on.

But there is a new part of DuckDV that definitely flew under the radar for me, But, this first highlight is talking about a new capability called Duck Lake. And what does this mean to an R user? So let's find out, shall we? This highlight has been authored by Novica Natkoff, who is, I believe, a data scientist at the Norwegian veterinary Institute, and he calls this post jumping into the duck lake with nothing but our on. So this has been, I guess, a newer capability offered by the the company behind DuckDV.

You I guess the way to describe this, and I'm very new to this, kind of this hybrid of what you hear in what's called a data lake where you often have a huge collection of data, but also some metadata associated with it to help with the storage and retrieval of various records. Well, DuckDB is getting in that town so to speak with Duck Lake bringing in these two concepts together. Now how do you actually get started with this? Well, that's where in the R community we've had the duck DB package available to us for, gosh, probably almost two years now I'd guess, somewhere around those lines.

But within the package itself of course you get duck DB ready for you. But one of the selling points of duck DB is the extension system. There are extensions for all sorts of things. I remember playing with the extension for object storage retrieval and and and sending of of parquet files and whatnot. That's played nicely. Well, there's a new extension called the Duck Lake extension, and that's where once you load up DuckDV in your R process and you create a database, usually an in memory database to play with things, you can execute via DB execute similar to the DBI syntax the call to install literally the Duck Lake extension from the official DuckDV extension repository.

With that, you can now attach a new database. They call this ducklake dot metadata dot ducklake as a as a new in memory database, and this will create now this duck lake version on your disk. Certainly in production usage, this Duck Lake artifact would live on probably object storage of our cloud provider and whatnot. But with that, you can now play with this in your inyourrcom. So again, it's written locally to disk on a temporary basis to start with. So what's why do you need to care about this? So in the example in the blog post, Novica talks about let's load the m t cars set. You can load that in with a typical DB write table call, and then you can now create additional tables with this, say with a different color or a different categorization.

Where in this case, he's looking at a a derivation of the miles per gallon over the mean of the miles per gallon as a new as a new variable and then creating a summary of that with some averaging and then grouping that by the cylinders. So you've got now a layer of raw data. He's calling that bronze in this case, but this second layer that I just talked about is sort of some cleaning and normalization typical of any data kind of pipeline. And now if you look inspect that that new file based database duckleg that you've ex that you created here, you've got three files associated with these different layers of the table that he has created here. And in each of each of that you've got a parquet file. So it's using parquet as a storage format.

Again, to me, parquet is like the new CSV. It is just everywhere now. You can't escape it. Not that you want to escape it. It makes your life a heck of a lot easier, I would say, once you get to know that better. So now those are three kind of separate tables based on these derivations, but you want to figure out then something that I think is still on the bleeding edge here is being able to track kinda how these tables are related to each other. He does have some code that, that would be in the ideal case of how you define, like, parent tables, child tables when it's created over metadata.

I believe that's not quite working as expected, but there are some additional metadata you can put in the table, you know, Duck Lake, I should say, nonetheless, such as executing different values into the filtering operation and then visualizing then the relationship amongst these tables. So So you got these parquet files that are kind of stored on disk. Right? And that's where you can now select into these from these relationships that you've established. And, again, some code works. I believe some doesn't. Haven't tried it myself.

But then you can create then a graph of the kind of ontology of the the way these tables relate to each other. And there's, there's an example here in the blog post. It looks like a kind of a DAG of sorts where it goes from one part of the table and I have to zoom in on my browser because it is kind of small on my screen here. From the the the first level, he called it the bronze version of MT cars, and then it branches off into separate branches of these different derivations that have been made here with this silver layer. And then from one of those layers that branches off again to this gold layer at the very end, calling it kind of a a catalog of sorts when visualizing how those are related.

So what he thinks the value of this is is that when you have these this metadata establishing the relationship amongst these tables, you can now put this relationship diagram of sorts, whether it's like this graph here in in the post or elsewhere, that could go in, like, your internal documentation for this, you know, set of databases or set of tables. Thinking of Quarto that Michael Mike mentioned he's been working on. Maybe a Shiny app that visualizes the relationships amongst these. There are a lot of things that this could be used for, and in fact, he even puts that at the end on top of the gist that has the code for the blog post.

Maybe you want to fold this into part of an existing data pipeline. We covered in, I think, last year's our weekly highlights a package called Maestro that helps you kinda take a workflow like approach to building data pipelines. This could be a good fit for those as well. So my take on this is that, a, it's standing on on well established format for saving these the disk, a parquet, but it's not just the derivations themselves. You are able to add kind of relationships for these tables as metadata and then have that available however you choose to visualize it.

So I'm still very new to this. I do want to play with this a bit once I get into more complex setup, but it's great to see that with an extension away, you can now play with a little bit at your fingertips of what the Duck Lake Promise sets to be. So really informative post and and shout out, Novica who sent this my way on blue sky, and I'm like, yes, please. Get me all the Duck DB content you can you can pass at me. So I'm enjoying this, but, yeah, Mike, does this get your creative juices doing what or going what you do with DuckDV?



[00:11:10] Mike Thomas:

It does very much so. And I'm loving DuckDV. It's still pigeonholed on a lot of projects with clients that we have on some of the larger, you know, data lake, if you will, like engines and providers or Databricks and your Snowflakes of the world. And I just anytime I'm working on one of those projects, I always feel like if I had the opportunity to start now from scratch, we wouldn't need it. We could just use DuckDV, Parquet files, and things like DuckLake here to essentially do most of the same stuff for free in a completely open source way that we would have more control over as well. And and I think the benefits of DuckDV are not something that we need to go into again because I've probably, talked about that way too much on this podcast. I just talked about that probably for five or ten minutes at Positconf, last week, really, how great of a project that is. And there's a lot more coming out on that project as well. Duck Lake being one of them. I think Mother Duck as well as sort of a tangential project that allows you to leverage, you know, cloud compute, and distributed computing, I believe, as opposed to needing to run things on your own laptop if you want. So a lot of advancements, going on in this space. And I think one of the the real pillars of of what Duck Lake is able to do and what Novica is, demonstrating in the blog post is kind of this concept, and you may have heard it if you're familiar with things like Databricks or Snowflake, of of time travel, really. Such that you can start out with an initial dataset or a table, which is under the hood gonna be represented in a parquet file, stored in a parquet file. And any changes that you make to that, you know, through SQL update statements, through SQL, insert or delete statements will also be tracked in subsequent parquet files within that same storage.

And there's this abstraction layer that you can write SQL sort of on top of that storage, and the way that the files are named and organized in this, Duck Lake convention, which is is not necessarily new as you mentioned. This this data lake protocol has been around kind of for a while. But the, sort of the protocol allows you to write SQL on top of it to say, you know, query, this table as of this particular time or as of this particular version. And under the hood, it can figure out what, data it needs to query, what inserts to apply, what deletions need to be applied as well to show you the data that you're essentially asking for.

So it kind of builds up this set of parquet files in your storage. And I I think this kind of time travel notion is the biggest sort of selling point of, what Duck Lake has to offer right now. And I really appreciated Novica taking that one step further in places where, you know, I'm not sure, DuckDV has, like, visual representations yet, but trying to actually show lineage as well beyond that. So I thought that that was really cool. And, you know, DuckDuck, I think, out of the box, tries to recommend leveraging Parquet for file storage and Postgres for essentially your data catalog, and allowing you to or your metadata, if you will, on top of your, data warehouse or your your data lake that you're creating. So that combination is is interesting as well, maybe good to know in terms of context here. But I I really enjoyed, this write up because I I haven't seen a whole lot of, you know, diving into some of the the weeds in these new developments in what's in DuckDV. I've seen a lot of getting started, but, in terms of getting started with, Duck Lake here, it's not something I had seen a whole lot of before. So I really appreciate Novoca, putting this together for us.



[00:15:04] Eric Nantz:

Yeah. I'm definitely gonna pursue this and maybe some lower lower risk projects just with some assorted simulated data, but I could I could see this time travel aspect. Not some ideal with a whole lot on, you know, strictly clinical data kind of stuff I do for most of the the day job. But certainly as we look at different evolving stages of, say, assumptions about data and different derivations across its life cycle, this this could be a great tool in our toolbox. So this is this is in the highlights for a reason. I think this is a a great way to to start off this episode.

Well, Mike, you mentioned you had a a spectacular presentation that you had at Posikov, and and I again, I watched the recording again. It was just as I thought when it was live. But you and I had something in common with with that other than just, of course, us being presenters at Pazitconf. We really like using Quarto. We've revealed JS to create our slides these days. It is just been an absolute pleasure compared to what I usually have to do with the day job, which I will save that rant for another time. You probably know what I'm talking about.

But it is really, really handy to take what I'm comfortable with, mark down syntax, and break these up into literally different sections of my slides. If I need to style it up, I sure can. You know, if I don't need to style it up and I just want nice convenient things like two column layouts, embedding code chunks, annotating through those code chunks, highlighting things as I progressively build, all that comes out of the box of reveal JS and quarto. Absolutely love it. And I have bookmarked a lot of stuff in my collection of bookmarks with the quarto tag, and they're either from people that made quartal or those that are like us making slides of quartal and they do some neat little trick and I'm like I gotta bookmark that.

There's a lot of these tips out there. A lot of them are quite scattered, but there's been one person at the forefront of collecting these in such a way that we could benefit as a community. And boy, he has taken that even up a 100 notches here. So we are speaking of the newly released online book called slide crafting, authored by who has been one of the ones at the forefront of this. Emil Fithfeld, who is on the as a posit engineer at Posit does a lot of work with the the tiny models team, but he has also been making so many great resources about Quarto and in particular making slides of Quarto.

His blog post that he called slide crafting had been featured as previous highlights before but now he has assembled them into one place called the slide crafting book and this is still early we always saw this come out I believe about a week ago or so it was after comp for sure and it's got a collection of all these tips that have been scattered or he has mentioned in his previous blog post but it's of it's not feature complete yet but what it has here is of definitely enough to get you started on the right foot.

All with kind of a crash course of creating your own custom theme for your coral slides in ten minutes or less. It literally has from start to finish what that process looks like, and then the rest of the book goes a deeper dive into each of these. What goes into the theme? Right? What colors do you select? What kind of fonts are you using? And how big or small are certain elements? Also, like I mentioned, what is the CSS styling associated with it? All of these are dedicated sections in this book, and I've already, just looking at this, have learned so much already that I'm gonna take into my next portal presentation.

And that's just the theming side of it. There's also the content block where we're gonna look at different ways that you can display your code, different ways you can highlight certain elements in your deck, changing layout. Where do things go? Left, right, moving it up and down? All these things that you can do with Quarto are all documented here. Like I said, it's not quite done yet. It's even got, other sections such as when you're showing a code demo using things like ASCII cast to help with that kind of recording like setup of a code chunk, ways you can bring interactivity via fragments and highlighting certain blocks of of of of elements, changing the styling on the fly, and even the the start of an extensions, section which is still not there yet, but I am looking at I'm gonna keep watching this quite closely.

There's a lot here. I've always wanted something like this because I want something where I can go to just in one bookmark, get all these tips. At some point, a lot of it will become muscle memory. It hasn't yet. So every presentation, I've learned something new. I do something different. In the past one at POSITCONF, I did the the highlighting of the code and narrated through each of those sections of, like, a shiny state block, but there's just so much. There's so much to learn. I think this is the start of something huge here. So, yeah, Is the slide crafting? Yes, it is, and I am here for it. What do you think, Mike?



[00:20:58] Mike Thomas:

Yeah. I I can't agree more. I am slightly upset that this came out how many weeks after Positconf?

[00:21:05] Eric Nantz:

I think maybe a week after.

[00:21:08] Mike Thomas:

Oh, man. I could have used, like, so much of this in my presentation. This is this is fantastic. And it's so many it answers so many of the little questions that I've had and just kind of not done the deep research into to figure it out, like image placement, layouts, things like that that are just, these great quality of life improvements for your presentation that I knew I needed to incorporate, but just didn't have the the bandwidth or didn't necessarily have a resource like this that I could go to to quickly get answers to my questions. And don't get me started on what the LLMs fail to know about, Quarto, reveal JS at this point because I I tried and and struggled quite a bit with some of the responses I was getting. But, anyways, this is it's just gonna be this is almost like a big book of type of a resource, and and I it's not super dense either. I mean, there's a ton of content here, but it's pretty easy and well organized to get to what you might be looking for. And even if you're not looking for something specific, you know, like I did, in preparation for the show today, I found myself, you know, scrolling through every section, you know, taking two minutes per section and and picking up something, constantly in terms of, theming and SCSS, which is, I promise, not as scary as it it may sound if you're not familiar with CSS or or SCSS.

It's maybe a little different to look at at first, but the amount of control that it gives you to incorporate your, you know, companies, or your organization's brand and logos and colors into your slide presentation that that that go a long way, believe it or not, to getting folks to pay attention to what you're presenting to them. That all everything is essentially right here. And some fantastic little things that I could have certainly used. I had, some slides in my presentation that I needed to kind of manually we had a clicker that we were holding, but I needed someone may out there may have the same type of situation.

But some had elements in the slide that I needed to scroll down, and your clicker can't necessarily do that. You would have to go to your mouse or your touchpad on your laptop to be able to do that. It's not just advancing a slider, not advancing a slide. Well, of course, ML has a section in here, I believe, under, fragments potentially that shows me exactly how I could have done that. You know, advance a tab set or change tab sets, I should say, or scroll, on some some content that's on your slide through, you know, the back and forward, buttons in the presentation.

Oh, man. I could've used that so much. But, anyways, just a fantastic resource. I can't even pick sort of one area to highlight because I was just picking up stuff everywhere throughout this book. So I'm excited to see how it continues to expand, but a ton of content here, a great start. I think it's easily digestible. And if you go to this URL and don't find something, that, you know, you learned that you didn't know before, I'll give you a dollar. It's how confident I am. This is amazing here. And one thing I'm

[00:24:35] Eric Nantz:

as I'm assembling my notes through various presentations, even if I see a potential to hopefully add maybe to this, I'm hopeful that this will be a community driven effort as well. I may have some things to add around the video, playing when you get embed videos in coral. There's some neat tricks you can do with that as well, which were really handy for my talk with the the the Mega Man inspired app demos that went without a hitch. Thank goodness for that because I was a late night hack to get there, but that that that would be another time.

But, yeah, even you, Mike, you utilize videos nicely in your part of the presentation and do a poll request and and issue notes. So, yeah, I could see. I yeah. This is definitely growing, but, boy, there is enough here now that for your next quartal presentation, whenever you're doing this at the day job or you have an external conference you're presenting at, I think you're gonna have enough here to do a a terrific job from both the UX side of it and, like I hope, have fun doing it because it is a lot of fun to make these.

Well, if I'm the curator for our weekly issue, you know I'm gonna have something with Shiny involved in it and, and, and, podcast. So, luckily, this was voted by others as well, not just me. But especially at Pazitconf and many avenues before this, we are seeing very much the intersection of large language models to enhance both development but also giving unique experiences to Shiny apps that just simply were not possible even just a couple years ago. There has been a terrific series that kinda takes you from literal start to finish, so to speak, on how you might approach building LLM AI model capabilities in your next Shiny app. And so we are talking about the third in this series called the shiny side of LLMs, and this has been authored by Vera Elfink van Leeenpoot who has been a prolific member in the shiny community since near the beginning.

She has done tremendous resources. She's also doing courses about Shiny as well. We have a platform called Athletics that I've heard great things about, but in any event she has been embarking on this journey to bring up to speed somebody who's new to the world of language models, what are the nuts and bolts behind how it works, but now we're at the third part where we actually plug this in to a shiny app. So the first parts are talking about what elements actually do and what they don't And then the next part talked about going from prompts that you put into the LMS interfaces and how the responses are constructed.

Like I said, now it's time to put this in action. And this comes full circle when we last talked about because the use case for this is going to build a Shiny app that will let you upload the contents of your quarto presentation and get feedback from the LOM on what's working well and what might be tweaked. So it is in essence a peer reviewer of your next portal slide presentation. Oh, boy. You told all things I wish I had had sooner. Maybe I could have used this before the conference talk, but nonetheless, better late than never.

So she's calling this app deck check, and it's gonna start with some narrative here and then the post about first why we choose Shiny. Well, I think the question is why not, but that's another point for another day. The point being is that within the ecosystem, now we have packages both on the our side and the Python side to integrate these chat like interfaces into your app this is called shiny chat on your side and chat last on the Python side so once you have that ready to go and and install either of those packages for your use case, you've now got to choose the LOM provider, of course.

I've been a big fan of Anthropics Cloud Code. Their models have done really well for me, but, nonetheless, whichever either hosted platform you choose, you're likely gonna have to get an API key because this is gonna go into your configuration based with the Elmer package to make all this work. But like I said there are code examples here to build this app literally from beginning to end and we start off with getting the chat interface going outside of shiny so I'm gonna focus on the r tabs here in this what looks like a cordial blog post with Elmer and configuring the chat tie into anthropic so there's a chat underscore anthropic object that you create where you just say the model the system prompt that you give it, again, plain text, and any additional parameters specific to the model itself.

And then this is something new. It's not just the chat object, but you want to plug this in the Shiny. Right? And for this, you want to be able to kind of have that nice streaming effect as the response comes in, and that's where Elmer comes with the stream method of this chat object. And then that's where you feed into it some language that you want displayed in the app itself, and then be able to tell the LOM what to do with that. I've never played with the stream object before, so I'm gonna or stream methods. I definitely want to play with this after after this, after this episode probably.

It does some unique generator based on the coral package. I believe that's a way to kind of do the looping correctly, but this is this is really neat. So it gives you that natural look and feel when the LOM is responding back after the user uploads their coral presentation. The other thing to keep in mind is that you don't want to have a bottleneck in your app as this response comes in. You want to take advantage of what's called asynchronous processing. This is great for the UX of all this so that your chat doesn't grind things to a halt with the rest of the app as the response comes in that's where it's a combination of again the Elmer package and in this case the promises package on the our side to do the special chat underscore async function.

So this is flagged to do asynchronous mode for this and it returns what's called a promise object afterwards that will be rendered or resolved when the app needs it, but it's not gonna stop everything else. So with that, you've got kind of the building blocks in place, and now it's time to build the UI. This is where, for preparing my talk at the conf, I did a, vibe coding session to prepare the UI in my demo and I got like 90% of the way there. But here she's done the hard work for you. A simple layout would b s lib, got a nice of sidebar or page fluid layout, I should say, where we've got a module for the chat component and a header at the top kinda at, like, a little placeholder for the user to ask anything about the presentation.

Very simple UI. Again, a module chat mod UI is gonna drive most of this from the Shiny chat package. On the server side, it's making that connection to the anthropic model and then rendering the server side of this chat interface, chat mod server. They've done the hard work for you. This is a module plug and play. You don't have to reinvent the wheel here. Really great to see the progress on this because I saw the early days of it. It was a lot of custom code. Now, right off the shelf with ShinyChat.

So once you have that, then you've got the scaffolding of a mess of a new UI. It's very straightforward right now. We're just seeing, does this work? And then you see in the screenshot here the prompt that was added as part of that metadata going into the chat object and then the response from the LOM based on the title that the user entered they call it the shiny side of LOM so what's your feedback and it's got the response there so we know the the plumbing works right now it's time to make the app look nice we can't just do that one column interface right so now she does a little whiteboard in here a sketching here of what the app will look like, a nice sidebar layout with cards that are added to the application. We got a main content and and then there's a nice animated screen of what they're working towards, the app in action.

The LOM prompt is on the left, the chat interface, there's a nice little CSS loader icon, and then once it's done it's got a whole bunch of like a series of value boxes at the top. Things like length of presentation and other content, and then the actual advice that it has for the the given presentation with various pieces of feedback. So all the code is here for both R and Python, and you get again this very nice UI and the app itself is doing a lot under the hood. Right? It's actually taking that uploaded quarto file and then it's running quarto under the hood to get the HTML version and then it's getting that that HTML file which is then fed to the l o m based on what they've added in in the back end of it. So this is a nice kind of diagram showing how this process works but in the end you can copy this app right from the blog post and try it out for yourself providing you've got that key to anthropic or whatever AI service you want to you want to do here. So there's other enhancements that she talks about here, such as a loading experience where you get that nice little loading icon when things are happening. So the user's not left with just a white screen doing nothing.

Those nice value boxes at the top, nice refinements. Again, just giving that extra nudge, that extra pizzazz to the user experience. But there's a lot more to this. There's styling. There's how you handle error checking. But this is as comprehensive of a post I've ever seen to take one of these apps from start to finish. So there's some nice nice examples, like I said, for error checking. And then when you're ready to go, there's a great section, what you would do to deploy this. Obviously, there are platforms like Pasa Connect that would be able to take this, Shiny apps IO, the internal I should say the internal Pasa Connect or the external Pasa Connect cloud.

But if I dare say so myself, another way you could host this would be containers as well and plug that into whatever app, you know, of service a, lets you host a container. Again, you build the app. You've got many choices where you host it. Either way, as always, if you're gonna put something like this for public facing, wherever internal your organization or in the public domain, be wary of the usage of that API key because you don't wanna break the bank too. But in the end, for a small app that doesn't have more than a handful of users, I doubt you'll you'll do very much.

Add Jason to this post. I did I had a colleague ask me, hey. You've been using Anthropic for a while. What's been your cost of this? Looked back at my cost. I kid you not, Mike. From January to now, my cost for anthropic has been a whopping $3.20. It's time you did some explaining. That's less than a lunch at the cafeteria. So I dare say I'm doing things right, I hope.

[00:37:00] Mike Thomas:

I don't believe I believe it.

[00:37:02] Eric Nantz:

Alright. But yeah. Yeah. That's so that's where just from little me developing and using it with Shiny or Positron assistant and in some very low level apps that that's pretty tangible. But, again, fantastic series of posts here. This is a great, great example of taking the concepts from the first parts. Now put it in action for a literal use case that I most definitely will be using before my next presentation. But, Mike, you've been we've been watching this space for a while. What did you think of Verway's exploration

[00:37:35] Mike Thomas:

here? It would be one thing for Vira to just, you know, use some sort of toy example and go through the entire process of creating both R and Python Shiny apps with the LLM capabilities in the back end, the ability to render the Quarto document. Doing that all in just one language would be incredible. She's done it in both languages and with a use case that is incredibly, I don't know, like, relevant or or tangible, practical, for anybody that's having to make a quarto presentation, which, you know, we just talked about in the last blog post and we've both done recently. And by the way, your presentation at Pazitconf was was fantastic. I think you gave me a little too many flowers there, compared to your presentation. But, this is, as you mentioned, probably one of the most comprehensive blog posts I've ever seen. I think it's well deserved that it's on, Pozits, series of shiny blog posts. And a I'll just pick out maybe a couple things that I I really like.

I think the error handling is something that we talk about a lot, Eric, and ensuring that your users aren't suddenly just seeing, like, red letters and error messages that are cryptic on their screen, and you do your best to try to this is an approach we always take is maybe pop up a modal when there is an error that tells them, hey. Something went wrong. Maybe you're able to to provide some pass or if statements that categorize what went wrong to give them a message, or maybe you even just voice the the message itself, you know, that you would see in the the terminal in that modal, but it's a a much better user experience than the whole app essentially just breaking. Right? We certainly don't want that to happen, if possible.

The the UI and the design here, I appreciated the time that she took to wireframe, what she wanted to put together, before that. And it's also you know, I don't think this is the purpose of the blog post, but even just some of the ways that she went about crafting her her prompt, yeah, a system prompt, if you will, to sort of instruct the LLM what it should do, you know, how it should respond and things like that were really interesting to me. She does have a system prompt in there, and then she has sort of a a more verbose regular prompt.

And the results look fantastic. I'm very excited. I have my cloud API key ready, and it seems like I can just sort of copy and paste this this code locally and try to run it myself. Myself. And I'm quite sure I'm gonna be blown away here, but a fantastic deep, deep dive into the art of the possible with Shiny, for R, Shiny for Python, and their associated LLM, counterparts these days.

[00:40:37] Eric Nantz:

Yeah. I am, literally, this afternoon, gonna build a custom next dev environment with Rex just for this because I am that eager to try it out. So, absolutely, that is on my on my list here. And, again, this was the third part, the culmination of a three part series. So definitely, like I said, if you're new to this, definitely check out her existing material. It does a great job of breaking down what can be very ambiguous terms if you're new to this. I mean, even me personally, I still get tripped up by some of the terminology thrown left to right on LinkedIn posts or whatever else in conversation. So I definitely like to have it explained to me practically, but not not in a condescending way. Verily does a terrific job with this. So, yeah, this is this is right in my wheelhouse, and, yeah, I'm doing still doing chordal slides. Might have to do one again before the end of the year, so this is this is gonna be utilized for for darn sure. But, we hope you also utilize the rest of the r weekly issue. We are running a bit strapped for time so no additional fines here. I do well, maybe selfishly speaking, I do have in this issue my adventures with switching the shiny dev series site to corridor so apropos of what we talked about today lots of interesting findings there and I'm back to blogging now. I got a blog component of shiny dev series so, who knows how long it'll last but I'm gonna try and be diligent with it, but it was it was it was good to get get the creative juices going again. So that's in there but also many many other things too. So with that we will, wrap up here but, of course, we love hearing from you in the community and we got many ways to get in touch with us. You can send a poll request.

That's time that's definitely appropriate after what I went through this morning, a poll request to the markdown files that comprise the our weekly project, especially the next issue. We have a draft markdown file. If you find a great resource, new package, great blog post, maybe you wrote a great blog post you want it featured, yeah, just send a poll request to that draft issue template and the next curator will be glad to merge it in. And, also, you can get in touch with us on the social medias out there. You can find me on Blue Sky these days.

I am at r podcast dot b s k y dot social. Also, I'm Mastodon at rpodcast@podcastindex.social. That's a mouthful sometimes. And I'm also on LinkedIn. You can search my name and you find me there. Mike, where can they get a hold of you? You can find me on

[00:43:10] Mike Thomas:

blue sky at Mike dash Thomas dot b s k y dot social, or you can find me on LinkedIn if you search catch Brook analytics, k e t c h b r o o k. You can see what I'm up to.

[00:43:23] Eric Nantz:

Awesome stuff. Then again, big shout out to all the listeners out there that said, hi, nose back at the conf. I will always appreciate all those kind words. We're trying to get back to the regular schedule here where we're doing our part, trying to anyway. But, nonetheless, we'll close-up shop for episode 100 212, I should say, of our weekly highlights, and we will be back with another episode of our weekly highlights next week. Hello, friends. We are back of episode 102 212.

[00:44:02] Mike Thomas:

Well, they so I had my speaker notes, you know, on one screen Right. And then my slides on the other screen. And I thought that if I

[00:44:10] Eric Nantz:

pressed play on the video on my speaker notes that it would translate into the video. They're separate. They're separate. I found out the hard way you know before. Yep. Oh, man. Okay. I didn't know he had videos in yours. I would have told you, but No, I know. I'll have to check yours out. You're good."
"7","issue_2025_w_39_highlights",2025-09-24,51M 33S,"Fresh off an amazing experience at posit::conf(2025), the R Weekly Highlights podcast is back with episode 211! Eric and Mike share their experiences at the conference and then dive into an amazing collection of highlights. We learn about a myriad of packages to programmatically write and parse markdown documents, initial impressions with…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 211 of the Our Week of Holidays podcast. And yes, it was true. We had to check the episode numbers. This has been a hot minute since we last talked with you. But this is the weekly, usually, weekly podcast where we talk about the awesome resources that are shared on this week's our weekly issue at ourweekly.0rg. My name is Eric Nance, and, yes, I am probably the biggest culprit for why we've had the long layoff. There was a a busy few weeks getting prepared for what we'll be talking about, I think, very shortly of a very eventful posit conference, but I'm happy to be back back settling down and processing what I all learned.

But thank goodness I'm not doing this alone because he's back as well, my awesome cohost, Mike Thomas. Mike, how are you today? Does it feel odd to be back into this?

[00:00:51] Mike Thomas:

Feels odd. It feels great. I've been on the road quite a bit lately. Had some conferences in on the West Coast. Had a conference, obviously, down in Atlanta at POSIT COMP last week, which was fantastic. I feel like I I'm just giving presentations for a living and not writing a lot of code, but I did scratch and claw together in our package over the weekend, while I was kind of on the couch down with the cold. So, hopefully, that will come to see the light of day soon.

[00:01:20] Eric Nantz:

Nice. We've both been a package mode recently. So, for those aren't aware, both Mike and I did have the good fortune of of being presenters at this year's Posikov. And I'm just gonna, you know, give the floor to you first, Mike, because your talk about kind of bilingual data science teams, you had so many nuggets in there and so many words of wisdom for everybody in this situation. You had you had the room in the palm of your hand, but how how was that experience like for you?

[00:01:49] Mike Thomas:

It was a great experience, and I think I'm sure as with you and anyone else that has presented in the past or is used to giving presentations, it's all about preparation. And the more you the more you prepare, the easier it is. And I felt that, I was well prepared, not not, at at least in part to the team at Articulation who posit, is a contractor that posit hires to essentially help us, make sure that our presentation is put together in a timely manner and that we do a good job of organizing our thoughts. So that was a huge help.

I enjoyed yeah. It's it's very easy to speak about a topic that you're passionate about. So I enjoyed giving the presentation, got a lot of great feedback. But how about you, Eric?

[00:02:32] Eric Nantz:

Yeah. I do echo the preparation, and I do admit mine went down to practically near the wire on a certain part of it, which I'll touch on later. But, I was able to for the first time, I think, ever in a non industry, like, type of conference, I was able and I mean industry. I got my day job. I was able to present something I've actually built in the open source world that actually has seen the light of day. And so I've talked about my newest r package called shiny state, which takes bookmarkable state to new heights, and I think I did a pretty nice, way of rounding that out and my motivation for it and, did spend a lot of time on the messaging because anytime you have, like, twenty minutes or less, and I think it's almost harder to prepare for those than if you get that nice one hour seminar or whatever. So well received, I think. And I'm also happy to say that the package a day after the presentation finally hit crayons. So my first crayon package in over fourteen years, folks, I I mean, if I was able to get it on by hook or by crook, and I went through that checklist probably about 50 times. It seemed like by the time I filed it, but it it feels good, and it also feels good. It seems like a lot of people have big ideas for it, so I've gotta watch that issue tracker pretty closely now.

But, overall, great experience. And then the day before the conference, I was busy with the Our Pharma Summit. Great shout out to all my life sciences colleagues out there that attended that event. And I was running around and doing some audio magic here and there, but giving a presentation there as well. So by the time the conference is over, I definitely was ready for a bit of rest. Not that I got much of it, but, nonetheless, it was, it was eventful and, yeah, all the great conversations hanging out with you during one of the lunches and hanging out with some other peeps. Yeah. It was it was just as advertised.

And no matter where posit goes in terms of their direction, the people make it make it the WER file experience. So I had a blast.

[00:04:40] Mike Thomas:

Likewise. Great talk, Eric. If you haven't seen it, hopefully, when the recordings come out for the folks that purchased a ticket, you can see it, today. And then in a few months from now, I think it'll be on YouTube. But I am so pumped about your Johnny State package. So thank you for all your work on that.

[00:04:57] Eric Nantz:

Awesome stuff. And, yeah, I can't wait to actually use it for my projects now. And I have it out there. It's my biggest motivation for it. And I am actually dusting off the cobwebs of the shiny dev series site. I'm I'm gonna put a blog section on it, and I have a blog post about the package in the near future too because I gotta I gotta get on the blog and train too when I get a chance and follow your lead amongst countless others that do a way better job with us than I do. So Definitely not me. I'm not sure what you're thinking of, but there are plenty of people out there. I've seen them, man. I've seen them. You did you do a bang up job. But we gotta talk about the awesome our weekly issue before we go too much further here and checking my notes because it's been a minute. Our issue this week has been curated by Jonathan Carroll, another one of our longtime curators and really one of the founding fathers of this project, I must say. And as always, he had tremendous help from our fellow Arrow Crew team members and contributors like all of you around the world with your poll requests and other wonderful suggestions.

So we lead off with a ways that you can not just write your markdown documents in the traditional way. And when I say markdown being what almost is like the kind of like the English language to me at this point in terms of technical documentation, I'm writing it every day. It's going in many different places, whether it's a GitHub issue, summary, a GitHub pull request note, or just my technical notes. As in the R community, we have many ways of authoring our mark our markdown documents. I even just said the first one. R markdown is what pioneered a lot of these efforts.

More recently, the quartal framework amongst others as well. It's one thing to manually write it. Right? What if you wanna programmatically generate and parse markdown content for further downstream or upstream processing? Our first highlight is just for you because this is a really big deep dive into many different ways you can approach this programmatic perspective of markdown, and this is authored on the rOpenSci blog. So, by the way, shout out to to Noam Ross from rOpenSci. I was able to meet him at the conference, and it may have been our first meeting ever, but been talking to him online for years and years. So that was that was fun to to talk shop with him for a bit. But this post on your open side blog was written by Mahal Salmon, former former curator of our weekly, along with Christophe Dever and Gian Convar.

And this is this is a a big one. So we'll summarize what I think the key points I'm taking away here. And, of course, Mike, your perspective is appreciated too. It first talks about what exactly is markdown. I hope by now most of you listening are familiar with markdown, but if you're not, there's a great intro to how you write the syntax. And there are a few different ways you can write certain bits of syntax, but it does go over what I mentioned earlier, the various ways that you can interact with markdown, especially as an R user. And and, again, there are quite a bit out there.

And, of course, one of the biggest selling points recently is that you have maybe some front matter in the markdown written in typical YAML format that might this or it might define certain options for rendering or other parameters. And, of course, you can embed programmatic code chunks itself using R, Python, or other languages, with the special kind of fence div kind of syntax there. Again, all that's in the in the blog post. We all if you've done markdown before, you you're pretty familiar with that stuff.

Now it's about what if you are in a situation you wanna build a template, but yet you wanna dynamically insert certain pieces from, let's say, an outside art program or whatever else, there are various ways to accomplish this just to get things off the ground. In the art community, markdown and r markdown rely heavily on the knitter package by EYC, who was that has been the glue of so many things for what I do in markdown. It comes with a function called knit expand, which will let you kinda part you know, inject into your markdown syntax with maybe those placeholders for variables. You can inject certain things in that as well, but there's more than that in town.

There's also the whisker package, authored by Edwin de Jong. That's actually used by the package down framework. I didn't know that until this post. There's also the brew package, which is a a long time package in the r ecosystem I've used in the past. And if you wanna get outside of R itself, there is, of course, pandoc, which is used by more than a handful of markdown rendering engines. So we got some nice examples based on if you were giving kind of a homework assignment and you want to inject the name of the student into the document as well as a customized solution, or I should say, customized mean and standard deviation for the question here.

And then the example talks about using, you know, a workflow based in the whisker package, which for those of you familiar with the glue package, you're gonna feel right at home with whisker. I've played with it a bit. It is pretty nice. And a new one I learned, once you generate those lines of dynamically generated content, if you wanna write that out, there's this newer package to me anyway called Brio, b r I o, which apparently is kinda like a souped up version of what base r can do for reading just arbitrary lines of text and writing out lines of text. So I'll have to put that in my toolbox for later, but the example basically has a custom function called make assignment that wraps this together. And sure enough, we got a nice little data frame out that has for each student the custom mean and SD parameters and then the file name that it's writing to. And and once you run that, you get a markdown file of everything filled in dynamically.

Definitely a great way to get started, and that is for generating. Now let's talk about parsing. This is where I've wondered. I've been through grep nightmares parsing stuff before in text in text documents over markdown or not. So that is of it is an approach if you're willing to get, you know, your hands kinda dirty and muddy with regret, go go for it. Not my cup of tea anymore. I've been down that road before. But there is the new the new thing in town for importing Isuzu's programmatic type documents is changing it into the abstract syntax tree layout or AST.

This is kinda like when in markdown, you have these common constructs to say headings, lists, code blocks, numerous other elements. And then you can think of ASTs are a way to give you this kind of maybe nested kind of list kind of structure where then you can parse that and then write maybe in or modify certain elements of that and then write it back out. This is being used in many different ways that I've seen in the community, but, the post continues with a real life example that I did not know happened, but, there is this great, reference that I saw in the g g pod ecosystem in the past called the g g pod two extensions gallery.

And apparently, somebody hijacked one of the links in that gallery for some of those. Not so nice, I'll say. And so what if you were on the receiving end of that, if you were to maintainer of that and you need to quickly find which links were hijacked, you know, you're parsing out this huge markdown file or set of markdown files where these links are scattered. What are the best ways to handle that? So there are many different parsers that do the AST route for markdown, some of which include the tinker package authored by Miles Salmon herself and maintained by Jian Kamvar, another coauthor of this, that translate that to XML from markdown to XML using the rendering engine called common mark, which is used quite a bit.

XML still gives me the heebie jeebies sometimes, so it's not my very desired way. But, hey, XML is quite powerful. I do acknowledge that. So, send your feedback to me if you have any differences of opinions or if you're successful with XML. I'd love to hear it. There are some new games in town here. One is called the MD four r package by Colin Rundell. It is in the experimental state, but it does kinda take that list approach to translating the AST to a nested list, all built in r. There is a development package out there.

Pandoc itself will let you do this, and and you can either use, into its, native form of the AST or in JSON format, but then you would have to use Lua filters to be able to parse through all that or JSON if you're doing JSON. If you're if you're familiar with JSON, you might be able to have some good stuff there that technically doesn't depend on another language. There are also newer ones out there like parse QMD that Nick Crane, who I also met at the conference, a new package that parses quartal markdown documents, and then it's basically built on JSON as well with JSON lite. There is also the parser m d package, yet another package by Colin Rundell.

Seems like they're really in tune in this space quite a bit, and that's using a special boost spirit library, which I think maybe based in c, although I have to fact check on that. There was a talk about that at a at a long ago, our studio conf in 2021 that might be helpful as well. There is a lot to digest here because you got a lot of choice here. So they in the in the rest of this post, they talk about what can you use to inform your decision on choosing a parser. And part of that will depend on if you're gonna import code chunks and do something with code chunks or not. That might that might switch the game a little bit because in that case, you might or benefit from XML as well because that might handle the code chunks a bit better.

But there that would vary by use case, of course. The post also talks about what about that metadata I talked about at the top that's usually in YAML type format? You separate that with those kind of three horizontal dashes at the front matter. I definitely have done things I in the past to grab those dynamically. Wasn't the prettiest in the world. But if you are using r markdown, there is a handy function called YAML front matter that will help you extract that through one function call. And I'd say, yes, please. The last thing I wanna do is rewrite that kind of stuff if unless I absolutely have to. And then there's also other ways to do this too with, say, the YAML package itself.

And if it was in a different type of markup format like TOML, t o m l, there are some other packages like RCPP, TOML, as well as others to be able to parse that as well. So I think generating documents, I would say, of the two things is probably the easier thing to to tackle at first. And with those tools we mentioned earlier, I think you can get underwrite very quickly. Parsing, you've got you got a few options. Just like everything in r, you got a few options at your disposal. I think it really depends on the complexity of what you're parsing that's gonna determine which way you go.

Again, I probably don't resort to XML based methods unless I have to, but that could be just me feeling like imposter syndrome with some of that stuff where if I give it a go, maybe I'll I'll feel better about it. But I am intrigued by a lot of these tools that we're seeing in the community. And if I am in this situation in the future, I'm gonna use this post as a as a great starting point for that, markdown generating and partisan journey. But, yeah, markdown all the time, no matter which way you slice it. Mike, what did you think of this?



[00:17:31] Mike Thomas:

Yeah. Markdown is the English language for us programmers. I I like how you phrase that, Eric. I had a client today who had developed a statistical model in r and did the model documentation in Microsoft Word. Oh, boy. And when we went to do some counts, just simple counts of the data, in the data that they provided us, that they used to build the model. Those counts didn't match what was in the table in the word document, and I asked them if they had ever heard of our markdown or quarto, and they said no. And a single tear fell from my fell from my eye. So Wow. We still have apparently, we still have work to do or this person lives under a rock. But, you know, I think this, article right here is is not necessarily about small changes that we're making. These are about making significant changes that probably need to be made in a number of places where it makes sense to use a programmatic approach. Although, I would say in the small case situation, I love Versus codes, search, find, replace all, which probably now exists in Positron as well. Very useful tool. Sure does. Seen that, and you need to replace that all week, baby.

If you need to find an instance of something across your whole entire repository and replace all instances of that with something else, it's a pretty sweet tool to check out. I was definitely intrigued by by the tinker package here that I believe Mel Solomon, kind of dreamed up as it stated in the blog post for going from markdown to XML via XML two. Eric, I'm happy to be the good cop to your bad cop with respect to our feelings around XML. I'm here for it. I've been doing a ton of it lately. This is all stems back to that package I was working on over the weekend where it's a rest API that sends back XBRL of x b r l file. So I had never heard of before. I guess it's sort of like Excel, but for the finance world, you needed some sort of an x b r l reader, and I was not about to do that. I just want it to end up in a data frame that we can do our cool r stuff with.

So fortunately, I was able to convert that XBRL to XML and then parse that into a beautiful data frame. So I, at first, was quite intimidated. And then once I figured out some of the wizardry that the XML two package can do for us, that with a little bit of purr, made it fairly straightforward to get that XML data into a nice data frame. So initially intimidated, I'm here to tell you, Eric, it's not that scary.

[00:20:25] Eric Nantz:

Okay. You I feel safer now. Okay. That'll be my next journey.

[00:20:30] Mike Thomas:

I did think that it was kind of important to note, especially as you go the other direction of trying to, you know, do this sort of round trip where we're going from maybe markdown to some other format like XML and then back to markdown. There's a whole section here called the impossibility of a perfect round trip. And, the author gives the authors give the example of using that tinker package to go maybe from XML to markdown where maybe list items in your markdown that were previously represented with an asterisk, would then be represented with a hyphen instead.

And I think asterisk is kind of old school anyways for list items. Eric, I I'm not sure if you use asterisks or hyphens in our show notes here. I have always used asterisks, but today, I have switched to hyphens. I'm I'm taking a stand. But I I just We have a full mix in our show notes, unfortunately. Yes. 100%. But I this this is all to say that there are are probably some gotchas when you are doing this this sort of round trip to go from some different representation back to markdown in situations particularly where markdown may have multiple ways to do the same thing. Right?

The author of that package that's going to go that direction for you will have to make a choice about which to use. So I I thought that that was interesting. And the thing I really love about this blog post is it's it's not just a showcase of a single tool. It really is giving us this whole entire world of possible tooling that you could use to accomplish and and solve these different types of use cases that we're talking about here and weighing their pros and cons and benefits, which I think think is extremely helpful because I'm sure that everyone in this situation has a a different sort of nuanced use case that they're trying to solve. And one tool may work better than another, or they may have more comfort over, you know, HTML versus XML representations of their mark markdown, information. So that may be helpful here to know the different possibilities that are out there based upon your preferences.



[00:22:39] Eric Nantz:

Yeah. And I do think, certainly, on the parsing side of markdown, that's becoming even more important to me lately. As I think about, you know, another, you know, like, every conference this year, a big theme is in, you know, building custom AI tools on top of things. But we have internal markdown based documents at the day job. I'd love to be able to parse and ingest very quickly so that I can use that for a multiple, you know, sets of purposes. Maybe that feeds into rag down the road or whatnot, and there may be cases where I have to get lower level of the content versus just pointing it to a directory or markdown file. So I'm definitely intrigued by the by the different possibilities of what we can do here, and there is, like I said, a lot of choice involved, and there may not be the perfect way, but I think with the combination of tooling here, you can get you can get quite far.

And markdown is not stopping anytime soon. I'm who knows? Maybe by the time I'm, like, 80 years or something, markdown will still be around. At least I hope so. You know, Mike, learning even new programming languages already can be difficult enough, especially depending on what background you have. Heck, learning new languages in and of itself in this world can be quite difficult as well. I can, speak from experience because I have been slowly trying to learn Mandarin, Chinese, because my my wife is from China. So we've been you know, I can say the basic phrases, but as a English native speaker, oh, goodness. It is just a different world, and it just makes you realize how illogical the English language is compared to other languages that do have a lot more structure to it. That's just just my opinion. You know, hot takeaway.

Well, we're not the only I'm not the only one learning a new language, albeit I'm not learning that well, but, our next highlight here comes to us from the aforementioned curator of the issue, Jonathan Caro, because he is on a journey to learn Japanese as a as another language. And along the way, he has decided to take matters in his own hands and not just create an r package to help with some of the written side of the language, but he actually vibe coded it too along the way. Well, so this is always fascinating to see how far people can go here. So let's learn about John's journey here.

So as I mentioned, he's been learning Japanese. I I guess his daughter is now, you know, learning has learned this in high school, so he wanted to tag along for the ride. And there are some, of course, external services or resources that help with language such as Duolingo, which I think has been good in his opinion. Another one that he was recommended to try is called one economy. And that again, they all have slightly different takes on how to use repetition and how they link words together. In practice, they got you know, they gamify things a little bit. You got a leaderboard to level up on your on your tiers of skill set.

And speaking and listening to it is one thing. It's a whole different ballgame when it comes to writing these type of languages because it is very easy for two, you know, characters. And this is what he calls a logo graphic based, which is not so much based on the typical alphabet like we have in English and other languages. They look like drawings almost. They're they're they're, you know, hand strokes in different symbols, and just one minor difference can mean a huge difference in how that that word associated with that character is spoken.

So I can definitely sympathize with that. I cannot write Chinese word for lick unless I get a lot of help. But, Jonathan Jonathan has definitely been on that same train with the Japanese language as well. So he thought, boy, it would be nice just to kind of sympathize some of this information together. And he looked in the, in the recent, you know, resources online, and he learned about this technique kind of, by a, a person named Alex Chan, who had a post about storing language vocabulary as an actual graph that kind of links to get almost like a knowledge graph of sorts, but you link together these certain characters that have a similar component and may look similar.

And you can do that for both chine for Chinese as the original author did. He wanted to try that for Japanese. So how does he go about building it? So, you know, obviously, you could just throw your LOM at it and just every time you need it, you just ask the LOM to build it, but he wanted to go deeper into this. He was hoping there might be a way to make a package out of this and not just turn to the LOM for one off request. No r package existing that does this. He did stumble upon, an older, resource from seven years ago that tapped into the API of the Wanna Connie service.

So that's interesting. So he was able to, you know, take the inspiration from that and then trying to assemble the data that would go into building these kind of custom graphs of the language characters. So he got an API key, went to the API docs, and was able to use again the hitter two package, which I use quite a bit for my web API needs these days, and was able to grab the endpoint as a big old set of JSON blobs, which typically is what we deal with with API data. And for that, some metadata was associated with it, and he's got an example in in the post of what that that code looked like, a good hybrid of of hitter as well as, per mapping and all that.

And you can you can, you know, be able to get that in a nice tidy data frame. Of course, he could have stopped there, but no no no. He wants to go further on this. He's been watching the space on, you know, tools like claw code and others. He even saw this interesting video about how it was linked to Obsidian, which is a very popular open source note taking software in markdown. There's markdown again. And it was able to ingest the markdown that was in these Obsidian notebooks and do some really interesting things with it.

So he thought, okay, this is interesting. What if I get inspiration from that and start the vibe code a package with cloud code and to be able to just see how far he could take building this new package to, again, generate these custom graphs of the language symbols. And this is where here we go. Right? So he booted it up and started to think of a plan of what to accomplish here and what it needed to do to build the package. First of which, you know, query that, that service API and then figure out which function to use, to start assembling this and then to write the documentation on its own, as well as the unit tests on its own.

And then he just kind of watched for a bit. I've seen these in action from other people. Just kind of starts with a checklist of going through things, and it just, kind of goes one by one. Some steps take longer than others, and it it it can be can be interesting to watch. But he was, using, I believe, something called whisper to accomplish some of this too. I haven't looked too carefully, but, once it was done, it built the package with modern approaches to the scraping with header two, as I mentioned earlier, even ran dev tools check on its own, and it mocks some tests using HTTP test. So that's pretty nice. That's a pretty modern way of doing it.

And then he would check check it around. Sounds like it wasn't too bad. And then he would instructed to do various, you know, commit messages, and it wanted to add itself as a coauthor. Apparently, it's, like, coauthor by Claude. That was pretty interesting. But but that's a good thing. Right? As long as you wanna be transparent on what it's actually doing here. And then once he had the package, he gave us some of that JSON output. Seems like it worked pretty well, but there were some issues along the way.

And he had to tweak things a little bit to not just get the IDs of these characters strung, but also the actual symbol itself, from this. And so he was able to interrupt claw code for a bit, have it change course a little bit, and then he was able to get get the actual symbols out for various ways. And once it all wrapped up, had a package over a 133 ish tests. That's pretty impressive. While I was ingesting the API, and then he's got the package on GitHub, it's all right there linked in the blog post. And in fact, he even goes a step further, and I believe he's also put together a Shiny app that puts as a front end to this. So he's got that link too as well.

Again, lots of additional Vibe coding here to get get all that squared away. But in the end, this is one of the best ways of learning this. Right? Is to learn by by doing. Like him, I would I would the caveats I resonate with at the end of this post are a, certainly, you may wanna think about this approach being used in very high risk situations. He jokes that you probably don't wanna do this to connect to your banking app or anything to do financial stuff. But and, and then the other thing to note is that, you know, things like Cloud Code don't come over free lunch. Right? You're gonna have to pay some fee for the back and forth with the AI service.

So you gotta, you know, keep that in mind. Hopefully, it's not too extensive, but you never know. You might rack up $20 or perhaps more depending how far you take it. So just be just be watchful of that. And and, again, he is definitely have the mindset that this is a good tool for assisting your development. I'm not losing my job anytime soon over an agentic agent, I think, but I definitely think it can help and speed up your process of going back and forth with a different dev task of a package development.

And he's he's not pretending this is gonna be used, you know, a lot from this package itself. It does what he wants it to do, but this is a learning opportunity for him. And, yeah, I've definitely seen a lot of people start to use cloud code for purposes like this and for a real novel need that John had. Definitely got him there. And he probably could have gotten there himself, but it probably would have taken more time. So, certainly, there are benefits in trade offs or how much time you wanna invest versus the cost of using this and hopefully not having to refactor a lot of code after a Vibe coding session.

But it is a sign of the times. Right? So happy to see John learning out loud here, and hopefully, it's supercharging his, Japanese learning journey here.

[00:34:20] Mike Thomas:

This is really interesting, and I think this whole blog post for anybody that's trying to, I guess, weigh how much they should care about this whole vibe coating thing or perhaps how they could, should, or shouldn't incorporate AI in large language models into their software development workflows. I think this is just a a fantastic end to end blog post that's very transparent in what worked well, what what didn't work well, and and sort of, you know, why it it worked the way that it did. So, Eric, your journey with Mandarin and its differences with the English language, resonates very heavily with me with a a three year old who we're working on reading and writing with. It's it's like you teach them one rule in English, and the next word you run into breaks that rule there.

There's so many edge cases, Mike. Yep. Yep. Which is very frustrating, and I guess we take it for granted. But on on some of the busy days at the day job, I'm sure you, Eric, feel like I'm over here doing Vibe podcasting. But I did wanna let the listeners, know that they should not worry. This this podcast is not Google's notebook l m, at least not yet. I No. Was taking a look at a Python package, that wraps an API, and I was trying to make a similar R package over the weekend that wraps the same API. And the developer who I'm very, you know, impressed with, the initial version of the Python package that he he made, the the API services recently updated its its protocol.

So he had to go in and essentially refactor the whole package. And Claude I could see in the commits that Claude did the whole thing pretty much, you know, wrote the test, refactored the test when things weren't working. And I thought it was incredible. And I and I I think to Jonathan's point in this blog post, similar to him, like, it works. Is it the best, most concise, you know, way to write the code to do what it's trying to accomplish? I'm not sure. That API endpoint I'm talking about provided data in a couple of different possible formats depending on your your request.

One was, semicolon delimited, and then the other option was XML. And it was using polars and could have just specified, like, a read delim with a semicolon, but Claude decided to ask for the XML and write, like, 50 or a 100 lines of code to parse that XML into a data frame as opposed to, you know, when I was doing it in our it was it was two lines of code, to be able to do that. So I think that's just an example, and I I think that plays out in a lot of what I've seen in terms of me just asking for individual responses from, you know, chat g p t or Claude and my my coding day to day journey is that it will give me code that tends to be a lot more verbose than what I would normally write.

And I think, again, sort of if you don't care about the the the part in the middle, the code that's actually, you know, executing the thing and you only care about the end end goal, maybe a lot of times, you'll be okay. Just kind of blindly, you know, five coding. And, again, it all comes down to risk as you mentioned, Eric, and as Jonathan mentions here. So weigh that into your decision, but these tools are also getting getting better day in and day out. One of the things that I also struggle with sort of on this topic are leveraging ensuring that I'm leveraging, like, the latest and greatest from the packages that I'm using when these LLMs are trained on, you know, older older data. You know, recently, I was leveraging a function or I was leveraging TidyR's pivot wider, and it was giving me an it's Claude was giving me an argument that no longer existed.

It it had been long since deprecated, and it's asking me to use that. And I looked in the docs, and that argument didn't exist anymore. So I run into a lot of situations like that as well, whereas we're developing software. We're trying to make sure that we're using the latest and greatest as opposed to some of what I think philosophically these LLMs do is just regurgitate the past so that we don't make a lot of progress forward. That's a big probably a big way to to end, this section of of the podcast. But I really long story short, really appreciated Jonathan's fully transparent walk through of his journey on this package.



[00:39:01] Eric Nantz:

Me as well, and I am early in this process too, but I actually had a very, I won't say, stressful situation. But, back to my shiny state presentation. Night before, I had an example demo app, but I just was viewing. I was like, no. I don't like it. I wanted to do something that kind of brought some fun to it, but also an easy example. So, yes, I booted up Positron assistant with the anthropic agent mode, and I basically vibe coded a retro eighties video game looking app for the demonstrations. At 1AM It was awesome. It was this is what I needed because otherwise, I could've spent probably a week doing that knowing my OCD of developing shiny UIs, but it it did the job, man. It did the job. You did that the night before?



[00:39:54] Mike Thomas:

Yes. The night before. Oh my goodness. Go watch Eric's presentation, please. That will blow your mind.

[00:40:01] Eric Nantz:

It was, it it it but it you know, it it did the job. I mean, would it be exactly the way I wrote it? No. But it did good enough, and that's all I need. I just need a good enough. It was just for the slides, just some prerecorded demos of OBS. And once I got that app on, there was none to the next part, but it literally did save my behind when I had that very literally down to the wire change in my demo direction. And that that kinda made me a believer that in the right situation, this can definitely be a big help. But as you said, Mike, cautions abound for certain situations.

But in the end, it can be a nice tool in your toolbox.

[00:40:42] Mike Thomas:

It's a heck of a Claude sales pitch.

[00:40:45] Eric Nantz:

Where's where's the where's the podcast revenue coming in, man? I don't know. And rounding out our highlights today, it it does feel like I'm going back in time on this one. I'm not meaning that in a disparaging way, but we back in graduate school when I was a TA in our stats department, we indeed would be part of the group that worked with our faculty to create dynamically generated quizzes and exams. I still remember making this in the Pearl and Lake Tech code of all things, so let's not get into those tangents.

But this next highlight, this last time I should say, if you're in the situation where you are want needing to generate, you know, dynamically generated, say, statistical based exams, and you're confined to certain, I'll say, older formats, this may be just for you. This actually is a blog post from the r exams project, and I think we may have mentioned them very briefly in previous highlights. Holy smokes. This thing is really comprehensive. If you are in this space of teaching with, with statistical knowledge or whatever else, this is a suite of packages that are literally meant for producing and parsing written exams in many different formats.

One of those being multiple choice format. And, unfortunately, many of these are still being done in paper and when the students are filling it out and then turning it in. And there can be some issues when you scan those results in and some really gnarly issues that this blog post highlights here. They call it quality control for scanned multiple choice exams. And I don't pretend to know all the internals of how the r exams tooling works in the r ecosystem, but the way they've conducted this and the way they have a solution to tackle some of these gnarly problems, it's definitely intriguing to me. So there's a couple demos here. One of which is there's a, you know, an exam that has some pretty typical problems where it's got five single choice answers.

And then somehow when it was scanned, it was rotated upside down and certain things have been marked up a little oddly. And some of these choices actually have multiple check marks in it when there should have been single choice answer. So this the the suite of packages that they have for our exams has a way to scan it and rotate it to the right way and then to literally run a wrapper function called knops fix where it'll give you kind of a visual what needs to be corrected and then be able to enter it in either in the prompt with the real value should be or click through a GUI like interface to to fix all that. So, yeah, it's it's kind of manual effort, but they that their clever trick is that when they import it, they actually make a zip file of the different problems as images. And then once you fix a problem, it kind of dynamically makes new metadata, I believe in JSON format or whatnot.

And then it ingest it pack into the parsed object that this exam or the importing this exam, you know, scan file generates. And then you can unlink kind of the temporary files and get this all cleaned up. Boy, oh, boy. Where was this twenty five years ago when I needed it? But, nonetheless, better late than never. And then their second demo has a lot of problems, not just the the rotating issue, but they've got, you know, many many other corrupted markers where it was supposed to be, again, one choice, many multiple choice, and and some fact some of the scan results are even off the page, so to speak, like really gnarly stuff.

And so the package again lets you fix these dynamically, but then you get a nice little looks like HTML based GUI on this one where it actually thinks there's more questions than there should be, but then you can at least select only the ones that are supposed to be selected, and then it'll write back out the actual amount of questions. And then you can, you know, review all this as you go. It keeps everything that you work on in the space so you can easily, you know, parse this through and with your human eye, so to speak, before you make the final call on fixing this up.

But, man, I can just imagine how much time this pipeline saves for not just creating these exams, but also ingesting them when you're dealing with these, you know, I hate to say legacy, but pretty legacy formats in terms of the way education is these days. But I'm just really impressed with what they've accomplished here. And like I said, if you're in the education space and you wanna start, you know, learning from what they're doing, they got a whole bunch of blog posts about what they're doing with this suite of packages, even some integrations with AI tooling as well to help, along the way. So definitely learn something new that, again, me from many years ago would have been all over this for my exam generation.



[00:46:18] Mike Thomas:

This is absolutely incredible. It does sort of remind me of the, like, memes out there where, say, like, men will do anything but but fix themselves essentially, which is like, you know, we'll do anything, in our, you know, to parse information and allow us to programmatically interact with scanned, exam documents instead of creating non, like, PDF, you know, scanned exams in the first place. But but c'est la vie, that's that's just the way that it goes. One area that I think is really, really interesting and I'd love to dig in further to see exactly how they did it, but it's these functions that sort of pop up this interactive session. Right? Whether that be some sort of HTML, widget or or or whatever it is, some HTML interface that allow you to either take a look at the PDF, maybe interact with it as well. We have a ton of use cases lately where we are using LLMs to parse data out of PDFs to provide, you know, answers standardized answers for downstream reporting or downstream decision making, things like that.

But our users are wanting some sort of audit step in the middle where they can see the section of the PDF that the LLM used to get its answer, and that's an area that we struggle with. And I feel like if I dove deeper into the internals of, the packages that are being used here, it might give me some some big pointers on how to solve that problem. Because I think that that's probably a problem that exists for a lot of folks who are trying to merge the the worlds of large language models and PDFs these days.

So very, very interesting to me. I'm excited to look a little bit further underneath the hood. But if you are in this space at all in education and needing to administer exams and then review them and grade them, and still doing that very manually and looking for tooling that can help. I mean, this is this is gonna be mind blowing if you haven't run into it before.

[00:48:25] Eric Nantz:

Yeah. I think this does just about everything you need in this space. So I I think it's worth worth checking out for sure. And and you know usually I I try to give credit to the authors but this blog post is this a project in general has a lot of people behind it so I have a link to their contact information in in the show notes if you want to get in touch with them but they're also on social media as well. So we'll have a a link to all that. But, yeah, really fantastic effort there. Just like the rest of this is yep. Go ahead. Yeah. It looks like maybe Akeem Zelos is tagged at the bottom of this. Oh, it is. Okay. Good eye. So, Akeem, yeah, fantastic job to you and the rest of the team on this fantastic, fantastic pipeline.

And the rest of the issue is fantastic as well with our weekly, and it never stops even when we were at our at our low for a bit, but it is back up and we're back up and running and luckily not a moment too soon. There are lots of great resources to look at and on top of these highlights that we talked about, we are a bit pressed for time so I will have to, you know, not do our additional fines, but again, you'll see a whole bunch that John's curated here that you can look at your leisure. And, also, we love hearing from you. Again, shout out to all the attendees at Pazit Comp. They've said some nice words about our humble little podcast and wonder when it was coming back. And I said, it was coming back. Trust us. So thank you to all of you that said hi to both Mike and I. It was it was gratifying to to connect with you in person. We always love hearing from all of you. But since we're not a cop anymore, you wanna get in touch with us, there are a few ways of doing that. We have the contact form directly linked in the episode show notes that's still up and running last time I checked.

We also have the project itself that you can help out with poll requests. Everything is@rweekly.org. Click the little GitHub icon in the upper right. You'll get directly to the pull request template so the curator of the next issue can benefit from what you found. And little birdie tells him that might be me next, so I need all the help I can get. But last but not least, you can find us on social media. I am at rpodcast@bsky.social. We'll try to post more often now that I'm getting through some of these crazy August, September conference and presentation stuff that I've been involved with.

You can also find me on Mastodon with at rpodcast@podcastindexonsocial, and I'm on LinkedIn. Just search my name. You're gonna see me there. Mike, where can the listeners find you? You find me on blue sky @mike-thomas.bsky.social.

[00:50:53] Mike Thomas:

You probably won't find me on on LinkedIn if you search my name. But if you search Catchbrook Analytics, k e t c h b r o o k, you can see what I'm up to lately.

[00:51:03] Eric Nantz:

Awesome stuff. And like I said, it felt it feels comfy again, being able to do do this. And hopefully, we can keep back to get back to a regular cadence, but we you and I both have been in the middle of a lot of stuff going on. So we'll we'll keep trudging along. But as always, we thank you so much for listening out there from Revier around the world. We really appreciate it, and we look forward to hopefully being back. We have a new episode of our weekly highlights next week."
"8","issue_2025_w_34_highlights",2025-08-22,44M 20S,"In episode 210 of the R Weekly Highlights podcast: The Positron IDE has officially been released after two years of intense development, and we share what excites us the most. Plus a new tool in your Shiny testing toolbox to bridge the gap between server-side and dynamic updating the user interface, and an entry point to making that URL of your…","[00:00:03] Eric Nantz:

Hello, friends. We are back of episode 210 of the Our Week of Highlights podcast. Yes. It's been the summertime. Our release release schedule has been a bit sporadic, but we're happy to be back this week nonetheless. And if you're new to the show, this is where we talk about the excellent highlights and other resources that are shared on this week's Our Weekly issue at ourweekly.0rg. My name is Eric Nance, and as always, life has never had a dull moment, especially lately. Lots of things happening in the day job and also my external efforts, and I'm actually just coming off of a very successful second annual r pharma gen AI conference that I get to edit the recordings of very soon. Well, lots of interesting talks there. But, yeah, I'm back here at the humble our weekly confines here, and I'm not joined alone. Thank goodness. I got my awesome cohost

[00:00:55] Mike Thomas:

virtually staring at me here, Mike Thomas. Mike, how are you doing today? Doing well, Eric. I know we have a strict policy of no free ads,

[00:01:03] Eric Nantz:

and there's somebody coming into the podcast space. But I did wanna shout out. Pauseit has a new podcast called The Test Set. It's pretty cool. They sure do. Yeah. We'll put a link to that in the show notes, and I've checked out the early episodes. And, yeah, I think they're on the right track and always excited to see more join us in the data science space, not just us by our lonesome here.

[00:01:25] Mike Thomas:

Absolutely.

[00:01:27] Eric Nantz:

Yes. And we got a fun I know we've haven't talked to each other in a bit. We got a lot to catch up on, but we're gonna use the highlights to do that as usual. And this week, our curator is the esteemed Colin Fay, author of the one of our favorite art packages of all time, Golem, amongst many of our great shiny initiatives. And as always, he had tremendous help from our fellow Aruku team members and contributors like all of you around the world with your poll requests and other great suggestions. And we lead off with more of a announcement of sorts, but one that's been almost two years in the making, but it is now official.

We have mentioned before in previous highlights that Posit, of course, the company behind the very famous RStudio IDE, has been cooking in the oven, so to speak, a new IDE called Positron. And as of about a week ago, it is now out of beta. It is officially released. The version one is out, and I have been an early adopter of Positron. I'll share my thoughts on that as we as we talk here. But if you haven't seen the beta and this is your first time hearing about Positron, what is it really about? Well, as you recall, perhaps from a couple years ago, maybe longer, time flies of course, our studio had rebranded themselves to be called Posit for many reasons, but one of which in particular was to emphasize that they are a multilingual data science, you know, tool, you know, maker in in this space.

And they've been diving into the world of Python quite a bit, and they had realized that RStudio itself, the ID, while it's been excellent for R users, certainly for other languages, it may have left a bit to be desired even with things like reticulate and whatnot. So part of their mission to become more multilingual was to reimagine the IDE, hence Positron was born. And what I can tell you is that it is going to look a lot different. If you're if you're used to our studio, it's gonna take a little bit of getting used to. However, there are definitely things that the team has done behind Positron to try and make that transition and custom customization ability a bit easier, especially if you're new to it for the first time.

And for those that aren't aware, Positron itself, while our studio had its own custom code base that Posit developed on, you know, standing on the shoulders of open source frameworks, of course, but really, it was their own their own making. Positron certainly is a highly engineered product, but we need to be clear here that it is actually a fork of another project called Code OSS, which is actually the underpinnings for Visual Studio Code. And we are seeing, I'd kid you not, Mike, a lot of companies and organizations forking the Versus Code code base and the code OSS, I should say, and putting their own opinionated spin on an IDE for their real particular domains.

Some of them are great. Some of them not so great, but I can tell that there's been a lot of time and effort put into the care and the development of Positron to make it not seem like literally just a reskin visual studio code. It definitely has a data science flavor. That's where you start to see things that are familiar to the RStudio experience, such as a data viewer, variables pane, being able to bring up help pages of, like, package functions and whatnot. But there are also many improvements over what you may have experienced in our studio.

We don't have time to get into all of them today, but I really like the fact that I can switch our versions with a little drop down. And there each of them are an individual process independent of the IDE's process. Whereas for our studio, it was all one major process, and if one thing went wrong, bye bye. You might see that little dialogue with a little bomb on there saying your session was aborted. Good luck to you. In not in not so many words. Here, if you do have an issue with pa of the r process or the Python process for that matter, you can just restart that console. It's not gonna break everything else. That to me is a huge quality of life feature right off the bat and to be able to go again back and forth between, say, running some code in r, maybe you have to switch to a Python script if you have a a team member or a project that's multilingual. You can go right there.

Use whatever flavor of notebook you like, whether it's Jupyter or, you know, I'm certainly on the chordal train. I've been doing a lot of chordal development in Positron. That's been a first class experience. Many of our nice enhancements associated with connecting the databases and also this is where you start to get into what the bet that they're hit hitching on so to speak of being on this fork of Code o s s, you get the ability to install extensions from the open VSX extension library. This is somewhat akin to if you're on a web browser such as a Chrome based browser, you have the extension, you know, ecosystem. You can make your browser do all sorts of things. Right? Well, positron is no different in that sense. You've got all these extensions available to you. So there's a lot that have been recommended out there in the community.

In fact, I remember it was Gary Gade and Bowie, Engineer Reposit that has kind of his own wrapper extension that does a lot of extensions that he finds very helpful in his development. So you see all sorts of these things out there, and it's just a click away really in the extension menu. So I really, really like that like that feature. So there are lots of other nuggets here, but there's also a nugget that I do wanna emphasize here. Not so much on the technical side, but on kind of the the life cycle of Positron going forward.

It is open source. The license, however, might be a bit different than what you're used to. This is called the elastic two point o source license. If this is your first time hearing it, I don't blame you because I didn't hear about this very much until recently as well. I won't try to pretend to be a a legal expert here. I mean, I don't don't go to me for legal advice, but I have a feeling this was chosen because there may have been situations where the Rstudio IDE, which was released on our more permissive license, was used in situations that maybe weren't quite as nice to posit or RStudio back then.

And I think they're trying to, if I had to guess, put a little bit of reins around just how positron is embedded in other services. That could be my speculation hat, but I think this is a reactionary measure to some stuff that has happened with the use of of our studio in other, I'll call it platforms. I am kind of biting my tongue here because I am recording this, but I have heard as much from some very trusted people in this space. So that caught my attention, but nonetheless, for an end user out there, a data scientist like yourself listening, no big deal. It's still open source. You can use that however many machines you like in your daily work. I've got it installed on both my work machine and my dev machine here. It's been working great, and I think the future is bright. And I dare say the timing of this probably is no coincidence that we're about a month away from Posikov, and I'm sure this was a a big feather to to shout out over there. But, yeah, Mike, exciting times. Positron's out of beta. What do you think?



[00:09:41] Mike Thomas:

Yeah. I'm very excited. And I think that whole elastic you know, the more that I'm reading up on it, that elastic license, mostly just means and I think this will be fine for pretty much everyone out there, that you can't, like, take positron and make it your own company's offering, like, turn it into some particular SaaS offering. And you're correct, Eric. I think we saw some instances of companies doing things like that, with the RStudio ID. So to me, I think it makes a lot of sense. You know, coming for soon for the the Posit workbench folks, which is exciting, exciting, you'll see Positron as a generally available IDE type. I'm imagining it was in pre preview for a little while, but for everybody out there, you'll be able to start using it, as your IDE of choice in Posit workbench if you like.

And one thing that I I did see in the article that I also liked was a sentence that said, we are committed to maintaining and updating our studio. So not just maintaining,

[00:10:41] Eric Nantz:

but also updating. That was a key find there. Yeah. It could have been easier to say maintain, keep the lights on, but they wanna keep improving it. Right.

[00:10:49] Mike Thomas:

So it's it's not like RStudio is getting deprecated. If you choose to continue using RStudio because that works best for you, then by all means, go ahead and do that. The positron assistant is very cool. It sounds like that's in preview. And the only thing I wonder there is if you can sort of bring your own LLM, like a local open weights model for privacy purposes, or if you have to use, like, a third party API. I'm assuming most of the docs point to the third party APIs. Yeah. As of now, I believe for co completion,

[00:11:21] Eric Nantz:

it's GitHub Copilot. And then for the agentic, you know, chat like interface, right now, it's Claude Anthropic. But I have heard from, you know, Joe Chang himself that they are looking at adding other providers, including bring your own model down the road.

[00:11:39] Mike Thomas:

Very cool. If, if Joe or anyone else is listening, you should check out the continue extension in Versus Code. It's it's really cool. It allows you to, you know, choose one of those providers or bring your own, local something that I'm actually giving a internal talk to our team today on how to configure that. Nice. Pretty exciting for privacy purposes, in some cases. Right? I think a lot of the times when I'm authoring code, it's not a big deal if if it's going back and forth to Claude or something like that. But if I'm asking it to reference a and use this context, like a proprietary document that, a client gave to us that has, you know, financial information or something like that on it, then I can't I can't do that. So I have would have to use a a local choice. And instead of having to, you know, spin up some sort of separate Ollama, it'd be nice to have that all sort of in that same UX.

So, just just rambling now. The continue extension allows you to do that, so maybe they can borrow some of that technology for Positron Assistant. And I think, you know, not to I think at, like, a higher level, just having everything ready to go for both R and Python is awesome. And you can't take that for granted. Like, that's not easy to do, to be able to just have in this particular IDE application just ready to go whether you wanna code up R or Python. There's for anybody that's either tried to install R or Python and and get packages and environments up and running and things like that, it's non trivial, especially if you're using the language that you're less familiar with. Right? If you're a Python user trying to start in our project or vice versa. And it may save you from also managing, like, a bunch of Versus Code extensions to have to do that.



[00:13:29] Eric Nantz:

If you're I can speak from experience trying to manage. You you recall, and others may be listening, I was an early adopter of the dev container setup and Versus Code and using the r extension a lot. And and they got me pretty far, but then when, like like you said, I had to go dive into either some Python coding or stuff, and it's like, oh my goodness. That is a lot to manage. So having, like, the batteries included, so to speak, as long as you've got R and Python available on your system, it's gonna hold your hand much easier than if you have to bring your own kind of dev setup with with Versus Code. So I do appreciate that quite a bit.



[00:14:07] Mike Thomas:

Absolutely. And I think, you know, one extension of that, no pun intended, is the database connection pane. Like, if I was to try to do that from Versus Code, I'd have to have a particular extension depending on what type of database I was connecting to. And I'm imagining in Positron, if you're using DBI or the pool package to manage your database connections, it doesn't matter if it's Postgres or a regular SQL Server or, you know, some some AWS, offering. If If you're, you know, leveraging those packages, then you should have this nice connection pane that will allow you to actually explore your database objects and and see that right in front of you, which is super cool. And then if you do wanna need to extend Positron, you can use any of those open VSX extensions, which is awesome. And and the last thing I think that's really helpful that speaks to my earlier point about having everything kind of ready to go for both r and Python is that there's, I think, ready made templates for UV and RN. And I think UV is kind of the leading virtual environment, manager for Python, and obviously RN, I think is still the big one on the R side. So I think that's possible, you know, to set up mostly through the UI instead of having to do, you know, pretty tricky, you know, command line, terminal type of stuff to get those environments up and running, which has been tricky for me in the past. And I know a lot of folks virtual environments are not fun. So anything that they can do to make that easier is awesome. So I'm very excited.



[00:15:34] Eric Nantz:

Yes. This boy, has that been a pain in my existence in years past. And one thing I've mentioned it before, but, boy, I I still have to give kudos for this. The very fact that I can just use a system wide install a positron on my setup, but yet have Nick's behind the scenes manage the R and Python installation, and it just works. Holy smokes. That is a game changer for me. It is very similar to what I felt with the dev container set up years ago. But now with Nyx, my goodness. It's the possibilities are almost endless. So I am definitely using that. And, I remember Bruno, once he shared that vignette on Rick's, oh, yeah. Positron. You just have to you have two ways of doing it. And I took the easy route, and boy, no easy route just works. So another another win for for for Positron on that sense. Because of RStudio, you have to actually install RStudio in that same Nix environment. That gets kinda tricky, especially if you're doing any WSL stuff like you have to do on Windows. So the fact that with this, that's all a thing of the past. Thanks to Positron and extensions. Yeah. That's a chef's kiss right there.



[00:16:46] Mike Thomas:

You'll be happy to know that in my slide on environment management for my Positconf talk this year, I discussed Posit workbench, dev containers, and nicks. I would never forget you.

[00:16:57] Eric Nantz:

Oh, my my heart flutters now, Mike. You know how to speak to me. Oh, I needed that this week. Well, since our esteemed, curator Colin Fay is at the helm here, it's no surprise that our next two highlights are definitely focused on the shiny space, and we're always happy to talk about that and some of the great, developments that are happening in the shiny ecosystem. And for this next highlight here, it is a new perspective and a new tool that is at our fingertips now for what can be a very difficult task of testing your Shiny app. We have covered in other highlights the the ecosystem around what Shiny has built in to help you test what you might call the server side logic of certain pieces of your app, including modules, which can get you pretty far.

And where it does not get you as far potentially is if you have a shiny server side process and say a module or elsewhere where it's not just the ReactOS from a classical set your input and, you know, the ReactOS downstream, whether it's a reactive dataset or a reactive plot or what have you, It just works to bring all that together in in the the way you do shiny testing in basic shiny itself would test that, you have a test server function that can help you along the way. What happens though if your logic also has a lot of interesting UI updates dynamically on server side via the update family of functions, or maybe you wanna update that value in the drop down, that value in the date picker and whatnot. It wasn't so trivial to accomplish that with kind of this test server framework. Well, there is a new package to address this very need, and this package is called Shiny Testers and has been authored by Ashway Baldry, who is a senior data scientist at Ascent over in The UK.

And and I'm I wanna say right off the bat, Ashway is not new to the shiny space. He has made a few, very novel contributions, and we'll have a link to his, kind of portfolio open source page in the show notes. There's a lot of great finds there that I'm gonna be looking at. In particular, he made a a module called designer It was like a an attempt at wireframing Shiny apps, which is pretty intriguing back in the day. Back to the the task at hand here, he has written Shiny testers to be kind of this one call kind of function in your test server logic that if you have any part of your server side process that does this updating of an input, which then, of course, would feed into additional reactives, you can incorporate all this with literally one function call called use Shiny Testers in the body of your test that function or test that test, I should say, that's leveraging kind of what Shiny does for the test server paradigm.

And if you have any in your UI of the app, any of these update, you know, any inputs that need updating in your test, you have, like, say, an update date input, it's just gonna magically work the way you would expect about you writing custom code. And if you want to attempt this before, that's where I also have a link to the the github repository of the package itself in particular the package documentation. In the getting started vignette in particular which we'll link to, he shows what that custom code would look like using a combination of our lang kind of like dynamic expression, you know, inserting that this package is gonna let you kinda have free of charge with just that use Shiny Tester. So in the example on the blog post here, he's got an observer, an observe event that's based on a button press that's gonna update a date input with a new value, maybe a new minimum value.

And then in the body of the test server you've got session dollar sign set inputs a way to, in essence, change the input values based on that. And then he also sets the input for the, I guess, the action button that's being pressed. And then once that happens, then he's able to simply check then at that point, does that input value equal what it should equal after that observe event? I admit I I will have to sit down with this a little bit, but I definitely do have some apps at my in my current, day job projects where I am updating observe updating via observe events certain inputs based on other choices that the user has made in the app or other downstream reactives that have happened in the processing.

So there's a lot going on under the hood here, but I think the key takeaway is you as the end user, if you wanna stick with kind of this test server paradigm of testing kind of the the behind the scenes of your shiny UI, but yet still wanna be able to dynamically update these inputs. It does sound like ShinyTesters is a way to scratch that particular need and to have a pretty elegant way of doing it. So it is still in GitHub. I believe he's playing the release of the CRAN in the near future, but certainly I am more than happy to have another, tool in the testing space in my toolbox that can be less of a footprint, so to speak, than the full blown, say, say, shiny test two suite of the whole dynamic, you know, Chrome process that's manipulating inputs. This is kinda getting you there almost all the way. So Mike is smiling here. He's got a lot to say here. Let's hear it.



[00:23:18] Mike Thomas:

No. I don't wanna talk about headless Chrome right now. I fought headless Chrome last night. Oh, okay. Five hours. Sorry for the bad bad memories there. No. No. No. It's okay. I did figure If anybody is trying to use types and Mermaid JS in a dev container, anybody sits at the intersection of those three things. I'm I'm here to help. I figured it out. No doc. You'll find a way to contact Mike in the show notes because there's a lot of you out there, I'm sure. Yes. Mermaid JS does not like, Mermaid JS in quarto does not like Docker. And they put that in the documents, in the the quarto docs. But, anyways, I digress. This sounds like, you know, as you mentioned, Eric, a way to do some Shiny testing, particularly across these update family functions without needing, you know, the the headless Chrome and and everything that goes along with that for Shiny test two. And I think that that's really powerful, potentially.

You know, one of the interesting things in the example implementation is there's kind of an entire server function stuck in the test that code. And I'm imagining that for the purposes of, like, reproducibility and as the app changes, you're not going to want that to be the case. You're going to actually want that function defined elsewhere, you know, in your R directory, and then be able to leverage it within your test that suite here, and probably still have your, shiny, you know, colon colon test server function in the second half of this this test that function. And I know this is probably difficult to articulate on a podcast, but it's it's fairly straight forward, this example implementation. If you take a look at the the blog post and sort of the first snippet of code example that's been put in there. And very, very easy to to do things like within tests that, you know, expect identical between, a particular hard coded date that you might have and the update that you're trying to push.

And, I think it's very obvious, very straightforward in its implementation. And we use, at Catchbook, the update family of functions constantly all the time, and I'm sure you do, Eric. If you are out there using a lot of render UI, you're making your app slower and clunkier than it needs to be. That's all I'll say. Eric and I could do a whole hour long podcast

[00:25:49] Eric Nantz:

about why that's the case and why you shouldn't do that. We have definitely touched on in our previous workshops too. I remember very well. Yep.

[00:25:56] Mike Thomas:

People are like, why? It's it's it's using, you know, overhead that doesn't need to be used, when this update family of functions exists. And a lot of people that I run into don't even know that that update family of functions exists, and it's it's life changing. It certainly was for me when I figured it out. So having sort of this testing format against those update functions, again, that we're using constantly, I think is is really helpful. Excited to see how this package progresses and ideally makes it to CRAN in the near future.



[00:26:27] Eric Nantz:

Yes. I'll be watching this space quite a bit, and I do have a, yeah, a couple of day time or day job, I should say, projects where, very, you know, complex shiny app that we're developing. We're We're getting into that testing phase. And no, I was not a good boy, Mike. I didn't build my test right away. I do have to bolt them on, but this gives me gives me kind of a way out of the, you know, I I hate saying it because Barish Swirk is a good friend of mine, but shiny test two is is sometimes a hammer that doesn't need to be the hammer if you can accomplish a lot of it. In a, if you're doing what we Mike and I would call the quote, unquote right way of having your business logic and fit for purpose functions that you can do and test that out alone, go for it. That's awesome. Maybe even a dedicated package, even better. But then when you do add that in between state of the app is doing something, it's probably calling a lot of functions you've built outside of, you know, Reactors or whatnot, but you still need some way to test kinda that cohesive experience with the dynamic UI updating that you're doing in the more logical and and more performant way with the update family. I think this is this is massive. So it flew under the radar, certainly. That's why we love doing our weekly. I would not have known about this probably otherwise, until maybe a random crayon, you know, message. But this is I'm gonna be playing with this probably in the coming weeks and, yeah, not a moment too soon.

And rounding out our highlights today, we've got another awesome shiny post about a feature that definitely does not get a lot of attention, and I definitely share, you know, the the same, sentiments of the author here of trying to get the word out there in many different ways. But our last highlight comes from Mauricio Pacho Vargas, and he is definitely not a stranger to the r community, and and he's been around quite a bit with many contributions actually in the c plus plus space amongst many other improvements. But he talks about Shiny in his, blog post here in the highlights about a a very neat trick that you can do to help ingest kinda new parameter values in a typical Shiny application via parameters to the URL.

Now what this might mean if you're thinking about what do you mean parameters in URL, I just go to a web page and it just works. Right? Sometimes depending on the page you go to, especially if it's more of a classically designed page, you might wanna try looking at what happens after the .com or this, you know, the the the TLD domain at the end. You might see a slash and then you might see what looks kinda like key value pairs or you might say some value or some name of a variable equals some, like, number or some really random string, and you might see a few of those separated by an ampersand.

Those are what we mean by URL parameters. And the default invocation of Shiny, you don't necessarily get that, you know, right away. You have to kind of build that in. How do you build that in you ask? And that's where the blog post comes in with one way of doing it. And, and, Pacho demonstrates this via a golem, you know, app, which again, you know, big props on that. We are all big fans of golem here as well as our curator, of course. And this is tapping into a feature that I think deserves more attention. I have more to say about that shortly of bookmarkable state, but there are different flavors of doing that. So in the flavor of this example here, we got a very simple, server app, which I think has only a a couple inputs, for the UI.

And then in order to register an app for a bookmarkable state here with the URL parameters, you first have to figure out how do you trigger this. And there are different ways of triggering, like, the act of generating these parameters or these values. In here, he's just using a simple observe block that is gonna change after every input change. And now that might be too greedy to some, but it it definitely works here in this, tutorial like example, converting the input object to a list via the reactive values to list, and then making sure he excludes an input that he doesn't care about. So you can give it the input ID of a input you don't want in this URL parameter and with the set bookmark exclude function. And then when you do that, the next call is just simply session dollar sign do bookmark. That's gonna help register these parameters.

And then the on bookmarked function, that's kind of a callback function of sorts. He is then I dynamically updating the URL that's gonna have these URL parameters baked in after that operation. And then in the UI UI of the of the app, it's very straightforward. It looks like a typical Shiny app with a sidebar layout, three select inputs, one main panel, and then a slider input. And then the key part is that you have to wrap this It's a the typical Shiny app UI function. You have to make it a function with the request parameter as its default, you know, you know, per one parameter, but then the rest of it is simply a tag list of the rest of the rest of the elements.

And with that, just some more Golan magic there behind the scenes, and then you can run the app. And then when you run it, and he has an example at the end of the post here, the URL has got kinda that local host URL, but then there's a slash question mark underscore inputs and then we start to see what the actual values are where it's maybe like for the species. It's the when the penguins say you set the idyllis, you know, species. The island, you can Bisco, I believe. But you'll notice that if the text has some kind of special character in it, in the URL parameters they have to do this fancy percent number to encode a a special character, I believe. Something like that. I'm not an expert on that side of it.

And that can get you really far. That can get you really far for these simpler apps. However, I will mention you might hit your limits on some of this, especially if your app has a lot of inputs that you want as a parameter. Because even though I have seen some really long URL strings in some of the apps I visit in the day to day, whether shiny or not, There actually is a limit to how long that address string can be, and you might find out the hard way if you use this URL parameter method for an app that has more than maybe a couple of modules of a lot of inputs inside.

So, there is another path forward, which I'll get to shortly. But first, Mike, what do you think about these, techniques here?

[00:33:49] Mike Thomas:

I think this is really helpful introduction for folks who are looking to get into bookmarking, particularly if you are using, Gollum. And, specifically, you know, there's I'll be honest. I haven't done a lot of bookmarking. I I should have, but we just haven't for whatever reason. And I think most of these bookmark, functions are sort of Shiny out of the box functions like set bookmark exclude, the session dollar sign do bookmark, and then the the on bookmark and update query string. Are those all pretty much native to to Shiny? Exactly right. Yep.

So we're not using anything, you know, any crazy, additional packages. This is all sort of native technology, which is great. You know, one thing that was interesting to me as well, especially as you talk about, having query strings that are or URLs, I should say, that get really long really quickly is this function called set bookmark exclude. So you're able to actually take the names of one of your input IDs that you have and just use that within double quotes within the set bookmark exclude, function here and ensure that that particular input widget, whatever it may be, is not used in the bookmarking function at all. So I would imagine if you sort of send this URL to somebody else who wants to access the app, you know, whatever you have that widget set to will not be sort of captured for them when they go to, you know, enter that URL in their own browser, which I think is is very helpful in situations where you have a ton of inputs but you only care about sharing the state of one particular input in your app. So that was something that I learned that I think is really useful. If you're coming from, you know, the Gollum world and you take a look at the code here that Pacha has in the blog post, I don't think it's very intimidating at all. One great catch, Eric, that I didn't catch the first time around, as you mentioned, is in the app UI code, which is a function sort of like everything in Gollum, it needs to take a parameter now called request.

So I think that's, you know, something that's easy to miss in this blog post that I think is critical to making this all work. But as you mentioned, Eric, these URLs can get real long real quick if you have multiple modules, multiple inputs, things like that. You have any, you know, additional reading material that we could brush up on on this topic?

[00:36:18] Eric Nantz:

I dare say so. I have been in this space for a bit, and I have fought these battles the hard way a lot of times. And for those that aren't aware, Shiny bookmarkable state, what we have here is one flavor of it, the URL flavor of it. There is another flavor of it that is definitely more akin to more complex applications overbuilt with golem or not, and that is called server side bookmarking. That is where instead of the URL being updated that you could copy paste and send it to your your colleague or your customer, for replicating a certain situation of the of the app UI, you actually would save the state of the app via the inputs and any reactives that you think are important enough. There's a way to do that as well. But it gets saved on the server process, which is either a local directory on your file system if you're running this in a vanilla, like, posit workbench, positatron, or r studio session on your computer.

But if you are the using this on, say, a posit connect server or a shiny server pro back in the day, these server side bookmarkable state files would be saved in some kind of cryptic area on the server that only an admin could have access to. That might be fine for or a lot of use cases. But for me, it wasn't quite because I wanted my app users to be able to access these objects that are saved with bookmarkable say, especially some of these reactives that I saved out and these input values into other situations.

So it all so this is all culminated with what I will be talking about at POSITCONF, my new r package called shiny state. I won't say I have the most creative naming, chops in the world, but nonetheless, this is a a kind of supercharged version of bookmarkable state where I'm gonna put the power of where you store these bookmarks in your hands, Mike, because with shiny state, I'm gonna let you tap into the power of the pins package and let you choose where those bookmarkable state files are sent and open the door for many possibilities that the default bookmarkable state feature doesn't quite allow for, including the ability to share perhaps a bookmarkable session with your teammate, maybe your colleague that's using the same app, and you can, in essence, collaborate on, application experience together.

It won't be quite like a Google Doc thing, but it'll be much closer than you get in the vanilla shiny bookmarkable state. So, which I got joked to many times, this is very much a conference driven development paradigm. I've got the the package on GitHub, and I'm putting my talk together for positconf, but this has been a huge this this, paradigm that I've put in shiny state has been custom code for a very long time in my one app to another kind of copy paste this stuff and you know, Hadley's in my ear at the moment saying if you're doing that three or four times, that's gotta be a function. Well, it was a function, but I kept doing that three or four times in many different apps. So it was like, make it a package in my head. So, yep, that's why shiny state exists. And I'm really excited to see others try it out. Very early days, but I think it shows great promise. And, like I said, the power is in your hands for Hayutayor Bookmarkable State. And I'm really excited to see where it goes.



[00:40:02] Mike Thomas:

No. We're extremely excited for, your talk and the the shiny state package that I know you've worked super hard on. And I think parlays with this blog post,

[00:40:10] Eric Nantz:

Bipasha really nicely depending on how complex your use case is here. So great way to round out the highlights this week. And I dare say, I'm, I've been to I've been very fortunate. I've been to every iteration of our studio in Posikov. I don't think the book marketable state feature has ever been mentioned once. That is how under the radar that has been, but those of us in the in the industry are consulting trenches. Boy oh boy, it's been a valuable thing, and I think once hopefully, the shiny state kinda takes off, then yeah you might find some interesting you know uses of this mic and you of all people I think will appreciate what what is in potential here for using this

[00:40:54] Mike Thomas:

absolutely gotta get this thing on crayon though because there's a couple other shiny states packages floating around on GitHub. Oh, really? Well Watch out. Watch out. Okay. That's good to know now. Maybe I should have known that sooner. Nonetheless, the name of it is Sorry. Most of them, don't look to have any activity in, like, the last three years. Okay. Well,

[00:41:14] Eric Nantz:

yeah. Like I said, I'm in the conference driven development mode. I do want this thing on Kranos sooner or later, so that will be another journey. But we hope that our weekly helps of your journey in data science, especially the use of R and open source in your data science journey. Every week. Tons of fantastic resources. We obviously can't get to the entire issue. We'd be here for almost five hours if we did. But nonetheless, that is for you all to catch up on after this episode. You can see all the excellent, you know, links Colin Fay has shared with us on this week's issue. And, of course, this is a community project.

This isn't like some of those things you might see on, cough, cough, LinkedIn, where they're doing their own fangled newsletter of sorts about the r space. This is a community driven effort. I I do throw a little shade there. I couldn't resist because I'm very proud of where r weekly stands in in the in the statuary community. Certainly, some talks that use r really inspired me to say that because r weekly has had a hand in some really interesting community efforts recently that I heard about. But we are also driven by your support too in keeping the project going.

Best way of doing that is to share those great resources you found, blog posts, new package, maybe a new thing on GitHub you discovered that's really interesting. Our curator will be glad to contribute that into the next issue. We're just a pull request away. Everything on GitHub, everything transparent. Absolutely love it. And also, we love hearing from you too. We are available on our, first, our podcast page that you get in the podcast player. We have a link to the contact page. Definitely send us feedback there. It's always much appreciated.

If I bumble something, especially your pronunciations, boy, I need to hear about it because I'm not the greatest of speaking names. But, also, we love hearing from you about our feedback on social media. I am at rpodcast@bsky.social on Blue Sky. I am also on Mastodon with that our podcast at podcast in this house social, and I'm on LinkedIn. Definitely not causing any weird posts there. Just some fun things here and there. Mike, where can the listeners find you?

[00:43:28] Mike Thomas:

You can find me on blue sky as well at mike dash thomas dot b s k y dot social Or on LinkedIn, if you search Ketchbrooke Analytics, k e t c h b r o o k, you can see what I'm up to.

[00:43:40] Eric Nantz:

Very nice. And I just saw I think it was a week or so ago. Your team is expanding once again. Got some great new talent coming on your team. It's always great to see your your humble little, consulting shop grow and not so little anymore, is it? You got a lot going on there.

[00:43:56] Mike Thomas:

Thank you very much. No. Very exciting and open source all the time.

[00:44:00] Eric Nantz:

That's the way we love it. Yep. So with that, we will close-up shop here on episode 210 of our weekly highlights. There may or may not be an episode because both of us have a lot of things going on next week, but we'll see what happens. Either way, we hope to see you back here for another episode of our weekly highlights either next week or soon after."
"9","issue_2025_w_31_highlights",2025-08-02,45M 3S,"In episode 209 of R Weekly Highlights we learn ways you can pinpoint just what is slowing down your R code, a novel framing for testing your next plumber API, and the adventures in recreating a NY Times chart entirely with ggplot2. Episode Links This week's curator: Jon Carroll - @jonocarroll@fosstodon.org (Mastodon) &…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 209 of the Our Weekly Highlights podcast, and this is the mostly weekly show where we talk about the latest happenings and highlights that have been shared on this week's our weekly issue. My name is Eric Nantz, and I'm glad you're joining us from wherever you are around the world and being patient with us as always as yours truly has a hectic schedule these days as summer starts to wind down. But as always, I am joined by my awesome co host who never has a dull moment ever, Mike Thomas. Mike, how are you doing today?



[00:00:34] Mike Thomas:

Doing well, Eric. We just rolled out a new shiny apps portal for a bunch of our clients. So we are shiny everywhere right now, which is very exciting.

[00:00:45] Eric Nantz:

I'm always here for it, and I saw your your exciting press release earlier this week. So really congratulations to you and your your very, talented team that hopefully I I'm not I'm gonna be seeing you obviously in in POSITCONF next month and hopefully maybe see some of your teammates again too. We'll see. But, nonetheless, really excited to to talk shop once again when we get there. I can't wait. Yes. I've already started the speaker coaching session, so it's that time. Gotta get that talk squared away, so I got some fun stuff to share too. But, we got some fun stuff to share today.

Speaking of sharing, yours truly will be a little vulnerable here for a second. Usually, as part of my, release process for each episode, I try to schedule the social media posts and, it was late at night. Usually, when you do coding late at night or everything's late at night, something will happen. Yours truly somehow on the transfer time in the future to next year because I had 2026 as the as the scheduled year. So, Mike, you call you definitely correctly said, where's the where's the post? I never even bothered to check. Realize, oh, jeez. They're stuck in the ether. So if you saw a note about the last episode a week later, the problem was not in the machines.

It was what's sitting in the chair here. So you're welcome. I thought it was gonna be a lost episode that, you know, would come out way later. But, nope. We caught it. No big deal. No harm. No foul. It happens. Yep. Yep. And sometimes you you need that that, extra hour sleep for you for you to send those out. So we probably will not have that happen this time around. You should see the notes the next day after I finish my fancy editing. So let's get the show on the road here. Our curator this week is Jonathan Caro. And as always, he had a tremendous help from our fellow Aroki team members and contributors like you all around the world with your great poll requests and other suggestions.

And, we've we covered this topic a bit before, but a lot of times when I develop either packages or applications at the day job, There is always the need, and I do mean the need for speed as a certain actor said in one of his racing movies. And sometimes that becomes obvious when, say, you've maybe built that great shiny app and then it's doing some crunching behind the scenes before you get that plot shown or that nice data table shown. You might be nice and at least put, like, a loading spinner in front of it, but then you may have a customer that says, what on earth is taking so long? Then you're thinking, okay. What is it really?

Is it the business logic? Is it the UI element? What exactly is it? And a lot of times, at least in my practice, it does become part of the business logic problem, which means you're back to regular our code to try to figure out where this issue lies. So in light of that, our first highlight we're covering today is some really great notes from Kelly Baldwin, who has been a frequent contributor to highlights in the past. And she has this great new blog post on, I believe, one of her, newer blog actually is called particles.

Great name there. And the article is entitled speed testing and three different levels that you can take with this with your r coding. And this was inspired by a recent, workshop that she jointly gave with Tyson Barrett, from a USCOTS, organization workshop, and this is inspired by a lot of material from there. And so to motivate the problem, she makes, a little bit of a pipeline here of doing some various dplyr operations with an inflated, New York City, flights data set. In fact, she basically are binded rows, you know, multiple times to get a much bigger set.

And, spoiler alert, I mean, you must use a little l m to get this example going quickly and that's, you know, very relatable. Group by processing, summarization, doing a little additional variable generation for different lengths or transformations and doing some filtering afterwards, and some sorting. This is all very relatable data pipelines. And so that might might take a while to because if it is a larger dataset. So before you can really do a lot of testing that's recommended here or, benchmarking if you will, it is helpful to put all this in a function for convenience.

So she ends up doing that with an appropriate name, do the thing function. And, I love these I love these kind of names because sometimes in my mood, I might name a function very random words as well. First level is basically the time itself. That's often where we start to see there might be an issue, so that might be your first pass, if you will, of just how long is something taking. There is a recommended approach, but if you're essentially using r for a while, you may be aware of the sys dot time function where it literally just gives you the the actual time when you run that function.

And you could save that as a variable, run your long running function, run that sys time again, but then do arithmetic and minus what you recorded as a start time. That is fine ish, but she correctly points out you're getting an object back called the diff time object. And it's not really user friendly from what she wants to do, which is more arithmetic around these operations. So there are some better functions to use in her opinion, which are both in base r called proc dot time and system dot time. So if you just swap out sys dot time or proc dot time, you'll get now an actual proc dot time object, which is a named vector, which is great because you also get the elapsed time right off the bat after you do your, arithmetic of the start time minus the the end time. So that's a little more relatable.

And, again, this is human type of seconds that are outlined here. And then you may be familiar in the r community, some of the wraps kind of this product dot time stuff, the tick tock the tick package or and the tick tock function to be exact. So you can basically flag it as tick, do your function, then do run another function called talk, and immediately you'll get the time elapsed right off the bat in the console. This is great for Marto's interactive, type of expirations at the time. And it might lead to more, you know, more investigation depending on that complexity of the function. But you can do some pretty interesting things with the result of that object too if if you're daring to do so, but that may not be quite enough.

So you may have found the slowdown. Now you wanna know, well, what can you really do to interrogate this instead and maybe look at alternative solutions to that alternative, code. That's where benchmarking comes in. There are a few different ways you can do this too. First of which is may first of all, having that alternative function to have something to compare it against. And because Kelly's been involved with a lot of the DT package, might say a reinvigoration or at least trying to bring that more to the mainstream of the art community, she makes a handy function called do the thing DT where she basically takes the d t plier package, compares that existing data set to a d t object with lazy underscore d t, does the same d plier operations, and then, you know, returns out. So that's a nice little convenient thing if you wanna still keep your dplyr syntax, but take advantage of data dot table functions.

So with that, there is a a package called micro benchmark where you can run iterations of these same functions multiple times because if you run the function once, depending on what your system is actually doing, you might get different results the next time you run it. Right? Because maybe your your browser is going crazy on you. Maybe you got another, you know, data hungry process to do and stuff. So having having Microbenchmark do a a set, like, a controlled experiment, if you will, running each of these things, say, say, a 100 times, you might get then the average, you know, the mean or the median of these times and resources used. So that's great. Now keep in mind, it's gonna run the code a lot of times, so I might have to sit back and do something else while it's crunching through that.

And that can show, in this case, that the d t data dot table version of her function here reduced the evaluation time quite a bit. So that was pretty neat. But you may wanna see, is that really gonna generalize the even bigger datasets? So you might wanna make different versions of your inputs. And then that's where you might wanna opt into the bench package, which has I think there's an offer by Jim Hester, formerly a posit. Now I believe at another company. And this is where you can do the mark function from bench and use this these set of functions, but also maybe a different input or different inputs throughout this process.

And there is, again, a way to do this with the mark function, and you'll get similar elapsed times as you would for the other, micro benchmark function. But this only runs something once when you do that mark function. It might be just helpful for prototyping, but the reason she wanted to do this says you get the memory results, not just the time results. So that might lead to, oh, wait a second. The DT version is using roughly half the memory as the default dplyr version. So is that going to generalize to larger datasets? And that's where the press function comes in, where you might, in this case, say, dupe you know, have these different scenarios of different sizes of the data and still use the mark function under the hood to do it, but then you're basically making these different versions of the flight's dataset.

And then that's where she could see that as the data got larger, this advantage of data dot table versus dplyr wasn't really growing at the same growth, as the dataset got larger. So it's not quite as fast in the larger datasets as it was on the slightly smaller set. So that's where looking at the bent the bench package gives you some additional insights and little ways to control your experiments even more. Last but certainly not least, and probably the most complicated of this is that you wanna really figure out in a fine tune way where these potential bottlenecks are actually happening instead of just bigger wrapper function.

Where is this actually happening? And that's where profiling comes in. Something I need to do more of, but also become more comfortable with the with the workflow here. You got a few options here too. You got the provis package, offered by Winston Chang over at Posit. This has been great for both traditional r code as well as shiny code for that matter. And you can feed in, in this case, kind of the body of this function that Kelly's made here, and you get the nice kind of flame graph that's interactive, and you can start to see that most of the time being spent was on the filter statement. That often is a a bottleneck in traditional dplyr pipelines.

But it's great that you can at least explore this interactively even in the blog post itself, toggle between the flame graph and the actual data that shows, like, the steps in a in a classable list. Really nice to see professor is being used here. You can go even more lower level than this with the r professor function and base r. In order to do that, you still need to save your results out, when you run r professor, and then, you'll have to do you know, create a helper function of sorts or use a summary rprof, function to do that as she has in the blog post where you can see roughly similar output as prof is. But that may be helpful if you wanna stick to stick to that native r prof structure.

There are some additional packages in this space and, in fact, profer, offered by my colleague, Will Landau, was a great, great, package in this space. So check that out if you're interested in. He saw some shortcomings and prop this, so he thought he wanted to take it to another level where proffer and if you know Will well enough, you know that when he gets his hands on some, he will make a nice interface out of it. Kelly concludes the post with some other additional thoughts about maybe in the future, profiling can be easier with maybe a pipeline kind of syntax.

She shows some kind of pseudo code that she does to illustrate this, in a more manual way. Maybe someone will take on that challenge. Who knows? But there I hope this post, illustrates to all of you. You've got different levels, if you will, of doing this kind of benchmarking or in speed testing exercise. Sometimes just the time of last might be good enough. Other times, you gotta go a little further, and that's when you gotta put your, you know, design of experiments type hat on with the bench package and micro benchmark, and then really start to get lower level if you need to with the profiling tools.

They can be quite rabbit holes. So you gotta, you know, right size this to what you need for your given project. There may be cases where you just can't do anything anymore and you gotta, you know, communicate that to your users. But you might see some significant gains of how that data is being stored or the types of processing you're doing. Things like data dot table are definitely a big help, but there may be more you can do. So many things like DuckDB and Duckplier have my attention to make data operations even more faster, than potentially this. So, again, great if you're new to this space and, again, some good nuggets to dive deep further if you want to, you know, go down the rabbit hole, as I said, in each of these different levels. So as Kelly always does, she teaches me something new every time she writes a great post here. So, Mike, what did you learn from all this?



[00:15:14] Mike Thomas:

Yes. Kelly Baldwin, former Shiny in production workshop attendee, asked great questions, asked me questions Alright. I couldn't answer, because I think she's smarter than me, which doesn't take much. But the the blog mentions that, Chad JPT wrote the data wrangling pipeline code, and I can't help but wonder, did Kelly do some vibe coding here? I guess that's sort of the big thing these days. I have not What is that to use it for you? Yep. Yep. But yep. Throw that out there. I am really guilty of using sys dot time, which is, as Kelly points out, is a lot more fragile than some of the other options that we have, even just in base r, like proc dot time and system dot time. Or, you know, that TikTok package is one that's been around for a while. It's just, you know, it feels like one of those tidy packages that just makes the output a lot neater and user friendly for us when when we're trying to do these types of exercises.

And there's some great use cases, and I never really thought about all of the nuances where you may want to employ, you know, profiling, benchmarking, you know, comparisons of the time it might take to run your pipeline. But as she mentions and you mentioned Eric, if you're looking to do experimentation and run your code a ton of times, because we know it won't take the exact same amount of time each time. Right? It could just be millisecond differences or, it could be longer depending on what you're trying to do.

You can see a distribution of those run times with that micro benchmark package, which can really be your friend. I think by default, it runs your code like a 100 times and shows you, summary statistics on how long it took across that distribution. And if you wanna compare trying to do the same thing a couple of different ways. You know, one example I could think of is if you have a dplyr pipeline and a data table, data dot table pipeline that are trying to accomplish the same thing, you can use the bench package, which can be be your friend in those situations. And there's always something for me just kind of oddly satisfying when I see these clever name spaced functions, like bench colon colon mark or bench colon colon press. There's a few other packages out there that, you know, I think are are clever in how they name both the package and the functions within that package that the the package and the name space version of the function, is I don't know. It it's just one of those things that makes me smile. And, you know, speaking of that twenty twenty three Shiny in production workshop that your favorite podcast host taught, Profis, as you mentioned, Eric provides some really powerful visuals and graphics about the time it took to run each part of your pipeline. And it takes some education to figure out how to interpret the output of Profis.

Once you do figure that out, it's it's really powerful. But as you mentioned, I wonder if we could make this prettier, or more interpretable in the future. If there's a way to kind of clean up what Profiz returns and and just make it, I don't know, feel like it's it's in the year 2025 in a way. I feel like that's gotta be possible. I'm kind of making my own call to action to maybe step into that package and and see if I can submit a pull request to clean it up. But that's probably some serious code and probably a little bit above my pay grade, but we'll see. Just throwing that out there into the world.

And and it may not be important to you in a lot of, situations that you're in to get your code down from maybe like a two minutes run time to one minute. But if you have, for instance, like a cron job running every day on on cloud compute, it can actually save you a lot of money in the long run. And it can just be, you know, these things add up over time. So taking the necessary steps and just being thoughtful about how long your code is running for, where the bottlenecks are, and just trying to write the most, you know, concise but legible code. That's something that we really try to practice day in and day out on my team.

And and it can be really important in the long run if you look at the big picture. But great blog post from from Kelly, and it's a a great reminder on a subject that I don't think we talk about enough.

[00:19:24] Eric Nantz:

Yeah. And a lot of times, maybe in a more interactive data analysis, you're certainly not really thinking of these of these issues. This is when you get to those ETL kind of processes or anything that involves heavy business logic and your interfaces, whether it's packages or apps or definitely a combination of both in my in my cases. A lot of times, the biggest wins are when you find that even after all this, you still can't really do a whole lot to minimize the execution time. If you do have an opportunity to somehow cache these things or run them on a schedule and then say your app consumes them, you know, on the on the fly, but that that heavy lifting's already been done, and the app is simply importing them. This will, free free consulting for those out there that may be struggling with this.

Packages like pins. If you can do your data ETL stuff and then put it as a pin as your summary or transform set, have your app consume that instead of the app doing all that. Oh goodness. They say the the time gains can be immense, and that was a recent project I'm about to put in production in the coming month. That's been our biggest win. Just cache all those results. The app itself didn't need to do any of that stuff on the fly anyway. And now I run a crown job that right now is every night or every morning, I should say. But I got the capability to run that even more frequently if I need to with with cloud architecture. So it's, little things like that, you kinda learn on the fly, but even, you know, before you get to that point, if you can speed things up, these are some great tips to do so.

And as I just mentioned, Mike, you know, trying to do things outside your app is a really helpful technique. And a lot of times being able to separate out the way your package or your app is is calling an analysis, you know, requirement or analysis function that gives you more control on, you know, the rest of that process can be really helpful too. You and I have both been knee deep in our recent projects and either building or consuming or frankly, both building and consuming custom APIs, application programming interfaces. We haven't heard that term before in our various utilities or or innovation capabilities we've been building here.

And in the world of the art community, a great way to build your own API has been the plumber package. That's had many years of history now, but, boy, it's been a lifesaver for me when I wanted to democratize, if you will, the use of this analytical pipeline or resource. So so many from our I could call it, could call from Python, could call from anything that can do a curl request. Plumber gives you that that opportunity. So you may be building this great API, but then it comes to Sage where if you gotta put this in production, they're gonna ask you, where's the test?

What do you wanna do for testing? You could go down a lot of different directions for this, but this next highlight talks on some really great points for you to consider, especially as you structure, you know, your testing paradigm and some common pitfalls that you can avoid here. And, returning the highlights once again comes to us from Jacob Sobolewski, over at Absalon who's been, as you may have heard in previous episodes, really active in the space of testing on the shiny side as well as on the general r side. And this blog post he has here is called testing your plumber APIs from r itself.

And so there is basically first set of advice in this blog post that I think can relate to us on many levels. It's a given that your API probably is gonna do some kind of business logic type function, wherever it's data processing, wherever it's munging, wherever it's just making new variables or whatnot. And that doesn't necessarily need to be tested in the same way as the API interface itself. So his first piece of advice is called the separation of concerns for that business logic stuff. Put that in its own function and use what's familiar to many, those in the r community of the test that package.

Use that framework that's been well established. Does that have anything to do with the API interface at that point? Get all that stuff ironed out first in the more traditional testing paradigm. Same thing applies to Shiny itself for that matter. Get your business logic in the functions, test that with test that, your life will be a lot easier. That gives you the ability to tailor the API kinda testing, which he calls API contract testing. Not so much focus on the exact response a year from the API because that's gonna be handled from the business logic stuff. This is more about the what he calls the shape of the response.

Is it the type of structure, say, a list structure? Maybe it's a data frame structure. And so having these two different layer approaches, you got the business logic layer and now this API contract layer, to really compartmentalize these two perspectives. So you may be thinking, yeah. I got I I can I can handle to test that framework now? Well, how do I get started with this kind of API contract testing layer? The first step is just getting the API running of itself. So as many things of our, by default, things run-in the foreground when you run, like, a long running function. Heck, when we run a Shiny app interactively, that's in a foreground process most of the time. So you wanna run this API, this Plumber API, process in the background.

And that's where, this is where I started to learn some interesting use cases. The Mirai package, author by Charlie Gao, who is now a solution engineer at Posite. This package is great for launching a plumber API as a very straightforward black backward process. I remember in the past and the plumber documentation, they would say, okay. Spin up another r process, run this plumber process on the fly manually, go back to your foreground process, and then run your, like, curl calls or whatever else to to interactively spot check the the API you're serving.

So this is a great way for me or I just to get that thing running in the background and then get on with your actual testing paradigm. Next, he recommends a new pattern or at least new to me the way he articulates it for the the way you test things. So you first got the arrange part of this, getting your API as a background process, get your test data ready to go, and then setting up whatever you need from a configuration or authentication standpoint. That's your setup time, if you will. Then once you got all that boilerplate set up, the background process, the authentication, now it's time to actually do the thing as I say, and make the actual request to your API using a package like h t t r two, hit or two, however you wanna pronounce it.

This is great. That is only just running the action that you wanna test with the required parameters in your API. Last but not least, you've got the assert phase. And this is where you're gonna look at making sure that you're getting the expected types of results. And again, maybe not the result itself, but the shape of it, the type of response codes, the status codes you might say. Is it a 200? Is it a 400? Is it a 500? Is it, and again, the structure, making sure it's fitting the structure, not necessarily the in-depth content of the response itself.

So though that in and of itself, that approach really resonates with me. And then also looking at, how you structure all this, he's gotta recommend a naming pattern of, like, test dash API dash endpoint, you know, whatever that endpoint ends up being for the individual end points, but keeping those in isolation from each other instead of doing one script that does all the end points at once. So it's a great way to focus yourself well on that particular end point and then running the emo, the associated, maybe business logic test alongside with that, but in that layered approach.

And then we can make the bugging easier. It can make setup easier and a lot, you know, a lot of things like that. And then, you know, he's got some other, helpful information about, again, the idea of testing the shape of the response, not the content itself. You may not necessarily care what the, like, a slot called ID or name, what those actually are, but what are the attributes of that? What is the logic in that? Use the business logic for the actual values. And then lastly, he concludes of some pitfalls to avoid, that can be difficult to avoid if you're new to this. Trust me. I know.

Is making sure you don't necessarily duplicate the business logic type of test in your API test. Like, those should be kinda self contained, their own function, their own tests, and you just simply call them as needed. And then, really, hopefully, you're not so much testing the implementation detail itself. It's more about the API endpoint. They it should not matter necessarily if you're using hit or two to do it. If you're using a straight system curl command to do it, that should not matter. An API calls an API call one way or another.

And then hopefully this is probably the hardest to avoid if you're in an organization. Hopefully you don't have to depend on an external service to actually conduct the API testing. But if you do use it only when you absolutely need to and maybe take advantage of things like mocking or other sit you other helper packages to kinda cache that what that setup, you know, external process looks like, and then use that mock function in your testing instead of having to do that repeated call to that external service to make that happen. So lots of great advice. And then also in the supplements, I will put in the in the show notes here. He's got a handy set of examples and what he calls the r test gallery, which I never he's I think he's had this for a while. I just I wasn't aware of it. And you can go straight to examples for the plumber API test, so it opens this little editor right under the blog post where you can kind of look at this almost like an iframe thing, I guess, through GitHub or something. And you can literally see in this hypothetical example what he's doing to illustrate the concepts in this blog post. So really convenient way to kind of see how all this relates to each other.

I've been building a couple custom APIs at the day job, so I've learned a lot through this and I've been using some of the best practices already. But certainly, I've I've learned a thing or two about this this recommended pattern of how to use, this arrange act and assert pattern when we think about constructing these API level tests. So great blog post here by Jacob as always, and he's quickly establishing himself as a real key thought leader in this space. I'm I'm gonna be reading this definitely again after this show. So hopefully you learned a thing or two, Mike, from this. I know you've been knee deep in API development lately. What did you think of this? I did, but Jacob always delivers seems like a whole lot of blog posts lately, really interesting content,

[00:31:18] Mike Thomas:

particularly around testing. And I couldn't agree more with the beginning of this blog post. You've got to separate your business logic from the service itself, and I'd preach this if it's a Plumber API or a Shiny app. I think it makes sense to do so in pretty much all cases. One thing that I was reminded of as we started reading through this blog post is there's a Plumber two potentially coming That is a rewrite of plumber. Right? Yes. That is coming. I mean, Thomas Lynn Peterson sent a note about that a couple months ago. Yeah. Yeah. So I see that out there. It definitely it has not hit crayon yet. It has no tags yet. I haven't taken a look at when the most recent commit was made. Oh, last week. So it's Okay. Looks like it's still under active development, and I wonder if some of these concepts that Jacob's talking about in his blog post may be introduced as features in the plumber two package as well. We will have to see. I haven't looked under the hood of the plumber two code enough yet, but just some food for thought out there. And I love the arrange, act, assert pattern. Jacob mentions, you know, writing unit tests for status codes, can absolutely be your friend.

In my experience, it can also be your enemy if you're too generic.

[00:32:35] Eric Nantz:

Yes. Absolutely.

[00:32:37] Mike Thomas:

Just a little word to the wise out there for all those those three, four, 500, status codes that could come back. One flavor of a 400 status code could be quite different than a different flavor of a 400 status code. If you've been there, you know what I mean. And also, in my experience, the with r package can really be your friend in unit testing for creating, like, sort of an isolated environment that you can throw away when the unit testing is done or this environment that's, you know, sort of separate than the development environment that you're using for your package, to create sort of these really interesting temporary environments. I think we've talked about a lot of use cases in the past where that may help. And Plumber or Mirai may already sort of offer that same functionality, or they may import with R, in those packages. I wouldn't be surprised at all. I don't have a whole lot of experience with Mirai.

I should. So, I know that it's great for spinning up multiple our processes, which can be great for parallel processing or concurrent requests. We don't have any APIs right now for our clients or internally that get hit really hard. So none of them we've found the need to allow for sort of concurrent processing yet, but if everything goes well, we will one day. So, and nan Nanotech seems very interesting for, that exact topic. Right? Concurrent request handling. And this all reminds me of, Jacqueline and Heather Nolis' talk. I think it was called, you know, we put our in production.

And they literally had, like, in a neural network, I think, API that they had stood up with R that was getting hit by the customer service team at T Mobile, like, a million times a month or something like that. It was an obscene number. I couldn't believe it. Yeah. And it was working great. So maybe I'll try to add those slides to the show notes because Yeah. I think they had a website called it was literally just called Put R in Prod. And we'll we'll try to link to that website, but it was one that when all the Language Wars stuff comes out about R slowness once in a while, I always just fire off a link to those slides in that website to prove that that is all bogus. But great blog post by Jacob. Lot of, I think really interesting nuggets to take away for your API testing use cases.



[00:35:09] Eric Nantz:

Yep. Love loved every bit of this. And like I said, I've learned a lot along the way. And one thing I'll I'll plug in this space, I think, can be a great accompanying to this is that, there's a there's been a new release of a package in this space by Scott Chamberlain called VCR. This is a great package that lets you record and replay these various HTTP requests that you might, that you might make if you're doing an r package that services an API. This is this is a great way to to test that. So I definitely recommend that approach maybe to augment with it. It's got a nice, straightforward way to augment test that with this, functionality. So lots of great tools in this space for in a space, I should say, and great thanks to Jacob for continuing to pump the knowledge into our veins about doing this in an innovative way.

Alright. And then lastly, the round out the highlights today. Admittedly, this one's gonna be a little more difficult to talk about because it's actually a video tutorial, but it's a space that we often touch on in this show about, the clever uses of visualization and art to replicate or perhaps even make better a lot of these fancy visualizations you might see in, in, you know, in the in the reporting space. And in this case, this high this last highlight comes to us from, again, a video tutorial, created by Spencer Shin, who is a data and policy strategist at the City Forward Collective over in Wisconsin.

And he saw a a a bar chart in the New York Times online blog. Be warned if you do try to link to it or click the New York Times link, you might get nagged by making an account. So if you get that just like I did when I was preparing for this, we have a link in the show notes to the GitHub repo that Spencer created, for this code demonstration that has the actual image in there for the plot. But, admittedly, the plot itself, the subject matter is a bit political, and I've had a hard enough week already, so I'm not gonna get political on you all. There are hundreds of other podcasts to listen to if you wanna get in on that train, but it is a bar chart about the the new bill that's been proposed, by the by the president called the big beautiful bill, and the way it could potentially impact different, income brackets in the next, ten years or so. So with that, it's pretty straightforward bar chart with each, threshold income is a separate bar, and the bar goes down below the the x axis if it's a negative threshold and goes up if it's a positive threshold or positive change, I should say, in the resources. So, in the video tutorial, I'll just kind of give a quick take on what Spencer did and some of the things I've learned from it.

There there was a hyperlink to the source data in the article, but, actually, the data is in the chart itself. It's annotated above each bar. So he made a simple tibble, for each, income bracket, the the average, threshold change or of the income, so to speak. He just made a tibble out of it. You know, brute forced it, but, hey, it works. And then after that, he the the finished product on the GitHub repo looks great, but he didn't start there. Started with a very basic ggplot, very bare bones, no updates to theming, no updates to anything. So he he illustrates what does he change along the way to start getting it to replicate the New York Times visualization.

He first, again, made the the data generation a bit easier, starts to think about, you know, getting the default theme out, changing the colors of the bars using, of all things, the developer console and the browser at the article to pinpoint where the colors are actually used in that online visualization, which I'm sure it was using some WebGL or web framework to do it. But the CSS is all there. So you grab the colors, start to update the theming of the of the of the code itself. There is a lot of interesting things. He was using positron for the first time, so there are little gotchas of the editor that he he showed and fast forwarded parts that he felt were a little little off, so to speak. But it's a great very much in the style of live coding.

He made this tutorial, so I can relate to it. I've done a few of those in the past, so definitely fun to see somebody flex their visualization knowledge. But honestly, it's native g g plot with the scales package. That's it. Like, it did not take a huge effort other than using some custom fonts to get this JGPOT version of the chart to look like the New York Times article. So if nothing else, it's a great illustration with just, like, a handful of time about a half hour screencast. He replicated this chart all with what g g pot two offers. So lots of clever use of the theming, the text representations, and all that. So check out the show notes. You'll get a link to the code. But, yeah, have a look at the at the video if you wanna see how an expert does this in action. So pretty pretty fun stuff there.



[00:40:45] Mike Thomas:

I feel like data visualization blogs and videos and things like that, they always offer something that you hadn't thought about before. Right? Like, there's sort of an unlimited amount of creativity that can go into the concept of data visualization. And this is, you know, a really cool exercise to go from scratch, something that he saw in the New York Times, to trying to build it yourself in Positron. And as you mentioned, love the live coding idea. Eric, you that's a a road that you have gone down many times. I have never live coded before. I am way too scared, but I think it's incredibly beneficial to people who watch live coding exercises because they can see the little hiccups that you hit and how you thought about, debugging them, right, on the fly. And sometimes someone else being able to just visualize that thought process can be more helpful than, you know, following this set of instructions on how to get around it.



[00:41:45] Eric Nantz:

So absolutely. We are all we are all human beings, my friend. No matter what you may think when you see the finished quote, we are all humans, and how we get there is not always as smooth as experience.

[00:41:55] Mike Thomas:

Kudos to Spencer. Absolutely. You know, one really sort of interesting thing about this particular chart that lends itself to telling the story of how flexible g g plot is and how customizable it is, is the output plot here has sort of date data labels on both sides of the bars. It has has data labels, you know, for the positive bars. It has data labels above the bars. And then for the negative bars, it has data labels below the bars that represent sort of the value. Right? The height of each of those bars. But it also has these sort of annotation data labels that I believe, you know, represent like the the quartile or something like that of each particular bar that are on the opposite side of the value data label. So for the negative bars, it's it's on top, and for the positive bars, it's underneath it. So you would think that that's a whole lot of data labels going on on the same chart here, but it's actually really clean and really informative.

I thought that it was pretty incredible that he was even even able to pull that off. And if you take a look at the code, I think the final piece of code is, like, 38 lines of code to pull off this New York Times bar chart with all the It's amazing amazingly

[00:43:13] Eric Nantz:

concise. Yeah. It's just amazing what you can do with JujubePot too.

[00:43:17] Mike Thomas:

I think that's sort of the long story short here. Amazing what you can see with g what you can do with ggplot two. Love the concept of bringing in the exploration of, sort of the browser dev tools. Right? Being able to take a look at the New York Times website itself and grabbing, you know, those exact CSS color elements, off of off of the page and incorporating them in the g g plot two code. Really handy trick for some of the the data viz folks out there that might be a takeaway. And I'm sure there's plenty of other things in this particular video here that you could gain, a lot of knowledge from. So hats off to Spencer.



[00:43:55] Eric Nantz:

Well, I couldn't have said it better myself. So, again, lots more in this week's our Wiki issue. And, again, check out that that screen cast. Again, it's a very entertaining, style that Spencer has, and I can relate to it on on many levels. But, yeah, our we better get out of here soon. Our day jobs are calling us once again. But, again, if you wanna get in touch with us, we have various ways of doing that. First of all, we we appreciate your input on the our weekly issue itself. If your new poll request for that great new package, resource, blog post you found, I have a few and I might be next up for curating so I can need all the help I can get. So that would be great.

You can also get in touch with us with the contact page in this episode show notes. You can also find us on the social medias. I am on in fact, you know what? Check the show notes. You hear me say it enough. We're you know where to find us. Just look at your podcast player. We're all there. So with that, we are gonna close-up shop here on our wiki highlights episode 209, and we'll be back with hopefully episode 210 of our wiki highlights. And again, hopefully, you never know where the summer goes next week.

"
"10","issue_2025_w_30_highlights",2025-07-23,32M 16S,"In episode 208 of R Weekly Highlights, the LLM train keeps moving along in the data science community: First impressions of the new Positron Assistant for code completion and basic package development, plus a friendly app for exploring the upcoming posit::conf(2025) agenda created by one of our talented R Weekly curators! Episode Links This week's…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 208 of the our weekly highlights podcast, and this is the mostly, I dare say, weekly show where we talk about the excellent resources that are shared in the highlights section and elsewhere in this week's our weekly issue. My name is Eric Nance. And, yes, I must have jacked myself, a couple weeks ago because I said, oh, yeah. We should be back to a regular weekly schedule. Well, sometimes things out of our control happen too, so it didn't quite happen that way. But we are back at least this week with another episode for you. And joining me as always in between, putting out various, you might call fires at the various, projects he has under running, Mike Thomas.

Mike, hope you can keep a level head here. I know you've been

[00:00:46] Mike Thomas:

knee deep in some, fire squashing. Right? Well, haven't we all? Haven't we all? But, none today yet, so knock on wood.

[00:00:54] Eric Nantz:

Alright. Well, we'll keep them keep that good positive vibe going. Alright. Told him about some various legacy things I've gotta fix. So, note of you out there that have maybe, like, IT groups that migrate servers from time to time. Better check those things after migration. Sometimes things happen. Hashtag just saying. Anyways, we got some great content to talk about today. You don't need to hear about my rants about that. And this week's issue was curated by John Calder, and yet another one of our OG curators on the team. And he had tremendous help from our fro, our rookie team members, and contributors like you all around with your poll requests and suggestions.

And later on in the show, actually, one of our other curators is quite involved with this, so stay tuned for that. First off, as you well know from our previous episodes and also in the general our community, you are aware that Posit, the company behind our studio's IDE that has been, you know, in production use for many organizations and data scientists for years, has been working on a new IDE called Positron, which is, at a high level, a very opinionated fork of the Visual Studio Code IDE. We have a lot of nice enhancements that tailor it better to a data science kind of workflow in R, Python, and frankly, even other languages, supports.

Things are going really well with it. I have been using it in my day job now at my local system for almost, gosh, six months, seven months now. Plays nicely with the Knicks, by the way. You know, I couldn't let that not go in there. But one of the newer things that we knew we knew was coming, so to speak, maybe rumors or whatnot, but now it is in preview mode along with the ID itself, which again, we wanna stress it's still in beta. So it's not like it's been a full production release yet. But one of the preview features that has landed is the positron assistant.

You probably can guess by the name what this is empowering. Yes. This is empowering the use of large language models for both code completion and agentic coding, which is in essence giving you that nice interface for you to help, you know, ask questions, and it will help you with development or answering questions much like a typical chatbot would, but powered within Positron. This is early days, but I am eager to say that I've had some use of this recently. I'll share that in a minute. But the first round up that we're talking about today in our highlights is a great blog post by Stephen Turner who just recently, changed roles, and now he is an associate professor of data science at the University of Virginia. So congratulations, Stephen.

He's actually been an earlier pioneer of using AI and LOMs in his codevelopment, especially with Positron. So he mentions that he's looked at this in the past in previous blog posts, which are linked in in the aforementioned post here, where at the time, this wasn't built in. So if we looked at different solutions, the plug in code completion, for example, as at that time, GitHub Copilot wasn't baked in. So you had tried frameworks like Codeum, tab nine, and continue, which I've dabbled in bit in the past.

But with this positron assistant, it now comes with batteries included, so to speak, with the use of GitHub Copilot, which is something you can opt into, in your GitHub account. There is a professional version, but I think you can get away of just using their free tier for this. But, you know, may I have to fact check me on that? But it can let you configure that, and then you can use then the copilot in your positron IDE for your code completion. It isn't a very radically new thing per se in the world of IDEs because even Versus codes may able to do this for years. Our studio had support for this earlier on, maybe a couple years ago, year and a half ago.

But he's overall, you know, it does it does work well. I will admit in my uses or at times there still hallucinates just a little bit of what I wanna do, especially with some shiny stuff. But it has saved me some time, especially with writing change logs or writing release notes or writing documentation. It's actually not too bad, not too shabby. It is much better than when I tried it a few years ago. So promising there. Where I think Steven was really curious about is the agent mode. And for this, within the positron assistant, you can configure not just the GitHub Copilot tie in, but you can also configure the use of anthropic Claude.

You bring in your anthropic API key or your OAuth. Yeah. I think it's API key for anthropic. That is the only one supported at the moment for this mode of the bot. So or the the plug in. Perhaps when it gets to production, it'll get future, plug ins or or I should say account tie ins. But once you have that, he was curious about what it would be like to use this agent mode to actually try creating a package. And when he did to test this out, he just simply created a skeleton of a package of use this. With the create package, you get your kind of scaffolding lined up, but a very minimal setup, maybe just your description, your name space, some placeholders.

And then he asked the agent in this positron assistant to basically create a punch or, wow, create a package with a function, to re reverse compliment a DNA sequence. Cause Steven has been heavily involved in genetics for years, documented with our oxygen and then write tests. And he has a screen capture of of the agent in action to build this. Every time I see one of these, there's a part of me that's like, oh, gosh. Does that actually work? But in essence, it wasn't too shabby, he said and you can see in the video, it basically, you know, scanned the structure of what he had in a directory, you know, creates a function, writes a documentation in our oxygen, and then wherever he wants to run the test that it helps create as well to check that that's working and then runs the dev tools check to make sure everything is working, and no errors were found.

He did have to make one little correction in the license field, but that was mostly due to the defaults of use this. He wanted to put the MIT license, but that's all, you know, very minor minor, enhancement there. But overall, he seems to be, you know, pretty impressed at least with this very fit for purpose, you know, much more streamlined example. Time will tell as how this works with a more larger scale ask. Maybe you have a more complex thing you're working on. I do mention for the shiny, enthusiasts out there, Mike and I included, positron does have support for the at shiny tag as well that you can put at the beginning of your query so that they can be shiny aware, so to speak. If you're trying to help enhance an existing app, maybe building a new app, then you're able to take advantage of its, I'm guessing, pretrained, hook into shiny knowledge and documentation or whatnot. So that's a a little note for the the shiny users out there. So he Steven's gonna look at this, you know, you know, in more robust situations, but overall, not too bad.

And one thing to note is that this really didn't cost a whole lot to, to run through his account. It didn't cost too much. It was about 9¢ for that trivial example. You know? Yeah. Obviously, if you do that a whole bunch of times, I might add up a little bit, but certainly way cheaper than some of the other services you might pay for in this space. So the key though on that, and there's a footnote about this too, is that a lot of people are paying, like, the default account access to these services, or it could be, like, $20 a month, sometimes even more depending on which tier you put in.

He's doing this via the use of, buying API. You might say tokens or credits beforehand. They're kinda like a pool that you can draw from and you pay as you go. And then when you run out of them, you buy more. In things like this, I think that's a huge cost savings compared to using the more traditional accounts. So if you're new to this and you're wondering just which way do you do this, paid approach, Definitely. If the service you want to use offers this kind of token bucket approach, take that and take it now. And that way you will, you will, your, your budget will person, wherever it's you or your organization, will thank you later, to make sure you don't run through these just with that default account access.

I've been using this a little bit. I'm I did it for some more basic, troubleshooting, and it did help very nicely about 80% of the way there. So as always with these things, I'm pretty new to it, but definitely promising that now I don't have to flip over to another interface just for that agentic chat. I can now do it within the confines of Positron. So I think there is improvement to be had, but overall, not too bad, all things considered. So I'll keep an eye on this space.

[00:10:57] Mike Thomas:

Yeah. This is pretty cool, and it's nice to see the sort of seamless integration that we're getting now with AI assistants in the Positron IDE. It's a really nice sort of chat pane on the left side of the IDE that, you know, takes up maybe 20% of the screen where you can sort of follow along and and type in your prompts and take a look at what, Claude, in this case, is is thinking and doing, and then sort of see it happen in the other 80% of your screen, within the scripts and and the editor and things like that. So that agentic mode was was pretty wild, and I would definitely agree with the notion of trying to leverage the APIs here, in terms of Claude. The from a cost perspective, you as, as mentioned in the blog post by Steven, you really don't have to worry about running up some giant bill because you can and this is something that we've done at Catch Brook with some of our enabled, AI enabled Shiny apps that leverage, Claude on the back end, is you can just do $5 at a time, essentially. I think that's probably the minimum that they make you preload.

And we have, you know, maybe 10 or 20 prompts hitting that that app a day. I know some of our our users are probably writing, leveraging that API, some of our developers, I should say, to, have these AI sort of chat and and edit features within the Versus code IDE. And I maybe have to spend $5 every six months. Like, it's yeah. It's so cost effective.

[00:12:37] Eric Nantz:

That you are saying that because I mean, this this will sound crazy. I spent almost double that when I'm at the cafeteria at the day job for lunch. Like, six months, $5. That's that's amazing, especially when you hear about all the horror stories of people with with AWS accidentally racking up their bill because some they kept turned on. This hopefully reassures you that, Mike, you of all people are not are not breaking the bank on this. That's amazing. No. And they give you a notification, I think,

[00:13:07] Mike Thomas:

when, I think we have it on auto renew, and they give you a a notification when they're about to charge your credit card another $5 because you're you're coming up to that limit. But it's it's so cost effective right now, especially, you know, I'm all about self hosting. And I think in terms of privacy, there's a lot of times when we do that and and have to do that, and it makes sense. But if you do compare the cost of the third party like Claude or or OpenAI, I'm not sure how their pricing compares, but I imagine, you know, that they're gonna have to be competitive in terms of their API offering, so it's probably fairly close. But from a a cost perspective, you know, versus self hosting and leveraging these third parties, the the third parties, I think, really have it now at this point in time. When you think about having to stand up sort of the cloud infrastructure, unless you already have your own server, you know, on your server rack like Eric has in his in his basement.

But we're not doing a whole lot of that these days, leveraging the cloud. But a great walkthrough, I think, of how to leverage this agentic mode within the Positron IDE that if you didn't believe it before, you can see it and believe it. Because there's a couple great sort of short one minute, one and a half minute video clips in here that Steven adds that I think really tell the story, you know, just as well as he articulates it in the blog post. So a great walk through of the combination here of Positron and Claude in this case.



[00:14:37] Eric Nantz:

Yeah. And like like I mentioned, I think, you know, right now, it's Copilot and and Claude. I would, again, not be surprised if we start seeing more offerings in that in that configuration that you do, including for those that are living on the self hosted side that might bring their own, you know, llama or service on top of that. I've experienced a few things, like you said, in that, nice little, basement server that I have here in three feet away, try with some various things. One thing to know, and I remember, you know, Joe Chang and others have said this, Claude still seems to be at least as of now, one of the better r code, you know, l o m, you know, model providers. I know there are some things that come in here and there, and there are there are some interesting work that, Simon Couch at at Pazit has done to help benchmark various models so you want to check out his his blog post, to get a good take on on those.

So my guess is these two will be kind of the standard for at least a short term but they're set up for success, I think, to bring in others as as as this matures. And, again, for the shiny fans out there, just put the at shiny before your the start of your real query, and it'll it'll basically call that shiny assistant right off the bat provide you have the shiny extension also in your positron extension list. So really, really exciting to see and I I think over time we'll start to see at least from deposit company standpoint, the fruits of the labor of going to this newer direction for an IV to give them more opportunities to hook into novel technologies to make the the data science development experience much easier down the road.

Well, Mike, in our last highlight here, it's a reminder that just a couple months away oh, it's coming up fast. We're headed to Posikov 2025. You and I have the good fortune of being speakers at the event. I'm also gonna be helping out with the our pharma summit there. Lots of cool things happening. And as usual, these conferences, there is a wealth of different topics that are gonna be talked about in the various presentations, the lightning talks, the full talks, and the workshops. It's a lot to take in. Definitely, wherever you're even on the fence are going or not, you'll probably want to look through the program and then you might see man that's a lot to digest.

Well, it wouldn't be a a theme. This episode has a definitely a theme to it because it's one thing to pursue the agenda via the traditional web page that they expose. But what if you had a friendly interface to kinda ask questions and then it would give you the information about the conference right at your leisure? Well, that's where our next highlight comes in because our very own curator at our weekly, one of our curators, Sam Palmer, a few weeks ago had released this quietly, a shiny app that's an l o m powered app to explore the POSITCOMF twenty twenty five agenda.

And, yeah, it definitely got some attention so much that in this last highlight, Isabel Velasquez from Posit has literally wrote a blog post on their official blog here about this new l o m powered agent, you know, agenda pilot app. So you can get to the link to the app. We'll put it in the show notes, of course, as well as, the link to the post. But this is really nice. It's a very straightforward interface. You ask it a question. In fact, in the blog post, Isabelle asked what are all the sessions related to positron and sure enough within, you know, less than a second, it lists out all the different sessions there. If you wanna learn more about positron like we were just talking about a few minutes ago, Lots of already I'm seeing two or three great talks in this screen capture, four even. Wow. There's a lot going on there.

So name your topic. You can find it in this, powered assistant here that Sam's put together. If he had asked me if this is possible a year ago, I would have said, you're crazy. But, no. We're not crazy. We're living the the good life, I guess, here because all of this, every bit of this is powered by open source in terms of the architecture and then leveraging one of these aforementioned services under the hood to power the l m itself. So what does Sam have to do to accomplish this? First, he used the h t t r two and chromote packages to scrape the agenda details from the conference portal. So again, there's a bit of homework you have to do to figure out, okay, what are my CSS selectors? How do I select this particular table information?

That's scraping one zero one at that point, but it is definitely achievable as long as these agendas, this is a note to future conference organizers, put them in a web format. Makes it easier for things like this to exist. So that was the cleaning exercise. He got all that data, then the little cleaning up with the tidy verse and certainly the per package is involved as well. But then how does the LM be able to ingest this? Because obviously, the pre generated, you know, training models that say open AI or anthropic claw to use definitely don't have this information.

This is definitely a novel development now, but typically speaking in this, workflow, you would somehow feed this data into a retrieval augmented generation or rag approach. Now there is a new package and the kind of AI ecosystem in our called Ragner that helps you create this more, readily available knowledge storage, so that the LOM can use this and query it for the information based on what the user asks. This is relatively new, but I remember, you know, my early days of exploring this, this seemed like a bit of a gap other than some real, you know, custom solutions. I've heard people like James Wade over at Dow talk about a couple years ago to kind of engineer this all yourself.

Ragnar is giving you a nice interface to do this all from the confines of r. So you got that knowledge store. So how do we let users use this? And that's where obviously shiny comes in because you can connect this with Elmer. Elmer is really becoming that hugely powerful engine to power a lot of this from the r side. And I dare say the more I use it, the more I'm really enjoying the experience. And while I have the the the audio mic here, an audio thanks to to Hadley Wickham himself who saw an issue I've filed on the Elmer GitHub about AWS bedrock support.

He turned it around in less than twenty four hours. So thank you, Hadley. I won't try to be spoiled about that in future PRs, but boy oh boy, that really makes my life easier at the day job. So Elmer combined with shiny chat is how this and shiny interface is powered here. Shiny chat is a great package that gives you that, you know, very familiar looking query prompt and then streaming the text results back in near real time as if you're on one of the other external services. So that would be great if you're just looking this locally. How do you share with people? So in this case, Sam published it on Posit Connect Cloud, a newer offering from Posit that has the, you know, the keys required for the authentication to open AI baked right in so the end user doesn't have to worry about this.

This is a part where I'm maybe say saying this for Sam's benefit. Obviously, this app probably is gonna take off a bit, especially in a month or so from now. Hopefully, it doesn't break your bank, but we'll see. I guess later on, hopefully, you're taking the token approach like we were advocating a few minutes ago. But nonetheless, obviously, you can deploy this in many different ways by Pawsit Connect Cloud. There certainly is a convenient way to make all that happen. So again, look at where we are now, folks.

A AI powered conference explorer where in the past, we would have, like, maybe say a shiny up to have, like, a dashboard of table of these, you know, conference talks and you can do filtering and whatnot. Yeah. Serves purpose at the time, but we know this is the way a lot of people are going to interact with this stuff. So she wanna find that one topic or that one domain. They wanna find it quickly. So I could definitely see people using this literally at the conference itself when they start to decide what talks they wanna go to next. So I'm I'm curious to see if it'll hold up. But overall, really impressed and credit to Sam for the awesome job on this and Isabelle for spreading the word.



[00:24:03] Mike Thomas:

Does this mean that filterable DT tables and reactable tables are a thing of the past, Eric?

[00:24:12] Eric Nantz:

Ask me another day, Mike. I'm not sure if I wanna talk about that now.

[00:24:16] Mike Thomas:

I think the answer is no. I agree with you. But this chatbot is awesome. And I gave it a shot, and I asked if there were any talks about building data science teams that use both R and Python. And, my talk is multilingual data science, essentially. So it's not like, you know, I'm not using all of the keywords perfectly in there. I wanted to see if it could figure it out. And my talk was the first one that came back. So I was very impressed with this. The response time was very fast, which made me immediately dive into the GitHub repository, which has been linked, I think, in the blog post as well as if you're on the app itself. There's a nice little hyperlink button on the left side of the app. And trying to figure out exactly how this goes on under the hood. Looks like there's a few different OpenAI models that take place, maybe one for, creating the vector database. It looks like there's some evals that Sam is doing, which is really cool to see how well it's performing.

And then obviously, the the call itself, to be able to retrieve the answers to the user's prompt essentially. So, really interesting. There's also a requirements dot txt file in here with a bunch of Python packages that says it was generated, by RS Connect Python. I couldn't find any Python code. So I'm trying to figure out if that's legacy or if this is truly a multi lingual project in and of itself. There's DuckDV going on on the back end, so I think it's a really cool repository as well. Sorry to get, you know, in the weeds here, but I think I think it's a great repository to be able to use in in future projects and take a look at sort of how all the different pieces came together between, you know, creating that vector database, having system prompts, working with Shiny chat and getting sort of everything to work together, including evals, which I feel like a lot of folks just sort of forget to care or have some sort of approach to evaluate the accuracy of your LLM model that you're creating. So awesome job. This is definitely a repository that I'm going to come back to quite a bit as well as hit this, especially as the conference comes closer, hit this app pretty hard, probably to help build my agenda, which I think is gonna be really cool. I imagine, as you mentioned, Eric, that there may be a lot of folks if this gains in popularity that will be hitting this pretty hard especially as it comes towards, September here.

And my hope is that that posit would be willing to split the bill here, but we'll see.

[00:26:59] Eric Nantz:

My guess is or if I had to guess, there's gotta be some kind of support here for this because this is gonna make their life a lot easier too to boot. Right? I mean, good grief. They didn't have to build this themselves. The community came through once again with one of these. It's it's it's amazing to see. And, yes, I I was, while you were talking there, see if they could pick up my talk because I just asked, can you tell me if there are any presentations about shiny bookmarkable state? And thank goodness mine didn't get picked up there for such a niche kind of topic. I found it very quickly. So I'm excited to see see what else I can and find in this in this exploration. But, yes, I am gonna geek out on the GitHub repo that Sam has put together because this is I'm sure you can attest to this, Mike. This is somewhere we are being asked time and time again.

How do we leverage tech like this in in apps that focus maybe solely on it or maybe there's a part of the app that focuses on having this tie in. So I think I'm gonna I'm gonna definitely check this out a bit because in my expirations with tying Elmer and AWS Bedrock, I just had a major breakthrough. And, again, thanks to Hadley for pushing that to the finish line. I think I'm gonna be starting to play with this a bit. But, hopefully, my, job security of doing, you know, fun interfaces with with filterable tables is still intact. I think it will be like you said, but, you know, we gotta give it the times too sometimes.



[00:28:23] Mike Thomas:

I hope so as well. Well, Eric, I just asked it, are there any podcast hosts who are presenting at the conference? Just to see what would happen. It says, yes, Eric Nance is a a podcast host presenting at Positconf twenty twenty five. Voice behind our podcast, and you co host the Our Weekly Highlights podcast. I was not mentioned, which just further goes to prove the correct the correct notion that you are the star of the show.

[00:28:51] Eric Nantz:

Oh, we don't wanna end on that. No. Did we? But, no, that's that's intriguing, nonetheless.

[00:28:57] Mike Thomas:

So How funny is that?

[00:29:00] Eric Nantz:

That is that that that that's a cute one. Yeah. You know what's next? Somehow, I've heard this on other podcasts. There will be some service that somehow takes my voice and Kabuki butchers it to do some random things. Like, it's happening, folks. It's just a matter of time. So please listen around there. Don't take that as a challenge. Somebody else is probably gonna do it. We're already there. We're already there. So yep. Well, on that, you know, rather, entertaining note, I think we I don't have much for follow after that. So we're gonna close-up shop here pretty quick here. We got, some aforementioned, things to put out our respective day jobs here. Getting back to the the weeds so to speak. But, you know, really fun issue to talk about today.

And, you know, much like what Sam demonstrated here, he from the community has helped the industry greatly. You all in the community can help our weekly greatly. You know how to do that. You can find the upcoming draft of this issue and if you found that great new resource, that new blog post, that new app, that great new package that's doing mind blowing things, you can send us a poll request with the template already provided for you to put that into the upcoming issue draft, and our curator for the week will be glad to review that and and most likely merge that in on the spot. So definitely value your contributions there. And we also value hearing from you and the community for all the various things that, you ever get wrong or things you like hearing about the show. We'd love to hear from you.

We have a contact form right in the episode show notes. Definitely take advantage of that. If you're just in your podcast, boy, you wanna quickly send some stuff to us. If you're on a modern podcast app, you can send us some fun little boost along the way as well. Details are in the show notes. And And if you wanna get in touch with us on social well, social media that is, you can find me on Blue Sky with at rpodcast@bsky.social. You can also find me on Mastodon with @rpodcastatpodcastindex.social.

And last but not least, you can find me on LinkedIn after you sift through all the AI generated fluff sometimes on there. You can find my me. Just search my name and you'll find me there causing all sorts of chaos. Mike, where can the listeners find you?

[00:31:15] Mike Thomas:

You can find me on blue sky at mike dash thomas dot b s k y dot social. You can find me on LinkedIn. If you search Ketchbrooke Analytics, k e t c h b r o o k, you could see what I'm up to lately. And although I am a business owner and a CEO, you will you will not find me at any Coldplay concerts.

[00:31:36] Eric Nantz:

Yeah. I I'm I'm I'm glad that wasn't your face that popped up on that when I saw that feed pop up. Yeah. Data science adjacent, though. Airflow. That was. Yes. That was. Yes. And, there were some ramifications as they might say. I that that that made the wave. So Mike and I are not that, that kind of person, so we are safe in our respect to homes here. But, we hope you are safe in your respect to home wherever you listen to this episode. We promise we won't cause you any additional chaos. That will conclude this episode of ROV highlights.

Thank you so much for listening from wherever you are, and we hopefully will be back with another episode of our weekly highlights next week."
"11","issue_2025_w_28_highlights",2025-07-12,47M 32S,"It's been far too long since our last episode of R Weekly Highlights, but we are finally back with episode 207! In this episode we learn about novel ways to automate fancy Quarto content, how we can be on our best behavior with behavior-driven-development, and finding that pesky portion of data breaking long data pipelines with a magical debugging…","[00:00:03] Eric Nantz:

Hello, friends. Oh my goodness. It's great to be back with episode 207 of the our weekly holidays podcast. And I do admit this layoff has been a lot longer than intended, but we're happy to be back here. And if you're new to the show, this is the what usually is the weekly podcast where we talk about the great resources that have been shared in the highlights section at rweekly.0rg, along so much more of our various adventures in the world of r and open source. My name is Eric Nance, and, yeah, as I said, the layoff was definitely longer than intended because a combination of company mandated vacation and, you know, other business to resolve, but I am back here keeping things together, at least somewhat, piece by piece. But keeping things together, I would never wanna do a, after a layoff an episode alone.

And back with a vengeance is my awesome cohost, Mike Thomas. Mike, how are you doing? And more importantly, how are you sounding today?

[00:01:00] Mike Thomas:

Sounding much better. And to be honest, Eric, when I hadn't heard from you for a couple weeks after my prior audio issues in our most recent podcast, I thought I might have been fired. So I am glad to hear that that's not the case and to have gotten back in touch last week and finally been able to get back on the mic with you this week. So I'm super excited. Hopefully, I remember how to do this.

[00:01:22] Eric Nantz:

You and me both. And, I was telling you in the preshow, you will you will know firsthand if you're getting the the pink slip, so to speak. But, no, you are you are very safe here. This is always one of my, makeshift therapy sessions every week after all the chaos I go through in my dev life. But nonetheless, we got lots of great things to talk about here. And as you well know, if you listen before our weekly is a volunteer effort and every week we have a rotating curator. And this past issue that we're talking about today has been curated by Rio Nakagorora, another one of our OG curators on the team. And as always, he had tremendous help from our fellow Arruki team members and contributors like all of you around the world. We've heard a poll request and other wonderful suggestions.

So in in the midst of this layoff, I have been, you know, put in my hand in various things, and one of them is definitely writing a lot of documentation and not just booting up a static word document. I always go to one of my favorite new tools in the ecosystem, and that is Quarto for generating scientific, you know, documentation that leverages R, even Python from time to time. And that's been really wonderful for me to keep saying a lot of the new capabilities I'm developing into. But where Quarto and Rmarkdown before that got their major, you might say, future and, you know, future use cases has been the world of data analysis to be able to make reproducible research. Right? So you don't have to copy and paste all those tables and graphs that you're making in r or Python or whatever else. It's all automatically inserted in.

And with the HTML format, you can do a lot of fun things with that format. I have been pushing the envelope quite a bit. I know, Mike, you have as well with your websites and other documentation you're making. But there are times that you wanna kinda merge those concepts together, really make a novel, you know, user experience for that web based report, but it's based on maybe different levels of data or different, you know, datasets, but they're all doing a similar analysis. So how can you kinda blend these two worlds together and take advantage of, you know, quarto and ours built in automation in play?

So our first highlight today is coming from Danielle Navarro who has been at the forefront of a lot of things and reproducible research, as well as some really awesome artwork and, you know, generative art, if you ever want to check her blog out for that. But in this post here, Danielle talks about a recent, you know, situation, which was, inspired by a data analysis, and they had to repeat many different types of plots or tables across these different partitions of the data. So in this example that Danielle comes up with here, they leverage the baby names, our package as a a fun way to look at all the different names out there to kind of look at how you would, look at the different permutations of the name that that they have used, throughout their life going from Dan, Danny, Danny, different spellings of Danny all the way to Daniella.

So little nice little, first off the bat, in quarto, they have created what's called a tab set of this visualization. Looks like a histogram of the years and the baby names dataset and the color coded by genders to, the frequency of that. But right off the bat in quarto, you get a nice tab based, you know, layout for each tab being a different name, and that is, a function that we'll get to in a little bit how how they built that. But that's great right there, and that doesn't stop with plots. You can do that with tables as well or even just print out the data.

And then this is where things get fancy. Maybe you don't wanna be limited just to the chordal tab set feature. You wanna construct this report and take advantage of some nice features in HTML, like putting text in the sidebar or the margin of the report. So there are some other functions you can create to leverage some more HTML features, like putting custom divs, which are like unique blocks of of HTML constructs, but they're uniquely identified where you could fit in some margin text with this. You could also fit on these nice little call out boxes in the margin itself if you wanna put it in this way. And throughout, there is an example function that has the contact, the content of this block of text, if you will, along with the class and any separators if there's multiple pieces of content.

And this is pretty interesting how you can do this, but how did how did Danielle pull this off? Well, they have created a new r package mostly scratching this own itch here called Quartos. I hope I'm pronouncing that right. But the motivation was, again, going to a reporting perspective, all these different groups are partitions of data. You don't wanna have to manually repeat these chunks that you're putting into this HTML content. So this package has a way via code chunks to automate and iterate through these many different groups or many rows and be able to put these HTML based content in dynamically and have it write the quartal syntax for you. So instead of you having the right, like the the three colons, the curly bracket, and then the ID or the the CSS attribute or whatever the construct is you're doing in quartile and then putting the text in manually, it's gonna use R to create these code chunks for you.

This is not a new concept in and of itself because I remember my early days of R Markdown and Knitter. I would very much use this same technique in my biomarker data reports where I might have 200 protein biomarkers. They're all doing a visualization or maybe a bolded list of key features of that biomarker. And I would use r and knitter to automate, say, making a bolded list of these key features and just dynamically put it into text via, you know, back then, just, you know, paste or now glue or things like that.

So this is basically taking that to Quarto, but now giving these additional nice HTML features that Quarto has, like these custom margin blocks, the tab sets, and putting it all in directly. Now Danielle is upfront at the end here saying not sure if that's gonna be useful to other people, but it's useful to them right now. And perhaps you might be able to learn something from it if you've been struggling with repeating similar analytical or data displays in your portal doc without having to manually type that in every time. I think there's a lot of potential here, and I may, definitely take inspiration from this for my next, data driven report that I make with Kortal.

And, no, it's not gonna be in Microsoft Word because HTML is the new hotness, although it shouldn't be that new to anybody listening to this show. Mike, what did you think about Daniel's exploration here?

[00:08:45] Mike Thomas:

Calling HTML, like, something new is, you know, as the language itself is is sort of funny, sort of silly, but I I totally agree with your sentiment. So we do a lot of reporting, and I I do a lot of reporting on a day to day basis. And lots of times, this involves sort of a traditional data analysis approach to maybe building the same chart, a time series, for example, for each independent variable in your dataset, if you're doing like a machine learning problem. And you could hard code this in your quarto document and have a section that says, you know, variable one, and then have your plot. And then, you know, do your two hashtag pound signs for your next section and just say variable two and show your next plot. But if you are trying to do something that's much more dynamic than that, if your dataset changes and you don't wanna have to change your code and those variable names or anything like that, a great trick that's existed sort of since the our markdown days and has been adopted in quarto is is this, output as is capability, which allows you to essentially put raw markdown HTML, in your chunk, and it will actually render that nicely in your report.

And you can wrap all of that in a for loop or take a list and stick it into a per statement. So I have to imagine imagine what's going on under the hood here in the Quartos package that Daniela has put together is quite a bit of that, probably. Some some output as is type of functionality, list type functionality, to be able to render these these lists in a dynamic way that spits out sort of multiple sections in your report or, you know, these multiple tab sets. And that's something that if you've ever done it, it's very powerful, but it does take a good amount of code to write to pull that off. And Right. I think what Danielle has done here is provide us with some really nice helper wrapper functions around that type of syntax to make this way easier than and way less verbose than what I, you know, have typically written in the past and what you may have typically written in the past to accomplish that same thing.

So very excited about this. I think it's gonna be a great helper set of tools and little functions. It's it's almost like an internal package that I would have loved to have written for myself if you've ever, you know, written yourself a toy package, maybe at the day job to, you know, do things that you do on a day to day basis, but you're not sure necessarily needs to see the light of crayon or anything like that. In this case, I'm very glad that Danielle put this out there in the world. I'm excited to use it.

Another thing I think while we're on, like, the per topic that I saw just as a tangent is I think, Charlie Gao has done quite bit of work, and there's a new release maybe today of Per that came out that allows for much more, parallel processing of Per. So use all those cores on your machine and see how fast it can go. But really excited to see this Quartos, our package and and to be able to start using it in my own reporting.

[00:12:00] Eric Nantz:

Yeah. There's a lot of wonderful concepts here at play that until you really realize and get tired of manually coding this up, you may not fully appreciate it, but I know I've been there in my early days of a data data scientist kind of says this role on the on the trials and these biomarkers. Yeah. I got so tired of having to manually code this up every single time. I would port over functions from, like, one report to another. And, yes, I should have ran a package out of that. But back then, I was like, at least I got something.

But, yeah, I can take a lot of inspiration from Cortos here. And then in the in the package of write up, Danielle notes that there are actually other packages in this space a little bit. One, I believe, is called Portable, by or Quartaps, I should say, by Suzuki Sasaki Usuki, if I'm pronouncing that right, as well as Q Report by doctor Frank Harrell himself who has been one of the more progressive adopters of knitter and r markdown in our quartiles. So there's definitely, an appetite for this kind of reusable and and very, very convenient functions to automate the process of entering these codes these code blocks or these output blocks, I should say, leveraging r itself. So great great resource here. And I'm definitely gonna dive into this, like I said, for my next reporting adventures. And, yeah, on the per topic, goodness gracious. I was an early adopter of the fur package when that first came out, and they've responded immense job to at least give me that that taste of multicore kind of goodness with per. But when now that Charlie Gao, who is now, working a positive full time, I sense big things for the world of iteration and take advantage of multiple, architectures for your, multiprocessing and and now, frankly, HPC components. So, watch this space. It's gonna get real fun real fast.

Now we're gonna go to a pretty big departure room as talked about, so more of a conceptual exercise, but one that I'm definitely curious about. But I might need to think about a bit more before I start adopting into my workflow. Let's let's talk through this here. Mike and I have spoken about there are different in previous episodes, there are different ways you can structure your development, you know, philosophies. One that gets a lot of attention in the DevOps kinda community is, especially when you're building, say, an application or a package that's gonna be used by lots of people, is making sure you're on top of, like, the testing paradigm alongside your development, and they call that test driven development. That has served a lot of people well. I have kind of hopped back and forth between that. There are times I should have done the testing sooner, and then I regret it. And I realize I got bolted on now, and I gotta, you know, get through that pretty quickly.

There is another spin on this that kinda goes a bit deeper than just testing in and of itself, and that is a concept of behavior driven development or BDD for short. And you can use this on a lot of different domains. But in the second highlight here, we're gonna talk about a way that this can apply to what could be either a package development or definitely in a Shiny app development. And this post has been authored by Jacob Sobolewski, who is a software engineer over at Appsilon. And he has been looking at this topic for quite some time. I'm we may have even covered a previous post that Jacob has written in this particular space. But if you've never heard about BDD or behavior driven development before, this post is for you because he's gonna walk you through just how this works in a in a very, accessible example here.

The idea around BDD is basically three major points, and this does have some parallels to the, you might say, famous or infamous agile philosophy. So don't turn off your podcast yet if you're sick of agile. We're not gonna we're not gonna beat you to the death over that. But the idea is that you still capture some kind of what's called a user story, which is meant to be a high level about what you want to accomplish. And in the case of the example we're gonna talk about here, one example might be that if you're a customer for a bookstore, you might say, I wanna be able to get a book, add it to my cart so I can buy it if it was on a website or even a brick or mortar store.

But then that's one thing to capture at a high level. What do you actually do with that? Well, you need to refine it somehow, and that becomes more specific examples about how would you actually fulfill this story. And then lastly, based on those examples, then you create what are called the specifications. What are the requirements? There are a lot of different terms throughout in development world. What are the rules or what are the criteria to actually accomplish this? So back to the storybook or the book example, you've got that general user story.

Now to refine this, you've got to look at a few different principles here. You need some context. And then when something occurs such as an event, then there should be some kind of result of that event. So if you want to be more verbose about this in the bookstore example, it's like you're in the bookstore, you walked in, you're gonna select the book such as, in this case, the the Hobbit, then you wanna add this to your cart, and then you should be able to see it in the cart. Now it sounds like I'm talking to a four year old about this, but that is the process of starting to break this out further.

Because in those statements, we didn't really say what kind of store this was. Right? It could be an online store like an Amazon type equivalent or it could be, to be really geeky, a a terminal application or, like I said, a physical place that you walk into. So then the next step, the final step is, okay, how do we get specifications from this? And that's where you start to really get into the nuts and bolts about how you pull this off and actually develop the software, in this case, to accompany this.

So in the case of this blog post, he's walking us through creating a new class, an r six class for this bookstore. And within it, there are certain methods that you're gonna have in this such as selecting the book, adding it to the cart, and then verifying what it actually includes. There's literally code that you can write to do this, and this is where the the stuff kind of flips in my head a little bit. He starts off with a test from a test that right off the bat, even though there is no actual code to do this yet, he's got the code in it of what he wants to accomplish.

No kidding. It's gonna fail when you first run this, but that's that's the that's the hook here. He's putting the code in a test before the code's actually been fleshed out. I still have to wrap my head around this a little bit, but I'm I'm keeping an open mind here. Once you've got the test established and what you wanna actually verify now, of course, you gotta actually flush out this bookstore class. That's where we start to see actual code here. And it's interesting where he puts this. He puts this in a setup chunk and test that. Not even his actual r library yet, for maybe an r folder or whatnot. Now he's got the bookstore r six class with a few different public methods on there. They're pretty much empty right now, but they're actually there. They got the skeleton of what he wants to accomplish.

And then the last step that to satisfy the specifications such as a function that selects a book you wanna get some result back with the book details, a function to actually add it to the the cart or the wherever you're putting these with a unique ID for that. And then lastly, returning the list of what's in the cart. So then you start to flesh out then in this class, or the specifications for this, just like boilerplate structures or functions to actually pull this off and then plug that into the test code.

So getting the specs documented somewhere in code and then putting that back in the original test script or the r six class, I should say, and then running the test again. And lo and behold, you've got it passing. Now that, again, I have never done it that approach before. I never started with a test with functions that don't actually work yet and then go kind of backwards from that. I will have to train myself on that if I do adopt this. It'll take a little getting used to. Now as I said, Jacob has looked in this space for a while because he's actually got an r package that may have made it to rweekly before called cucumber, which lets you kinda set up this BDD approach in a very, you know, very elegant syntax that looks very much like a YAML kind of structure.

And that's made in what's called a features text file, in this case, bookstore dot feature, where it's got the feature is called the bookstore then nested in that scenario. And then these different, like, ways of parsing the example use of that user story, the given, when, when, then statements. And then the cucumber package will somehow import that. I've never used it before, so I'm just looking at the at the blog post here. And it will basically help you turn that into test specifications or or testing functionality.

And then it's got its own test function to take advantage of that text file with the test that you put in test that, and it will give you the results if you if you pass or not. I will definitely need to sit down with this a bit. I think there's potential here. I'll have to figure out just which use case this best, benefits me. But maybe for a new project, I might give this a whirl and see what happens. I can definitely see some value in being deliberate early on with the general parts of this and then getting more specific later on. So definitely some good food for thought here, Mike, but I'm curious.

Have you even come close to developing things like this before?

[00:22:53] Mike Thomas:

No. I haven't. I'm still awful at taking the test driven development approach. I I'll just be straight up. I don't do it. I should. I should. It's the way to go. But I don't me both. You're you're among friends here. But this is really, interesting to me, and I'm I've tried to trace this down, like, all the way to the source a little bit. And it looks like there is this open source project that I believe is beyond, was it Jacob at, Epsilon who authored the blog post? I believe it's beyond that. And it's it's this open source project called Cucumber that lets you write automated tests in plain language.

And what, the the type of document, that dot feature document that we were talking about, the the Cucumber r package being able to, parse and take a look at. I guess it's called a Gherkin document, which is another word that I had never heard before, g h e r k I n. And it uses a set of special keywords to give structure and meaning to executable specifications. This is right from this documentation on the open source Cucumber project, website. But this is a a massive project. It it seems like that I just happen to have never come across before. We can put it in the show notes. It's cucumber.i cucumber.io.

And this is essentially, I believe, this open source project is what led, Jacob to take the initiative to create the Cucumber R package, and that's why it's named Cucumber. But it's it's pretty interesting that we're able to parse something that, as you said, Eric, looks much more like a YAML file into essentially a set of unit tests that are very, you know, sort of stepwise driven development, feature driven development. It lends itself to a shiny context, I feel like, in a lot of ways. Right? And when I was looking at the the r six code that Jacob had in the blog post, it made me feel like, okay, we're almost in a a shiny setting, and I know he works at Absalon that does a lot of shiny development work. So I imagine that all ties together well there. But if there is a world where we can make unit testing more accessible to people who prob the percentage of, you know, data analysts out there that may not even know about unit testing or have ever used Test That, which is probably nonzero, unfortunately.

But if there's a way where we can make that more accessible to folks, you know, through this Cucumber framework or YAML specifications. And, also, this is something that these these dot feature files, these Gherkin documents, this is something that I could bring to, like, a business analyst who has no Exactly. Concept of of coding or anything like that. I don't wanna bring them my test that script. Right? That's gonna look like Wingdings to them. Much for how they bring them something like this. And if I can not only bring this to them to review and edit, but then not have to take the step of, okay, here's the plain English version of it. Now I need to translate it into, you know, our code. If this is already done for me, that's one less step in the process that can really streamline your your testing workflows, which are are not trivial. Right? I think as much as we probably don't want to admit it, AI nowadays in the chat GPTs of the world have made things like, you know, Roxxon documentation or, docstrings in Python or unit testing a little easier for us, which is awesome.

And and I think this maybe sort of piggybacks on that in a way that can make these types of things that have been in the past sort of tedious for us to do, the things that we put off till the last minute, just make it make it easier for us to to get done, and and wrap into our workflows, into our our packages, and things like that. So this has opened up a whole new world for me, this Cucumber framework, and I I'm really interested to dive into it further.

[00:26:59] Eric Nantz:

Yeah. I I remember I tried something kinda similar to this. There is, a great there's a excellent book that's been offered by Martin Frigard, on building shiny apps as our packages. And he talks about, one of our favorites, Golem, of course, amongst others. But he does have a section around this specification driven kind of development with developing your test. So it is kinda like a hybrid into this. So it's not the first time I've I've seen something like this in action. But, yeah, the Cucumber framework, it the I've never seen anything like this, indefinitely, treated like this before. So I do have a potential project that I've done a lot of you and I talked about this before.

A lot of times in our Shiny apps, we'll do the business logic testing via separate r package or just test that with the functions that don't depend on Shiny. And I've got some of those established in, but for more of the higher the higher level things, I might take a crack at a cucumber and see how that might help me out, especially in the case of it's not just testing the business logic. It's testing what you might call that end to end usage of the app. Or in the past, yeah, I would work with an analyst or somebody on our team. Be like, I've got this set of steps.

Just take a half hour, run through it. Let me know if it works. Nobody wants to do that anymore. So if we can automate that side of it with a combo of Cucumber and then maybe some frameworks like shiny test two or playwright, there there's lots of opportunities to take this into into future directions on that cohesive user experience. So I'm there are some lot of great examples on the our package, Cucumber site, which, again, we'll link to in the notes. So I would say, definitely, if you're in a collaborative environment and you need some advice from either just your end users or fellow developers about getting mired into the language syntax of how the tests are done, I can definitely see how this, Cucumber approach, can really help here.

And last but certainly not least in our, highlights episode today, it's, it's a rite of passage, folks, showing you doing a lots of data. Everything looks great for the first few rows, maybe in the first 100 rows. And it's just really deep in that large data set. There is just one partition that just breaks everything apart. You may be using that per package you just referenced to do a lot of iteration or just straight up looping for that matter, and you just don't know why it failed at that particular time.

If you're like me in the past, you would say, okay. I'll try to trace back or do print statements of, like, which variable was processing. And then when it gets to the one that fails, I'll try to subset the data manually for that particular one that failed, try to run stuff manually to see what broke or whatnot. That might be good for smaller cases, but when you have a large data set and lots of functions that you're running, that's not scalable. You need, you need you need some help, so to speak, to make finding that problem a lot easier.

Well, our our last highlight today, it comes from a very authoritative source in this space of novel data science with r. It is authored by foe our former r wicked curator, Miles McBain, who has been, of course, one of the forefront pioneers of, you know, you know, novel data analysis. Also has been an early adopter of the targets package. He's been one of the main vocal advocates of targets. But in this post here, he calls it diving into the hunt. And when you say hunt, it is very much like trying to find a needle in a haystack here.

So the situation was he had about a large dataset or large sets of datasets, maybe more than one. And there's a lot of steps, so he's got the example here, that look like something you might write in an interactive notebook fashion, such as like a Jupyter notebook or like a chordal document or a markdown document. You're making new variables, making new variables based on those previously made variables, you're transforming them, and then you're more transformations. It's like a step by step thing, but he said imagine doing that for thousands of lines of stuff like this split across different, files entirely.

And he does make the analogy that this might look familiar to somebody either a as an interactive data analysis or somebody that's a wizard in Microsoft Excel and has a whole bunch of formulas in each row of that of that table that build upon each other or build upon columns and stuff. Very much a literal step by step by step, you know, representation of that data flow. Well, that's great for them. But if you're if you're inheriting this kind of code and you encounter one of these issues, how do you actually figure this out?

Well, in one, there is some kind of grouping. Right? And might be just the row of your data or it might be a group of rows. Well, let's say row for the sake of simplicity here. There are derivations being made on each row. And how do you figure out, once you know there's a problem, how to get to that environment where the problem resides. So in essence, he kinda wants a way at a high level to zoom in literally on that set of data or part of the data, that row that's causing the problem, but do it in a way that's native to the environment that in this case, the tidyverse kind of packages like dply are operating on.

Here is the trick that Miles has come up with, the, simple function he's written called dive. I say simple because there's only two lines of code, but I don't think it was simple how he got there, not in the least. Goodness gracious. So here's the first step. Some I don't take a much advantage of is that you can take a list of things in r, and you can literally transform it to an environment. Mind blown moment number one. Number two, if you're familiar with the r constructs of objects, a data frame is a special type of list.

So with this first line converting the list of the data frame to the environment or an environment object. And then I so can't wrap my head around this. Within a low a function called local, which I believe means you're can pertaining to that function's environment, you run the browser function, which I use every single day in my debugging. But I'll just put the browser statement in, like, the top of a reactive or the top of, like, a table output or whatnot, and just literally just do the debugger from there.

But you can feed in in this local function, not just the name of the function you wanna run, but the environment you wanna run it in. So this second argument to local is this object of the data frame environment. So now he can simply put all this pipeable code, but in the end, pipe that to the die function. So that and that's after filtering for that specific role or ID that he knows has a problem. So once you isolate where the problem is, use your familiar piping syntax, put it into this dive call. And then when the browser kicks in, you're not in your global environment, You're in the data frames environment. And that's where you can do some really nice diagnostics about like trying those mutations that you have and those mutate calls, really getting to know what those variables are representing so that you've really taken away another source of maybe discrepancy or variation when your debugging environment isn't quite the same as what's in that pipeline environment.

This takes care of that. He says he's now added this to his dot r profile, so he has it wherever he is in his data analysis needs. And he doesn't really have to rewrite anything substantial other than finding where the problem is and piping it to this dive function. Oh my goodness. Could I have used this about ten or twelve years ago when you're analyzing 55,000 genetic markers, and there's just that one that's out of domain range, and you just don't know why this function would have been hugely helpful for that. So definitely take a look at this post. It's very short to the point, but it it is very relatable based on my past experience and my debugging adventures with complicated datasets. So credit to Miles for once again blowing my mind. Well, it looks like a simple function, but it wasn't simple how we got there. So I'm gonna be adding this to my R profile for sure. Mike, how about you?



[00:36:40] Mike Thomas:

This is wizardry at its finest, and no surprise that it's coming from Miles. But if this is, I guess, sort of exploitation of R's ability to have multiple environments. Right? And that idea. And I would say if you are a little earlier on in your R journey, if you don't know about the browser function, please learn about the browser function and how to use the the debugger in R. It can be life changing in terms of being able to much more quickly diagnose and address issues than if you're trying to do that without the browser function or the debugger.

But this is, as I mentioned, you know, sort of combining that debugging concept and capabilities with multi environment specifications. And as you said, it's a simple, quote, unquote, simple function, a two liner, but there's a lot going on here and a lot of power. And this is one of the coolest tricks that I think I've seen in a long time in any programming language. So I'm absolutely gonna add this to my arsenal and my repertoire as well. Probably stick it right in my dot r profile as as you mentioned, and I think this will be really handy for a lot of use cases that we have.

I have seen so much code like the example code that, Miles described at the beginning where everything is very flat, not very well modularized. There's really no no function extraction or anything like that, and you're just sort of creating data frames and overriding them as you you do your next step. Not a lot of chain assignment going on. I have been critiqued in the past, and our team has been as well, in some of, like, the model validations or the code reviews that we do for for caring too much about software development best practices.

And the code works. So, we just wanna make sure that the, you know, the the correct inputs are getting converted to the the correct outputs, essentially, and that there's no bugs in the logic. And, technically, there's no bugs in the logic, sure, but you've got a lot of fragile code hanging around here, and potentially the next person that steps in is gonna have no idea how to address it, especially when something either breaks or you're asked for that inevitable enhancement. And it's tricky to know exactly where to apply that.

And anyways but I digress. I I can just very much relate to, Miles' sort of story here that set off this blog post in his code review of some some tricky code and trying to find a needle in a haystack, and what a useful little function he's put together there that for those similar use cases that I face on a fairly, unfortunately, consistent basis, he was able to drum up something that makes his life easier and it's certainly gonna make mine easier in the future as well. So much thanks to Miles, not only for the blog post, but for the handy trick function.



[00:39:43] Eric Nantz:

I couldn't say it better myself. Two additional thoughts on that. First, as you alluded to, our comes of a lot of these great features built in. This is not something you had to add as an extra package. Right? The environment conversion, the, you know, building the run functions and specified environments, the browser function. I have not met any other language that comes with this so elegantly, readily available off the bat. I'll stand on my soapbox for that. Yes. Hot yeah. Get to get the hot take out. I'm I'm not done yet either.

While we were away in our little hiatus, you probably saw this, Mike. There was some chatter about a very provocative post that alluded to r is on its last days. No. It's not. I can tell you for sure in my industry is most definitely not. And how do I well, so I'm not gonna pretend that life sciences dominates everything. And in data science, no, it doesn't. But when you look at robust development of capabilities that do have a very important need and need to stand the the test of production usage and not just have been our company's firewalls, but also to our health authorities or whatnot.

Yes. We are seeing some promising avenues from Python, and no shade on the Python listeners out there. But in the world of statistics, data science, r has a leg up on a lot of these things, and it's gonna be hard to close that gap anytime soon. And I and but, again, it's not even either or thing. But I do not feel that we're in any decline here, so to speak, even with the world of AI taken off. We got Elmer, folks. We got Elmer to help us on the our side with that. And I've heard people like Hadley and Joe say that the Elmer interface is just fun to use. I get it's not like you don't feel like you're making a compromise going to r for this stuff compared to Python. It is an elegant interface because of our extensibility of the class system and whatnot.

We can do a lot of interesting tricks. So, like, Miles opens up this post here. You got so much of this available. You can tackle almost anything with it. It's just the limits of your imagination, and I I I fully agree with him. In fact, you're you're calling this kind of wizardry post here. He actually gave a talk at, previous RStudio Conf, like, the magic of open source. Right? So he is definitely our our expert wizard here in the world of of ours. So really, really fun post here. And, yeah, those those reports of ours, demise, I'm gonna say are greatly exaggerated.

Hot take over.

[00:42:39] Mike Thomas:

I couldn't agree more. I think it boils down. That was ridiculous. I saw that as well. I'm glad that you called it out. I saw it all over LinkedIn. And I think what has clearly happened is that the great things about R folks have been trying to port over to Python, and the great things about Python folks have been trying to port into R as well. So it's very much a use whatever one you want, but don't trash on the other language just because you don't like it as much. You don't need to do that. There's no there's no need. So I'm gonna shameless plug while we're still on the the subject. Can I shameless plug?

You got it. It's gonna be a talk at Positconf this year given by yours truly titled Yeah. Building and Managing Multilingual Data Science Teams. So I am a big proponent of of both. It doesn't necessarily have to be only one or the other. They both have their strengths. They both have their weaknesses. But we don't need to trash on one side or the other.

[00:43:40] Eric Nantz:

That isn't a great teaser for to come to Positiv to watch Mike's talk. I don't know what is. So, yeah, I can't wait to to be hopefully in the front row for that one when I'm over there. It is is very much the state of the world we're living and not in our dev lives. Right? So, I mean, I'm I'm got some colleagues on the on the Python side doing some great work, and then I'm integrating this in my R workflows, and it's all it's all working well. But, yeah, like Miles says here, we got a lot at our fingertips here.

You may, in your R journey, encounter some hairy situations, whether it's in the data analysis, the Shiny app side, package development side, chances are there is a way to get out of those bugs. And, yeah, you may use AI for that. You may not need to, like, in this in this blog post. Sometimes the tried and true is the best way to go. Speaking of the best way to go, we have lots of other content here in ourweekly.org. We invite you to check out the rest of the issue. We're running a little low on time, so we won't do our additional fines here. But if you're new to the website, it's got a great set of sections, all clearly labeled, whether it's package updates, tutorials, events in the community, lots of great things that are happening in this in this ecosystem and our weekly, through I don't remember how many years it's been since we've started running this this project as, stood the test of time as the truly open and community driven way of giving you this great content.

And no no bots are powering this. No organization is overseeing all this. This is all driven by us passionate, you know, advocates of our and data science. So that's why I'm a little, mini soapbox number two, but nonetheless, we definitely invite you to get in touch with us. If you have some interesting takes you wanna share with us, I, we always welcome all opinions from all sectors, and you can get in touch with us multiple ways. First of which, in our podcast episode show notes, we got a little contact form. Feel free to send us a note there.

And also we got our our availability on social media these days. I have been a bit quiet, but I'm hoping to get back into it. I am available on Blue Sky with @rpodcastatbsky.social. I am also on, Mastodon with @rpodcastatpodcastindex.social, and I'm on the aforementioned LinkedIn. I try to stay away from the clickbait stuff. I just post some relevant stuff. I search my name, and you'll find me there. And, Mike, where can listeners find you?

[00:46:13] Mike Thomas:

Likewise, it's been a bit quiet, but certainly, if it's the podcast today is any indication, hoping to get back, back out there a little bit. And you can find me on blue sky at mike dash thomas dot b s k y dot social, or you can find me on LinkedIn if you search Catchbrook Analytics, k e t c h b r o o k, you can see what I'm up to.

[00:46:34] Eric Nantz:

And I believe I saw you on a, golf course recently. I was jealous already because I wanna get my golf game back on. Maybe someday, we'll we'll hit around the nine or eighteen, and you'll poem me to death with your skills, but we'll have fun. That will not happen. But we can certainly golf. Yeah. I I'm I'm gonna enjoy that at the best of them. But, nonetheless, we're gonna we're gonna sink the eighteenth putt here and close out the shop here for our weekly highlights, for this week. And hopefully, things are back to normal. Of course, you never know with the way life goes, especially in the summer.

Shout out to all of you that have to shuffle kids around day, you know, summer camps and whatnot that can wreck havoc on schedules. I'll do my best to wrangle that. So we'll close-up shop here. Thank you so much for listening to this episode 207 of our rookie highlights, and we hopefully will be back with another episode of Arroyuki highlights next week."
"12","issue_2025_w_24_highlights",2025-06-13,42M 21S,"The summer schedule has been crazy, but we finally have a new episode of R Weekly Highlights! In this episode: How the new shiny2docker package eases your entry to the world of containers, the power of WebAssembly in full ggplot2 glory, and how the latest solution for speeding up R code draws upon a classic computing language you may not…","[00:00:03] Eric Nantz:

Hello, friends. Yeah. It's been a minute, but we are finally back with another episode. In this case, episode 206 of the Our Wicked Highlights podcast. We have been off for a bit, but this is the show where we talk about the excellent highlights that have been shared as well as some additional things as time permits in this week's Our Weekly Issue. My name is Eric Nance, and hopefully, you didn't forget my voice. I feel like it's been a bit since we last were able to speak to you all and your favorite podcast players.

I've had, definitely a whirlwind of a few weeks, and I just would say a note of future self agreeing to do three conferences in a matter of two weeks may not be the best for your, your stability, but I got through it. And the last one was especially fun. But, nonetheless, I am here. But I am not alone. Thankfully, who's able to carve up at a time today after a bit of chaos is my awesome cohost, Mike Thomas. Mike, how are you doing today? Doing pretty well, Eric. Apologies to the listeners. We'll probably sound a little bit different, today. I'm

[00:01:04] Mike Thomas:

using, unfortunately, my earbud headphones to record instead of my typical microphone, which has officially bit the radish. And I think this is one of the byproducts of having not recorded for a couple weeks as I decided to test out my microphone, about five minutes before we jumped on and realized that it is officially out of commission. So I will be hitting up Amazon very shortly

[00:01:26] Eric Nantz:

and should sound better next week. No. We we know how it goes. And just like with infrastructure, we've been spinning up. Sometimes when it fails, it fails at the most inoperative times, and we ought to put those, put those things out. Well, great for you. It's an Amazon, click away. Some other times, there's a lot of blood, sweat, and tears, so to speak. We're getting everything back set up. I've been down that road too, especially in my Docker adventures literally right before I record here, but I think I got it solved.

I anybody that runs Docker on Windows, we're gonna talk about this a little bit. You definitely have my sympathies. Man, WSL is a thing, to deal with. Anyway, I digress. I tend to do that from time to time. So let's look at the notes here. It has been a minute, Mike. We always do our, prep show notes together. Let's look at the curator here. Well, of course, it would be. Whenever I have so many things going on, usually, that's the time when I have to curate an issue. But thank goodness, I was able to get one out there, and we got a lot of great selections to talk about. So this is a pretty easy one for me to get out to the door, but as always, I could never do it alone. Had tremendous help from our fellow, our rookie team members, and contributors like all of you With in this case, I think it was a whopping five or six poll requests that had some great resources that has been shared in this week's issue. So without further ado, let's get right into it. I already teased it out. Right?

There is a novel technology that's, you know, hit the software development landscape for the last almost like fifteen years it seems like, and that is the world of container technology. The way that you can encapsulate not just a quote unquote application or a library, but its own system dependencies and what you might say is a cross between a full blown virtual machine on your system any more fit for purpose architecture. In this space, the Docker, runtime for containers has been what usually gets the most attention.

It's, had the most mindshare, you might say, but it's not the only game in town, of course. There are some other ones like Podman, which has a lot of compatibility with Docker. And if you wanna get really low level on Linux, you can use what's called LXC for those kind of containers. As an R user, you may be wondering what errors what are some of the best use cases for this? And I still stand by what I'm about to say is that in the context of shiny development, containers are a massive help. It opens up a lot of doors, a lot of possibilities for I can scale this to use those cliches, and also more transparently, actually where you host it. There are a lot of options available to you.

You may be new to Docker and you're wondering, oh, gosh. I have to be a Linux expert to get this started as an R user. Not necessarily. You can grow into it because our highlight is talking about a fantastic package to kinda ease your way into the world of Docker, especially in the context of Shiny development. And it comes to us as the package called Shiny two Docker with the number two in the middle. And we have a great blog post from the team at think r, in particular, Vincent Guider, who I believe is a coauthor of this package to introduce Shiny to Docker and give us the use case of just why it's important and in the mechanics of how it works.

The team at ThinkR has been using Docker for quite a bit. We've I've had multiple conversations with the esteemed Colin Faye, also a fellow curator on the art weekly team on his container adventures, and, of course, web assembly adventures on top of that. But Docker has been a key focus in one of Mike and I's, favorite packages, Golem, to help with those deployment concerns. And I'm gonna I'm gonna say it again because I can't resist. Once you go, Golem, you never go back. Waste of my humble opinion. I'm getting a virtual applause from Mike here, so I'm not too off base here. So they've always had hooks to use Docker, to help generate a Docker file for probably past year or so. But now they've encapsulated that functionality into a standalone package, hence Shiny to Docker.

So what does it get for you on on the tin, so to speak? You get to have what's called the Dockerfile, which in essence is the instructions that you give the Docker runtime to build your, what you might call image or application based image. Shiny the Docker is gonna help automate that process to create it for you. It's also gonna interact nicely with RM for rev. We're out of the box. It's actually if you don't have one already, it's gonna generate an RM lock file and detect what versions of packages you are using on that application library and boost strap that for you to ensure that reproducibility of the package version.

I do have additional thoughts of that towards the end. I'll save it then, but what's also nice is a shiny to Docker in the role of the CICD, especially with GitHub actions, get lab runners and whatnot. It's going to help you create either a GitLab CI configuration or a Git GitHub actions workflow YAML file, which is wonderful to not only build a container on that infrastructure, but also they'll push it to a registry of sorts for these container images. The most prevalent one being the Docker hub as one of those.

But you could also choose additional repositories as well. And that is a huge deal because then with that image in place, you can then choose where you deploy that. So how do you get started? Well, of course, you're gonna install the package with the typical install dot packages because it is on CRAN as a record. So you can get it straight from CRAN or our universe or other places like that. And then the rest of the blog post gives us kind of a case study for preparing a relatively simple shiny application, where it's in a simple app dot r, very much the hello world type geyser, oh, faithful, I should say, histogram that we all know well very well.

And then once you have that encapsulated, it's time to generate the Dockerfile. It's simply a call to shiny to Docker. Give it the path to where your app dot r is or rest of your Shiny files, and then it kinda takes care of the rest in a very use this like fashion. Gonna give you the Dockerfile ready to go. A special text file called dot Docker ignore, which if you're familiar version control with Git, you can have a dot git ignore to tell Git don't touch that file. Environment variable file say hello. Don't ask me how I know.

But you can have the same thing with Docker ignore. There may be some things that your pack your application has that just were more for development. You don't already want that in your Docker image because you do wanna keep the size in a reasonable image size because that's gonna, you know, potentially long your download times when you actually call this to run. And then with throughout the step, it's got a lot of nice kinda air, you know, checking. Like I said, it's bootstrapping RM file of the box for you to help with getting that lock file, and then it's gonna give in that Dockerfile the command to actually run the application using the Shiny run app function, which we typically put, like I said, at the end of these Docker files, these instructions.

And then once you have that in place, then you can step back to Docker itself, run the assuming you have Docker on your system, which again is a prerequisite. I do wanna say, in my humble opinion, working with Docker is much easier on Linux than some of the other operating systems, whoever Mac or Windows. But I feel especially, sympathy to the Windows users because you do need the Windows subsystem for Linux version two of that in particular to run Docker on your system. Trust me when I say that can be finicky depending on your corporate environment.

So best of luck with that.

[00:09:49] Mike Thomas:

Mike might have something to say about that. I'd say Casper can help. We've done that a lot. But, yes, you're exactly right. Once you get WSL two installed, I would just highly recommend installing Docker Desktop to automatically install the Docker engine and give you a nice UI into all of your running containers and images as well.

[00:10:08] Eric Nantz:

Right on. It's exactly what we are recommending. Some of you may be aware as a side tangent, but I am part of the submissions working group under Doctor consortium. Literally, as I record tomorrow, I am set to transfer the next version of that pilot with the Docker version of a Shiny app. And, yes, you better believe in my instructions for the reviewers, we have install WSL and then install Docker Desktop. We need the easiest way for them to get containers running. So, yes, as a a very, on the point recommendation. So once you have that in place, you're gonna build the image with docker build. You can give it a what's called a tag, like a little label for that, and it's gonna inspect that docker file and then let the magic run, so to speak. Build it where by where. And then once you have that image in place, then you can use that at any point on your system with the docker run command.

Give it a port that you want your Shiny app to be exposed on. Give it a name if you wish, the name of that image afterwards, and then you're gonna get a process in the terminal to say, okay. Your app is running on local host or, you know, port whatever. Then you can browse to that port. Typically, you can map that to port 80 if you wanna make it super simple, but you can choose any port really at that point. And then you've got yourself a Shiny app. It should work just as if you were running this in your R environment, like in our studio or a positron or or whatnot, it should look just like that. That's the whole idea.

It's just another way to execute your application, but your application source code is the same. So the post concludes with some nice kinda practical tips to give you the most smooth process to get yourself in this container journey. One yeah. As you're iterating, I'm sure you're gonna have, like, cruft in your app directory as you're tripping stuff. Once you're done, like, you've got a stable thing, try to keep that relatively clean. And when necessary, add things or add directory names or file names to the docker. Ignore if they're really not meant to be used by by the container.

They do recommend, and I actually I'll hardly agree in this context. RM from the start just outside of this Docker file or shiny to Docker pipeline can be helpful. Although although, I just gave a talk at our medicine about how I think Nyx honestly can be an even more attractive alternative to this. Mike someday will be converted to that. We're not there yet. But I got proven, in multiple projects now. Next can also fit nicely with Docker as well, where you can have the next help of your development libraries for the app and even at the system for that matter. And Docker, instead of using r and r m to boost wrap that, You can just boost wrap that same next recipe if you will. You'll get that same version of packages.

Myself and Bruno Rodriguez will be doing our talks at our medicine. Hopefully, the recordings will be out soon. We think in Shiny, there's a there's a pathway there. So I digress. Other great things to think about, definitely look at things locally before you, you know, rely on CICD to help with a lot of this. Trust me when I say where things can happen in your CICD scripts or it's the YAML instructions that you give it or like me or go hunting for punishment and put some bash shells shell scripts to automate creation of files. You may just mistype one path and it just all goes haywire.

Try to test that locally You'll you'll thank yourself later. Also, you wanna make sure that you're thinking about security in mind. You don't wanna put a lot of sensitive credentials in these images just in case. Hopefully, there's ways around that in the future via environment variables in the platform you choose to deploy this to. Many of them support that where you don't have to put it in the code, so to speak. And, yes, this is your gateway to Docker, but it does help to have a little bit of background into what's happening in, like, that Docker file, set of images.

How do you get system dependencies inside? Where does our fit in the picture? It does take a bit of getting used to. I won't pretend I have everything figured out. But knowing the way the layer system works, you wanna put the things that are most frequent to change towards the end, not towards the beginning. Because once you change something, everything below that step will have to be recompiled anyway. So if you think about it, if your app is really iterating, you wanna put references about the app code as far down that Docker file as possible, which, again, Shiny, the Docker is gonna help you with that step and then keep the package installation stuff towards the top. That way, that that's pretty stable. You just iterate on the code. It'll be much less time to compile these Docker images than if you had the app stuff at the beginning, and then you're, like, doing all the package installation.

Yes. That can be, it's like watching paint dry sometimes, so you just gotta be prepared for that. In the end, very nice, you know, gateway as a shiny developer to get into containers. In my humble opinion, I think this can also work nicely with Nick's. Maybe I'll talk for the authors of this in the future by having an extension of sorts. With that, with Ricks, who knows? The ideas are out there. But, Mike, you and I are are big fans of containers. What did you think about Vincent's post here? Yeah. We try to containerize

[00:15:57] Mike Thomas:

everything, especially Shiny apps. I really like this post and the introduction into the Shiny two Docker package that seems to utilize, heavily the Docker filer package under the hood, which I have been previously familiar with because that's a dependency of Gollum. Gollum has some functions, I believe, in the dev folder, the o three deploy script for those, Gollum developers out there that allow you to create a Dockerfile for, you know, some of the different, sort of end places where you might be landing your Shiny apps. And it's a one liner that does a lot of this type of thing behind the scenes, and it looks like Shiny to Docker kind of extends this a little bit.

Some of the packages I think that really help in this process. One of my favorites is the attachment package. And I know Shiny2Docker leverages that. And there's this function from attachment called create RN for fraud that Shiny to Docker uses under the hood, which can actually, I believe, sort of ensure that all necessary R packages for your app are accounted for in the Docker image via your RNs dot lock file. And one thing that I will mention is getting, you know, r n's and Docker and everything to to play nicely can be a little tricky, especially when it comes to sort of updating your app and updating packages in your app. But once you get the hang of it, and I think thanks to some of these other helper functions, it can be a huge, lifesaver, especially around dependency management and making sure that what works on your machine works in production.

And I will definitely echo your statement, Eric, about making sure that you test these things as you create a Docker image and and run a container before you just throw it at your CICD process. We've seen a lot of folks who will do that and they'll test things, but then they'll make like one more final little change, to their app before they create the pull request that kicks off the CICD process and they don't retest because they really don't think that's going to actually affect, anything else. But, of course, it it does and that CICD breaks and they you know, everybody has to have a meeting and a conversation around how are we gonna fix this. And if you before you you push things out to that CICD process, if you promise yourself to test it which means that you're gonna run a Docker run command locally, you will cut down on many of those meetings that I have sat in before, to hand off sort of the app locally to production and the DevOps team that's going to actually stand that up. So that's my word of wisdom here.

I know, Eric, that you are a big fan of Nix as a potential replacement for our end. I probably shouldn't even say this, but we build a lot of shiny apps that don't even leverage our end, but use the the r two u, image from Dirk butyl, which just grabs the latest version of all of the r packages that your app depends on. Obviously, the trade off there is is full reproducibility. If you only have a couple dependencies and if you're actively maintaining that app on a day to day basis, then I think it's okay to make the argument to leverage that approach.

It's it's just sort of a choice on our end. But if full reproducibility is really important to you, then leveraging our end can can definitely be the way to go. One of the interesting things that I found is that, Shiny to Docker creates this Docker file that typically uses a base Docker image, called Rocker Geospatial. So it's the geospatial image, from the Rocker project, which comes with R Shiny, and I have to imagine a lot of geospatial levers as well. So I believe that you could sort of customize this after the fact. Right? Because we're just creating this Docker filer object that we can then edit. So it might be a good idea to look into that and determine if that image is the base image for you, especially if you have some constraints around size of Docker images that you're allowed to have and that your DevOps team is sort of expecting you to hand over the wall to them. I know that this this geospatial, image from the Rocker project is already over one and a half gigs by itself.

We tend to create a lot of apps that are actually just around one gig. Again, we we like to use that R2U image, pretty often, which keeps things quite small. So if you do have performance limitations like that, it just might be a consideration. But in a lot of circumstances, that might work perfectly for you as well. I think one of the interesting things that you may not know is is tools like Posit Connect and shinyapps.io. They are doing this behind the scenes, their own person of this. So when you submit your code to either one of those services, they're going to build a a Docker container and they're going to use some sort of workflow that they have. I don't know if it uses Docker filer, if it uses Shiny two Docker, but they're going to try to essentially parse your code, scan scan it and take a look at all the dependencies that you have and what's gonna be needed from an R package dependency standpoint, a system package dependency standpoint, a version of R, and I imagine the operating system standpoint as well. And they're gonna try to do their best job of putting that Docker file together and doing this for you. So I think this is sort of the next step to try to take this in your own hands. I think it can be a really helpful introduction to Docker for those who are not necessarily familiar with it. And as you continue to use this workflow, maybe you'll start to also get comfortable with taking a look at the Dockerfile that gets generated, seeing if there's optimizations or enhancements that you can potentially make to fit what you're trying to do a little bit better than what is trying to be automated behind the scenes. But this is a great gateway project as you mentioned and really excited to see it. Yeah. There when you think about building it locally, sometimes you're not as concerned about these

[00:22:21] Eric Nantz:

other issues you identify, like the size or the base image you're basing off of. Sometimes you just want all the things right away, make your development easier. But, yeah, once you narrow down the hosting platform you're gonna throw this on, a, totally agree. The more you can run this locally and iron out any issues before you throw it over there, the better off your life is, whatever CICD or, like I said, just applying to these cloud based platforms. Because if you can narrow down let's say something happens on the hosting platform and you you're convinced that your app works.

You can show your IT support that, guess what, running this on Docker locally, whether on my own machine or what I had to do recently. Some may know that GitHub has the product called Codespaces where you can, you know, boot up what it in essence is. I containerize environment for for Versus code. I would install my Docker image on there. Pull it down, run the app, and make sure it works there. And that's obviously not the biggest, you know, resource footprint that it has available. So you verified it works in these other situations. And if your hosting platform is still not working, then you've got some stuff to tell IT. It's like, I did my homework. It's working on these things.

This literally came from personal experiences or experimenting with Kubernetes stuff at the moment, and I was driving my myself batty because it was working fine in these other instances, but not there. But now we've got some things where you can troubleshoot. So that's more in the trenches, situation here. I think the big picture is the container technology, great for a lot of reproducibility. It also just gives you a lot of options for where you put this that you don't necessarily have if you just keep the app as is. We didn't even mention there's another platform that you work with a lot, Mike, called ShinyProxy that also is very heavily based in the container footprint, as well. You bring the app as an image there, docker image or a container image, if you will. They have different options even in that orchestration engine. So there's a lot out there. You You give yourself a lot of possibilities.

I've actually talked to the positive folks about, well, it's, you know, positive connect. Cloud is great. If you could give me, like, a way I can bring bring my own container, oh, we're golden, but we're not there

[00:24:42] Mike Thomas:

yet. Yep. No. You're exactly right. Shiny Shiny proxies. Another option where it is a bring your own container option, you know, you have to it's open source. You have to set the environment up yourself and and manage it all yourself. But if you prefer to go that route, then that is a a great possibility in the many choices that we have for shiny deployment these days.

[00:25:19] Eric Nantz:

Well, jeez. We just got finished talking about container technology. We're sharing your shiny ass, but especially you've been following this show along with recent developments. You know there's another challenger as I say for how we distribute shiny apps in a much in a very novel way, and that is of course with WebAssembly. In particular, the terrific work author by George Stagg, with WebR and now with the Shiny Live package where we can take a Shiny app that has, you know, minimal dependencies and throw that into a WebAssembly process. And then our individual browsers become that engine much like how Docker was the engine to run the shiny app in our highlight.

Why am I transitioning the web assembly now? Well, because our next highlight here is indeed a shiny app in the contest of exploring what is actually under the hood of a g g plot two visualization, but it has been deployed as a web assembly app. This has been authored by June Cho who has been a very prolific developer in the visualization space with g g flat two. And, apparently, this idea has been in the works for a while because he's actually talked about the idea of understanding how the different layers in a g g plot come into play as you're construction these visualizations.

He's written papers on this. He's actually given talks such as that the JSM conference as well as our studio comp before that about his his take on the layering approach and a package that he has coauthored in the g g plot two ecosystem called g g trace, which is another novel contribution here to basically look at the internals of g g file two. And in fact, one could say that this g g file two layer explorer app that we're looking at literally right now as we speak is an intelligent GUI front end to the g g trace functionality.

So right off the bat, like, if any WebAssembly app, you put it the URL in, say, Microsoft Edge, Google Chrome. Firefox can be hit or miss in WebAssembly just saying that, but at least for this one, it seems to work fine. Once it loads up, you've got yourself a pretty typical looks like a a scatter plot with a linear regression line best fit. It's a predefined plot as option one, but you got multiple options here such as a bar chart, looks like a a density curve. I'm just clicking through here, a box box and whisker plot as well as a, violin type plot split horizontally.

In each case, you can look at the plotting code in the little code editor at the top, and you can make tweaks as you see fit. Like, if you have your own favorite plot, you can just throw it in here if you like and regenerate the code because WebAssembly, again, has that our console kinda baked in. You can just run this code at your leisure and try stuff out. And once you have your plot, you've got really interesting ways ways of selecting the different components of these layers on the left side of the app with this little radio, group button here choice of, say, the compute position, the geom, type of data inside of that. And on the right side, you've got now a way to inspect what is actually in that layer in that particular call to to the g g plot two functionality.

So just like in the case of this violin plot, I can look at what's happening with the geom violin code here, look at the data behind the scenes, look at both the input and the output as well in a nice little data table. And if I want to run some of these expressions that ggTrace is exposing, I can run that on the fly and refresh the the dataset or the summary based on the code I put into that yet another editor. This is fascinating. You can even get information on the layer at a higher level of a model that pops up of the different classes, the subclasses, and the methods that are composing that layer.

This really is I use a cliche looking under the hood. This is as close to, like, really getting into the internals of a car engine, but with g g paul two speakers I've ever seen. And the fact is I can do this all interactively. This is this is amazing. I'm not I can only imagine to be out of work that has gone into building this. But like good documentation practices that Mike and I espouse many times to our projects, there's a very comprehensive about page in this app where you look at at a high level how to use the app, what's the general workflow, and how you can do some common operations in here, like being able to compare the inputs and the outputs, try to run custom expressions to kinda massage that data a bit, and as well as exploring the graphical type of object output for each of these different geomes that you can, you know, you can run here. And there's a little button called hijack plot and literally run what it would look like if you draw this onto the existing visualization.

This is amazing. He does mention there are some, you know, various fine scope that is exploring here with respect to g g proto methods. Again, I don't know as nearly as much as what June knows under the hood of g g plot two, but I do feel like this is, I could bring my own plot to and really go to town with just seeing what are the building blocks of putting this together. A very technical deep dive into the magic that g g spot two gives all of us. So very straightforward streamlined app. I can see myself, spending a lot of time with this in my next visualization adventures. But, Mike, what are your impressions of this?



[00:31:29] Mike Thomas:

Well, one of my impressions this week, I think, is g g plot, I believe, turns

[00:31:34] Eric Nantz:

18 years old. Did it have a birthday this week? Yeah. I kept I kept seeing that or even 21. Like, I don't even know which one's correct now.

[00:31:41] Mike Thomas:

I don't know. I think it was 18 from everything I saw, which would take us back to 02/2007. I think sounds about right.

[00:31:49] Eric Nantz:

Did you ever base plot before g g plot or before you realized what g g plot two was? Oh, yeah. I was a, heavy user of base plot. And then once I knew g g plot two, it's like my life changed forever. If you believe we base plot it. Well, one of us on the call, I'm sure people are not surprised about.

[00:32:08] Mike Thomas:

No. No. I did the same thing. We were starting out with our I think it was because I had a professor that didn't know that our packages existed and anything outside of base are which from my conversations with others in early academia around that time, it it's not unique, unfortunately. But, yeah, this is really, really interesting. You know, the WebAssembly stuff still blows me away. Every once in a while, we'll be trying to do something sort of obscure from a plotting standpoint with g g plot two where we have to go into a particular layer or a g g plot object and try to make a modification, right, to get it to show us this customized thing that we want. And I imagine that some of the wizards out there that do things like tidy Tuesday or is there a competition specifically around database as well or did there used to be one on the r side? Oh, there used to be one. Yeah. That's a good point. I forgot what that was called. But, yeah, we definitely seen it. Pretty incredible, you know, g g plot based visualizations.

I imagine that they would occasionally do the same thing, and it's always felt to me like a little bit of the wild west. Like, you're going into uncharted territory when you start really diving into a g g plot object and then looking at these, particular layer attributes. But this this web application that's been developed really makes it much more tangible and obvious for us to be able to take a look at those internals on a layer by layer basis. And I think it's a really interesting approach that they took to doing so. It's really helpful. I was playing around with the little r code editor to update the plot that they were showing us. I'm I'm looking at predefined plot three, and making some changes to the labels and seeing how it sort of propagates down themselves.

And, I I just find this really interesting. It's a great tool that's been developed for us to be able to do that, and it's a really good education in the g g pot.

[00:34:13] Eric Nantz:

Yep. And, while you were talking, I was doing a little smoothing, and I have found the GitHub repository of of this app. I put it in the show notes for all of you as well. There's some really handy tricks that he's doing from both the Shiny perspective and the WebAssembly perspective. So this is you know, I'm always on the lookout for where people can take the direction of WebAssembly, especially in the case of you might call educational, you know, really getting to know certain packages or certain analytical, you know, methods. But in the case of visualization, oh my goodness. This is just a an excellent use case. Because, again, not only does it come with the predefined plots, the five choices, you can run your own and have it updated in real time. That to me, the magic of WebR knows no boundaries in my opinion.

So I'm gonna be definitely looking at this in more detail. Again, great, great example here. I hope George Stagg is aware of this. I'm sure he would be super thrilled to see where where web assembly is being taken here. But, yeah, credit to judo or judo and the rest of our of his contributors here for, a massive accomplishment here. Alright. For our last highlight today, we're gonna shift gears a little bit, just getting a little more low level with r itself. And those of you who have been around r for a bit or maybe new to it, you may find that one of their critiques is, well, with r being an interpretive language, sometimes it's just a tad slower than, obviously, the lower level solutions based on, say, c plus plus, Rust, whatever have you, especially lately.

But there always have been, you know, attempts to make our code faster such as the aforementioned conversion to c plus plus via RCPP. You know, that's always been a tried and true. And like I said, Rust is getting a lot more attention lately. But an unexpected, repository appeared, in the last week and a half, and this is on GitHub at the moment. This is a brand new effort, but a new r pack is has been authored by Tomas Kalinowski, who I believe is an engineer at Posit. He has authored the quick r package. And what does this really mean? Well, on the tune, it just says it helps make your r code run faster.

How is this actually accomplishing it? Well, you have a function that exposes called quick, and then you feed into this the r code, I. E. A function that you've created that you want to make faster. And apparently with some of these, examples in the readme here, and again kind of simpler arithmetic type of situations with four like, double four loops, we are seeing some pretty substantial gains in the in the reduction I should say reductions in the time to run a quick r version of a of a function going from a to, you know, only four milliseconds. So I guess if you scale that up that could be substantial gains there.

What is it actually doing under hood? From what I can tell in the read me here, it is utilizing Fortran folks. Yes. Fortran, a language you may not have heard about unless you were, you know, like a real old timer like me in grad school and we did have some Fortran happening in our statistics curriculum, it is apparently compiling this r function that you supply into the the quick the quick function into Fortran type of code. Well, there. That's that's, that's an interesting, creation if I dare say so myself.

So I am I'm intrigued. Now there are some caveats here that Thomas, outlines here. This will not work with every type of function because it is dependent on the type of return value that you're gonna use out of this. So there are some caveats to play, but I am I'm definitely intrigued and apparently there is experimental support for using this in an r package. But, yeah, some of these restrictions I had to scroll to see this. You might have you may have to make sure that your arguments are declared explicitly, their types and apparently their shapes as well.

Not every type of input supported. It's just integer, double, logical, and complex. The return value must not be a list, along with some other caveats as well. So as I can see, this is early days and that there is a subset of the current built in r vocabulary that is supported, and he's got a printout of the 66 or so of those under the hood based kind of functions that are supported. But he does a he does plan to expand this out, in the future. So we'll keep an eye on on this. There was a lot of, enthusiasm on social media when I saw this shared out, but, yeah, yet another contender to make your r code run faster with Fortran of all things. So Hello. Again, Eric here. Unfortunately, during our recording, we had a snafu occur with our back end system and our connection for our our awesome co host, Mike. So his his, take on this last highlight was unfortunately lost, but, you know, it's assured he was impressed with QuickR as well. So, anyway, we apologize.

We won't have his audio, but let's go ahead and close out the show. So back to our regularly scheduled podcast. This is gonna be the coolest party ever. Yeah. This is one of those cases where I love to know kind of the genesis of building this was the big motivation here. Now I I am doing a little smoothing on their GitHub repo. I do see a a fellow contributor is the esteemed Charlie Gao who is, of course, the author of Mirai. So my guess is there might be an async thing in play here. I don't know. But I, I I may wanna talk to Tomas sometime to see if he's around, to get get behind the store behind the code a little bit on this. But I'm intrigued and knowing what Charu is cooking up over at Pazza, there's a lot of things this could this could lead to. So fascinating discovery here, which again, you you always learn something new in Aarushi. Right? This is definitely one of those.

With that, that will wrap up our main segments here. I do have to get out of here because the day job calls again, but we definitely thank you so much for joining us. And once again, the rweekly project, if it runs on your contributions, if you have a great highlight or a resource you wanna share with us or a poll request away at r0k.0rg, you can find the link in the top right corner. We have a custom, you know, issue draft or poll request draft, I should say. Fill that out and our Curator of the Week will get that in in the upcoming issue most likely. So with that, I usually talk about our social media handles, but I do have to sked out out of here. You can find that in the show notes. I I write those show notes. A lot of work goes into that, so definitely check those out. And also, we have chapter markers too if you wanna, you know, skip to your favorite highlight. Just hit that little skip button if you're on the modern podcast player. It'll be right there for you. So with that, we'll close-up shop here for episode 206. We're glad that we're able to record this, talk with you all again.

And hopefully, we'll be back with another edition of our weekly highlights next week."
"13","issue_2025_w_21_highlights",2025-05-23,52M 51S,"Have you wanted a chance to rewrite your own history? With Git, you certainly can! We learn that and other amazing tips to supercharge your version control skills. Plus a promising new package to let your Shiny app users make the call on their preferred layouts, and how mocking is not something to dread when you build unit tests interacting with…","[00:00:03] Eric Nantz:

Hello, friends. We are back of episode 205 of the Our Weekly Highlights podcast. We usually do this every week, but real life happened last week for both of us. But we're happy to be back this time around for another fun episode to share the latest highlights and additional resources that have been shared on this week's our weekly issue at ourweekly.0rg. My name is Eric Nance, and I'm delighted that you join us wherever you are around the world. And as always, I'm joined by my traveling cohost, but he's back home, so to speak, Mike Thomas. Mike, how are you doing today?

Doing well, Eric. Yep. I had a a one day trip to to Kansas, the Midwest part of The US last week, which was a lot of fun, but a quick turnaround that ended up, taking place smack dab when we would typically do the highlights. So no travel here this week and happy to be back on the mic. Yep. Never never feels right without you, so I've decided I had my own, escapades, albeit internal escapades here that I had to take the week off of too. But, yeah, happy to be back here. And just a programming note, I don't know if we'll have an episode next week because then I will be traveling next week. So we'll try to squeeze one in, but if not, then we'll we'll be back the week after. But, we got a fun issue to talk about today. Some really interesting highlights here that have me, both, you know, thinking I need to learn a lot more, and I got some cool stuff to try out. Though it's not a believer at any longer, this week's issue is curated by John Calder, number one of our longtime curators on the RWBY project.

And as always, he had tremendous help from our fellow RWBY team members and contributors like all of you around the world. We've heard Paul request and other wonderful suggestions. And we will lead off with, a fundamental technique that if you're developing anything that's outside of that quick little exploratory data analysis script, Your future you will thank you later if you invest, some time in learning version control, in particular with the Git version control mechanism, which has become synopsis with, many of the development world, so to speak, and software development, but also data science as well.

Mike and I here use Git literally every day for our projects, and we could not live without it. And every time I think I've, you know, I I think I've got a pretty good grasp on some things. And then there are instances where I do feel very humbled based on how well some other people in the community have learned Git even when they didn't come from necessarily a software development background as well. So our first highlight is one of those showcases from a very renowned expert now in this intersection of Git and research software engineering and data science.

And in particular, it was a meetup that was recorded at the Salt Lake City, our user group meeting. And this is one of the top our user group meetings across the entire world, in my opinion. They always have fantastic speakers, fantastic talks. This is, run by, Pazit's own Julia Silgi as well as Andrew Redd. And in this particular instance in their May session, they had the esteemed, Ma'al Salman, who has been a long time both, curator for Art Weekly, and now a frequent contributor just of her daily work that spans our OpenSci as well as the synchro group, which we'll be hearing more about later as well.

And in her, presentation at this meetup, it was a both, presentation and interactive demonstration of be getting better with your history of git commits and a lot more than just that. So I both must confess I didn't watch the entire meetup yet that is on my list to do, but I have had a good, review of a few key sections as well as the slides, which we have linked to in the show notes. And Al always does a wonderful job of her presentation materials. So this is kind of a tour de force of, like, some really core Git concepts that, you know, you may use a little bit every day in in basic ways. But then if you take it up a few notches, you can really help future you immensely, especially when you get to reviewing when things maybe went wrong or reviewing what the heck you did last year in those various commits.

So as I was watching this, there are some practices that I've latched onto with a vice grip, so to speak. One of those is, one of her recommendations right off the top is instead of doing that massive commit of, like, fifteen, twenty files of a whole bunch of line changes and just putting this commit message, like, just get this in before the deadline or get this in before my break. You're not gonna be able to make heads or tails to that no matter what fancy get interface you use to examine that. And so her first piece of advice in this talk is make smaller commits with a focus set of files.

I wouldn't say really matters if it's one file or a set of files, but the changes should be related to each other in some way so that in your commit message, you're very specific about what exactly you're trying to fix. Maybe it's a bug in your code. Maybe it's, a refactor of a CI process. But if you lump that in with a whole bunch of other changes, oh goodness, that that that's gonna be rough to debug and rough to especially in the world of collaboration for your collaborators to see what the heck that commit was all about. So I have gotten better with that. Now again, we all sometimes fall in old habits from time to time, especially when things go haywire. But when I'm in the flow, so to speak, I am making these more nimble fit for purpose commit messages.

And the other part that she recommends highway and I wholeheartedly agree is branching. Branching is the methodology where instead of just being on that single stream of development, you can then separate another stream of development. Maybe you wanna experiment with a new feature. Maybe you have this really critical bug that you wanna fix and determine if you got the fix working before you push that back to your central or main branch. Git branching. Once you get in the flow of that, it is such a time saver and frankly, an effort saver as well.

When if something say doesn't work as expected, you just delete that branch and go back as if you never did anything wrong. And, also, when you wanna merge that into your main branch, having that separate branch as part of, say, a pull request in GitHub or a merge request in GitLab, it gives other collaborators an opportunity to review those changes, maybe even test it locally on their setup. But, again, you haven't touched that main branch just yet. You've got a way to have that separate stream of development, and it is very helpful for experimentation while avoiding making the infamous, say, copy of files that we often did, at least for me back in grad school when I didn't know Git yet. And I'd made tons of copies of my dissertation files when I was experimenting with something. And if I'd had Git, would have been much better.

That's where things and the rest of the presentation is where I start to feel a bit of humble, so to speak, and things I really need to level up on. So this is Eric being a little more vulnerable here in my get adventures, but I'm I'm always of the spirit of transparency here. When things do go haywire and you're not sure where it went haywire in your commit history or maybe you seem to have that file working before, and then you've done, like, five or six commits since then, and then suddenly without you knowing it, you've broken that file or you've broken that script or a Shiny app, and you don't know where the heck it went wrong.

That's where the git bisect command comes in, and this is where you can, in essence, enter a different mode of Git where then you can interactively step back in time to these different commits that you made and get the the you might say the environment of the files at that particular commit as you step back and then experiment with the code, see if things are still working or not. And you kinda go back and back until you found that place where it was working and then maybe go to that commit afterwards and then really start to isolate what you did in that script that might cause things to break.

I have only used this sparingly. And boy, oh, boy, every time I enter that mode, I feel really nervous. It's almost like going into the black hole and wondering if you're ever gonna come out. So one nugget that I saw in this presentation is that, yes, this is for all the maybe the VIM and VI haters out there and hate how you it's so hard to get out of VI. Right? Well, there is a way to get out and get bicep if you get too scared and you're like, just get me out of here. There is a way to exit it out, which I and then we didn't know about very well other than googling it the first time I did it. So that that is something I don't feel I probably shouldn't be so nervous about, but boy oh, boy. The first time I did it, I felt like I was in a a different world during it.

But one thing you'll see reinforced throughout this talk is that there are a lot of nice Git wrappers or, let's just say, gooeys out there that can help you with certain operations and Git, and they are really helpful. But sometimes there is just no substitute for being skilled in the terminal we get. There is some things that any GUI out there just is not gonna have an elegant interface for. Bisec might be one of those things where you just gotta get your hands wet or dirty, so to speak, and get in the terminal to do it. I'll be interested to see if that improves over time. But get bisect is definitely something I wanna practice.

And one thing that you also see throughout the presentation, especially in the demo parts, is that Ma'am Elle realizes that it can be really scary to do this on a real project if you're trying to learn Git in these concepts. So we've covered this before in our weekly highlights in the past, but she's made an R package that's been inspired by a very, fun, reference that she links to in the talk called, oh, beep, get. And it is called, I'm gonna try to say this right, suparlipopet. I probably didn't get that right at all. No nonetheless, not even close.

This is a package that you can play with different exercises that that demonstrate these concepts. So I could get bisect, like some of the things we'll talk about later. And you can do this in a very low risk setting because you're gonna make temporary directories that have, like, a might call a borked git repo to let you practice in in your R session and throw this into your favorite IDE, like RStudio itself, like, more recently, Positron. So you could try this in your preferred environment and get a feel for how things work. So we'll put a link to that in the show notes because it is a very fun package to play with.

There are some other concepts here that I've seen used, especially by really talented developers out there that I have not done as much to my liking, such as when you get to a pull request merge, not just merging in all those commits from the the branch that you pushed up, but doing a squash and merge, where then you can consolidate all those, like, smaller commits into one commit that maybe now, again, maybe that doesn't work as well in certain situations. But if a pull request only had, like, a few commits and they're all related to each other, it might be better for the history of the project to, in essence, squash those into one with a more detailed summary and bullets in the commit message of each of those individual changes.

Your mileage may vary, but I have seen that in the past used with great success. Another thing is she makes heavy use of the git amend function, which again is a way to kind of change something you did in a previous commit. Maybe you had a typo in the commit message, so you can just simply amend what you typed in that message. Other times, you could just add a file that you forgot about. And there are some GUIs that can help with this. So you might be able to use, like, even the RStudio GUI to do this or in positron with Versus code, you know, extension library. There's an extension called GitLens that does a great job with some of this as well as, you know, some of our, utilities like GitKraken, which I like, can help with this as well. So that's a great tip. As well as rebasing, which, again, that's one account I won't say scares me as much as BISEC, but I've always felt intimidated by trying that. But it is a way to kinda like in the squash thing, kind of consolidate certain commits and make the history a bit cleaner, especially if you're really iterating through things. And maybe in the long run, it doesn't have to be that verbose, but you just wanna, you know, make things a bit better, especially when you're merging things, so to speak.

So again, I haven't played with that as much. But again, you got demos in her package to try all this out. So definitely worth the watch. Definitely look at her slides, and she has links to her various blog posts that definitely most of them haven't featured in our weekly before. So it's not the first time we talk about version control, but boy oh boy, it's great to have a reminder from time to time just how powerful this can be in your development journey. So I ate some humble pie with this, Mike. How would you think about her her resources here? Always something to learn when there's a a Mael,

[00:14:27] Unknown:

presentation here. And I'm very jealous of the Salt Lake City, our users group. It seems to be going really strong for quite a few years. They get great speakers, cover great topics. So it's it's a great resource that we have out there for folks who are looking for, you know, live presentations and the community acts aspect to be a part of as opposed to maybe, you know, watching prerecorded things. But this is a fantastic walk through. Git is such a bigger language, I guess. It has such a big footprint of what it can do that maybe you don't realize.

And I'm sure even maybe the males of the world still may not know every single little last thing that Git can do because it to me, it it just feels like there's always some new thing that Git can do, some new function it it has, command that it has that, I had never heard about before. So there's some very, as you mentioned, Eric, I think there's some pretty low hanging fruit, in in terms of additional, you know, functionality that Git has that you may not have known about. The the amend the the Git amend is a fantastic, command in in my opinion. I often find myself making a commit and then realizing that, you know, I may have impacted a few files, but there was one last file that really, should have been changed as well that lines up with that that previous commit message.

And instead of creating a whole separate commit for it, I can I can run git commit amend and actually overwrite, if you will, that last commit with the additional change that I want to bring in? So that's fantastic. You can also update the commit message with that command as well if you felt like the message, wasn't encapsulating everything that you wanted to say. So that's fantastic. Git revert is is another one that Mel walks through, which I think is very useful. And those last two, as you mentioned in some of the GUI tools that we have, the GitHub desktops of the world, the GitKrakens of the world, may be able to do that. But git bisect, which I think is is what Mael may have spent sort of the majority of the time walking through, rightly so, is extremely powerful.

Right? But it's it's certainly, detailed to get the hang of. And I think this sapphire le paupet, package, which apparently is French for my goodness or good heavens, something like that. It's like a French exclamation, which if you've ever worked with Git, you've probably exclaimed something like that at one point in time. But it's a really useful tool to be able to try some of this stuff out in a very safe setting. And I think probably as Mel mentions and as you mentioned, Eric, as well, some of this functionality may not be as important when you're working by yourself.

Right? I don't necessarily need to amend a commit message, you know, for myself if I forgot a file. I can just create a new commit, you know, on top of that that previous one. But when you start getting into team data science, this stuff becomes really important. And when you are reviewing codes, submitting pull requests, and other folks are going to be looking at the changes that you've made to the repository and the the Git history that is included in those changes that you've made, you want to provide them with a clear set of breadcrumbs that they can follow to understand the changes that you made and how you got from what existed in the repository to the repository to the fix or the enhancement that you created. And I can't tell you how much more efficient your workflow can be if you have a good grasp of these Git concepts, as opposed to to if you don't. It it can be a tremendous time savings for that code review process and for that collaborative process.

So if you do find yourself looking at some of these Git tools and thinking that, you know, maybe this isn't really that important to me in my current workflow because I'm sort of a solo data scientist on my team. That may be true. But if you do get into a situation in the future where you are collaborating with others, maybe even collaborating on open source outside of your your day job, I would highly recommend trying to pick up some of these additional Git tools that will really make that collaboration a lot easier.



[00:18:53] Eric Nantz:

Yeah. You wanna put yourself as well as, like you said, a team of, you know, data scientists and developers on the best path for success with this. Because one thing I've always learned is that, you know, not to to not to do a humble brag or anything, but when I get on a project of a team, usually, I'm the one that's doing the get kind of orientation first, just because just by by my work, you know, history, I have the most experience of it, other than a couple other people on a various projects, especially when you get to more of the clinical facing side of our data science area in our in our company.

So I'm trying to arm them with resources like what Mel has written here, but also just some cheat sheets and other, you know, tools like these interactive, you know, I mean, it's GUI based editors or frameworks around Git that can help along the way. And they may never need to be as proficient with certain parts of this, but that's where if there's just some attention to detail on at least the basics of this with those fit for purposes, commit messages, branching. And then as we grow into it, learning things like bisect, learning the amend functionality, I think that comes through experience as well.

But we we can expect to be experts right away. So getting them onboarded correctly or say efficiently, I will say that it's always been one of the most requested things I've ever had about, you know, like, say, education or training resources is resources around Git. So what I try to do is just assemble all these great resources from online. These online books that Mel mentions are also great reading as well, and I just try to distill the key points in various resources because there's no way I personally can duplicate what has already been done so well. It's just more of, I think, the practice of it, which, again, her package, I think, is a great way to practice it so that we all come with a foundation that we can contribute, you know, right away, but also not feel so intimidated by this whole get operation. So that first experience does matter.

And I don't know if you'll ever get in Git kind of like the experience we have on the r package development side with things like use this and other packages to just get you up and running really quickly. Get I'm not sure if we'll ever get to that point. No pun intended. But I think we we can definitely get some resources to share along the way, and this is certainly one of them. Absolutely. I do remember that

[00:21:28] Unknown:

Greg Wilson who was, sort of a famous software carpentries teacher, instructor, developer, once famously said on a podcast I was listening to that he has taught probably tens of different programming languages in, you know, these educational settings. And Git is by far the hardest language he has ever had to teach, like head and shoulders above.

[00:21:53] Eric Nantz:

Yeah. I, I can hardly agree with that from my limited experience in teaching this, but we should send some graduations to Git itself. I believe it just turned 25 years old recently this year. So, for those who don't know, Git was created by the author of the Linux kernel himself, Linus Torvalds, as a way to help manage all the kernel changes, and it's still being used to this very day. So talk about scratching your own itch, so to speak, for all of us to benefit. But, he he he had an entertaining interview a while back where he wished he didn't have to invent it, but he there was nothing else out there that met the needs of the kernel development. So we're all we're all beneficiaries of it even if we have some, less than stellar things to say when things go haywire, if you know what I mean.

Well, we're gonna transition now to, one of our favorite corners of the development world in the art community, and that's the space of Shiny and web applications. And in this next highlight, I will admit I got a preview of it, albeit not realizing where this was really gonna go. So a little step back in time with yours truly here. It was in last year during our r and pharma conference workshop series. I had the pleasure of hosting an a workshop by David Grange, Karma Tarip, and John Coon about this new blocker, framework that they used for a client project to help, application viewers, in essence, design a Shiny application of a no code like solution. It was kinda like a gooey editor for a for creating a Shiny app and then sending it to your users.

I was even put on the spot to do some exercise for it. It was a great way to learn on the fly. But throughout all of this, I realized they were doing some really slick things with how they were orienting these different parts of the app and being inserted in different places, be able to move it around. I had never seen anything like this before. Turns out that became a very important functionality and not just as blocker package, but in some other projects. So in this next highlight, we are pleased to talk about a new r package that has just been, released called doc viewer.

And it comes to us on this blog post author by David Grangen, Nelson Stevens, and Nicholas Bennett from Synchro, who we had just mentioned, and my Al was also supporting as well. And the doc viewer package under the, I guess, on the tin, so to speak, it is a way to give yourself these grid like elements, these panels. They can put any type of content inside and have some really neat capabilities to kinda mix and match, move them around wherever you see fit as well as some other nice customizations too. So in the blog post, you guess what? I know David's been a pioneer in WebAssembly.

There is a WebAssembly powered app right in the beginning of the post where you can actually see this in action with a set of about three panels here, one with a slider and a histogram, another with a VIS network graph, and then one with a data table that you can just move them around. I'm literally doing it right now as I speak. I'm just moving them within another panel with different tabs, or I'm blowing it out to a different side of the panel, with its own separate box. Very, very interesting here. And so you may be wondering, okay. Well, how does this actually work?

It is is using a doc view function where you put everything in, and then you put in a list of what they call panels. And in this panel, give it an ID. You can give it a title. And then the content, which, again, can be just a simple string or it can be a shiny type of content, like, say a slider input, like a plot output and whatnot. So again, we've seen they mentioned in the post, they've seen attempts at this in the past with, like, this kind of drag and drop functionality, such as the Dragua, framework, Dragua JS, also Shiny j q u I for wrapping jQuery extensions, and then also the sortable JS library where there's a sortable r package that I've used in the past.

They said each of them work pretty well for certain cases, but they needed something a little more customizable. And that's where they discovered the doc view JavaScript library that this one wraps. So there is a JavaScript library under the hood that if you're just doing any kind of web app, you could definitely tap into. But now we've as our users get to tap into the same functionality that that blocker package I mentioned was using under the hood. So you may be wondering why is this helpful for me as, say, a shiny developer listening here.

I've always said you can't please everybody with your user interfaces. There's always some client or some customer that just is tired of the sidebar layout. Maybe they just don't wanna have that plot be so big and in the in the frame. They just wanna kinda move it out of the way and move something else to get the key focus. Well, instead of creating, like, copies of apps that please, like, individual customers, maybe the doc viewer gives me a way to have one part of my app, maybe multiple parts, where I say, here's a predefined layout, what I think is, you know, a good way to present the data, present the controls.

But you know what? If you have a preference, just drag and drop to your heart's content. One interesting nugget that they mentioned here is that they do have functionality in this package to save the state of where these panels are and to restore it later. I wanna look into this a bit because some of you are aware I am working on a package called shiny state that's wrapping the shiny bookmark. We'll say, I think this is a little different, but I'm definitely intrigued to see how they pull this off. So they mentioned there's a vignette that talks about this, so I'm gonna definitely look at that after this recording.

And there is some stuff they wanna work on in the future that the JavaScript library has, such as grid view without drag and drop or collapsible panels. They say they are working on that, but this is early days. They're looking for feedback, and I am definitely intrigued to try this out on one of my future projects, especially in those cases where I know I'm gonna have a few key elements on that on that interface, and I don't know exactly what is the best layout. Maybe somebody prefers, like I said, that data table front and center. Maybe they prefer a plot front and center, and they want the controls in a more, you know you know, open way or or more larger view.

So I'm gonna play it and see what happens, but I'm intrigued by what DocViewer could give us for customizing our shiny user interfaces. So, Mike, what do you what do you think about DocViewer here?

[00:29:26] Unknown:

Nope. This is super cool. It scares me in that it's could be one of those situations where you make an app that contains DocViewer functionality and then all of a sudden, your end users want all the other apps that you provide to them to also be doc viewer, functional if you will. Yep. But, it it's sort of like having a chat interface with your app. Right? Exactly. But I think it's it's really, really incredible. I think that there's a lot of potential here. I really thought that it was, you know, interesting how the the team arrived at at finding DocView after exploring a bunch of other sort of JavaScript tools that we may be more familiar with. They mentioned, you know, sortable.

Js, which has an R package called sortable that I use a lot for filtering. That's fantastic. There's Dragula, grid Stack, but but nothing really, fit exactly what they were looking for in terms of a a nice they call layout manager until they found dock view. And it's very smooth, sort of when you start to drag some of these different panels around, it's it's incredibly smooth. Yeah, I would have expected, you know, maybe one of the first packages that does something like this, at least that I've seen in Shiny, to be a little bit clunky, but it's it's pretty easy. And I think it plays into maybe another concept that we've probably all heard about, which is the idea of, like, self-service BI, alright, that end users potentially want, but is always tricky because I think a lot of times it's it's better to just put something in front of them that's a little more fixed, as opposed to doing the self-service thing where maybe they're not applying a filter that they don't know needs to be applied. But I'm gonna digress from that potential tangent. And one of the really interesting things to me in terms of the the code syntax is how you create these layouts is actually sort of starting with one panel, it's called.

And then the position of the other panels are all, defined with respect to a prior panel that's already been established. So, there's a within this this panel function, which is how you create a panel in the grid layout, there is an argument called position that you can serve a list to. And that list should contain, I think at least two objects. The first being reference panel, which is a different panel that you want to reference and then the section the second, argument there, the second list element would be direction.

So if you're creating a second panel that you want to be to the right of the first panel, that list would say, you know, reference panel equals one to reference the first panel and direction equals right. So that my second panel is to the right of the first panel, which I think is really cool. You can obviously, decrease the width or decrease the height of any of these panels. But in situations where you want to set a minimum width, which is what they have in the example here, there's a minimum width argument so you can't decrease it, more than a particular number of pixels on screen, which I think is fantastic. So they've thought through, I believe, a lot of the functionality. I think that's sort of most important to creating these these layouts for end users. And I'm really impressed with what they've put together here with the doc viewer package.

I am most certainly gonna try this one out, in a future app here. And this is this link is going straight to our team's internal Slack channel because I think it's gonna be a hot topic.

[00:33:01] Eric Nantz:

Yeah. And one other area that I kinda glossed over until I reread again is a lot of times when we're starting these projects, we get into that wireframing stage, right, where we're just trying to figure out what are the just the basic components and from a UX standpoint that we want. And we've seen attempts at this a little bit in the past, from Posita, actually, that Shiny UI editor, framework that Nick Strayer had authored, and that can be a great tool as well. But with this, I could combine this with, actually getting to our friends, I think, are, Colin Fay had written a package called shiny ipsum, I believe, where you can have these random placeholders, like a data table, a plot, or or or another rendered output or input. And you could combine it with this doc viewer, and then you've got, like, okay. Here's my initial guess at the u at the UI layout.

Now in this meeting with maybe your stakeholders or other developers, let's just kinda drag some stuff around and see what really sticks, and then maybe we align on that fixed setting, or you might keep it flexible for the main app. So I think wireframe is another underserved area in application development that does get glossed over a bit, but yet is something that the last thing I wanna do is to bust out a PowerPoint slide for something like that. I want something that's more visual and interactive without paying for a proprietary service. So no shade to the Figma lovers out there or anything like that, but I'm I'm cheap. I like it free, and I think this doc the doc viewer, package might give me might scratch my itch on that on that side of it too. So as I mentioned, it is early days with it, but but as I'm as I mentioned earlier, the team has used this quite a bit with that block r package. So I know they've been putting it into a very realistic use case right off the bat, and I am intrigued to see what I can what I can do with this in a future project. And maybe there are cases where you have an app where maybe the majority of the app you wanna key fix, maybe that one module in there where you want to be more flexible.

I wanna see if I can, like, opt into using doc viewer in only one part of an app versus other parts that might still have that more

[00:35:22] Unknown:

fixed layout as a way to gently introduce it. So, again, things for me to try, but I'm intrigued to explore more. No. That wireframing case is a a great potential concept and there are certainly a lot of potential wireframing tools out there. I think this one would be a great one. I know we say no free ads, but I do wanna give a shout out. We do a lot of wireframing in Excalidraw, which is an open source tool that's just really fantastic for quick drawing sketches, things like that. And, there is a pull request out there to merge a new library in Excalidraw. So if you're familiar with Excalidraw, there are certain libraries that have like AWS icons for example that you can import and then you'll have access to using those in your wireframes.

And there is one called, Shiny Excalidraw and it has all Shiny components and there's open pull request into Excalidraw for it. It looks like, let me figure out. Mike John Page. I gotta figure out exactly what his name is on GitHub. It's Mike John Page, and, he's presenting this at the UseR conference

[00:36:32] Eric Nantz:

this year. So Oh. Give it a thumbs up if you see the poll request. I've I've definitely checked that out because I have used XCalDraw in the past as well. I wanna level up a bit more. I mean, having those shiny icons will just make it even easier. So yeah. Great. Great call out there. There's lots of interesting tools out there. And like I said, Doc Fear, I think, is gonna be in my, toolbox in the future. And I dare say the pedigree of the talent behind this package, I've always had a lot of a lot of appreciation for what David Grange and has has done for the shining community and his, previous efforts. So, certainly, it's in good hands as they say.

And lastly, in the highlights, episode today, we've got, another concept, from a development standpoint, especially when you get to, again, either application development, package development, another, maybe ETL pipeline development. You wanna make sure things are working as expected, not just by running code in your session, but you wanna repeat that, especially as you're making changes in an automated fashion. And that is where we have the test app package, which has been hugely instrumental in the automated testing of our packages when you build test functionality, test scripts, whatever you wanna call that.

But there are cases where maybe some of those functions you have in the package, you know, it either is an expensive operation to run or it's using a resource that would be really difficult to just manually keep configuring over and over again. And that's where the idea of mocking and testing comes to play. And no. I'm not talking about what the kids like to do when they think their daddy is talking about some old time concept. Not that kind of mocking. I'm talking about making things seem like they're real, but yet they kinda have a little maybe shim in front of it.

So our highlight talking about mocking with Testac comes to us from Muriel Delmote, who is a data scientist at ThinkR. And our, ThinkR has always been a frequent contributor to the highlights as well. And her post is titled mock them all. Simulates a better test would test that. So what is the motivation here on top of what I said? But let's say you also are building some business logic tests in an application like with Shiny. And maybe you have a piece in your app where the user uploads a CSV file or whatnot, and then your app's gonna do some processing on that.

Well, imagine trying to build a test around that. Right? You can't offer using Shiny test two, perhaps. If you're not really interested in doing the interactive experience per se, but you just wanna test that what happens after that upload operation is working as expected. You want a way to kinda capture that uploading feature without really uploading a file. Maybe you have a test file already in your package infrastructure. Well, that's where mocking comes in and tests that. They have functionality called with mocked bindings, where then you can kind of put in a shim to that function for, say, choosing a file or whatnot.

And then you can just give it a hard code of path like she has in this first example here where she has mocked the choose dot files function in base r just to illustrate. So you can use your imagination. That could be used a lot of other ways as well. And then the part that I've been more in the weeds of, so to speak, is that when your testing involves some kind of external resource outside of your package or app, let's think a system process. So let's think an API or maybe some other resource like that.

So this can be also used with, with mocked bindings as well. And in the case of API, she does have an example where she's wrapped the idea of fetching the data from an API using the header package, h t t r. But you could also, again, have a mock binding where you simply give the result of that fetch operation as, like, a list or a JSON file or whatnot. But I will put in a plug of a package I've had used great with great success, in a recent package I've built for wrapping an API, and that's called HTTP test two, which I'll link to in the show notes, which is a way to give you these mock bindings for for a hitter two or h t t r two, function calls where you kinda run it once, it'll capture the response. And then every time you run that test again, it's gonna use that cache response from that API request instead of hitting that API all the time. So that's a great accompaniment to, technique here.

Now last but not least is that there are some situations that that with mock binding or even that a HTTP test two is not gonna help you because there are some functions that really depend on that. They're either very primitive in the language itself or they really depend on the environment at that specific time you run it, such as, say, sys dot time or getting an environment variable. So you can't really put that in with mock bindings because it just doesn't know how to how to capture that effectively.

The workaround, an understated workaround, is that for that function that you're trying to wrap, you put a wrapper function in front of, say, sys. Time. And then you can then do with mock bindings on your wrapper function instead of the base function itself. So in her example, she's got a wrapper function around sys dot time where it's, it it's simply that's all it calls under the hood. But then in her with mock binding, she then wraps her, get current time wrapper on it to just give a static timestamp instead.

So that might be really helpful if you're in a situation of testing in some kind of external environment or system environment where you can't just put in the with mock binding, that actual function itself. A wrapper function may be inconvenient at start, but in the end for testing, you'll you'll thank yourself later that you can put bindings for that. So very, straight to the point post, but I think, the mock bindings is an understated, underutilized utility so that you can, either save some cost by not calling that API so much or, please your IT admins. So don't want you hitting that that system resource all the time just because you're developing a new feature for a package. So really great advice here, and I'm definitely gonna take that to heart in my next project.



[00:43:51] Unknown:

Absolutely. And I didn't even realize, I guess, that, this with mock bindings function was in the test that package. It's fantastic. It's one that I admittedly haven't used yet, but but need to begin using. I think I've tried to do some things like this with the with r package. And I think the combination of test that and with r can be a really powerful one for doing things exactly like what's being discussed in the blog post, you know, simulating hitting an API, simulating a file being uploaded, simulating connecting to a database, things like that, and allowing you to build tests that, right, aren't going to be resource intensive and unnecessarily resource intensive, of the resources that you might be using in production, but still ensure that you're testing your business logic robustly.

So this is fantastic. I I think something that I often find myself doing, especially in my Shiny development, is having to make a change and rerun the app and, you know, upload a file, for example, and continue to do that to test the change. And it's not a huge deal, but it takes time to do that. And I think if I change my workflow, and this may be beneficial for some others out there as well, in that I try to make that change and instead run the the unit test first before I actually open up the app and, try to test that functionality, then I might be able to save myself some time by not having to to boot up the app, wait for it to to spin up, wait for the file to upload, and instead ensure that my unit test is running successfully and maybe that results in troubleshooting a few failures of the test first after I make my change.

And I will probably spend a whole lot less time doing that iterative process of continuing to boot up the app as I develop. So, this is definitely something that I want to bring into my workflow and I'm very grateful for, the the author here to be able to point out this with mock bindings that I definitely knew about in the back of my mind but have not been leveraging enough.

[00:45:58] Eric Nantz:

Yeah. I'm working on an app as I speak that does a lot of AWS processing under the hood, and I do have a dedicated package for that piece itself. But in that dedicated package, yeah, I absolutely wanna combine with mock bindings and this HTTP test too so that I can wrap these API requests, cache them after an initial run so that I can keep looking at these resources. I'm pulling things from, like, object buckets, DynamoDB databases, and whatnot, and I don't wanna keep hitting that Amazon API because, not that it's a huge cost. But if if you if you rack this up over time, it it can it can raise some eyebrows. So definitely caching that, I think, will be will be very helpful. And it does give you another on ramp to something that you and I agree with wholeheartedly when you search again, aspect of Shiny development.

Try to do the majority of your testing on the business logic side outside of Shiny first. And if you can use win mock bindings and to aid in that journey, I think it'll make testing a heck of a lot easier and save your usage of, say, Shiny test two or these other frameworks that are out there for when you absolutely have to do the UI stuff. But the business logic stuff, yeah, try try to use TestDaT for that if you can. And I think you and I are not alone in in thinking that. There are some other great people out there in the community that say the same thing.

And what we also can agree on is this issue, we got a whole bunch of other additional resources, and we invite you to check it out as always. I won't give a specific one per se, although it is kind of a plug for an effort later on. But in my, this three week window where I'm doing, conference presentation, but also I am gonna be part of the Our Medicine conference the week after. And, two guesses what I'm gonna talk about. My latest craze is the intersection of Nix and R, where I will be giving a hands on demo workshop of how I'm using Rix and Nix to do my Shiny app development.

But then a couple days later, we have the man himself, Bruno Rodriguez, is giving a workshop on using Rix for Nix and reproducible analytical environments, and he does have a post in this our weekly issue and our additional finds here called multi language pipelines with Rickspress, which is his newer package that's augmenting the idea of targets, reproducible pipelines, but with Nix. And the biggest, motivation for this is multilingual data science pipelines. The example he has is using R and Python. But imagine you could generalize this to say R and Julia, Julia and Python or whatnot.

Anything Nix can do, we're express can do with you as well in the context of r as well. So definitely check that out, and we'll put a link to the r medicine, schedule in the show notes if you wanna check out Bruno's workshop and my, fun little adventures of my, Rick's journey. Kind of an encore for when I did a Shining Comp, but we get to really dive into the details quite a bit on how this actually works. So I'm excited to be a part of that.

[00:49:18] Unknown:

I'm always impressed by, what Thomas Lynn Peterson comes up with, and I think he's in a very unique niche within the R space in terms of graphics and fonts and things like that. And there's a great blog post that he recently authored called Fonts in R on the Tidyverse blog that walks through all sorts of different typefaces and and really a lot of interesting concepts that I don't spend a lot of time thinking about. But when I run into these issues, I am grateful for folks like him because there always seems to be an answer to the problem.

So, really interesting one to check out if you're into that type of thing.

[00:49:54] Eric Nantz:

Yeah. Fonts have always been one of those things where I I use sparingly in customization, but they always kinda scare me when I have to go outside the confines of what's included in the system, especially wanna up my game with the g g pot visualization and fonts. So this is a very, very helpful resource to get started on the nuts and bolts of what's really happening. But with the recent advancements, as you said, with the system fonts package, as well as some other great resources, out there for both the visualization and the text representation, side of it, I think it's a great time to up your game, and it's not just for web apps. Right? This is for, again, those static visualizations, those static data summaries.

A new font can go a long way in that in that overall presentation for sure. So great find there. And, again, we invite you to check out the rest of the issue for your other, additional great resources of new packages, other events, and other top notch tutorials out there. So we love to hear from you as well. If you've been enjoying our weekly, one of the best ways to help the project itself is to get in touch with us via a pro request. If you find that new great resource, that new blog, that new package, we always have a link in the top right at rweekly.0rg for you to send that poll request with a template already prebuilt for you. Our curator of the week will be happy to get that merged in for you, to get your impact on our weekly itself. And, also, we love hearing from you in the audience. We have a contact page in the episode show notes so you can send us a fun little message with. You can also get in touch with us on these social medias out there. I am mostly on Blue Sky these days of at rpodcast@bsky.social as well as Mastodon with @rpodcastatpodcastindex.social.

And I'm on LinkedIn. You just search my name. You'll see my random exploits over there. And, Mike, where can the listeners find you?

[00:51:56] Unknown:

Yep. You can find me on blue sky at mike dash thomas dot b s k y dot social. Or on LinkedIn, if you search Ketchbrooke Analytics, k e t c h b r o o k, you can see what I'm up to.

[00:52:09] Eric Nantz:

Alright. And, looks like you got some great content coming our way soon. I'll be looking out for that. And, as always, I've like I said, I'm in the midst of this mini summer conference season, so I'll be sending some posts about what my adventures are there. But as always, we thank you so much for joining us on episode 205 of ROH highlights. And we may or may not be back next week, but rest assured there will be a new episode in the near future. So until then, enjoy your our journeys out there, and, we will see you next time.

"
"14","issue_2025_w_19_highlights",2025-05-07,42M 0S,"Episode 204 of R Weekly Highlights is jam-packed with four highlights! We discuss the recent improvements to the recipes package in the tidymodels suite, Nicola Rennie's adventures with this year's 30 day chart challenge, going back to math school with Jon Carroll's latest explorations in digit rotation, and our picks out of the excellent…","[00:00:03] Eric Nantz:

Hello, friends. We are back at episode 204 of the Our Weekly Highlights podcast. Editor's note, I still am not used to saying 200 something yet, but I'll get there eventually. Nonetheless, this is the podcast where we talk about the awesome resources that are shared every single week at rweekly.0rg. My name is Eric Nance, and I'm delighted you join us wherever you are around the world. And joining me at the hip after some, rather nervous firmware updates is my awesome cohost, Mike Thomas. Mike, how are you doing?



[00:00:34] Mike Thomas:

Gotta love Windows, Eric. Yep. It was one of those mornings where I booted up the computer and had to sit and wait for about ten minutes for a firmware update,

[00:00:42] Eric Nantz:

But I think we're okay. We made it through. We made it through. I still remember one of my older Android cell phones did one of those firmware updates, and it never woke up after that. So I'm one of those people, even in 2025, I never see anything do a firmware update. I'm crossing all my fingers and crossing my toes, and nothing pulls up. But but you are here, and I am here as well. No, no car failures this time around. So luckily, from that, tech seems to be kind to us. But what else is kind to us? Our weekly itself, we got a lot to talk about today because we have not one, not two, not three, but four highlights to talk about today. So we're gonna keep you keep your listening ears busy here. And our curator this week is Beto Almerzak. And as always, she had tremendous help from our fellow Rwiki team members and contributors like all of you around the world with your poll requests and other wonderful suggestions.

And we'll get your appetite wet for our content here. And because we are talking about one of the mainstays in the tidy models ecosystems for getting all of your data and getting your models into shape for that eventual cooking that you do with your machine learning and other types of models. And that is the recipes package. Recently had a 1.3.0 release updated on CRAN and, tidy models software engineer at POSIT. And Mel is back with, a nice roundup of the key new features that you may wanna pay attention to.

First of which, admittedly, this one gives me flashbacks to my early days of r and especially reading certain CSV files. You'll know why in a second. Because there is an argument that's been around in the recipes ecosystem, strings as factors. Now before you run away from your earbuds here and wondering, oh, no. Why are you bringing up such bad memories here? No. This is this is important because there are situations where by default, a string variable that's in your predictor set of variables that you're gonna use in your model would be converted to factors by default, but that argument was located in a function called prep.

Recipes, like a lot of the tidy models factors, have a lot of function names that definitely either make me hungry or make me wanna cook right away, nonetheless. Now that argument, string as factors, has been moved to recipe itself, the recipe function to be exact. This is important because now you can take advantage of probably disabling this conversion in many different places in your recipes workflow. So, Emil's got a a nice example where they can see what it looked like previously and using in the prep function, and now using it in the actual recipe call right off the bat so you don't have to keep using it over and over again if you have multiple prep calls in your in your pipeline for getting your recipe ready to go.

And then also, there are some deprecations to be aware of, one of which is for the step underscore select function. This was apparently due to maybe some issues people are having in their recipe workflows, and it didn't really play as nicely what they felt was going on in the rest of the workflow. So there are cases where you can kinda migrate to some more, fit for purpose functions on this. One of which is step underscore r m, which then you can feed in the columns that you want removed in that particular step without doing the negative or minus sign annotation that you might have done before with step select.

And also for selecting variables, they do recommend that you are able to use some of the more select helpers, which actually gets touched on later in the post from the tidy select package that's often using dply or tidy r and and the like. They're bringing better support for that in the recipes package itself. And then lastly, there is, a new argument being added to the step underscore dummy function. And there's nothing dumb about this. This is the the convenience function to convert a variable into indicator like variables often in categorical outcomes or predictors so you can use these in many of the machine learning models.

And in the past, if you want to specify how the contrast associated with that categorization were performed, you had to feed that into an option statement such as options, contrast, equal, and then you might give it the either treatment, c o n t r dot treatment, or the c o n t r dot poly. Those are two ways to define contrast. Now you can define that in step underscore dummy itself. To me, I tend to not use options unless I absolutely have to because I think it does break a little bit of reproducibility.

And somebody that I'll help debug code with has sent an option way, way, way somewhere else in their dot r profile, in their home directory, or some other place. And I'm like, what gives? Why is this digit rounding happening here? What what what's going on? So nothing I'm bitter. So I think anything you can have more transparent in the function itself is a win in my book. So I think that is a great, great new, new, capability. And then also, there has been some performance improvements where the step underscore impute underscore bag function, which is often used whenever you have to do the bagging pro preprocessing for your machine learning model, such as random force or gradient boosting. And when I've used those in the past, there have been memory issues in the past for tree models.

And now they've minimized the footprint quite a bit. And, in the example, m o sites here going from 75 megabytes run bagging step now down to 20. That's a nice nice win when you scale this up immensely to different, CPU architectures, especially if you're trying to be nice to your HPC admins, which I try to do. And there's a lot more to this re release. The blog post has a link to the release notes if you wanna get more details. But it's great to see the momentum on tidy models keeps pushing forward.

And, yeah, even the, artwork in this blog post makes me wanna eat right after this, Mike. How about you?

[00:07:19] Mike Thomas:

Same here. Yeah. A couple interesting things, you know, on the removal of, the step select are essentially deprecating that. It looks like the recommendations there are that you use just dplyr select to select which variables to use in your model before passing the data into the recipe where possible. And now that step select is deprecated, they're recommending that you use step remove. So that will, by default, remove the variables that you include in that statement. But if you negate them, it will include the negated ones. So it's almost like a flip of what step select was previously doing. So you may have to wrap your mind around that a little bit if you wanna continue to leverage this inside of your recipes workflow. And then the other one, Eric, that I'm not sure if you touched on was that tidy select can now be used anywhere, and everywhere within recipes. And I think previously, there were a few different steps or or functions, if you will, like step underscore p l s and step and puke bag that we also talked about that have maybe specialized arguments that previously asked you to select variables for the arguments of those functions in a particular way that was not compatible with your typical tidy selection, and now that is no more. Tidy selection can be used pretty much everywhere throughout all of the different functions within the recipes ecosystem. So that's definitely a great quality of life improvement here. And, you know, that memory reduction for step impute bag for those that fit a lot of bag tree models, I think is great. You know, I was a little surprised that Ames dataset that they use in the example, I think is pretty small. Right?

And that took 75 megs previously and now takes 20 megs. So if you scale that out to maybe a significant size dataset, I don't know, you know, what the relative size of your average machine learning dataset would be compared to the Ames dataset, but, not only would that that memory footprint probably be bigger than those numbers, but also your what you're getting back is is gonna be bigger as well. So that's a great improvement there. And always love to see that the Tidymodels team continues to work on the recipes package because, in my opinion, and I know that it's an opinionated framework in and of itself, but it is just fantastic for doing machine learning for the hardcore machine learning folks out there to be able to set up these workflows and sort of like this purr like fashion that allows you to really iterate over all sorts of different combinations and different algorithms that you wanna leverage really quickly in a way that, you know, we used to take a crazy amount of time from my days in Karat.



[00:10:01] Eric Nantz:

I was gonna say the same thing. That's how I first got into machine learning of ours was the Karat package. And so I remember being one of Max Kuhn's, early days of the Karat package. He did a workshop at a local stat conference here in the Midwest, and I was mesmerized by the power of it. But, boy, it took me a while to get my head around it. Recipes, alongside Tanya Miles itself, I really like the having dedicated packages for each part of this process. And if you're wondering, like, where do you go to learn more, we'll put a link in the show notes to the Tidymodels website, which has tons of resources, the online book. There are actually more than one online book about all this, some of which is more of the practical day to day, and others more on the statistical theory behind a lot of the choices. There's a lot a lot to choose from here. And like you said, Mike, it's really great to see see this getting pushed forward. And, even though my day job doesn't do as much of the machine learning side as it used to, there are cases where, especially when you get to that infamous exploratory analysis perspective, you're asked to do a lot with less, and you could do much worse than throwing some tiny models action in front of that dataset to find that hidden subgroup of of records that shows that improved effect. Don't ask me why I say it that way. It's a volatile industry. What can I say?

And up next, we're gonna visit the, visualization corner as I like to call it here on the highlights program because, it's an annual tradition since about 2022 or perhaps sooner. Around the springtime, there is a thirty day chart challenge that's, released online, and the thirty day chart challenge was originated by Cedric Sher and Dominic Royal. This is, like I said, been running a few years now, and we've seen a lot of esteemed members of the art community take their, take their, visualization chops to work and learn some new things along the way. And in particular, a frequent contributor to the highlights program, Nicole O'Raney, has bat is back with her roundup and her perspective on what she created for the 2025 edition of the thirty day chart challenge.

And she's been doing this for a few years now, but she kinda did things slightly differently this time around, which I think are kind of intriguing. And one of which is she tried to challenge herself in different technical stacks. She is definitely primarily an R user, but as we've seen in previous highlights, she is venturing into other visualization frameworks along the way, notably, Observable and d three for some great interactive web based visualizations. And also alongside this, in this thirty day chart challenge, they don't necessarily tell you what the data is you're visualizing, unlike the TidyTuesday initiative that does ask you to create a visual based on the dataset in mind for that week. Here, you might be asked to do a a challenge of a visualization showing relationships between variables or maybe a time series visualization, but you get to pick the data.

So what she was trying to do to save time and re and kinda challenge herself at the same time is to reuse datasets in multiple ways. And we'll get to some of that shortly. And then lastly, try the upper, annotation game, so to speak, on some of these visuals by adding more text and annotations to these charts to really, you know, show some key insights in these, in these visualizations. So when we talk about the data she used, there was one particular dataset that she drew upon for multiple visualizations and that was a set released by the our world and data project on income inequality as shown by the share of income that's received by what is deemed the richest 1% of the world population.

And she thought this would be a great way to you visualize not just that dataset on a whole, but also look at where how it relates to other data sets that may or may not be in that same domain. And this rather, fun plot we're about to talk about here is getting back to maybe some examples you've had in either a statistics or data science course where the underlying theme is correlation doesn't necessarily imply causation, because this awesome line plot here is showing how, apparently, as the rich got richer, so to speak, and taking the more, the share of the of the income, some reason, the population of wild birds in the world just started to decrease in that time span.

Are they buying the wild birds and just keeping them capped? I I hey. Hey. You didn't you didn't hear from me. I have no idea. But if that doesn't make sense, you're right. It doesn't make sense. That was definitely for the, spurious relationship side of the challenge. But that was, an interesting take on it because usually you hear about the ice cream and murder rate example. This was this was a fresh take on that, so I I enjoyed that. And then also, that that's just one, plot that she made. If you're interested in all the plots that she made for each thirty day or thirty of the thirty day challenge, she actually created a Shiny app powered by Shiny Live and WebAssembly, which we'll link to in the show notes. And you can look at each day and what she created, which will have links to the source code behind those visualizations. So I have fun, in the in the prep for this episode, just thumbing through that. And there was one other one that caught my eye maybe for, again, a lot of the wrong reasons, but this time very much intentional.

There was a part of the thirty day challenge where you were asked to create a chart associated with extraterrestrial domains like aliens or whatnot. So she makes this, I I do I had to I had to say this abomination of a bad chart, which looks like my screen is glitching in different parts, like if somebody hit the hit the screen in the monitor because it's all weird, jaggy, fuzzy resolution. But she made this chart. You'll have to see it to believe it. Apparently, in future blog posts to illustrate just what can go wrong in these visualizations, and I can already see about five or six things horribly wrong with that chart. So that was, that was a doozy there. But, again, you can check out the, the rest of the visualizations in her Shiny app that she created. Well, again, I have a link to that in the show notes.

But again, this isn't just about being creative. She actually learned some great, you know, new and interesting technical observations too, Mike. So why don't you walk us through that? Yeah, Eric. Some of these plots are really interesting. That extraterrestrial

[00:17:12] Mike Thomas:

one is pretty funny as well. You know, one of the things that Nicola among the many things that she was trying to try out, you know, knew during this thirty day chart challenge is she leveraged not only g g plot, but also explored the tidy plots package. And she was really encouraged by how easily portable her ggplot code was to the tidy plots framework, it seems like, and just how clean the syntax was. And I know that that's one that we've touched on a few times in the highlights. It's not one that I've had the chance to get to yet, but I have been blown away by everything that I have seen from a documentation perspective.

On the tidy plots package, I think there's a great landing page for tidy plots that has some fantastic examples. And the visuals just look beautiful. And when you take a look at the code behind the visuals, it's it's almost shocking how, concise the the syntax is, how few lines of code it takes to get to really nice plots using tidy plots. Another thing that she explored on on the R side was using the g g I r f package in JavaScript to create drop down menus. I'm a little interested in how she did that and if that ported over to her Shiny Live app as well.

If it was some sort of an HTML widget that she was able to create. Are you familiar with doing that, Eric?

[00:18:29] Eric Nantz:

I have not tried that, so I'm really looking at that code too after this episode.

[00:18:34] Mike Thomas:

Definitely. And then she built a a function to do something which is something that I always struggle with depending on what operating system I'm working on and what dependencies are there and which aren't, but take a screenshot of charts that you built. And she leveraged, the h t t p u v package, particularly, a run static server and then a stop all servers function from that to, I guess, spin up a server and then use web shot two to actually take the picture, to take the screenshot and then store that as an image in a particular folder, using this really small, nice, little tidy function she developed called save j s p n g. So that was great. And she also leveraged some Python as well. And she still wants to work on some examples of combining Python data processing and visualization in Observable via Quarto, which I can attest to is super easy because we have that just great ability to pass our and Python objects into observable OJS data frames, I think, with, you know, very, very little code, and that's all documented in the Quarto website. So I would encourage you to check that out if you'd like. It's great for static servers, like GitHub pages and things like that, or even just sharing an HTML file with someone if you don't have really the capacity to stand up, you know, a whole server for a a shiny server type of web application.

So that's awesome. She had some advice here for chart challenges if you're somebody who's interested in getting into these thirty day chart challenges to try to push your data visualization skills. And one of her biggest pieces of advice is try to use the the same data for for multiple charts. And I think that that's a great piece of advice to try to get you to think less about the data and more about the visualization side and and really try to open up that creative side that at least me and maybe some other data scientists out there, may be weaker on, you know, compared to the the data processing side. She had a couple highlights of some other favorite charts that other folks built throughout the challenge, which are are absolutely beautiful. And and like you, Eric, I loved checking out her Shiny Live app and just taking a look at all of these different charts through the past four years that she's developed across thirty days. So I guess that's a 20 different possible charts that you can take a look that Nikola developed. And not only can you take a look at these beautiful charts, you can see the source code behind them. And it, again, makes me wanna create my first Shiny live app that is something I still haven't gotten to either, but it's, it's high on my list

[00:21:11] Eric Nantz:

here. We gotta rectify that pretty soon. I know. Because, you know, it's moving fast, man. It's moving fast. But I've yeah. I really enjoyed all these visuals here. And, you know, I have the the creative chops of, of a stick figure when I do visuals, but, boy, it sure is inspiring to see what Nicola and other great, community members, are able to come up within these in these chart challenges. So, yeah, if you're ever if you're on, like, the Mastodon or Blue Sky, you'll see a lot of these charts being thrown out there on social media, much like the TidyTuesday visualization. So it's a lot a lot you can be inspired by. Sometimes you'll see them in the most unlikely places too because I actually did see it wasn't in the thirty day challenge, but, you know, during the hockey playoffs, I've I frequent the hockey subreddit just to see what, you know, angst or excitement is out for various teams. And there is somebody that did a visualization of the brackets of the teams progressing to the next round, but they did it in the style of Lego bricks.

Bernie, you would have little Lego dots indicating the wins and then and and showing the team with, like, a nameplate with the the names on them. It was and the Stanley Cup in a gray kind of, Lego mosaic, like, setup. Man, there if I got, like, two months of just uninterrupted time, I would create that in our just see how, you know, ugly or good it looks because there's lots of no matter where you look, there's always places to up your game. And I definitely appreciate Nicole taking, you know, some new ideas, new frameworks, and seeing, you know, where she could push the envelope on.

And, also, you know, when you're doing interactive visualizations, not necessarily the topic of this post here, boy, I just love those those packages out there with what we call the batteries included, so to speak. I've had another project at the day job where ECharts four r by John Coon has saved my behind on this really nice line plot. Interactive zooming on the x axis right off the bat, tool tips right off the bat. It just is so elegant. I think those are great. When you have the idea for the visual, you just don't have the time to get into the nuts and bolts of JavaScript and CSS land.

EChars4R is one of those that just takes care of so much for you. So I'll never turn on a chance to plug that awesome package. It's the best. We're gonna take quite a diversion in the highlights because we're gonna go from a visualization corner to really challenging yourself with some more fun math teasers. But, admittedly, after I play with this, I almost can't believe it works, but it actually does. So this blog post for this next highlight comes to us from a fellow Our Weekly curator, Jonathan Carroll, who as you've heard in previous highlights, he loves to test his might both on interesting mathematical challenges and interesting languages that conduct them. And this post has both.

So in this case, he was inspired by a post that he saw on Mastodon, that was sent by Greg Egan on in this kind of brain teaser that actually has a solution. So imagine you have a number that has a a set amount of digits, let's say, four digits. And then you want to, what we call, rotate the numbers. So imagine, you know, the number one, two, three, four, that's four digits, and you wanna rotate it so that maybe the the one and two go behind the three and four. It's almost like if you have pieces for this, you're just kinda shuffling the order a little bit.

Apparently now I'm gonna try and narrate this in audio as best I can. There is a mathematical derivation to get to that solution for any number if it meets one of the following constraints. If that number n is not the same as 10 to the power of the number of digits of that number minus one, there is a formula you can use to get that derivation. It goes like this. The n the number n times 10 to the power of the number of digits, k in this case, and the mod or module, which is like in division, getting the remainder of that, the re and the the right hand side of this is 10 to the power of the number of digits minus one.

Yes. This works. I tried this out. I voted my r session. I did that number one, two, three, four. I wanna rotate it by three digits and then see if I could get that right answer, which is four, one, two, three. And, yes, that formula actually works. I'll put that in the well, John did that as well. He almost didn't believe it worked either, but he thought, okay. How do I make this more general? It's great for a specific example. He has solutions in his blog post, both in the r language and the Julia language, which as you read it, they do make a bit of sense. He's able to get the length or the number of digits in that number.

An interesting trick in r to do that is that the n char function will automatically convert, like, an actual double or integer to a character and then get the number of digits. That's kinda slick in and of itself. But you have to watch out if it's a negative number. It won't quite give you the right answer in that case. But then he's able to plug that in and then do, you know, the mod operator and r, which is 2% signs, and get a function, you know, very simple function of one liner to get this done. And sure enough, it works.

Not quite vectorized, though. So that's where he has an interesting package called vec, which he's written about in his his blog before, which apparently does some neat trick with ring buffers and using the indices of vectors to get to that same solution. So, again, all the code in this in this in the, blog post, and this function rotate indeed works in both ways on that. Julia, again, pretty similar, albeit he does think it might be a bit simpler because there is an actual n digits function that is fit for purpose for actual numeric values and not just strings, which means it isn't as susceptible to that negative sign issue that the r solution might be.

But there are some other little gotchas along the way you have to be careful of. But, again, it's a one liner. And when you read the code, even if you're not a Julia programmer, you can kinda get a hang for how that works. And they do have a built in way to deal with vectors with using, you know, built in Julia primitives and doing cyclic rotation. And, again, all straightforward, at least in my opinion. Here's where things get odd, folks. Because as John has done in his previous blog post, he ventures into very niche languages for mathematics, and he talks about two of them here.

And this is where you have to flip your perspective a bit because now you have to deal with glyphs as the operators, not just typing just to be fancy. These are actual operators in these languages. And even in the case of this APL language, which he has written about before, and I think we featured on previous highlights, the order isn't left to right. It's right to left. So that's a lot to get your head around, but he narrates through how he found the different glyphs for these various operators going from the typical assignment all the way to the mod operator, the negative operator, and and men and and more. And then when you read it, I mean, I I can't even narrate it here in audio, but you'll see it looks pretty fancy, like, almost like a mathematical equation.

But my goodness, it works. And he puts a link to the, try a pl.org interactive web based editor. If you're not convinced, you can try this stuff yourself and convince yourself, but it indeed works. And then if that wasn't enough, there is a newer language out there called UIUA or Yuya. I have no idea how to pronounce that one, but it is using stack based language approaches, which, apparently has its own kind of domain specific operators, again, in glyphs as well. But this is a newer language. So there's not as much documentation out there ever than the official docs. But sure enough, he's able to get a solution in place, but he has to think in terms of, like, stacking things on top of each other to make this happen and not just the typical left to right or even the case of APL, right to left.

So that, that that definitely got my head scratched a little bit. But, again, fun brain teaser to look at. But this is a great way to challenge yourself. I'm one of my wife's group chats with some friends. She has some friends that do these math teasers, like, every week, and they all I think they would be all over this kind of language if they haven't heard about that. Hadn't heard about it already. But again, if you are not convinced that works, I I would say the proofs in this post and the best part is you can try this out in multiple languages to feed that curiosity or in the case of me, humble myself for what I don't know about how this stuff works. So, Mike, they may talk me talk me up a bit. Am I that bad with math or is this just really out there?



[00:31:18] Mike Thomas:

I'm gonna decide on this is really out there. Makes me wanna go back to the data visualization post because I feel like I'm over my skis. But this is I'm just teasing. This is it's a good blog post for me to get back into mathematics, get back into my mathematics, get back into my roots a little bit, and, you know, tease my brain on this. And like you, I I implemented this in r as well just to try it out for myself. And that's sort of the name of the game for how I was able to, I think, learn math more efficiently than I did prior to learning programming because, you know, if I could program it, I could understand it. Right? And I think that that goes for a lot of folks out there. And initially, I think when I I looked at this just reading the blog post, I was like, what is going on? And then put it to code a little bit, and it it started to make more sense to me a little bit. It's a cool little brain teaser. I think sometimes you are able to create some of these, you know, mathematical brain teasers if you if you'd like sort of backwards. And I'm curious to understand who originally sort of came up with this concept and and how they arrived at it. But it's it's super interesting. It's always funny to me sometimes I see on social media, like, somebody claiming to do magic with math and and, you know, telling, hey, what's what's your age? And then figure out, you know, okay, subtract that from what year it is and then multiply it by nine, and then they arrive back at their age or something like that. People are mind blown, and it's it's like it's, it's pretty straightforward. So this one, certainly not as straightforward, especially when you get down to the last two sections where we get into APL and UI, UA, where we're starting to use hieroglyphs, in my opinion, to, stitch together some of these equations. And we're going from, left to right to to right to left. Is APL the one that literally stands for a programming language?



[00:33:10] Eric Nantz:

You got it. Great callback to a previous episode.

[00:33:14] Mike Thomas:

Oh my goodness. I can see why. But I really appreciate, you know, taking a a small little brain teaser like this and, implementing it across these four different languages just to showcase these different languages, the differences between them, the strengths and weaknesses that they have, and serve as an introduction to some of these languages for folks like me who are certainly less familiar with them. Always always really interesting to read a Jonathan Carroll blog post.

[00:33:40] Eric Nantz:

Yeah. And I do have some, co colleagues at at the day job that I think would be all over this if they get a free moment. So I'll probably send this their way and see what kind of brain teasers they can come up with and and see if APL makes sense to them or not. Boy, I just I'm I'm amazed, all kidding aside. I'm amazed at the the creators of all this because I don't think I could come up with this if you gave me ten years to come up with something like this. My goodness. I have a I have a hard enough time just of art itself. So this is this is amazing stuff.

And and speaking of amazing, so we're going back to r for a second. We will close out this episode with our last highlight here, which is, been a mainstay in the art community in terms of this content for years and years. It's may have had different platforms have been shared on, but Joe Rickert, who has been, one of the founding, you know, members of the art community that I've been following for years and years is back with his at monthly top 40 r packages roundup. I'm sure if you've been around the community for the years that Mike and I have, you may have seen this in his previous Revolutions blog post back when he was at Revolutions.

That was in a deposit, and now he's got this in the Our Works blog, which is a community effort led by him and Isabel Velasquez, with other contributors as well. So, again, we definitely don't have the time to talk about all all 40 packages that Joe has come up with here. But as usual, he organized these by different domains, different industries, or or different topics. And there were two that definitely caught my eye because they are relating to different projects I'm working on at the moment. The first of which is a package called AutoDB.

This one's interesting because this one here is a way to, in essence, help you normalize a data frame by going to the third normal form. And, admittedly, I don't know the nuts and bolts of that definition. But if you dealt with databases before, this will look a bit familiar to you. This has been authored by Mark Webster, and he does point out in the vignette that we'll link to in the show notes, he is using this as a way to inform data cleaning operations, investigating his data. He stresses that this is not meant for database design.

Even though you will see graph you know, tabular representations of the different relationships in these datasets, it'll look very similar to a schematic you might have for a robust database table relationship diagram or an ERD, I believe they're called. But, again, really interesting because I am dealing with data at the moment coming from this API of this, vendor with some other datasets we have in house, and we do have to merge them together. And more importantly, we have to make sure that the constraints that we have told the vendor to follow are actually being met. So I could definitely see this as a tool to investigate these relationships and make sure things are are sound.

And maybe it goes in ICU of a package like point blank by Richie Yoon that also looks at their data quality as a whole if you have an ETL kind of process like I'm dealing with. So that one caught my eye. And then on the visualization side of it, a package called Kite Square offered by John Weidenhauft. Hopefully, I said that right. This is a way to visualize contingency tables, which if you are dealing with you know, if you have, you know, often you'll have, like, observed versus truth or rater agreement. You often do contingency tables for this.

This is an interesting way to have kind of a square grid, but then showing these, either discrepancies or similarities between these variables. And he he he likes the name kite because it rhymes with chi as in the chi square test that you often see in contingency tables. So I'll have a link to the GitHub repository for the package in the show notes, but definitely an intriguing visualization to kind of bling up that what can be a pretty utilitarian table representation. Maybe get a little visual around those marginal effects and those counts all in the same, umbrella, all powered by g g plot two under the hood. So you can feed this in a lot of different places.



[00:38:26] Mike Thomas:

But that's only two of the 40 here, Mike. Did you see any that caught your eye as well? On the risk management side, which is a space that I operate in quite a bit, there's a new package called EQRN in all caps. I got a version one dot o, zero dot one dot o release, and it provides a framework for forecasting and extrapolating measures of conditional risk, for example, you know, of extreme and unprecedented events, which we seem to have more and more of lately, including quantiles and exceedance probabilities using extreme value statistics, which is a concept I'm familiar with, and flexible neural network architectures, which is interesting. There's a couple papers behind it and and quite a few folks that worked on this package, and it has a nice package down site, which I am in the middle of checking out right now. So I'll one, if you're in the risk space to look out for. Awesome. Yeah. I I thought that might catch your eye, so I wanna make sure

[00:39:22] Eric Nantz:

you got that one. And, yeah, very nice documentation here. So I could see this being very useful in your in your toolbox for risk based assessments and analysis. Yep. Great. Great find there. And like we said, there's a ton more in this blog post for you to choose from. And even for the for the, colleagues in life sciences, there is, I kinda say this with with with kindness. Yet another tables related package, called Clinify. I I I laugh about this because it was not long ago, folks, that we didn't have any great table packages in our and now I don't wanna say too much, but let's just say there's a lot of choice to be had here, and that's been a hot topic in some of my circles recently. But, again, great to see advancements on on that side of it too.

And, again, that's just scratching the surface's entire issue. Right? There's a lot more to choose from of additional packages that have been released, a great new set of resources, and we don't have time for additional finds because we had a whopper of an episode today, but definitely definitely check that out. See what catches your eye, and thus, yeah, you'll be able to get in contact with either the source code behind that great package or blog post, but also maybe make some connections in the future.

And we also like to connect with all of you. First of all, the projects definitely keeps going because of contributions from you out there with your poll request for new content to be shared in the upcoming issue. How do you do that? Let's go to rww.rrg. We got a link in the top right corner in the, in the page, a nice little ribbon structure. You can click there and get to the GitHub repo right from there with a nice issue template. You can also get in touch with us personally on the, episode show notes. We have a link to the contact page. You can send us your feedback there and get in touch with us on the various social media outlets.

I am on blue sky the the I am on Blue Sky these days with @rpodcastatbsky.social. Also, I'm on Mastodon with @rpodcastatpodcastindex.social, and I'm on LinkedIn. You can search my name, and you'll find me there. And, Mike, where can Alyssa get a hold of you?

[00:41:35] Mike Thomas:

Sure. You can find me on blue sky at mike dash thomas dot b s k y dot social. Or on LinkedIn, if you search Ketchbrooke Analytics, k e t c h b r o o k, you can see what I'm up to. Awesome stuff. And again, thank you so much for joining us for listening to episode 204 of our weekly highlights, and we'll be back with another edition

[00:41:57] Eric Nantz:

next week."
"15","issue_2025_w_18_highlights",2025-05-02,44M 19S,"In this episode of R Weekly Highlights we hear from industry experts on how they choose a programming language for their projects, a big boost to the use of copilot for building your next Shiny app, and the learning journey of a new R user. Episode Links This week's curator: Sam Parmar - @parmsam@fosstodon.org (Mastodon) & @parmsam_…","[00:00:03] Eric Nantz:

Hello, friends. We are back at episode 203 of the Our Weekly Howards podcast. We are a little bit later, than usual because, yeah, real life happened for both of us in different ways. Poor Mike here was the victim of my diatribe in the preshow that we may or may not retread here. We probably won't. But we're happy to be back this week with covering the latest highlights that have been shared in this week's our weekly issue. My name is Eric Nance, and, again, I'm so happy you've joined us wherever you are around the world.

It is, already month of May. My goodness. Almost halfway through the year. It does not seem real, but it is a happier time of year. I always feel like once we get out of the February doldrums that things start to pick up a little bit amidst all the chaos that can occur. But as always, I'm joined by my awesome cohost, Mike Thomas. Mike, how are you doing today? Doing well, Eric. Yeah. A couple extra days for us this week before recording has helped me fully charge my batteries at least. Oh, be thankful that you didn't have any failures on that. So Mike here is referring to a recent car mishap I had the overnight, which, never an ideal time for those things. So, yeah, folks, if you do drive cars regularly, check those battery levels. Sometimes things go haywire, man. Thank goodness for warranties.

Okay. The good news is I don't need a car to do this show. I am right here in the humble confines of my, recording environment here, and we get to come talk about some really fun stuff in this week's Our Weekly Issue. And as always, if you're new to the process, we always have a curator that takes the issue every week. We rotate among our team of curators. And this week, our curator is Sam Parmer. He also did a terrific job as always, and he also had tremendous help from our fellow Aruki team members and contributors like all of you around the world with your poll requests and other great suggestions.

So we lead off with a very typical kinda tale in terms of building solutions, and you might be facing kind of a fork in the road, wherever you're new to data science, new to software development, often trying to figure out what is the best tool for the job. And sometimes that tool is, in essence, the programming language itself. And our first highlight here is actually a recent new article from Nature, the Nature journal, highly regarded in the world of science, authored by Jeffrey Pirkle. And this is a very, quick, you know, very short article, but in a good way. It's very concise. A lot of interesting feedback from various esteemed colleagues in different industries on what they would say are key questions and answers to key questions for those that are new to the world of software development and and coding in general, and how they can make the appropriate choice for what language they might use for their given task.

So the article starts off with, you know, just what is programming in a nutshell. And guess what? Like many things in this article, it'll kinda depend on what perspective you're bringing here. There are a lot of programming is at a lower level. Think of trying to build a solution that needs to be highly performant, cross, operating system compliant, and like I said, you know, very, you know, fit for performance. You're probably looking at one of the compiled languages where you write code. There is a compiler in the back end to translate that into the machine readable state that your computer needs to crunch through that processing.

In the role of r, there are many packages that do include compiled code from c plus plus. We are not and, of course, historically, languages like Fortran, which I saw in grad school, but also more recently, the Rust language has come, you know, in a lot of focus as well. So when you're in that kind of stack of software development, these compiled languages are often what's turned to first to optimize these algorithms or these lower level utilities. But with that said, if you're coming in from data science, you're often used to having the you often need the ability to explore your data interactively, look at what kind of variables you have, do some interactive plots. And that's where you have the interpreted type of languages, where you're writing code, and then there is a process that's often to the side of your code developer window where you can send that code and get feedback right away.

And here you're looking at, of course, the R language as well as Python or MATLAB as various options depending on where you go for your scientific needs. And more recently, there are frameworks like Quarto that let you put multiple languages in one place if you wanna hop between R, Python, JavaScript, Julia in terms of running that report or that data analysis. So with that said, there's also, obviously, the world of web interfaces where, of course, Mike and I are huge fans of Shiny, available both R and Python. There's other ways that you might wanna look at the language of choice in that domain.

But then we get to, you know, the other meat of the article about these various, industry experts and how they are choosing the language they're using and what is leading to those decisions. So we have, from, Eduardo Secrete. Hopefully, I'm saying that right. He is doing a lot of applied statistics research in The Netherlands, often looks at multiple languages, and looking at the ecosystem around them, like how many packages are available. Are they in his specific domain of psychometrics? And that's where things like MATLAB have been very appropriate for him.

But then others in the communities such as Yanina Balinese Sabini, who you've heard quite a bit in the world of rOpenSci as their community manager along with other initiatives. She is a big fan of r, because often everything she needs to do in r, there's a package for that. She jumps in the article. The only thing r doesn't do is make her breakfast or coffee in the morning. I'm sure there's gonna be a package for that someday. You know? What what else is new? That that will be a big hit whenever that hits.

And so that's that's great advice too. Just what do you need to do, and does the ecosystem support it? Another key aspect is the type of data you're looking at, especially in the world of genomics where not only is the data highly specialized, but the volume of it as well. It has been, you know, known for a while that within the our our ecosystem, the Bioconductor suite of packages is very important in the world of bioinformatics research. And that's where having that available to you to import, say, those gene expression data files and to be able to have custom classes that are tailored to that type of data is really, really important.

And and other situations is, again, the size of data. A bioinformatician named Titus Brown comments in this article that a lot of the hardships that can occur is when somebody's new to r. It's working fine for smaller data, But when they get to analysis that involves thousands of genomes and other genetic data, you might have to look at other languages that have packages ready to deal with large data. Your references, Python might have a broader way, array of tools to do that. I think R has come a long way in the world of big data too. You just gotta know where to look. Sometimes it's not obvious to a new user, but being able to manage data in a file format, we're big fans of things like DuckDV.

Hopefully, things like that start to take a lot of foothold in the world of bioinformatics. And then lastly, another key consideration is when you're encountering issues, where can you find help? That's where, again, Yan Yanina has a great comment here about the R language having a welcoming community of everyone to support each other as well as extensive documentation both with the core, our language, as well as the packages that are available in many of these, industry specific domains. And with the advent of GitHub, many people are sharing packages in the open, So you have an open dialogue to file an issue. If you find an issue with a package, both R and Python are also, you know, very much not strangers to that area.

And, you know, in the advent of AI as well, you know, that's gonna be another consideration as a lot of newer students are are newer to the language. They're probably leveraging a large language model to help with some of that. And at least we are seeing that, you know, with the right models, you can get some help with R, for Python coding. But, you know, we could go on for hours about, you know, how how to best use AI responsibly. But I think in a pinch, I can definitely help you out, especially for some esoteric needs.

So really great perspective from multiple industry experts in this field. And is there a is there a winner in all this? Well, no. We're not gonna pick a winner in this. This is about in your specific domain, there may be an ecosystem around a language that's already ready for you, to take advantage of to get your job done and to really start where you see that that most, you know, wide usage and, you know, a great community around it. So, of course, we're big fans of R here. It literally does almost everything I need. Of course, I am venturing into other niche, you know, languages or niche frameworks like JavaScript when needed. But R is still the engine that performs all my all my, analysis needs at the moment, but it's great to see if I was new to the game. Just what kind of questions should I be asking if I'm looking to make a optimal decision?



[00:10:13] Mike Thomas:

Yeah. I really enjoyed it. Yeah. It kicks off with a discussion about how Python has sort of overtaken JavaScript, from, I think, some GitHub research that was done. And I'm sure that's in large part to everything going on with AI, but that's that's a pretty big deal. And I I think there are probably also a lot of data scientists and data analysts out there that may not consciously always realize that they're developing software. Right? Sometimes if we're just playing around in R, it can can feel like magic or or power Excel if you came from that world. Right? And I really like Jeffrey's articulation of compiled versus non compiled languages. That was really helpful for my own understanding, and a great articulation of sort of the long standing debate over the usefulness of notebooks and struggles that they have with reproducibility.

And the shout out to the Marimo project, which seems to be a great option, it sort of creates a dependency graph in the background from what I understand. I haven't tried it myself yet. And when you change a value in one cell, the dependent cells will be updated accordingly. Kind of feels like or sounds like targets to me. I haven't tried it yet. We use quarto, you know, because it's easy to switch between R and Python within the same notebook tool. But if I was more exclusively on the Python side, I think it would be, my tool of choice, it sounds like. And I also think sort of that big data discussion is pretty much been squashed, right, with the likes of DuckDB and that the parquet format and now we have APIs from whatever tool we choose to be able to successfully leverage those technologies from R or Python.

And Jeffrey, you know, mentions and rounds out the article, a couple different resources, including the Carpentries and the Data Science Learning Community. Those are are two of my favorite resources and maybe the last thing that will touch on is I agree with him that I there's a lot of benefit in my opinion in choosing the tool that your colleagues are using and Eric I think you'd share the sentiment as well but I also think there's value in really going deep in one particular tool and learning it really well one particular programming language I mean and I think then it will become easier to adopt the next tool as opposed to trying to learn two simultaneously.

You know, I did this with R going really deep into R over over multiple years before I really started to try to pick up Python, and I truly believe it helped me pick up Python a lot quicker than I would have otherwise. You know, though I'm I'm no expert, I can get around pretty well in Python these days. And a lot of that stems from, you know, Googling Stack Overflow, ChatGPT, whatever you wanna call it, saying, hey, I do this thing in R. How can I do this in Python? Right? And knowing those keywords to ask are really what unlocks you to be able to get that answer, to be able to to, you know, incorporate that functionality in that lesser comfortable programming language that you're trying to to leverage and use. So I think that that's been a really good strategy for me, and hopefully, it's helpful for others. But this is a really interesting, blog post. I thought it was sort of unique to our our weekly highlights, this type of a discussion, and really enjoyed it.



[00:13:23] Eric Nantz:

Yeah. Me as well. And one thing that really helps as you're, you know, trying to go deep in in these languages, it's always helpful when they even though they are they can have fundamental differences and some bits and pieces of it. But I I I do share your your your, experience there that if you if if I had started with an of a language that was, you know, deep rooted in, say, object oriented principles and other, you know, more traditional frameworks that say R and Python utilize versus starting with SAS.

It was really hard to translate what I've learned with SAS being, quote, unquote, my I guess, tangling my second language. I don't even count Java because it was a nightmare back then. But going from SAS to R was a massive jump to say the least because they were so so different. So I've I've I think we're seeing without me getting on my soapbox about the whole SAS stuff, We are seeing that whatever our Python, that's becoming what most people in such in data science are getting introduced to when they get to their respective coding classes. So I think the principles you learn there will set you up for success success even when you do have to venture off into some of the more niche side of programming.

Obviously, we didn't hear about Julia much in this article, but I know that's getting a lot of momentum as well. But, again, with it striking the balance of the open source paradigm, the object oriented paradigm, I think with the resources out there, the key is seeing the community around it, knowing how to ask the questions that you have, hopefully, having a mentor along the way or user group you can talk to. Again, we're biased in the our ecosystem. We got the great data science learning community.

Wonderful place to join if you're new to the language. You have so many people ready to help you out with the book clubs and other adventures. That's where you have to go kind of outside the confines of just what the language document documentation has to to offer to you. But the time is now to take advantage of those resources. And up next next in our highlights, we did talk about some of the newer ways you can get help for information. And one of the ways that came from a code helper perspective a few years ago is when Microsoft introduced the Copilot, functionality in Visual Studio Code and and in GitHub in generally.

And what can be nice is, you know, I'm using that to help develop, you know, maybe that snippet that you need for that function you're trying to put in your in in your Shiny app, for example. When first co when, GitHub Copilot came out in the very early days, and, admittedly, I was kind of intrigued by it because I never really used a code completion thing before. I did try it out. It left a lot to be desired in the world of shiny development, and I was kinda turned off after that. Now this was about two years ago. I knew things were gonna get better. I just didn't wanna wait around that long, and I kinda gave up on it.

Well, we have learned since then. There are some interesting advancements not just in Copilot in general, but in the way that you can develop a shiny app with Copilot. And so our our next, highlight comes from Peter Stryanko who is, one of the leading, AI engineers and thought leaders at Absalon. And first he had given a wonderful talk at the recent shiny conf, which I invite you to check out the replays if you wanna look at that after the fact. But his article in the Absalon post here is talking about now building more reliable Shiny code with GitHub Copilot, but also a new extension in the Versus code ecosystem from POSIT.

So what are we talking about here? Well, actually getting back to that shiny comp that just concluded a week or two weeks ago, there was a keynote by Winston Chang that in about midway through had introduced some recent work that the positive team has been doing by introducing a new tag in Copilot as an extension in Versus code, specifically the shiny tag. So what does this actually mean in practice? Well, let's say you install the the shiny Visual Studio Code extension, which again is available in the in the Versus Code extension marketplace.

And if you have Copilot already wired up, you just fire up a, an interface for that chat in Versus Code. And then when you're ready to ask it a Shiny specific question, you do the at Shiny tag before you put in your request. This was not around when I first started Copilot. There was no shiny tag or even our tag for that matter. So I'm kinda trying to narrate what I want, and I would just get a whole bunch of junk out. But in this example here, he has, where he's supposed to add shiny tag and he has create a simple app with a drop down menu to select a variable from the m t car set and then a plot showing that variable's distribution.

Now that add shiny tag is doing a lot under the hood. What it's doing is it's basically behind the scenes, really injecting the prompt that is often sent to these AIs with much more additional context around shiny itself. Whereas, if you don't have this, that shiny tag, it's just gonna kinda go through its typical resources that the model's been trained on to do this. So he's got an interesting before and after, situation here where he tries asking this question without the shiny tag versus with it. And there is a stark a stark difference where it is a the without the shiny tag, it looks like a gobbledygook of API, custom API, JSON files, CSS files, YAML files, and other weird stuff under the hood. It ain't shiny, folks. I mean, we can tell that much.

Where, if you look at the example with the shiny prompt, you get at the end, after some questions that the prompt gives back to the user, in this case of which language, r or Python? Chooses r. And then, okay, would you like the where would you like the app to be put? And makes a subdirectory for it. And then sure enough, becomes a simple Shiny app already using bslib under the hood. So that's great. You know, taking advantage of that. And it you know, it's a streamlined app, but it it it got the job done, apparently.

So if you are developing Shiny and you're leveraging the Copilot, you know, extension, you owe it to yourself to try this out because I think you're gonna get a much better results in this than if you just don't have the the shiny tag to help you out with the prompt behind the scenes. Now, again, I have not tried it with the new shiny tag. It's on my list to do. I wanna make a note for those who may be wondering, wait a minute, Eric. Why didn't you mention Positron? This is not available in Positron yet. I know that's in the works, so we may not see that until later this year. But if you're on Versus Code already, yeah, maybe give it a shot and see if they can bootstrap an app for you in a seamless way. So,

[00:21:04] Mike Thomas:

again, caveats abound, but, hey, there's progress to be made here. Yeah. Eric, I really appreciate this work by the Posite team. You know, I'm sure it's no small effort to convert all of the shiny documentation to markdown that can be injected into the user prompt. I'm assuming that's sort of the approach that they're taking, and it sounds like they did so on both the R Shiny side and the Shiny for Python side. So I'd be really interested to learn about how they went about doing that, I'm assuming in a some sort of a programmatic approach so that when, you know, that R Shiny package gets updated or when the Shiny for Python package gets updated, you know, that shiny tag in the Versus Code extension will reflect those updates for the sort of current best practices for those two packages.

And this prompt stuffing approach is one that we use very often, to try to pretend that the LLM was trained on a specific set of documentation or or context that we wanted to know about. With this approach, you're definitely going to get way better results than you would without doing any of this prompt stuffing. But behind the scenes, my understanding is that the LLM is sort of using a combination of the prompt that you provided, that context, as well as what it was trained on and and hopefully, weighting much more heavily, your prompt than the context that it was trained on. But I think you're still running probably a a non zero chance of it hallucinating on a particular question that you're going to ask. So like any of these AI solutions, don't take its output as pure gospel. Make sure that you, have a some sort of a workflow and approach where you're ensuring that either you have the the documentation for Shiny pulled up on the other screen and you're just, you know, leveraging.

You're you're using that as a gut check and leveraging the the LLM to try to get you to your final result faster as sort of a co pairing, a pair programming guide, which I think is a fantastic way to do it. But, yeah, I think this is, as I mentioned before, going to get you way better results than probably what you've been trying to do in terms of asking ChatGPT to write your Shiny apps for you.

[00:23:17] Eric Nantz:

I actually have, I'm I'm intrigued to try this out for a a real case, I would say, where I'm about, you know, without reviewing too much here, I'm about to get an influx of requests from different statisticians at my company to help build some shiny apps that may vary in the level of complexity. They may start small, they may end up getting bigger and whatnot. That's beside the point. I've been asked to, you know, at least look at sketching something out to give a demo that may or may not, you know, get the project get green lit for more robust development.

I've been contemplating whether I try something like this to boost wrap the initial version of this. Now putting this, your thoughts on this, Mike, do you think I should, if I was gonna do this, ask it to start building an app that kinda follows what typically you and I prefer for our app structure, I e a golem powered app as a package? Or do you think that might just be a bit too much for it to do right away? Maybe I should stick with, you know, a more traditional app layout. And then down the road, I convert it to a goal on that. Do you think an LOM could handle something like that?



[00:24:27] Mike Thomas:

I don't know. It's a good question, you know, and I think in a if you could, in a similar way to how Posit has converted the documentation or leverage the documentation for, Shiny for Python and and R Shiny, maybe you could also stuff the context for the documentation for Golem or b s lib. Right? I would imagine, into that prompt as well. These context windows are getting bigger and bigger. So I think it's it's worth a try, but if you were to to sort of do it do that approach without providing it any context around Gollum, I'd be interested to I wouldn't have a lot of confidence that it's gonna give you great results.



[00:25:10] Eric Nantz:

Yeah. I I've learned that, you know, when I'm building these initial prompts, I'm a very detailed oriented person with these things, whether it was back in the old days. If I had, like, when I say old, this is only, like, ten years ago. If I had a colleague that was instructed to help me with programming support for, like, this custom biomarker analysis, and I would give he he or she the very detailed specs of here's the input data. Here's the layout I'm looking for. Just code this up, and then let me review when you're ready. A bio would always leave no stone unturned with, like, type of variables to look at, the type of output, you know, these considerations, any derivations that they need to be careful on. I typically use that same approach for prompts, but I always wonder, am I being too detailed about it? So far, I haven't been, but I think, like you said, the context is really important. I can't expect in only a few sentences that this thing is gonna know what to do because what human would be able to do that either? So another perspective to keep in mind, I guess. Yeah. No. I envision a world I think everybody does

[00:26:12] Mike Thomas:

where someday instead of using this approach to provide, you know, additional context to the LLM about what you wanted to know to be able to have a better approach to fine tuning, I guess they call it, these LLM models so that really it only has the knowledge, you know, of the context that you want it to have.

[00:26:34] Eric Nantz:

Yep. And I've been seeing bits and pieces in in these areas. So you often see, you know, these in in within industries, some of these specific, you know, bots or models that are being shared in Hugging Face or other areas. And, yeah, I'm really intrigued to see see where this ecosystem goes. And, yeah, I'll give this a play. And in the end, would I ever just sign off with some about reviewing it first? Oh, heck no. No. I'm I'm not gonna if any of my colleagues are listening to this, don't worry. I'm not gonna I'm not gonna throw something over the fence if I don't vet it first. So take that to heart, folks.



[00:27:09] Mike Thomas:

Well, if at Catchbook, we develop an AI agent for Shiny apps specifically, I promise we'll name it Eric.

[00:27:16] Eric Nantz:

That's it, man. Game over, man. It's game over. Well, in in about as soon as you hear our last last highlight is kind of going back that we talked about at the very beginning for those that are maybe new to programming and informing what kind of choice they make for their language or programming language they're gonna use to accomplish a certain task. Well, let's say you've already made that choice of you're gonna use r to accomplish that task, and maybe you don't even know what that task is. Yeah. You're just learning from the ground up here.

Our last highlight here is an interesting perspective on what's been helpful for a recent, undergraduate to learn our in the humanities, area. So this, blog post comes to us from Bruno Pone or or Pone. He is a now a data analytics consultant at the data school in Deutschland. He is talking about what some of the things that he has encountered with applying, you know, data science type principles to history and the humanities. So his first encounters with r itself, was when he was in his master's of studies at the Hertay School in Berlin in their statistics department.

And there were a couple mandatory courses, and one of them talked about some of the statistical concepts that he became, you know, interested in. Things like validity, selection bias, principles like regression to the mean. They kind of captured his interest and noticed that, hey, these could be applied to multiple industries, not just, like, statistics as a whole. So within within that kinda area, he encountered his first R programming assignments, and it felt a little frustrating to him. It was just totally new to him. Lots of function syntax that got a little frustrating, debugging errors, and was, you know, trying to figure out how to best proceed here.

This is where, going back to what I said in in the earlier part of the show, knowing where to get help and knowing any communities around that language can be extremely helpful. In in his case here, Bruno, discovered that there was, as we've heard in the community for years, access through his, master's, program to a platform called DataCamp. And for those aren't aware, DataCamp is this is definitely not free advertising. I'm just saying what they do. They just offer focused courses with video content and in browser exercises all within their platform. You don't have to install anything on their system.

Now Eric's editorial comment here. You may wanna think about different sources in that, but we'll leave it at there. But with that, there are plenty of services like DataCamp out there. If you're interested in some more interactive content, that can be a great place to jump start your education. So like I said, there's a lot out there. Definitely, let us know if you're interested in what those are. We're happy to send you links about those other services. But once you get through that up, you know, that little hump, if you will, in your development, now he gets to actually use some of the things he's been learning in this domain that he's looking at in policy analysis and really starting to leverage in those statistical concepts that you mentioned earlier within our deduce visualizations, maybe do some ad hoc simulations to illustrate the the impact of certain concepts.

And then, of course, like all of us, we we became the victim of the pandemic, which changed the world in a lot of ways where we often were, you know, confined to our various homes or or other locations. And because of that, apparently, because of budget considerations, his institution, stopped providing those, Datacamp access. And that's where then he turned the platforms like Stack Overflow to help get questions and answers. And then also started reading more of the official documentation for functions and packages.

As we know in the R ecosystem, there is varying levels of documentation for packages. Most of the time, the ones that are robustly developed will have great documentation they can draw upon, but there's more to that. He also turns to these great books that are specific to R and different ecosystems around R. He mentions O'Reilly Media, which, of course, has been a great publisher of various books like R for Data Science and the like. There are many others out there like CRC Press and others. He mentions the R for Data Science book. We literally, at the day job, had an open office hour and some statistician was calling us and said, hey. I'm just picking up R again after many years.

Where can I go to learn more? And we both my colleague, Will Landau, and I pointed out r for data science is a great place to start. So we'll have a link to that in the show notes. It's also was the genesis of the data science learning community. So lots of great resources out there if you're new to the language to make you feel not so alone and and learning about all this. And touching back on the last highlight. Yes. In this day and age, AI, when used in, you know, responsibly, can be another aid in that journey.

But this is where we do with my 2¢. I do stress getting a pretty solid foundation in your understanding first before you start vetting the AI solutions because depending on which model you're running, depending on what kind of question you're asking through that prompt, you might get a solution that would have been great, say, five years ago, but maybe is missing some of the more modern approaches that maybe the tidyverse is giving you. Or let's just be real. A real example might be, hey. I'm dealing with this large data. How do I deal with memory management?

You know, it may not pick up some of the newer advances like DuckDV or Parquet that Mike mentioned just a few minutes ago. So just be careful when you're using those AI prompts to help you learn something that you do build that foundation first through these more, traditional resources that, again, if you're new to the language, I think can get you really far ahead of the curve, as well as you just reviewing other people's code. Let's face it. In the role of the shiny, I'm always reviewing what people like David Grange and, Mike, when you share an app online in the public, I'm always reviewing your stuff. Many others that help inform my style, my learning. So I can't stress enough leveraging GitHub, looking at repos of packages or apps or even just tidy Tuesday analysis if someone shares their code. That's a great win and of itself. So in this day and age, not to be like that guy that says, good off my lawn, but we didn't have this stuff when I was learning r. So taking advantage of these resources by hearing Bruno's perspective is certainly insightful and how he started from being both new to r and new to statistics into really loving the use of that language and getting some real interesting, analysis completed. So, again, great food for thought here, and I think a very relatable blog post for many people that are listening now.



[00:34:56] Mike Thomas:

Yeah. I agree. And, you know, this blog post takes me back to some of my own journey learning are and some of the things that I wish I had known and some of the things that, did help me or or hurt me along the way. I really like the the key takeaway around, you know, visualizations being a really great way to see data come to life and to put meaning right towards the code that that you're developing and actually sort of see the value firsthand there right in front of you, as well as, you know, having a goal or a project that you're trying to work on and and using R as a tool to get you to that that end goal as opposed to just sort of trying to blanket learn a programming language without any, you know, specific particular use case that you're trying to tackle.

This might sound silly, but one other thing that helped me when I was learning data science, maybe less, you know, specific to programming, but data science in general, podcasts. So I was a huge consumer of podcasts. So if you're trying to learn r, maybe r highlights podcast or just our weekly.org, shameless, shameless plug right now, could be a great resource to be able to help you, you know, just keep up with the conversations of everything that's going on and the the verbiage and, you know, some of the acronyms that get used that you may see online in, you know, some of your your research as you're trying to learn this that that may help some of these concepts start to click a little bit better. But overall, you know, I think a really great rundown. This this was sort of nostalgic for me, taking me back to my journey as well, but some great fantastic resources for folks who are trying to pick up R.



[00:36:39] Eric Nantz:

You give me the feels, man. That's why I started the R podcast back in the day because, a, no one else is doing it. And, b, I had learned so much about Linux through podcast. I thought, why is anybody doing this in data science and R in particular? So I do remember in the early days, there were some listeners out there who said, oh my goodness. This is such a massive help compared to just reading that online doc. So, yeah, I I think we can we can plug what we're doing here. I have heard people say it's been quite helpful because in this day and age, so many ways to consume content. So when you're doing the dishes or you're mowing that lawn, you wanna level up your r game, tune in to the back catalog of our weekly. We got a lot of things that we cover both in our experiences, but also more important highlights like this that share some some great insights too. And like I said, the online resources in general, so much out there.

Sometimes it can be overwhelming at first. That's why, again, I'm gonna plug it one more time. But John Harmon, who runs the data science learning community, top notch place to go to. Again, we'll have a link in the in the show notes. That is a wonderful way to collaborate with others no matter where you are in that learning journey. So great great post here by Bruno, and and, certainly, I'll keep a lot of these things in mind as I hear from others that are also new to the language. And as we said, there is a lot going on in our weekly, not just when we talk about these, you know, selection of the highlights. So we invite you to check out the full issue, which, of course, we link to every time in the episode show notes. And I do wanna give a very quick shout out to an additional package that I found in our packages section that I think could have really it will be useful now. It would have been even more useful a few years ago, but there was a time in a day job situation where we had a project manager that wanted the status of all my issues on my GitHub repo for a big large scale project because she wanted to feed that information into her, I guess, Gantt chart visualization thing to track a milestone forecasting and all that jazz.

So I stitched together some custom code with the GitHub API and r using the g h package. We kind of pull all this stuff down, do some massaging. Well, there's a new package that does that for you that just hit CRAN, and it's called issue tracker, author by Tengi Bertamili. Probably butchered that name. But it does what it says what you might expect on the tin. It's got some great functions to basically retrieve a project's open or issue information, both the issues as well as milestones and other interesting metrics.

You can and then you can save that to a local area so that when you wanna do processing on this data, you don't have to keep hitting the API to do it. You can have a cache version of this so you can refresh at any point. So and then you can update this database when you need to if you know there have been new issues filed or whatnot. But once you have that, there are also some convenience functions to filter the issues based on fields, maybe certain values or keywords, as well as some default sorting you can do of the issues based on, again, maybe milestones, other metrics.

And I would have used the heck out of this back then, and frankly, I may use it now because guess what? As much as I use GitHub project boards for my issue management, I live in a Jira shop, so sometimes I might have to pipe some stuff from GitHub to Jira, and maybe I just use this as an intermediary. I don't know. Mike's already scowling at that, so I'm sorry for bringing up bad vibes there. But, nonetheless, issue tracker is definitely a package I'm gonna be looking at. No. Sorry. I was just recently

[00:40:27] Mike Thomas:

added to an external team that uses Jira and sort of my first foray into using Jira, and I'm still still getting adjusted to it. Let's put it that way. But that's a great call out. You know, I'd be remiss not to to call out Albert Raps Scalable Reporting with Quarto blog post that just dropped this past week. It's a use case that I think a ton of people have. And as always, Albert does an awesome job of walking us through that concept.

[00:40:54] Eric Nantz:

Yeah. I love using Quarto in many different ways here. And the possibilities that you can do once you use the parameterized, you know, report functionality, There's just so much as possible. And also check out his, back catalog, if you will, on his site. He's been also talking about Quarrel recently with types for doing optimal PDF reports and theming those up. So if you're in the world of static reports, you have our sympathies if you are because, man, I love the HTML lifestyle. But if you are in that space, types is something I'm keeping an eye on for some really attractive PDF reports with that hints of CSS kind of style when you get, in web reports. So definitely check those out.



[00:41:38] Mike Thomas:

Yes. Types is awesome. Super, super fast compared to LaTeX. And, it's pretty much, I think, built into your install of Quarto. So if you have Quarto installed, then types is installed. No no tiny tech.

[00:41:52] Eric Nantz:

Yeah. Yeah. And that that can be really important, especially those in the enterprise that get a hard enough time asking their IT admins, can I have tiny tech or can I have this latex thing? You might get like, nah. No. So anything that comes bundled in can be can be a big help there. But like I said, our Wiki bundles up all sorts of great content here. We just mentioned a couple additional fines, but you may have an additional fine just by reading that issue. We'd love to hear about it too. And and speaking of hearing about it, of course, our project largely depends on the community for help. And that's where if you find that great new resource, we are just a poll request away. Everything's on GitHub.

It's all marked down all the time. Marked down if you can't learn in five minutes. The author and knitter would give you $5 back in the day. Maybe not now, but at least he told me that ten years ago. Nonetheless, you can file an issue or a poll request right from our GitHub page. It's linked in the top right corner of our our weekly issue, our weekly site. And if you wanna get in touch with us, there are multiple ways of doing that. We have a contact page in the episode show notes. If you wanna send us feedback in the traditional way, I will get that in our fancy our weekly inbox, and I will be able to share that on the show if you're interested.

Also, you can get in touch with us on the social medias out there. I am on Blue Sky where I'm at rpodcast@bsky.social. And, also, I am on Mastodon with @rpodcastatpodcastindex.social. Oh, those are hard to keep straight sometimes. And I'm also on LinkedIn. You search my name and you'll find me there.

[00:43:27] Mike Thomas:

And, Mike, where can the listeners find you? You can find me on blue sky these days at mike dash thomas dot b s k y dot social, or you can find me on LinkedIn if you search catch broke analytics, k e t c h b r o o k. You can see what I'm up to.

[00:43:44] Eric Nantz:

You bet. Always, great follow the hair on LinkedIn and the like and, yeah. I'm trying to get in my head above water so this week. I have some recent day job projects. I got some more open source stuff I gotta get back into, like, some, quote unquote conference package development for my upcoming let's talk at Pawsit Conf. I gotta button some things up there. So, well, we're really happy that you joined us for this, episode 203 of our weekly highlights. And, hopefully, unless real life gets in the way, we'll be back with another edition of our weekly highlights next week.

"
"16","issue_2025_w_16_highlights",2025-04-16,41M 56S,"It's not every day that we get to dive into a brand new release of R, but we get to do just that in episode 202! We share our takes on the major new features of R 4.5.0. We also preview the re-imagination of the engine powering R-based APIs, and the LLM-powered helper to get some of those R package development chores done. Episode Links This week's…","[00:00:03] Eric Nantz:

Hello, friends. We are back at episode 202 of the Our Weekly Highlights podcast. This is The Weekly Show where we talk about the excellent highlights that have been shared on this week's our weekly issue at ourweekly.0rg. My name is Eric Nance, and I'm delighted you joined us wherever you are around the world. And joining me virtually, as always, is my awesome cohost, Mike Thomas. Mike, how are you doing today? Doing great, Eric. Coming off the back of Shiny Conf recently. So it's a a fun time to be, you know, a part of the art community and a lot of exciting things to be looking forward to and talking about. You bet. I felt like I was, definitely drinking from the fire hose those past, last week with Shiny Conf. I did a lot of new things to talk about, and I'll talk about my adventures on that as we get towards the end of the show. But lots of lots of great happenings there, and it goes too quickly. I like all these virtual conferences do. They do, especially when the content is hot and really, really innovative like we saw last week. So certainly, if I don't say already, congratulations to the fine folks at Absolon for running that conference right now. I believe it's the fourth year in its existence, and it went off without a hitch and really, really well done, everybody.

It's no small feat. It it sure is. And I know from organize helping organize the Our Farmer Conference, there is never a dull moment behind the scenes. And at the end, you're kinda like, oh, I can breathe again. So I'm sure they're getting some well deserved rest after that one. But we're not resting here. We're talking about our our current issue. It is, been curated by old check notes. Oh, yeah. Yours truly was a real hustle on because right after my presentation at Shining Comp, I threw this issue together and somehow made it in one piece. So as hopefully, it's a good one. We will find out. But as always, I had tremendous help from our fellow our rookie team members. They They always keep me honest and really straight to the point, as well as our cure as our contributors like all of you around the road of your poll requests and other great suggestions.

And it was a pretty clear, quote unquote, winner, so to speak, and who would lead off this section because it's not every day or every week that we get to to talk about a new release in the R language itself. And what are we talking about? Well, recently, our version four dot five dot o has hit the crayon sources out online, and this code this release is code name, how about a 26. Now you may be wondering what in the world is that all about. So fun fact for those who may not know their R history as well, and I didn't either for a while, all the code names for each R release do have origins with the famous Peanuts comics, I e Charlie Brown and their famous characters.

And usually, when I see a code release, I can kind of remember, oh, yeah. I remember reading that script or I remember that from the old Charlie Brown movies. This one, however, I was like, what is that one all about? So yesterday did a little digging last night because it was a a geeky moment. I'm like, I gotta figure this out. What is this all about? So turns out one of the characters named Lucy, who tend to tease Charlie quite a bit, She kinda ran her own, you might call a makeshift psychologist or psychiatry booth where she would charge, I believe, 5¢ for a consultation.

And then Charlie Brown came to her one day and asked for some advice, and she out of boots says, well, how about 26¢? So I don't know what happened after that, but that's where the 26 comes from. The more you know. Very cool. Thank you for that. If I'm useful for one thing, it's the Charlie Brown trivia.

[00:03:45] Mike Thomas:

I remember Lucy's, yes, psychologist or psych psychiatrist booth very fondly, but I didn't remember the 26.

[00:03:54] Eric Nantz:

Yeah. Neither did I until the little of sleuthing last night. So, again, credit to the hardcore team for always finding a new one, the the spring upon us at each of these releases. So let's talk about what's in this release. And first of all, how this was put together. This is an excellent post that comes basically after every release from our friends at Jumping Rivers. And I the author this time is Russ Hyde, who is one of the data scientists at Jumping Rivers, and he's done a fantastic job summarizing a lot of the key points in this release. And we'll go through a few of the ones that caught our attention.

I was, telling Mike before the show that, it was this past weekend. It was the end of my kids spring break, and I ended up taking them to a a zoo over a couple hours away in Cincinnati. My youngest one, his favorite animal is not the one you might expect. It's not like the tigers or the gorillas or anything. It's penguins. He's a penguin fan. Guess what? I'm gonna try to get him to use Linux later, but that's another story. But speaking of penguins here, guess what? In our version four dot five, the Palmer penguins dataset that's made a lot of positive, headlines in the last couple years is now included in base r. This is massive achievement for that team that put this out there.

And the origins of penguins, we don't have time to get into it today, but, nonetheless, one of the motivations was to find a relatable data source that could maybe in the future serve as quote, unquote, the new iris or the new set that you could use for examples of, say, statistical inference, classification, roles that the iris dataset did play. And for all those, you know, textbook examples, examples you find online, the penguins dataset has a lot of interesting features. And now, like I said, it is available in Besar itself.

And so I'm really excited for that, and congratulations, for getting that into Besar itself. So that caught my attention. This other one is another case where you might say other computing languages tend to have an influence in r from time to time. We all know its origins from the s language and, of course, it's, you know, tie ins with c and Fortran and whatnot. This influence, I think, comes from a certain language that might have a snake as a mascot because we are now talking about the use function. This is brand new to r. Albeit, it was quite re released in version four dot four, but I did not get radar of it until now.

What does this mean? Well, if you are coming from the world of Python, you may be used to loading your modules or packages with a couple different approaches. You might do the import function, such as, like, import NumPy if you wanna use that, Or you might only want one specific function from that module, and you might say from NumPy import, say, the array function. In that way, you've only taken what you needed. In r, historically, the way we've done this is that you would do the name spacing trick. Not so much trick. It's a it's a feature.

You put the name of the package, colon colon, the name of the function, if you wanna use it in a quote unquote one off setting. That works I get to my muscle memory for years. I I do this a lot, especially when I only need that function once or twice in my overall function. But now with this use function, you can now call the function use, put in the name of the package, and then the next argument is one or more of the functions you want inside, which again will now modify the search path that R uses as it's trying to figure out what are you referencing when you call that function as an alternative to the namespace prefix and as an alternative just doing the library statement on the package itself.

Now, again, I I pride myself on being up to date with this, but this did come out of left field as I would say. But I think for those making the transition to r, this is probably something that many of them were wondering why wasn't something like this out there. So another fun fact is that there have been other packages in the art community that have exposed this kind of functionality. One of which I know from my exploration, especially with the Rhino, framework from Absalon, is the box package. That has been basically replicating this for a while, and you get more control over that than you do even with this use function where you could use this box package to import, say, functions from an arbitrary r script, not just a package. So you do get a little more control there for your needs. But I do find it interesting that the influence of another language has now landed into bazar, and I guess time will tell just how much this is gonna be used in the wild. So I think that that one definitely caught my attention for the future.

And then the last one that caught my attention, especially someone that's kinda, you might say, greedy whenever I install a new package and ends up having a bunch of dependencies. I just wanna get it done quickly. Right? We've seen approaches in the art community, namely the pack, you know, framework offered by posit to help, you know, give you a little more modern way of installing packages using parallel downloads and curl and whatnot. Well, as of our version four dot five dot o, the install dot packages function now lets you download functions and update packages in parallel.

That can be a huge time saver, folks. Like I said, especially if you're installing, you might say, something from the tidy verse or, or another, you know, string processing package and you got it to pull down five, ten, sometimes 15 dependencies, but, you know, how far the rabbit hole lies, this could, you know, may give you a bit of time savings. So I'm I'm I'm I'm happy to have that now land in the base tower itself without you having to do a lot of tricks to make that happen. There is a lot more to this release, and we'll definitely link in the in the show notes to the full post from Russ here as well as the full change log, which has its full assortment of bug fixes, offer new functions, and offer, you know, great developments in this release.

So, Mike, what caught your eye on our version four dot five here?

[00:10:57] Mike Thomas:

You know, a lot caught my eye. It's very exciting to have that palmer penguins package, you know, built in to ours that we don't necessarily need the external package. Although, just a word of warning, it's not a perfect one to one if you just wanna pick up, you know, the base dataset that we have the base R dataset that we have now in your existing scripts that leverage the palmer penguins package previously, some of the column names, I think, are slightly different. So, unfortunately, we love open source. There is another new package that just came out called BasePenguins, and it's authored by Ella Kaye, Heather Turner, and Akeem Zellas. And it allows you to easily convert your script between, references to the quote unquote old Palmer penguins package and leveraging the new palmer penguins dataset that is built into our four dot five dot o. So thank you to the three of them for authoring that package to make our lives easier as we try to convert, to leveraging the new the new built in dataset, which is awesome. Yeah. They use, the use function is really, really interesting to me. I almost can't believe, if you think about all of the function names that exist or namespaces that exist in base r that use was not, you know, previously used. No pun intended.

I guess they were reserving it just for this particular use case, which is which is pretty awesome, very Pythonic. As you mentioned, It looks like based upon some chatter that I've seen on Blue Sky, you know, one word of caution is you can't, have two separate lines that are importing different functions from the same package. Like, you can't have one line that says, you know, use the arrange function from dplyr and then another line that says use the filter function from dplyr. You would actually have to, supply sort of a character vector that says from dplyr, you know, essentially use a range and filter at the same time. Unfortunately, you can't do that downstream the way that you can in Python. So a couple of things to be careful of as you begin to, you know, hopefully try out and experiment this new use function. And then the last thing, you know, that really caught my eye beyond the parallel downloads, which is absolutely awesome, is a new function called grep v. So for any of anyone else, perhaps like myself out there who has always struggled to remember whether we need to use grep or grepo, and grep returns the position and grepo returns a true false, I believe.

And all you want is the actual values that get matched. The new grep v function does exactly that. It's what you've always wanted grep and grapple to do and what you've probably used string r to do. But now we have a a nice base function called grep v that can just you can supply a pattern to, your first argument would be the the strings and then your second argument would be, the pattern. Actually, excuse me. I think I got that backwards because it's base r. The second argument is the string and the first argument is the pattern. But it will return just the matches. The the perfect string matches, not the positions, not a logical, you know, vector of true falses, it'll return exactly what you want. So I'm super excited to have that now in base r for those situations where I need to do some string matching and, you know, it's just a one off use case where I don't necessarily want to, you know, install the entire string r library and try to stick to base r here. It's very very helpful. So I'm excited about that one as well. Yeah. And Russ also concludes the post that

[00:14:41] Eric Nantz:

a lot of the other features and bug fixes that are are called out in in the release notes are also stemming from really successful r dev day events that have been held after, say, various conferences or other, meetup settings. This is a wonderful way that we're seeing those newer to the community, newer to contributions to base are really starting to make a real impact on the language itself. And I think that's only gonna speak volumes to the success and the and the future life of our itself, bringing bringing contributors from all backgrounds, all experience levels to help with the project itself. So really excited to see all that. And I'm not sure of the time this recording of the binary versions of R have landed yet for, say, Windows, Linux, Mac, and whatnot.

But you do have ways of trying this out yourself without messing up your system, so to speak, or or putting into a bind, one of which is that you can use containers of Docker. They'll pull down a new image that has the latest r release from source already built in. You can also utilize the rig utility offered by Gabor Sarty that lets you install our development versions on r 4 5 alongside your r installations. And because I couldn't resist saying this, if you use the RIX package offered by Bruno Rodriguez, RIX now lets you put an r four dot five in your your environment there. So you may hear more about that a bit later. Nonetheless, you've got ways of trying this out. So, yeah, we encourage you to try it out today and and definitely give it give it a good whirl and enjoy those new penguins and all those other great features.

So in our next highlight, it's not every day that we get to talk about one of the more important frameworks that both Mike and I have been utilizing for years and years in our intersections of leveraging R with building custom APIs for our R based services or or analyses. And it's an interesting situation where you can see in the life cycle of a package, there are often two paths to take as new features are requested and maybe infrastructure needs updating. Do you keep building upon what's already there, or do you take that other fork in the road and kind of start from scratch?

Well, this next highlight is kind of that second route here because we are now talking about the plumber two package. Note the two. If you're not aware, there has been a package called plumber offered by Barish Slurkey over at Posit, and it was originally offered by Jeff Allen who was at Posit beforehand. And that's been my go to package for making R become an API. Whether I was doing a statistical output, like analysis, or doing some other utility, R was my way to make an API that then any language, no matter R, Python, JavaScript, whatever you throw at it, if it could compile if it could call an API, it could I could use Plumber to make that API.

Well, now we have plumber two that has hit the GitHub, you know, development, over at Pazza. And this one is authored by Thomas Lynn Peterson, who you might know as the one of the the wizards and maintainers of g g plot two along with many visualization packages. But Thomas has also been pushing the envelope a lot of web based request packaging and API packaging because he has authored in the past a package called Fiery, which was kind of like an alternative take on both, you know, API development and Shiny itself.

So he's definitely looked at this mode before. But there was a recent, I believe, you know, blue sky post or a mastodon post from Thomas asking for feedback on this new Plumber two package. So it's a great time to talk about it and spread the word about it. So what is different about plumber two as opposed to the first plumber? Well, even in the outset of the package site introduction, this is one of those cases where plumber two was born on kind of writing some of the things that they found inefficient from the previous plumber development.

And it sounds like it was just easier to kind of start this from scratch instead of trying to extend the existing plumber code base to fix all these holes. And he mentions that there are a few breaking changes if you're coming from the first plumber generation over here as part of this infrastructure improvement. So we will have a link in the show notes to the the the full, packaged down site, and there is a great article if you want us try plumber two, what you need to do to upgrade from plumber one because that's kind of where I'm coming from on this since I've been using plumber for years and years.

First of which is the function itself changes from p r to API, which I guess in retrospect does make more sense. Right? But it sounds like p r was kinda harder to remember, and it was meant to be an acronym for our plumber route. But Thomas says in this documentation that when you build an API, it's much more than just a route. It can be a lot of things under the hood. So they think the API, function name is going to be a lot more easier to remember for the majority of users. And there are other key changes, one of which is now if you have an argument for your path and your query parameter, if you do, like, slash something, slash name of endpoint, or whatnot, those are now passed as named arguments, and those are the only things that are passed as named arguments, not some of the other parameters, like query parameters or parts of the body of the response that you would have to tweak.

Now it's just those path parameters. There is a separate argument called body that's gonna let you have different slots of things to modify in the response that you're returning out. There is a lot more under the hood here in the types of objects such as new objects for both the request and the response. And they are now explicitly called both request and response. And they're gonna they're gonna really, you know, make things a lot more streamlined there. But I think the key part here is that they are looking for feedback.

There is a boatload of existing documentation out there, but they wanna make sure users both that are new to plumber itself as well as those that are coming from the first generation of plumber. They're able to give this a whirl and give them feedback because they are at the stage where they wanna they don't wanna lock anything down yet. They wanna make sure that if there is a better way to word something, a better way to have a function parameter, that this is the time to get in touch with them. So I have looked at the GitHub repo for plumber two. There is a a lot of nice dialogue already on issues on maybe things that soon need to be ported from the first plumber, but this is a a great chance again to have an impact on where plumber two takes you. And, again, we'll have a link to the the package down site with a lot more documentation, where, plumber two is going and the new API, bad pun intended, the new API Plumber two itself as compared to the first generation of Plumber. But as I look through the site, there's lots of great articles on takes on hosting, on how you render output, and like I said, the migration from the first plumber.

But it does look like there's been a lot of thought put into this. Not too dissimilar that I'd imagine many years ago when Hadley Wickham was trying to decide, do I keep building on the original g g plot? It's time for g g plot two instead. It seems like something around those lines. Again, early days. We'll see what happens. But I'm excited to try plumber two out. What do you think, Mike?

[00:23:27] Mike Thomas:

Yeah. Me too. I think that there's some really, really interesting improvements here. You know, I know you touched on quite a few of them, you know, the the parsing blocks themselves because, you know, Plumber and, you know, Plumber two as well, it's a it's a unique RoXygen syntax compared to what you might be used to if you're authoring sort of a regular r package and leveraging RoXygen two documentation. Right? The syntax or or what you stick in these tags and how you author this documentation on top of your API function drives, you know, how it works, which is is pretty interesting and and quite different, right, than just typically how we oxygenize a function in our r package.

So I think it's really interesting the way that they were able to migrate there. I'm really, impressed by the level of documentation that they already have given that this is so early in its development. There's 10 different articles as you you referenced that span from, you know, just routing and input and, runtime and hosting discussions across all sorts of different services, including, you know, Pasa Connect, DigitalOcean, Docker basic and advanced usage. There's discussions on security, great vignette on migrating, as you mentioned, from plumber o g to now plumber two. I thought one additional interesting thing that's sort of at the root of the package downside is that there is a new async tag, I believe, that allows you to convert your handler into an async handler automatically.

I don't know if that leverages, you know, future and and promises and how exactly that works, if there's additional dependencies that get introduced. But it seems like they're trying to make it easier to build APIs that can return responses asynchronously, which is really cool as well for larger workloads that folks might be faced with. So I'm excited to see how this package continues to progress. You know, I would imagine with the amount of effort that's clearly already been put into it that this is the direction that the folks at Posit are looking to go with, you know, Plumber as a concept itself.

So I'm certainly excited to see how this progresses. I know that there are some other, libraries and packages out there that are a little bit more legacy that try to do the same thing, allow you to, create an R based API service. And I imagine that, you know, Posit probably looked over sort of the entire ecosystem of those packages as well as plumber plumber one to try to come up with, you you know, what's the best possible way that we can author an API based, R package. So excited to see how this progresses. I'm sure that this is the result, like you mentioned, of a a lot of lessons learned and, you know, appreciate the effort here in in trying to build something that's going to be easier for us, our developers, to stand up API services.



[00:26:29] Eric Nantz:

Yeah. I'm glad you called out async. That was something that flew under my radar. And as I'm digging into this further, it does let you have two different options for the back end of that asynchronous operation. One of which, as you correctly guessed, is the future package. There is a future async method, but there's another one. Is that what I'm thinking? It is what you're thinking. It is from Mirai, the Mirai async offered by Charlie Gao, who we heard from the shiny comp actually. One of his excellent presentations on how Mirai is is supercharging async and shiny itself. So that seems to be some fruits of flavors there with some recent synergies. But this could be another case where on top of that, you know, implementation, I think the community may have had an influence on this too because I remember hearing about a package called Fawcett, authored by, a former Absalon engineer, that was also letting you leverage plumber with containers to do async as well. And I think all that's kinda converging, and I think that I I think just that need has spit has spoken volumes to make sure that if you're gonna use this as scale, I mean, you and I know we can use r in production. We do it every single day in our respective our daily work lives and then elsewhere.

You you don't want any excuses being thrown at you of, like, yeah. Well, but if you did that thing in r, you're not gonna be able to scale to all these thousands of requests every minute or whatnot. You can now. So I think with Plumber too, I think that's another way of pause and recognizing, you know what? We're gonna make sure there's no excuse left at the table for running this at scale. And then our last highlight today, I admit, as a parent, it's really difficult to get my kids to do the the things around the house that, you know, would make my life easier. I mean, I'm the one that does the dishes every night. I'm okay to do that. That sure would be nice to have a little help around. Like, you know, just clean the table a little bit or, you know, just pick up after yourself a little bit.

Things that my wife and I literally joked even last night, when in the world we're gonna have robots do some of this stuff. And we know in the future's not off. Right? You might have some of these things for your, you know, our development as well, especially as you're filling out, like, boiler plate for, like, your package structure or just trying to get an 80% of the way to something, but you know you gotta type it in manually to do it and whatnot. Well, in the in the vast expanding, landscape of AI development and integrations with data science, our last highlight is, again, one of those situations where as a package author and our developer, there are ways you can help, you know, get a lot farther, a lot quicker with a lot of these manual tasks you might call chores because they end up feeling like chores. Well, that is an apropos description of our last highlight here, which is the new r package offer by posit called chores.

And this is, blog post. The introduction to this is authored by Simon Couch who we featured on previous highlights on his expirations with the Elmer package, which is causing a lot of positive momentum in the art community for interfacing both public available LLMs via these host of services as well as self hosted services. And the ecosystem that seems to be spinning on this. So chores is actually, again, a kind of a new generation of a previous attempt at this called Pals, which is a way for you to leverage as, say, a package developer, ways to help you get to the finish line more quickly on on, you know, the more routine tasks that you have to do.

Such as you're writing a new function for your package, you're writing the function itself, you got some nice parameters, you've got, you know, you got your body of the function all set, then you realize, oh, yeah. I gotta document the thing. So what path what chores will let you do is you will be able to highlight that that function syntax, call up a little add in, and select the type of helper you want. In this case, you would do the helper for r oxygen. You're just talking about in the plumber too, how that uses r oxygen type syntax to call its parameters.

You can highlight that text. If you're a function body, run this add in. So a little keyboard shortcut to run that, press enter, and then the r oxygen documents are gonna stream right into your your current r script. And will it get you a % of the way there? Well, let's face it. Probably not. You're gonna have to fill in more details, especially around the context of your parameters. But just if you have, like, five or 10 of these, obviously, that's gonna save you some time and get you there. And it's gonna try and guess the type of parameter in that, which can be, again, quite helpful.

Now under the hood, how does all this work? This is an interesting approach, but I'm starting to see why they're doing it this way. There is, in the chores package infrastructure, a directory of markdown files, which are, in essence, the instructions that the LLM is being given for these various tasks. We just talked about the documentation tags of our oxygen. There are a couple others as well, such as helping bootstrap, testing framework, as well as perhaps even helping with a CLI conversion, converting some previous tax you might have done in, say, another package to the new CLI syntax. So it comes with those three helpers, but in essence, that markdown file is being parsed and supplied to the LOM every time you run this add in to give it the additional context in place.

They have set this up in such a way that if you like that framework and it's working well for you, you could extend this to any type of task that you like. Chores is just kind of like a a reference type of solution to this, but that's got me intrigued by what other ways could you leverage this paradigm. And at the end of the post, Simon calls out some other, you know, packages in the community that I've been building upon Elmer that kinda have their own take on this. One of which that definitely caught my attention for a potential time waster, I e I need a break from my r code, is this package called Krios, which lets you turn r into a turn based adventure game with characters and other steps along the way. The l m's kinda gotta be the moderator of this game that you play. So that, again, this whole paradigm seems to be leading to similar directions. So chores looks like a great start too, as an r developer, helping automate at least parts of these manual tasks so that you can get to that correct answer. Maybe not correct answer, but that finished product more quickly.

We always throw the caveats. Right? Look at these before you sign off on it, all that. You'd probably sick of me saying it by now, but I come from an industry where I have to say it. So we wanna make sure y'all are aware of the caveats, but I am intrigued by this. And I'm I'm gonna give this a whirl next time I write in our package. If you're gonna use this, please use version control.

[00:34:24] Mike Thomas:

I think we take that for granted, but I could not imagine, utilizing something like chores, which is going to alter your code, right, without so please make sure that you're using version control when you use this. But, yes, it is it is absolutely awesome. And even for the AI, the sometimes AI denialists like myself and perhaps like yourself at times, Eric, I think we can probably all agree that things like, you know, roxygen documentation certainly do feel like a chore. And sometimes it doesn't exist because it feels like like a chore and it gets left behind. It because, you know, priorities and, timelines and deadlines and things like that, hinder our ability to, you know, write good documentation.

So I would say that even if it's generated by an LLM, that's probably better than not having any documentation at all for your function. And ideally, right, the the the real concept here is that the LLM will generate that documentation and you'll it'll save you a whole lot of time from, you know, some of the the boiler plate that comes back, and you might be able to edit it a little bit to make it more perfect. Right? But it'll still save you maybe 80 to 90% of the work. And maybe some of just the mental work of actually scaffolding out the roxgen on top of your your function, which if you're just starting with a blank cursor, can feel perhaps a little daunting.

I know we have some keyboard shortcuts in our studio that that help us do that, which have been fantastic. But I think this can be another way to facilitate that process and and expedite the ability for us to, document our functions. And there's a really nice gallery of, these different utilities from the chores package besides just, perhaps the three that are in the video and the blog post, which are convert error in code to use CLI, which I think is really cool because I love the CLI package. Convert your unit test to migrate to test that, third edition.

And then template function documentation with roxygen, which is the one that we've been talking about. But there are also four others. One is for migrating to quarto style chunk headers. So that's the the pound sign with the vertical pipe after it as opposed to having those particular chunk headers, you know, within the the curly braces in the old r markdown style formatting. There's the ability to format news files. That one was authored by Hadley Wickham. There's one for adding or modifying reveal. Js SCSS theming, which is fantastic, especially for perhaps others like me who really don't know much about SCSS theming, but would love to beautify our reveal JS slide decks. And then there's a a ggplot two assistant as well. So a really nice gallery here in the packaged down site for chores. And, yeah, I I think it's a fantastic resource that, Posit has put together because I I do feel like they're taking a targeted approach to bringing AI into workflows in really constructive ways for us data scientists out there. So I appreciate all the work that's been done on this by Simon Couch and and all the other folks at Pawsit who've contributed.



[00:37:41] Eric Nantz:

Oh goodness. I coulda used that reveal JSSCSS one a week ago when I was putting together my slides for a shiny cup, and was filming my way through it. So I will be taking note of this for sure. But this is this is really intriguing part to me, right, is this community aspect of sharing these additional helpers that are out there. And so on top of this great gallery that you you mentioned, Mike, I'm gonna also link to the vignette on building these custom helpers because I that is a potential to take this, you know, in many, many different directions.

Yeah. That's a that's a good preview of what's possible, but I could see even if it stays within your respective firewall, if this is gonna help you do a proprietary automation in your company, having this, you know, within your organization, this set of helpers out there. I'm oh, boy. I I I see lots of lots of potential here. So the the sky's the limit as they say. And and like you, version control, if you're not using Aphrodis, oh, you're probably gonna pay for it. I can just tell you that much, and I think Simon would tell you the exact same thing. So nothing bad will happen to you if you version control. Even though it may seem tedious at first, I had a situation at the day job where I put in a new thing by accidentally uploaded or tried to upload a hundred megabyte or more dataset in GitHub, and it didn't like that.

Yeah. That says, oh, that's too big. You gotta wipe that outside. I'll go clean my history and then cherry pick commits that were without that. That was a fun afternoon. But if I didn't have Git, I would have been absolutely hosed without that. So don't put yourself in those situations. Use Git, and then that way, you can commit it when it looks good. So we shall see.

[00:39:35] Mike Thomas:

Absolutely. Yeah. And as you mentioned, these like we like to say, these prompts that you can create yourself, these templates,

[00:39:42] Eric Nantz:

all marked down all the time. It is. It is. If you can speak markdown, you can speak the language of these helpers as well as what's built with our weekly itself. So you set that up, and I knocked that out. My goal of our weekly is built with markdown because everything is transparent. We wanna make it as easy as possible for all of you that contribute to these issues. We're running a bit well on time, so we probably won't get into a huge amount of of additional fines here. But I will give a shout out to, Bruno Rodriguez and Philip Bowman for giving me excellent material to talk about at the recent shiny comp. You know, my presentation was called in the nicks of time, a new approach to shiny development with nicks and ricks.

It was a bit too much for twenty minutes. I do have that one regret, but, hopefully, when that recording comes out, you'll enjoy it. But thanks to them for putting Rix out there because I I it's definitely inspired me, and I've got a lot more to say about it as I push this in many different directions. I even saw something with Plumber too I could do with Nicks that I'm really intrigued about that I may, touch on in a future episode. But, nonetheless, there's a lot more to this issue, so we invite you to check it out at rwicked.0rg.

And put that in your bookmarks if you haven't already. And as always, you can also get in touch with us on social media. I am at rpodcast@bsky.socialfor blue sky. I am at rpodcast@podcastindex.social

[00:41:12] Mike Thomas:

for your Mastodon lovers out there. And I'm on LinkedIn. You can search my name, and you'll find me there. And, Mike, where can the listeners find you? You can find me on blue sky at mike dash thomas dot b s k y dot social or on LinkedIn if you search Ketchbrooke Analytics, k e t c h b r o o k. You can see what I'm up to.

[00:41:33] Eric Nantz:

Absolutely. Always love and follow what you're up to there. And as always, we thank you so much for listening wherever you are. We'd love to hear from you. If you like us to cover some additional things besides the issues, we're always happy to hear your feedback. But that's gonna put a bow on episode 202 of our weekly highlights, and we will be back with another edition of our weekly highlights next week."
"17","issue_2025_w_15_highlights",2025-04-09,52M 15S,"In this episode of R Weekly Highlights: We have a six-month follow-up perspective from an early Positron user, how the current landscape of AI tools perform when learning the ropes with the Tidyverse, and how you can create your first Observable plot while using R for data munging. Episode Links This week's curator: Jon Carroll -…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 201 of the Our Weekly Highlights podcast. This is the weekly show where we talk about the terrific highlights and other excellent resources that I shared every single week at rweekly.0rg. My name is Eric Nance, and I'm delighted you joined us from Revyar around the world. And, boy, it's been a crazy times in some parts of the world lately, but we're happy you're here. And I'm not joined here alone. I am joined at the hip here virtually by my awesome cohost, Mike Thomas. Mike, how are you doing today?



[00:00:35] Mike Thomas:

Doing pretty well there. Can't complain. The weather out here on the East Coast, about twenty minutes ago, it was snowing, and now it is sunny and beautiful. So it's as crazy as, the world seems to be these days.

[00:00:51] Eric Nantz:

It is. And that's no April Fools a week later. That actually happens, folks. And we had a freeze warning here too where it's like, I thought we were done with this, but, nope, we are not. So my my little hands here are still frigid from from being here in the in the humble basement when did I record this. So first first world problems, I guess. But nonetheless, we got some stuff to heat up our our, our knowledge here with the batch of highlights we're gonna talk about today. And as usual, the our weekly effort is a volunteer effort where every week we have a a new curator, rotating into their shift, if you will. And this week, that was Jonathan Carroll. Again, one of our longtime curators on the project.

He's also does a lot of interesting programming exercises, so definitely check out his blog if you're interested in what he's up to. But as always, he had tremendous help from our fellow Arrowki team members and contributors like all of you around the world with your poll requests and other terrific suggestions. So if you recall, it was back in 2024. The company, Posit, who, of course, have authored what has become one of the standards in data science tooling with the RStudio IDE, that's also been branded as Posit workbench for their enterprise products.

Well, they made a splash last year when first, it was kinda quiet, but then it was a big splash at Positconf when they talked about their new IDE called Positron. And for those that aren't aware, Positron is, in essence a wrapper around Visual Studio code, which has been used heavily in software development for quite a few years now, but with a data science flavor to it. And one of the main selling points is that it is a polyglot type of IDE where you can have r, Python, Julia, and almost any other language that Visual Studio Code supports. You can have that right into your Positron session.

I have been using Positron almost exclusively for now about four or five months. It was a little bit here and there in 2024. But with some recent advancements, especially around Nick's, I've been able to drive it a lot more as my daily driver, but I'm not the only one. And our first highlight today, we have a great blog post here called Positron, Current Joys Joys and Pains offered by Athanasia Milenko. She is a neuroscientist, and she gives us a great recap on her experience after six months of using positron.

She first leads off with the positives. We always like talking about the good things before the not so good. As I mentioned in the outset, a very, very useful feature of positron is, like I said, this multiple language support, which doesn't feel like something that was bolted on midway through a product's life cycle. Where if you remember running, say, RStudio in your daily driver and you maybe wanted to do some Python development with it, you would have to use reticulate. Sometimes it wouldn't feel quite as native. You got a lot of handoffs going on there.

But because this is based on Visual Studio Code and it's got access to all the ecosystem that Visual Studio Code brings, you have either extensions or built in support for the common languages, especially for data science, such as Python, such as Julia, and others as well. And if you're, let's say, a JavaScript developer, you can tap into things like ESLint for linting, lots of other extensions. And a new one that came out somewhat recently is the air extension to help you format your code as you're saving your file, both in r and, I believe, on the r side of things.

But in Positron, these things are easy to set up, and you don't feel like you're going off label, so to speak, when you use these multiple languages. So while I've been mostly doing R with Positron, I definitely have dabbled in Python before Visual Studio Code, so so it shouldn't be any real difference here. As long as you have your Python environment and your R environment set up, you should be good to go with positron in these multiple languages. Another thing that is growing on me, it took a little bit, but now it's starting to really become nice is the environment viewer, which now you're starting to see what positron brings to the table as opposed to just straight Visual Studio code, is that this environment viewer definitely takes a lot of inspiration from what we saw in the RStudio IDE.

Ways that you can view your dataset in a rectangular kind of display, do filtering on the spot. It may take a little getting used to at first if you're new to it, but once you get going with it, I think there is a lot of great ways you can explore your data there. Not all perfect. We'll get to that a little later, but I think it's it's coming along pretty nicely. And one nice thing that coming from those that use RStudio is that there was a viewer pane that you could say run either your shiny app or maybe document a report.

I knew that would happen. You didn't see it. You could document you can look at a render report in the viewer and whatnot. But guess what? Anything that the RStudio viewer could do back in RStudio itself, positron has support for that as well. This means also if you're maybe not on the portal train and you're still using a framework like blog down, one of my favorite packages for writing a blog with Rmarkdown, you can actually run your preview of the site in that positron viewer as well, just like you could with the r studio ID. So, again, they've done a lot of engineering under the hood to make that pretty seamless.

There's also support, for the add ins ecosystem that our studio brought to the table, again, midway through its life cycle. But because those are basically embedded Shiny apps when you run those most of the time, You can run those in positron just as well. However, not all roses and unicorns here. There's still a few things that are troublesome. So, Mike, I hate to make you be the bearer of bad news, but apparently, yeah, debugging's still not quite a seamless experience. What what does she have to say about that?



[00:07:26] Mike Thomas:

Yeah. In terms of debugging, it's it's not quite as easy as maybe what you would be used to in Versus Code. You know, running code while you're in the debugger, it can be done with, control enter, allows you to to run line by line from your browser statement, or breakpoint or other debugging marker that you've used. And Athanasia notes that if she's moved the cursor for some reason, whenever she does control and enter, it just is stuck at the browser call without running the other code. But if you've moved the cursor manually, it might not work, and and she can't really understand exactly why that is the case. I don't know if this is also the experience for any others. Eric, I know you've been a user of Positron a little bit. I don't know how much debugging you've done or if you've sort of ran into the same thing.



[00:08:20] Eric Nantz:

Oh, here and there I have. Now one thing I've kinda had in my muscle memory is I've been using debuggers both in RStudio and in Positron is I'm I'm one of those old school folks that just likes to type in that console to run code and either hit end to do the next line or just print out, like, an object structure. So but I can I can see I've had a little bit here and there where that focus seems to randomly shift away, and then sometimes I have to quit the debugger with with capital q to get back to the normal session and then try again? So there is some finagling you might have to do, so I can I can see where she's coming from here?



[00:09:02] Mike Thomas:

Absolutely. And and as well, you know, if you are if your previous workflow like Athanasius, in terms of how to run code when you're in the debugger, I think that's exactly what you're talking about. If you would copy and paste sections of code that you you wanted to run into the console and sort of have that interactive experience between the debugger and the console, that's not necessarily gonna work in Positron currently. And I think that's sort of the other big frustration, that she has at this point right now. However, you know, she says that her experience actually in Positron has helped her better understand the R Debugger as a tool and leveraging, you know, the keyboard commands in the debugger, like q to properly exit the debugger, you know, as opposed to maybe doing those things in a point and click fashion when you're in the debugger in our studio. So, you know, I think there's some pros and cons there. In terms of the data viewer, which is how she wraps up this post, it seems like it maps to Versus Code's data viewer, if you're familiar with that. So it's it's pretty good, but I think it lacks a a few features that you may be used to if you came from RStudio.

First being no handling for labeled data, which she points out is something that Shannon Pileggi feels strongly about, and there's a great link to Shannon Pileggi's blog post on labeling data in R. You can explore lists in the viewer where, you know, in RStudio, you can expand and collapse nested lists if you click on them from, you know, your global environment. And the data viewer also tries to guess delimiters for plain text files and doesn't always do a great job. So Athanasi has been preferring to use the Rainbow CSV extension, which is, you know, one of the benefits of Positron being built on the open source fork, I believe, of Versus Code, which enables use of that open VSX universe of extensions, which is is fantastic and something that you wouldn't have had access to as an RStudio user, which is a pretty cool universe for those of us coming from, you know, the Versus Code world.

And then the last thing the blog post wraps up with is being able to sort of set up your Positron environment, leveraging a a JSON file. And I don't believe that you can do something like this with RStudio. Is it mostly point and click to do that?

[00:11:26] Eric Nantz:

They're they're they did laid much later in the in the life of RStudio, give you a way to do a text config file, but it was never, in my opinion, documented very well of how to interact with that directly. You can back it up and then restore. But this I admit what we see here for Positron and by proxy Versus code. Yeah. It's JSON, but it it makes more sense, I think, to configure it well. I agree. And it's really nice to be able to do this. It's it's pretty easy to wrap your head around.

[00:11:56] Mike Thomas:

It should be familiar for Versus Code users, maybe less familiar for RStudio users. But I think taking a couple cracks at it, you'll get used to it really quick.

[00:12:06] Eric Nantz:

Yeah. And and that's a it's a great phrase to to end with. I admit I I may have had a jump start in my positron journey because, Mike, you know this. I was on that whole dev container kick with Visual Studio Code and the r extension with Docker containers. And I was kind of, for my open source work using Versus code extensively for my development environment. And, yeah, I mean, I got pretty long, pretty long way, in most of my Shiny app workflows especially. But there are always some pain points here and there. So when Positron came to be, it wasn't that huge of a of a lift or I should say a shift in in perspective because I've already kind of stress test Versus Code a bit.

I still can see if you're coming from our studio itself, this is gonna be a bit of a jump. I think no one will will discount that and that's expected. It'll take some getting used to. That's why I always recommend when something new comes out and let's let's be honest here, it's still considered beta. It's not a production release yet. Although, you know, if we know our history well enough, we'd imagine around September. This is gonna be a production release if you know what I mean. But my point being is that try with some low risk project, get a feel for it, and then slowly start to more fit into maybe your your daily workflow. But, you know, the best way to to learn is by trying. So I think there's a lot of lot of positive momentum here. Still some paper cuts as they say. That data viewer, aspect without the label attributes, that's actually a kind of a bigger one for a lot of my, colleagues here in life sciences because when we import data from SAS, say, using the Haven package, we get those labels right off the bat. And to be able to see those in the in the viewer is a big win for us. So, hopefully, that that comes to play.

And I think the other thing that maybe you have to set your expectations for is that this is a very, very customizable experience. Like, Versus Code, there are all sorts of blog posts out there, people tricking out their setup with various extensions, various paying layouts. In the end, Positron tries to give you, like, three or four good choices to start with of your paying layout that you can toggle kind of back and forth for the preferences. But I wouldn't I wouldn't get too bogged down into just how much you can customize. I would kind of build it step by step until you get to that that state that you like. So I can tell from Athanasios, post here and her post before this, she's been learning along the way and I think the evolution you can see in her settings file shows just that with, settings for both positron and some of the popular extensions, say, for git, there's a git lens extension and other, you know, nice things with GitHub directly.

Speaking of extensions, you may be wondering, well, sure there's so many out there. Which one should I pick? Well I will put a link in the show notes to really handy what I'll call wrapper extension author by Garrick Adenbuie over at posit I believe it's called positron x 1,000 or something to that effect where it's actually a collection of extensions that he is benefiting from from his role as a shiny developer and an engineer at posit. You can even just see if you don't install it right away, you could just see what's included in that and see if there's specific ones that meet your needs, whether it's from linting, auto spell checking, you know, navigation layout for your your files. There's all sorts of interesting goodies there.

So, again, easy to get bogged down a little bit, but you've got a lot of choices at your fingertips for how you can tailor the, positron experience to meet your needs in your data science journey. Well, we're at two zero one, Mike, and the street continues for yet another, interesting highlight involving artificial intelligence and our world of learning and data science, and we're gonna hone on that keyword of learning here because one perspective might be it's one thing to use AI for getting specific help to problems that we're encountering. Maybe it's a, you know, an error message in our console.

Maybe we have to tap into this other language that we rarely use like XML or whatnot in our projects. But going back in time as if you and I had these tools available to us and we're just starting our data science journey, just what would that actually look like? And I think this next highlight here is showing just that, with, author by Minnet Chechnya Rundell who is a professor of statistics at Duke University and also on the Pazit team where she is talking about using AI tools for helping learn the tidy verse.

Now she mentions off the bat, and you're in good company here Minhay, that there are a lot of opinions on AI out there and, we have no we have no shortage of ones that we have here on this very show, but she's gonna frame this in the context of being, you know, a newer learner in data science and maybe trying to do a more common task in, say, r with the tidy verse to get, like, a data analysis or visualization done. So there's a few different case studies here to kinda run the full spectrum of what we're talking about here.

The first one is leveraging chat g p t to help with reshaping your data and plotting. So this example is based on an already worked out example, from the r for data science book using the billboard music chart, dataset where she's asking the the AI, you know, chatbot, use the billboard dataset in the tidy r package to create a visualization of rank versus week number for each song in the dataset. And the answer comes back not too shabby. It's loading the packages right off the bat. It's doing a little bit of, reshaping with the pivot longer function. That's already a step in the right direction.

Doing a little bit of conversion for the week to become numeric and then doing a jiji plot, which, you might call a spaghetti plot type setup where each line is one of these, trends and week number on the x axis and the rank on the y axis of that longitudinal profile. And the so chat GP does give a little explanation at the end and Minay says yeah there are some things that are promising first it is using the tidy verse it's using a newer version of that tidy our function to go from wide to long with pivot longer, and it's got the y axis actually being reversed with zero at the top and a hundred at the bottom which is what she was looking for.

But this is where things get tricky, folks. There's always a little bit more than you bargain for here sometimes. Like, I colored the lines by track. That, you know, looks kind of interesting from a visual perspective, I guess, as an art piece. Maybe that's not exactly what you're looking for, perhaps. And so it's also loading other packages are necessary involved. Like, there's no dplyr used here. Right? It's just tidy r and ggplot2. But yet, sometimes this throws this in for about a lot of rhyme or reason.

So what you can do and what a learner might want to do in this case is they may not know the best way to just not color by track, so they may ask the bot, hey, in a follow-up chat, can you just do this about coloring each line? Sure enough, you get a new plot out of that, and they're all blue now. But they did more than just changing the color and also change the alpha level inexplicably, from point six to point three. So okay. Maybe it just thought that was better when there's no color there. So, again, the bot may be making some choices. The AI, you know, chat might make some choices that you don't necessarily agree with, underscores the importance of reviewing the code as you're seeing this to see if it is really something you want.

So there's there's stuff more at the end of the post we'll get to about kind of practically how do you handle these situations. But again, she's putting herself in the shoes as somebody who's still not quite comfortable with r and and say the tidy verse yet and trying to get to that more polished answer through additional prompts. In the end though, some promising trends there. We move on the case study too. More of a data cleaning exercise, which I think will be very common, a problem to solve for a lot of those, using data science in the real world. And she kept this one kind of vague. There was a column in the dataset called membership status where it can be either NA or select to register for a group but she wants us to again instead say closed or open otherwise so there's this time she's using quad to do it.

So in this, there's, the first response that it gets is a help cleans up the variable, this recoding. But note that if you look at the post after you're listening to this, it's using base r to do it, which, again, not bad. Right? But she wanted tidy verse approaches. So she asked the problem, well, can you do this with tidy versus set of base r? Then it pivots over, to the if underscore else function, and then to use that in a mutate call with dplyr. Looks fine. We're getting there. But it also does some additional things that maybe because it's been trained on resources that aren't as up to date as what we have now, that kind of raised a few eyebrows. One of which is using the Magritter pipe instead of the base pipe. So again, that can happen, right, when you have data that's trained on this.

Some of the the styling wasn't quite up to snuff, with the line breaks, and it didn't quite, show everything correctly. That I didn't need a head call for that because it's using a Tibble. So little little little things like that. If you know r well, you know to look for this, but maybe there are better opportunities and additional prompts to make that happen. The last case study is web scraping. This one can get a little messy, right, because there are multiple ways we can do this. Often multiple languages as well, and she kept this prompt really general. She just asked write code for scraping data from this resource on North Carolina weather data and this time using perplexity AI. So she's using different services and each of these the compare and contrast.

Well, this should be the surprise of relatively few people. She got an answer right off the bat that was using Python with the beautiful soup library which often gets talked about in web scraping. So she had to follow-up that prompt with, well, use r instead. And then you get some, you know, relatively readable code. It's using the r vest package, which is, again, one of the great packages in the in the tidy verse to help with scraping. You know pretty utilitarian type code again using the Magritter pipe.

And you but there's one thing that it didn't do well. It didn't reshape the data the way that she wanted it. She wanted to have the months, as rows and the temperatures as columns. It didn't quite do that the first time. So she asked it to do do the reshaping. And then you get a lot of new code to do this, but it's still not quite what she wanted. So she had to try one more time, and she was very explicit this time in the instructions to get two tables, one from January to June and then July to December, and then put them all together and then reshape them.

And no. This is where things get really dicey. The code that she got back for this step actually doesn't work. It doesn't work. And even though it looks like it worked, but as it gives some output, she has no idea how that output got there because the code actually doesn't work. So that's another red flag, folks. Sometimes it may look correct, but you really got to evaluate this. So this was, this is a fun little journey. Mike, I think you've got a lot of thoughts on this too.

[00:25:19] Mike Thomas:

As you're seeing these case studies here, what jumps out at you that people should be watching out for as as you're as you're seeing this? Yeah. I think one of the biggest gotchas and is the fact that, especially in the last case here with perplexity, and I've seen it in all the other services as well, both Claude and, ChatGPT, which are the two that I have been experimenting with the most, admittedly, Claude much more so recently than than ChatGPT, is that they will pretend that they, you know, they'll spit out an answer and make it look like the code should run successfully.

Right? And, that's not necessarily the case here where in, you know, Mel's blog post here, it's the same thing happened. It's spat out the answers, which not only were incorrect, but the code that it it's also spat out to, give those answers doesn't run. So I think that that is going to confuse a lot of new learners. I I think it's going to, you know, I I was listening to a another podcast, shameless plug for the Vanishing Gradients podcast, with Hugo Bowne Anderson, who had, Joe Rees on. And, you know, there's this idea that I think we're gonna create a ton of technical debt in in terms and, you know, in the next maybe five to ten years as more AI generated code gets into code bases. And then when something goes wrong, who's going to solve it?

Because it looks like these AI solutions are not great at handling the edge cases. Right? They're they're generalizations as it is. And I I think that we're not by having students sort of, you know, lean heavily on these AI tools, they're not maybe not learning the fundamentals of programming, which I think is a drawback, to, you know, sort of where we're headed in the future. I think it's it's going to be difficult, for folks who are just starting out in their career to to get jobs unless they really focus on a lot of these fundamentals. And maybe, you know, the folks like us, Eric, may not have as much work to do, I guess, if we're gonna be employing a lot of these AI tools. But when stuff hits the fan, we are going to be extremely highly valued.

So, you know, there's that dichotomy there. There's a lot of talk about agentic AI lately, which to my understanding is like stringing these things together in a process. And if one of these is writing poor code that doesn't execute, how can we expect to multiply that concept in a Markov chain type of, right, environment and not have things go wrong. Right? I don't think we've solved for the single agent, if you will, doing a good enough job, that, you know, in this case, you know, handling code execution. And maybe it's it's better in the Python ecosystem or JavaScript or some languages that are much more heavily used depending on your use case. But I think when it's gets applied to, you know, a lot of business settings, there's there's just domain knowledge there that the AI application, a lot of times, will not have to handle those edge cases and to really fit the code to the problem. So I could rant about I could rant here for for the next hour, but I do want to go through, sort of the the tips and good practices that Mine outlines here at the bottom of the blog post, which I really agree with.

First is, you know, provide context and and engineer your prompts. And I think we all know this, that the more explicit that you can possibly be and verbose in what you want and maybe how you want the AI to go about doing it, I think the more likely that you're going to get accurate results or results that you were looking for. So that whole concept of of prompt prompt engineering, I think, is actually very important. Second, you know, is to check for errors, and it's it's sort of obvious, but don't just take what comes out of the LLM as gospel. Make sure you run it in your console. Make sure you understand what it's doing.

I can't implore, you know, younger data scientists, data analysts in their journey enough to, you know, take a look at what comes back and and don't just don't just use it. Right? Make sure that you understand all of the parts of it. Go line by line. And if there's something that you're not sure about, take a look at what the LLM gave back because sometimes they'll not only provide the response, but they they will also provide sort of a step by step, explanation of the code that it gave back and and why it used things. Or if you're not a % sure, ask the LLM why, you know, why did you include this particular line of code as as opposed to doing something different?

And I think if you are not taking those steps to do that, I think you're gonna be certainly holding yourself back pretty significantly. But if you are, I think it could potentially be just as good of an education and maybe a more streamlined education in a lot of ways compared to, you know, Eric, you and I googling things. That's the way that we did it. Right? For hours and hours on end. And I still catch myself doing that too. It's it's been amazing how it it it's still not a

[00:30:46] Eric Nantz:

a first reflex to me to go to the chatbot. Like, I'm I'm slowly getting there, but I still catch myself on that Stack Overflow exercise.

[00:30:54] Mike Thomas:

Yeah. And then I think, you know, we talk about code smell. That's that's tip number four here. And I think that goes back to probably these LLMs being a little bit behind best practices in present day. So it's using the Magritter pipe instead of the base pipe, for a lot of cases here and and things like that that, you know, unfortunately, it's hard to make progress when the LLMs are trained on historical data. Right? And there's this sort of gap between present best practices and what these AI tools know. So, I don't know how exactly we're going to solve that, but it doesn't make me doesn't make me excited or optimistic, I guess, about the ability for us to continue to to innovate and make progress at the pace that we currently are.

The last couple tips are, you know, potentially starting a new chat. If you feel like you are just, you know, continuing to send prompts to your current chat and just not getting exactly what you're looking for, potentially start fresh and have the LLM sort of ignore all of the prior context that it's perhaps using in your current conversation. Code completion tools, she recommends to to use them sparingly if you're a new user, such as GitHub Copilot, and I would I would certainly echo that sentiment as well.

And I think the last tip here is use AI tools for help with getting help, and that's a fantastic idea. I think, sort of the the last one of the last sentences here is leverage an AI tool to develop a rep rex. Maybe that you can share in an issue on GitHub or that you can share in, like, the data science, learning community Slack channel to be able to get help, solving your problem if the LLM is perhaps not doing exactly what you hoped it could do.

[00:32:53] Eric Nantz:

Yeah. I really like that that that feedback there. And it's something that I think the more specific you are in what you're looking for, I think the better off you'll be. And and, again, it's it's kind of an art form to get these prompts in a way to get you to that, quote, unquote, right answer more quickly. I don't think I figured it all out yet, but I did get a lot of great advice from, you know, many leaders in this in this space such as, Joe Chang, the author of Shiny, when he was showing me a lot of those AI tools before Positconf last year.

It was amazing to see the effort he took in those prompts to get to that, you know, that data viewer, application that we saw in his presentation and whatnot. It it it's gonna take practice. Definitely takes a a lot of practice, but I really thought try to maybe it's just my nature. I've always been one of those more detail oriented people when I tell maybe a resource at the day job what I need to get this task done, and I'm very explicit about here are the input datasets. Here are what I'm looking for on the output. Here are the key variables I I need you to look at. I I don't leave anything unturned.

My wife will often joke I'm like the menu guy because I always like to go things by a menu so much. Well, I think it can be a good thing in most cases. And I think with prompts, it's better to be specific than to be too general. So that's one thing I've learned over the months or so that I've been using it. And I admit I still am not on the train of code completion yet even though I've done it a little bit here and there. I was jaded by my my first foray of a GitHub Copilot a few years ago and it was just giving me nonsense for my shiny app deployments and I or which is my shiny app code completion and I just never really turned it on since. But I got others that seem to be doing better with this. So maybe I just need to have a fresh look at it in, 2025.

But, overall, lots of thoughts that were provoked here with this post, and I think the key part that you and I and many likely agree with here is that these can help you, but you still need a fundamental, you know, baseline to help judge what is quality code and to not take it literally the first time around. If you have a careful eye, I think it'd be really helpful. But I've seen too many times already people are just taking what it's spitting out and running with it at the risk of going into a complete dead end because they didn't know any better. So hopefully, over time that becomes becomes solvable, but I won't hold my breath.



[00:35:29] Mike Thomas:

Me neither, unfortunately. They can certainly help you, but they can certainly hold you back.

[00:35:48] Eric Nantz:

Well, we mentioned at the outset of the episode how products like Positron are really positioning themselves to be a language multiple language supported development environment. Well, you can also say the same thing with the Quarto ecosystem where you have the ability to leverage, you know, pack languages like R, Python, Julia, and one that I have not used as much until kind of recently, observable for interactive JavaScript. And you may be wondering as an R user, yeah, I've heard about this observable thing either and and and talks about quartal or other blog posts. But what does it really mean for me? How do I get started with it? Our last highlight is a terrific way to to make this happen. It's called observable for our users, and it is a blog post authored by Nicola Rennie.

She does a terrific job here getting straight to the point of how you can still do both r for parts of your data processing, but then turn to observable for your visualization needs. So what she starts off with at the post is using r for what it does very well, which is data wrangling. So she's got, based on a a previous tidy Tuesday dataset that was used based on the Himalayan mountaineering expeditions. That was from January of this year. She imports that dataset into r, does a little bit of cleaning here and there.

Not much really. It's just a little bit of deep wire code for filtering and selecting various columns, and you got yourself a nice rectangular type of data frame. And normally, if I was just staying with r, it might then turn to ggplot two or one of the extension packages to do the visualization. But now it's time to see what observable can do for the visualization needs. So in quarto, just like it was in the predecessor r markdown, you have your code chunks where you give it the language that's being used for that. So we have used r as a language up to this point in the post.

Now we're gonna use the o j s code block to start those or o j s side of it. But the first step though is to hand off the data you just process in r over to observable. This is the part that seems kind of magical to me. I I'm I'm sure it's it's documented somewhere. But Quarto comes with a function called o j s underscore define, and you can feed in the object name of that data frame or table in R that you created, and then that is going to become available in any code chunk after with observable nonetheless this is a convenient a convenient way now to get the data you wanted from r into observable.

And so what do you do with observable? There are some things you might need to be aware of for a lot of the plotting libraries is that sometimes it expects things to be in more of a row wise format instead of the columnar format that we often see in our in our datasets in r. So there are functions, such as transpose to get you into that row based format that the visualization function will need later on. So just got an example of using that with the data that was handed off from r to get that into the row based layout.

When you're in observable, Azure itself comes with a lot of nice built in libraries for visualization processing and whatnot, but what's nice is that you can bring in other JavaScript based libraries into your observable session. And this is the other part that seems like magic. In R, we're used to loading a package in your R session via the library function, but that assumes you installed it first. Right? Well, in observable, you can use a function called require, put in the name of that extension library, and it's gonna kinda take care of both installing and using it for you. Because now we're in the world of JavaScript. Right?

These libraries are often externally hosted in some form and this is kind of like giving you that pointer to that library so she has an example here where maybe you want to use the d three library you could do that and then give it an annotation like a version number afterwards and you can load that into your session so that's a nice to have I've done this before with the Quero library as well that's kind of like a tidy verse inspired data processing library and and JavaScript, but for the rest of the post she's gonna use what's in observable itself.

One of which, libraries is called observable plot, which as the name sounds like you're gonna do yourself a plot with observable. She starts off with a basic scatterplot with the year on the x axis and the height of the of the mountains on the y axis and once you have your data in the right format that you do previously there is a function conveniently called plot and this is where this kind of resonated with me a little bit with my explorations with plotly and some other JavaScript libraries you got to tell the plot what are the variables they're gonna use on your plot and so there's a function or an attribute called marks and that's where you give it then the data argument but then what is your x variable and your y variable you do that then you got yourself your your basic scatterplot right off the bat but why why stop there let's let's make this a little cleaner shall we do you want to make some adjustments that the x axis is representing years but it's like using actual number or comma notation to do it so we've got to transform that into a more logical type of attribute which would be a date right so there are ways that then you can change your data set using you know what looks like pretty you know tidyverse like code where you can give it a new year or a new variable called p year and convert that to a date object.

So just like R, JavaScript has date objects, numeric objects, you know character objects and whatnot for your datasets then you feed that into your plot and now you've got at least what looks like a more robust year on the x axis and you can give it other nice attributes like using a grid on the back end with a grid true flag and giving it labels as well with, x and y. There's an attribute called label where you can change the label on that, to meet your needs. And then you can get to a color palette where there are built in color palettes. She has a link to the observable documentation where if you want to cover the points by another variable, you can definitely do that such as a region.

There's a pallet she uses called set two and you get a nice selection of the different colors. And then last but not least, you can add some titles as well. And again, all these arguments sound pretty logical. It's just a matter of where you fit them. So you have a title, subtitle, and a caption as well if you wanna put that at the bottom of the plot along with tweaking the size and the margins. So you can get very low level with these, but you can get to a pretty basic looking plot that looks pretty nice right off the bat. And observable, one of the biggest selling points, of course, is the interactivity that'll be built right into your report. If you compile this a quartile, you might get those tool tips. You might be able to zoom in. Lots of different, attributes that you can tap into here.

And then if you want you can save that as a static image afterwards, just as a PNG. There's you could tap into a package like webshot in R to grab that from observable back into R referencing the output of that cell that was doing the visualization. I've only scratched the surface of what's possible here, and Nikola does a great job of, like, orient yourself with a here's how to get started with it and little teasers along the way to really make this really polished. So I'm intrigued what we can do here. And, certainly, the handoff from r, the observable, could not be easier, in the Corto ecosystem. What do you think, Mike?



[00:44:35] Mike Thomas:

Well, I've done a deep dive and it looks like we can't and I'm not sure if you're asking if we can go from Python to OJS or if you were asking if in Quarto we can go from r to Python data. It was more of the latter. Just curious. I don't think the answer is is yes for the latter unless you have reticulate. I think that's what they recommend. Gotcha. But you can go from, obviously, in our data frame to a a JavaScript data frame that OJS can consume using that OJS define function. And similarly, on the Python side, you can go from a Pandas data frame to, again, using OJS define to an OJS data frame.

I have looked beyond that to see if you could do the same with, like, a polar's data frame, but there are no documentation and no links on that. And I can't even find the documentation for if any other, p Python libraries besides pandas are supported for that particular handoff. But obviously, pandas is pretty ubiquitous. Most other Python libraries, even such as Pollers, have a function to convert that type of a data frame to a Pandas data frame pretty quickly if you need to do that handoff. But, yeah, this is a fantastic walkthrough. You know, OJS for blog purposes, I I absolutely love, you know, for static website purposes because you you don't necessarily need that full server behind the page, but you can still get a lot of the interactivity and the tool tips and things like that that OJS, defines. And the syntax isn't that bad. Obviously, coming from a different language, it's gonna look a little bit different.

But if you just sort of take the time, and I think this great this blog post is a great walkthrough and some really nice snippets of OJS code. It's pretty understandable, pretty legible, pretty consumable, and not too scary. I think if somebody was, you know, trying to recreate these types of plots or try to use trying to use OJS for their own use cases, whether that be a blog post or some other type of, dynamic document, I I think you'd find it not too difficult of a process to convert from your R and Python, you know, visualization code to OJS code. And I think there are probably some syntaxes here that I might actually prefer OJS to what we have to wrangle on the R and Python side to get the plot to look the way that we want it to look. You know, one pretty cool thing about OJS that I read up on that I did not know is that the the documentation on the Observable website has an interactive color palette viewer, where you can browse different sequential, diverging, and discrete color palettes.

And the built in options include the ColorBrewer palettes, you know, which we are all, very familiar with on the our side. So I thought that that was cool. Last night, I was, you know, just messing around trying to come up with nice contrast between text and background color on a particular presentation that we are putting together and didn't really have a great workflow for doing that. I wish I had known about that observable, documentation site.

[00:47:42] Eric Nantz:

Yeah. Well, I've linked that in the show notes too. And I've I played a little bit of visualizations here, but my foray recently with observable and and quartile was, I maintain a website in quartile for our our consortium submissions working group. And I have, admittedly a geeky page on there that's meant for authors of the site like me and and other contributors, a way to use observable to dynamically generate the code based on a widget that the user can or the developer has of the possible attendees to a working group meeting. There's a little checkbox interface where you can check the names that attended it. And then below it, there will be a prefilled quartal snippet with one of those call out blocks that you can expand and contrast with the attendees, with the date that you select, and then you can copy that text into a new portal document.

It I mean, it was an over the top. Yeah. Absolutely. But it was a good way to learn nonetheless. So I I use that when I start drafting the minutes, for a working group meeting. Put that developer page up, get the attendees out, and then copy that over, to quartal. But that showed me this connector, which again seemed magical to me, going from r to import a spreadsheet of the possible attendees over to that widget and observable to reference that on the page itself and then give you that interactive element to select the name.

So there's there's untapped potential here, and I think in the case where you don't need a server, it's hard to deny the impact that observable is having these really clean and and polished interactive, summaries and visualizations in our in our quarter reports. So I think that the time is now to get a little observable action here. And, speaking of which, we had, a previous workshop, I believe, at our pharma a year or two ago, talking about observable for our users. So I'll put a link to that in the show notes too if you're interested.

And there's a lot more awesome stuff happening in this week's our weekly issue and, you know, of course, we have a link to that full issue in the show notes. Running a bit low on time today, so we probably won't do our additional fines. We'll invite you to check out the excellent issue that Jonathan Carroll has put together for us. Some great, highlights or additional fines that if you do wanna tap into, I'm already seeing some good stuff about Parquet, DuckDV. You know us. We love we love soaking up that content. So there's some great data processing, content there as well.

And if you wanna help the project, one of the best ways to do that is to send us that suggestion for that new resource that you found. Blog post, new package, anything in the world resource that you found, blog post, new package, anything in the world of data science and R, we love to hear about it. So you can do that via a poll request linked in the top right corner, that little octocot there right off the bat. Get a link to this week's, current or upcoming issue draft. Just send a poll request right there, and our curator of the week will be sure to merge that in. And a little birdie tells me that that might be me this coming week. So I could use all the help I can get folks.

Send those suggestions my way. And, also, we love to hear from you on the social medias as well. You can find me on blue sky where I am at our podcast at bsky.social. I'm also on Mastodon where I'm at our podcast at podcast index on social, and I'm on LinkedIn. You can search my name and you'll find me there. And by the time you're listening to this, the the twenty twenty five shiny conference will be underway, and you'll be able to hear my talk on Friday, talking about the cool stuff I'm doing with Knicks and shiny. But it should be a wonderful conference. So if you're,

[00:51:22] Mike Thomas:

not registered for that, go register now because there's some great content coming your way there. And, Mike, we're gonna listen to us find you. I couldn't agree more. We are super super excited for ShinyConf twenty twenty five over here as well. You can find me on blue sky at mike dash thomas dot b s k y dot social, or you can find me on LinkedIn by searching Ketchbrooke Analytics, k e t c h b r o o k, to see what we're up to.

[00:51:47] Eric Nantz:

Excellent stuff. And, again, great to always record a fun episode with you. And, yeah, right after today, we'll be getting our our shiny geek them on at the at the shiny conf, and I gotta finish my slides, folks. So nothing like conference room development. Okay then. Well, that's a good time to sign off here. So we'll wrap up episode 201 of the rweekly highlights, and we'll be back with another edition of r wiki highlights next week."
"18","issue_2025_w_14_highlights_638791571265579746",2025-04-02,38M 52S,"By some minor miracle (even on April Fools) the R Weekly Highlights podcast has made it to episode 200! We go ""virtual"" shopping for LLM-powered text analysis and prediction using the mall package, and how recent advancements in the grid and ggplot2 packages empower you to make use of highly-customized gradients. Plus listener feedback! Episode…","[00:00:03] Eric Nantz:

Hello, friends. Do not adjust your earbuds. Guess what? It is episode 200 of the Our Weekly Highlights podcast. And there was much rejoicing. We somehow made it, folks. Yes. We made it to the big 200. And if you're new to the show for the for tuning in for the first time, this is the weekly podcast. Well, mostly weekly anyway, that we talk about the latest happenings and resources that are shared on this week's our weekly issue. My name is Eric Nance, and I still cannot believe we made it this far. So maybe before I go too much further, let's put a little, little PSA out there.

Yeah. This 200 thing, yeah, after this. You know what? I've had a change of heart. I think of episode two zero one and and more, we're gonna call this the SAS weekly podcast because they don't get enough love. April Fools. If if you didn't catch it, you should have. But it as we're recording this, it is April fools day, which may be a bad omen for things going forward, but we're gonna make it work. But, as always, I'm joined by my awesome cohost on this show who's been with me definitely more than half of those 200 episodes, Mike Thomas. Mike, how are you doing today?



[00:01:18] Mike Thomas:

Doing great, Eric. What a milestone. Very excited. Yeah. You scared me there for a second, but, it is in The US here at least. I don't know how international the holiday that is, but it is April Fool's Day. So watch out. Beware. Especially, I'm sure for those

[00:01:34] Eric Nantz:

like you who have young children who'd love to take advantage of that type of thing. He already did yesterday as a preview for it. I've I discovered these, tablets we use for his, like, online art class were suddenly taped to my desk, taped to it for some reason. I'm asking why you do that? He's like, hey. We're fools, Danny. Well, okay. Well, I I'd imagine I'm gonna be in much worse shape after he gets home from school today. But, but, nonetheless, we're not gonna try to be fools here. We're gonna keep it real as usual with the awesome highlights that we have up for tap today. And this week's issue is curated by John Calder, another one of our awesome contributors and curators here on our weekly.

And as always, he had tremendous help from our fellow our weekly team members and contributors like all of you around the world with your poll requests and other terrific suggestions along the way. Just to put a little PSA out there, poor Mike here's, actual mic wasn't quite working as expected. So we had a little bit different audio quality. We'll make it work, though. So, again, we're not immune of the shenanigans on this day for sure. Well, I guess it's the time capsule. Right? It is episode 200, and typically, every week now, we got something to talk about in the world of large language models. And, yeah, we are leading off episode 200 with another great showcase on some of the tooling that's out there for an aspect of the usage of large language models that I personally haven't done as much, but I can see in future projects this will be extremely helpful if you're doing of a lot of textual based resources.

And this guest post on the Pazit blog was authored by Camilla Levio. She is a research professional at the Center for International Trade and Security at the University of Georgia. And this very practical post leads off with, I think, a use case that many of us will be very much in relation to when we deal with our documents in the enterprise or maybe from clients or other projects where the motivation for her recent effort was being able to compile a list of final reports. These are in PDF format from an annual conference called the Conference of the Parties.

This is spanning from nineteen ninety five to two thousand twenty three, And this is I haven't heard about this until this post here. This is a yearly conference for global leaders to talk about climate change, some strategies involved, and, you know, some new new avenues to address those issues. So these, conferences do release the, I guess, the the summaries of these. And what looks to be a pretty, dense, pretty, comprehensive PDF file for each year. And so the question is, how can her team be able to get generate these summaries and insights that are part of the contents of these reports?

Certainly, before the advent of large language models, you might tap into the world of text mining, textual analysis, sentiment analysis, where you're gonna be setting up, you know, those, keywords, maybe those patterns of searching. There are lots of avenues to do that. I know Julia Silgi has been a huge part of the efforts in the art community to bring text mining packages to the art ecosystem. So I've we've actually covered, I believe, a fair share of those on some of the previous 200 episodes of this, program here. But with the advent of large language models, you got a new take on making life a little bit easier in these regards.

And in particular, this post is highlighting the use of the mall package, which is authored by Edgar Ruiz, one of the software engineers at Posit. And this is a LOM framework in both r and Python. There are separate packages for each to let you run multiple large language model predictions against a data frame and, in particular, looking at, you know, methods of textual prediction, textual extraction, and textual summarization. So there are two steps that are outlined in this post about the use of the ball package.

The first step, I guess, the precursor of this is that we have a cleaned version of the report data that Camilla has assembled here in a CSV file. But, of course, you might have to leverage other means and perhaps even LOM itself to just get these from the PDF, which you could do with, say, the Elmer package and the like. But the the data set in in the example here has three columns here, one of which is the name of the file. Second is the raw text that's been scraped from this file. And then another another column, which is slightly more, slightly cleaner version of this text, but it's still, again, the raw text from this PDF just maybe without the formatting junk inside.

So step one of the mall package, she wanted to generate a new column in this dataset that gives a summary of each of these, PDF reports. And that's where you can leverage after you plug in your model of choice, which in this case is the, Ollama three dot two model here. You can run a function called l o m underscore summarize, where you give it, you know, the the clean textual column that you want to, to extract here, and then give it a name of the new column, and then more importantly, an additional language for the prompt itself that's gonna be feed into the model, where it was a very basic prompt here just saying to summarize the key points in the report from this proceedings.

And then she gives a little more extra help on the different categories that she likes summarization on. Again, one thing I'm learning is that the more verbose you can be of your prompt, the better within reason to give a little bit a little better context. And then when that function is completed, she gives a little preview of those, extracted columns here, and you can see, looks like pretty straightforward, the three key areas that she asked for highlighting, such as the decision making process, mitigation adaption, you know, emission reduction, and other key points were covered in these clean summaries. So, again, a llama model does pretty nice job there, it looks like. Again, this is only three or or maybe five or so reports here, so we're not getting a huge set here.

But this does look promising. That's step one. Step two is now with this clean summary that, again, is much easier to digest to read, you know, verbatim. What about extracting some key parts of this summary? Perhaps some keywords, if you will. Maybe the relay to certain topics. And she has those here using the l o m extract function from mall, where you can give it these different labels, that you want to basically be extracted from here. And in this case, she's looking for energy transition. This is where things get a little more flexible than in the typical large, text mining area, where you might have to look at all these different synonyms or other adjectives that would just that would, say this phrase and try to grab all that at once.

But in this case, with l o m extract, she's able to leverage just this labels column with a very fit for purpose, you know, description here, like I said, energy transition. And the model is gonna be smart enough, hopefully, to extract this phrase, but also ones that are, you know, adjacent to this as well. So after running this function, we have, again, an additional prompt that was supplied here. You now get a new column here that shows that phrase or an adjacent or, you know, like phrase of that in this extract energy trans column. And you see renewable energy is put in here, energy transition, solar energy.

Again, that's one nice benefit here. She didn't have to prospectively think of all those that could be possible. It was just one phrase, one label, and then the model took care of the other patterns that this could be, you know, closely adjacent to. So she then did a little visualization on these different keywords that are identified, and you can see that renewable energy was the top of the list with the other ones of about, one occurrence each. And, again, you can, you know, build more custom prompts if you wish, such as maybe telling the LOM prompt to answer yes or no, whether this text would mention any challenges in the transition to other forms of energy.

And, again, you can feed this in with an l o m custom function, which gives you that more customized prompt to grab this information and sure enough, you get this quote unquote prediction column afterwards with a no or yes that addresses that particular question. So what am I seeing here as someone who's doesn't do a lot of day to day of textual mining or textual analysis? There have definitely been projects at the day job where we have these say study reports or study protocols, and there's a lot of information there. Some of it's, quote, unquote, structured in tables, and some of it is not. Some of it's more free form.

This would be a great utility, I think, to extract those different pieces of a study design, maybe those different variables of interest that maybe have a little difference in their in their characteristics from one study to another. This seems really promising here. And to be able to leverage MALL combined with, large language models with LOMs seems like a a pretty nice example here to supercharge your textual mining efforts. So really great blog post here by Camilla and makes me wanna go shopping virtually at this mall, so to speak. What about you, Mike? Absolutely.



[00:11:49] Mike Thomas:

Yeah. You know, we're getting so many different LLM related r packages that sometimes it's hard to keep track of them all. And I I really appreciate the blog post here around the mall package that, you know, certainly seems to be tailored specifically towards, the open weights models, right, for use cases, perhaps when you don't wanna hit a third party API or you wanna use a local model, with the Ollama package. You know, I was taking a look at the beautiful package down site that we have here, and I think this references a blog post from our prior week's highlights In that, we have these tab sets under the getting started section of the, sort of home page for the package down site where we have code in both r and Python for leveraging mall.

And as soon as you switch from one to the other, all of the tab sets switch from from r to Python, so they're grouped together, which is pretty cool. It's, you know, incredible that we have these tools or or common sort of, interfaces to maybe more back end APIs across whatever language we care about and not to, I don't know, get a little too meta. But I recently was selected, for posit comp twenty twenty five for anybody that's going to be there. Whoo. Sort of myself pat on the back to talk about this exact thing, how we have a really great data science ecosystem now that's evolving where the language doesn't necessarily matter, because the syntax looks pretty similar in both cases, and we're just, you know, able to interface to these these common underlying APIs. So I think that's one of the the cases that we have here in the mall package. And if you take a look at the differences between the r syntax and the Python syntax, it's it's really, really minimal. So it's pretty cool that, you know, you can sort of bring your own tool and get the same results at the end of the day. You know, from another higher level perspective, you know, one thing I wonder about in a lot of these use cases is accuracy.

Right? Versus what you spoke about here, you know, traditional sort of text mining. And I think that that's always probably going to depend on how simple or or complex the use case is. You know, for simpler cases where you have specific keywords and and patterns that you really know are going to, you know, be the case 99% of the time or or more, you're probably gonna benefit from, like, a more traditional text mining approach that's a white box solution. You know exactly what's going on and and what it does miss, you know, why it missed.

But, obviously, when the problem becomes too big for traditional text mining, that's when we can bring in, I think, these LLM based approaches that are probably really the only tool that we have to be able to accurately do this, but with the trade off that we don't have as much of a white box to understand when it missed. So we have to do sort of manual evals a lot of the times, if you will. But, yeah, the the fact that we have these tools at all is fantastic and that, you know, folks are bringing not only developing tools like mall to allow us to leverage them as comfortably as possible from our own R environment, but we also have folks like Camilla who are drafting up fantastic blog posts that walk us through exactly how to do this stuff. So hats off to everybody involved, the mall team and Camilla for developing this this fantastic deep dive walk through.



[00:15:22] Eric Nantz:

A great start to the highlights this week. You bet. And I can see, you know, from from my case, you may have this, you know, large set of documents, and it's not like we're eliminating a human in the loop here of, like, reviewing these results. This is a way to get that that that intermediate step to make life easier to look for maybe more targeted summaries and maybe whittling down a list of, say, 150 or 200 some reports down to maybe a list of five or 10 that then become a lot more interesting, a lot more easily, digestible to review those results. So, again, my my, gears are turning in my head here because there are projects on top of these study documents that we have, but also just research manuscripts out there of given, you know, therapeutic areas like, say, Alzheimer's or other disease states. And we pay a vendor a lot of money to curate this stuff manually, and it sure would be nice instead of paying all that money to just get, quote, unquote, that study table out or that, you know, high level summary out that we grab that with the LLM and then, of course, you know, vet it through a human kind of in the loop for review. But my goodness, this could this could save a lot of money. So I've got I've got I've got some people to talk to, the the higher ups about this because we have we have the models now. This is clearly demonstrating this could be a plug and play for the model of interest over it's a llama. Or if you're in the enterprise, maybe you could use, like, Claude or other models on top of this. Seems like right it's a right time to start exploring this further.



[00:17:00] Mike Thomas:

Should we pivot to an hour's worth of hot takes on whether LLMs are going to do peer review for science?

[00:17:09] Eric Nantz:

I'm I'm here for it.

[00:17:11] Mike Thomas:

Alright. Let's go to highlight number two.

[00:17:25] Eric Nantz:

And rounding out our highlights today, we're gonna take a visit to the good old visualization corner, which has always been a staple on these previous episodes of our weekly highlights. Because as we've seen throughout the life cycle of this very show, my goodness, the things you can accomplish with packages like ggplot two is absolutely amazing to create publication quality and to be honest more than just publication quality, but really eye catching visualizations that would not look like they came from R.

So in this highlight here, we're gonna talk about a newer feature that landed into R recently that I think give your plots a little extra pizzazz especially with the use of colors. And this post comes to us from James Goldie whose name may sound familiar. He is a data journalist, but he was also one of the co leads of that recent quarto close read contest that we talked about a few weeks ago. And he's been a very active member in the visualization and kind of data storytelling, you know, piece of the landscape here.

And so from his blog, we're gonna talk about how you can make novel uses of gradients within r and, in particular, the g g plot two package. So he leads off with the fact that in today's world of g g plot two, we can do a lot of awesome, you know, features or a lot of awesome visualization techniques with use of fonts, you know, size of of labels, and, of course, colors. But sometimes he would find himself before some of these recent advancements getting, like, 90% of the way there, but then maybe just give that extra little polish. He had to pour it over to Adobe Illustrator or some other software and just give it that last extra bit of polish, especially in the world of colors.

Well, sounds like that might be a thing of the past because now with g g plot two and some recent advancements in the grids package, you have a lot more flexibility to make control or use of gradient color scales into your visualizations. And as I mentioned just now, this is building upon the shoulders of the grid package, which actually comes with r itself, but you have to explicitly load it into your session to start taking advantage of the lower level functionalities. But Grid has seen some really awesome advancements that we've covered from Paul Merle and others, contributors in this visualization space.

And with the Grid package that g g bot two is standing on the shoulders of, you can leverage two great functions. I'll talk about the first one, and I'll turn over the mic for the second one. But in the case of, say, a bar chart or other types of visualizations, you might wanna consider a linear type of gradient. And that is literally using the linear gradient function under the hood to do all this. So in this first example with a bar plot of the empty car set, it's actually a histogram. But for the fill of the bars, it's not just a single color. He leverages the linear gradient function going from red to orange.

Now, again, this plot itself doesn't look that great, but you're already gonna start to see the potential here. With within these, linear gradient calls, you've got a lot of flexibility for how many colors you feed into this and how you distribute it. So you can feed in any number of colors. And in this other rainbow like example, he's got about, looks like seven colors here. And then the the accompanying, you know, data's part of this, call here is a vector of what you call, like, stopping stops between zero and one, kinda like giving a threshold from going from one color to another and another.

So that's all well and good. But you can also change the actual size and position of these gradients using coordinate type arguments of, like, x one y one, x two y two to give a little kind of a bounding box, if you will, of where this is gonna be fit in. So you could do this horizontally by default or you could do it vertically. And on top of that, you also get flexibility to the type of units that's gonna be using in this kind of gradient transition within the the bar itself, and that is using a different unit. In this case, the SNPC or square MPC units that will give you kind of customization on the angle of how this gradient is gonna transition.

As usual, audio podcasts, we're trying our best to that now right here, making clearly see a difference between when he used this default unit, being specified of this SNPC where the transition from red to orange is a lot more pronounced, especially earlier on at the bottom of this chart. So you can see you got a lot more flexibility here, and all this can be plugged in to many different aspects of your plot such as with groups or within the scales themselves. So that if you have a bar chart of, say, positive and negative values, you can easily feed in these different positional arguments. So it may be at the above the the y equals zero of the axis.

You've got a more orangish red color versus below you might got a bluish color to really distinguish that that threshold going from positive to negative. Lots of great ideas here on the linear side of it. But as I've seen, you know, visuals don't always just have those nice little rectangular boundaries. We gotta get some circle action here, Mike, and that comes to us with the radial gradients.

[00:23:18] Mike Thomas:

Yes. And radial gradients work pretty similarly. You're gonna have this radial gradient function, and it has a few different parameters, c x one, c y one, c x two, and c y two. Those are the four parameters that really drive, the the center points of the gradient, and then r one and r two establish the the radius of the start and end radii. That's how you sort of define this whole entire entire circle gradient within this radial radial gradient function. Excuse me. So there are some fantastic examples here in the blog post that really demonstrate this nicely.

And one thing that I'm, you know, really sort of taken aback by is this this group parameter that you had had foreshadowed here, Eric. I guess that's available since our 4.2, and and it controls whether a gradient applies to individual shapes or to a set of them. And I guess it's true by default, which in the case of this scatterplot example, the the points have different colors sort of based upon the gradient sort of applies to the whole chart, if you will. So the points have different colors. As opposed to in the second case when the group argument is set to false, all of the points look exactly the same, but they have this radial gradient inside each point. And it creates this sort of three d look, where, you know, each of these points on the scatter plot looks three-dimensional itself. It looks like a ball that's sort of coming off the page, which is absolutely incredible. It makes me wanna throw everything on this this blog post. It makes me wanna throw away every single g g plot I have ever made, because they don't look nearly as nice as what, James has put together here in terms of his data visualization. This is a quarto blog. You would almost not know it, just because the theming and the CSS that's going on here is is absolutely beautiful. And and as you mentioned, you know, some of these, different bar charts as well where we get into this this grouping possibility really drive pretty stark contrast depending on whether you set group equals true or group equals false.

You know, another thing that James employs here is the g g force force package. And I have a awful confession to make today, but I have never used the g g force package. And I need to use the g g force package because I think it's doing a lot of the the really cool stuff, that's going on here or at least making it easier than just doing it, you know, in raw g g plot. Although, I imagine that's probably possible. Have you been a user of g g force, Eric?

[00:26:06] Eric Nantz:

I have not. And like you, I'm getting the notes to try this out because I'm seeing a lot of novel use cases for it these days.

[00:26:12] Mike Thomas:

Me too as well. So the the blog sort of concludes with, you know, being able to sort of abstract a lot of things that you may have done in CSS, to, you know, our code for these radial gradient specifications that you can create sort of backgrounds on your plot as opposed to creating gradients within the elements of your plot, really sort of setting kind of these interesting rainbow backgrounds on these simple plots that really provide for some cool theme here. I I would say, and this is a lot of stuff that I would have never really thought about or considered in the type of data visualization work that that I typically do. And there's a really handy preview underscore gradient function, that allows you to, you get a small bar, I I believe, in your your plot viewer that shows the gradient before maybe you apply it to, you know, a particular element of your chart. So that's a really, really nifty option. Sort of reminds me of the the the, theme previewing that we can do in b s lib, if I'm not mistaken.

So a really handy utility function here, but a phenomenal walk through of applying gradients, in our in g g plot two. I think, you know, James really covers sort of the whole universe of what's possible.

[00:27:34] Eric Nantz:

Yeah. And what James is showing here, especially as you alluded to in this last part of the post, this kind of stacking mechanism of these different gradients. This is one of those pain points that he used to have with going to illustrate was to create these more custom layered approaches to these different gradients for backgrounds and the like. And now with a a great example of code here that he has, he calls this function stack underscore patterns. You can see the recursive nature of this to be able to wrap all this into one type of gradient and to be able to, you know, leverage that as much as you like from both the linear side of it as well as the the radial side of it. And you can see at the end of the post these really eye catching backgrounds on on these different scatter plots and bar charts. There's a lot of wonderful features here.

And, yeah, like I said, even those that may be more familiar with CSS, he ties it all together of how that works on the CSS side as well. Lots of expandable details here. He he makes great use of Quarto here. This is a fantastic post, and I'm bookmarking it right now before I leave this episode because I wanna up my visualization game, and gradients looks like a

[00:28:51] Mike Thomas:

a wonderful way to do just that. Excellent post here by James. Coming straight back to this post the next time I make a GG plot.

[00:28:59] Eric Nantz:

Yeah. And count down with, like, the the twenty or thirty hours that we covered in recent weeks, it seems like. There's so much in this visualization space that I'm just scratching the surface of for sure. I've been on the kick of interactive plots, so we but, man, at some point, you gotta get to those static plots at the end, whether it's a report or I hate to say it, even one of those PowerPoint decks. So if I if I have to be in those static confines, I'm gonna I'm gonna bling it up with this for sure.

And there's a lot more you could bling up too by reading the rest of the issue, and it's a jam packed issue that John Calder has put together for us, and we'll take a couple of minutes for additional fines here. And, well, he's this is someone who we've covered their journey on reproducible analysis and especially in the case of leveraging Knicks as part of that journey for, gosh, over a year, if not more. But, in in my additional find here, Bruno Rodriguez has turned, you know, what I knew about Nick's and Rick's upside down, so to speak, because he is announcing a new package called Rickspress.

This is, to put it mildly here, a new take on reproducible analysis pipelines powered by Rick's. That should have a keyword that you may, may latch onto if you know some other key reproducible analysis analysis pipeline tool kits we're talking about. But I'm still wrapping my head around what he's accomplishing here at Rick's Press. But in a nutshell, when you have a pipeline of analysis, Rick's Press is leveraging Rick's for each step in this process, which opens up the possibility that maybe for a given step for analysis or text mining or whatever have you, because of the power of NICS under the hood, you have the flexibility in these steps to go from, say, R to Python or another language that Nick supports.

This is unbelievable. Now, again, he is very much stressing this is a prototype. Do not use it in production yet. He's still working out the kinks of a few things, such as how we pass objects back and forth through these steps in the pipeline. But he is, the elephant in room, so to speak. He has been heavily inspired by targets, and this is not replacing targets. Let's let's not kid ourselves. But what this is showing is the potential for a multi language analysis pipeline in the spirit of targets, but with a lot of granular control at the dependencies at each step, not just in the overall pipeline.

Immensely, thought provoking here. I am gonna wrap my head around this, but I admit this is kinda timely because I'm gonna be speaking about the virtues of Nicks and Ricks with my shiny development at the upcoming shiny conference, which is happening in a week and a half. So Rick's Press, is someone that's new, but I'll make sure to plug that in my talk at the end. So, Bruno, as usual, every time I think I figured it out, you change the question, so to speak, as Roddy Piper would say. But nonetheless, it was a excellent find here. So, Mike, what did you find?



[00:32:18] Mike Thomas:

That's awesome. I found a recreation of FiveThirtyEight's hack your way to scientific glory, website that was done by Andrew Heiss, who's always just pushing out incredible content. And it is a dashboard, so to speak, but all in observable JS OJS. So what that means is that I I think there's essentially serverless. Right? No. It's a static site, if you will, but it it is very interactive and really feels almost like a shiny app, if you will. And it's it's beautifully done. I I really love the theming here. And the idea is that you're a social scientist with a hunch that The US economy is affected by whether Republicans or Democrats are in office.

And you can choose a few different toggles here, your political party, which sort of politicians you wanna include, how you what measurement do you want to use for economic performance like GDP or inflation or stock prices. And then what you're gonna get is is a p value at the end of this, based upon whether that political party had a negative or positive impact on the economy. So it's a little bit of p hacking going on here to a cool exercise to be able to just, you know, kind of switch dials until you get a what's called a publishable result, which would be a p value of less than, I think 0.05 or zero point, zero one in the case of how I I believe Andrew has put it together here. But it's awesome. It's a a fantastic, I think, use of of OJS, and I imagine maybe quarto to to publish it and, really, really cool work.



[00:33:57] Eric Nantz:

This is fun to play with. I'm playing with it right now, and this is one of the advantages of bringing in Observable JS and and a portal doc. It is just snappy responsive, and this loaded right away. Obviously, this is just taking advantage of OJS, so we don't have the web assembly stuff going on here, but you don't need to in this case. It it is fit to the point. And, obviously, in my industry, I take the issue of p hacking quite seriously. But in this, this could be a fun way to to exercise that. So I'll I can play with this about fear, and I'm gonna lose my job, so to speak.

This this looks fun for sure.

[00:34:34] Mike Thomas:

Absolutely. Yeah. I think it's great in situations where you have fairly small data, right, to leverage OJS and, the quarto here. And probably, if you wanna continue using a static site in OJS and Cordova, your data gets big, that's where we get into maybe DuckDV WASM.

[00:34:53] Eric Nantz:

I think that's the future, man. That's the future. I can't wait to Got it. Can't wait to play some of that further. Absolutely. And, before we close out here, we have put a call out for, you know, any, you know, kudos or other appreciations for our weekend. We did hear back from one of our dedicated listeners, Maru Lapora, who I've had a great chance of meeting at previous Pazit conferences and other avenues. He had input a response to one of our, posts on LinkedIn about our discussion on some of the issues of CRAN recently. But first, he he had said he was loving the use of the continue, service as a way to leverage LOMs in Positron as a way to have a front end, you know, Visual Studio code or Positron extension to these services, and I am literally using that now on my open source setup. It is really cool. So great great, great recommendation there, Mauro. And, his feedback on the issues that we were talking about with CRAN recently, he says, I hear the pains of developers maintaining packages on CRAN.

Also, I understand the effort the core team puts in into allowing the art community to live at the head, so to speak. This is a rare, costly, and beneficial approach that I came to better understand. Thanks to this section in the book, and he plugs it, software engineering at Google, which I'll put a link to in the show notes. And that's a pretty fascinating read if you wanna get into the nooks and crannies of software engineering. But Mauro, this is a a terrific summary here, a terrific piece of feedback.

In the end, this has always been a multifaceted issue with the place that CRAN has in the art community combined with some recent issues that we've been seeing. But in the end, the there are things that I would say CRAN is still a step ahead of some of the other languages, which can be a bit more of a free for all in terms of package repositories. Sometimes not always of of very of varying quality, so to speak. So, again, great take on that. It's never a black or white issue, I feel, with these with these things, but great great piece of feedback, and we enjoyed hearing from you.

And with that, we're always welcoming for more feedback. So from the post episode 200 on, if you wanna get in touch with us, we have a few different ways of doing that, one of which is on the contact page in this episode show notes. Take you a link right there to a little bit of web form for you to fill out there. You can also, send us a fun little boost along the way if you're on a modern podcast app, like, CurioCaster, Fountain. In particular, it makes it easy to get up and running with these. I have linked details on that in the show notes, and you can get in touch with us on these social medias.

I am, blue sky where I'm at rpodcast@bsky.social. I'm also on Mastodon with @rpodcastatpodcastindex.social, and I'm on LinkedIn. Search your name, and you'll find me there. And you also like I mentioned earlier, you'll find me as one of the presenters in a in a couple weeks at the upcoming shiny conference I'm super excited about. And, Mike, where can our listeners find you?

[00:38:01] Mike Thomas:

I'll be there at Shiny Conf, watching you present here. That's super exciting. You can find me on blue sky at mike dash thomas dot b s k y dot social or on LinkedIn if you search Ketchbrooke Analytics, k e t c h b r o o k, you can see what I'm up to.

[00:38:18] Eric Nantz:

Awesome stuff. And, yeah, I remember you gave a a recent plug to some great advancements of using AI in your shiny app development. That was a a really great great, teaser there. So, hopefully, you're getting a lot of success with that as well. I really thank you all for joining us, wherever you are listening for this episode 200 of Haruki highlights. Who knows if we're gonna get to 200 more, but nonetheless, we're gonna have fun along the ride one way or another. And, hopefully, we'll be back with another episode of our weekly highlights next week.

"
"19","issue_2025_w_12_highlights_638779498699649835",2025-03-19,39M 3S,"Thriving in a multi-lingual data science lifestyle while authoring your next Quarto project, putting LLMs to the scientific test with parsing manuscripts, and replicating a life-saving spatial visualization originally created over 170 years ago! Episode Links This week's curator: Ryo Nakagawara - @R_by_Ryo@mstdn.social (Mastodon) &…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode a 99 of the Our Weekly Highlights podcast. This is the weekly show where we talk about the excellent resources that are in our highlights section amongst much more in this week's our weekly issue. My name is Eric Nance, and I'm delighted you join us from wherever you are around the world.

[00:00:22] Mike Thomas:

And very happy to be joined as always by my awesome cohost who's been with me for many of those a hundred and nine ninety nine episodes, Mike Thomas. Mike, how are you doing today? Doing great, Eric. Yeah. It's it's pretty crazy to think that we're coming up on that number. I don't know exactly what my count is. I'll have to do a little web scraping or, leverage some APIs to be able to to run some deep wire and figure out how many of those I've contributed to. But it feels like a lot, and it's been a blast. It sure has been. Maybe we should throw the the RSS feed in some LOM or something, and it'll tell it for us. You know, that's all the rage these days, which

[00:00:56] Eric Nantz:

Wah wah wah. Well, you know, what can you do? And there will be a section where we touch on that a bit, but, nonetheless, we are happy to talk about this latest issue that has been curated by real Nakakura, another one of our OG curators on the our weekly team who has been very helpful for to get me on board for many, many years ago. Seems like many years ago. Maybe it hasn't been that long, but, man, time flies in open source, doesn't it? But as always, he had tremendous help from our foe, our working team members, and contributors like all of you around the world with your poll request and other suggestions.

And it is a lifestyle, Mike, that you and I live of every day. We're we're just talking about this on the preshow where we may be losing multiple languages often at once in our projects. It is the reality, whether you're a solo developer and you've seen that great utility and it just isn't in your maybe primary language of choice, but there's an open source equivalent in another language, and you wanna bring it all together. Certainly, that's been a key focus of many vendors, consultation companies, and enterprises in today's world to build tooling and capabilities that can have the ability to leverage multiple languages at once.

And there was an interesting chat with two very prominent roles or leaders in this space about their thoughts on interoperability across different languages and data signs. So our first highlight is a blog post that has been offered by Isabella Velasquez over at Posit, and it is summarizing a recent fireside chat that consisted of none other than Hadley Wickham, chief scientist at Posit, author of the tidy verse, and many other important contributions in the art community. Obviously, needs almost no introduction to our audience here. And he was joined by Wes McKinney, the, you know, the, obviously, the architect of pandas, and it was also been working on, with posit as well.

And they were recently, as I mentioned, part of a fireside chat that, was hosted by posit where they invited, a small gathering of fellow data scientists, much in the style of the what they've done basically every week called the data science hangout. It was kind of a mini hangout of sorts. The YouTube video is online. And if you wanna watch that or listen to that after you listen to this very show, we'll have a link in the show notes, of course, to that recording. But there was one aspect that Isabella touched on in her blog post that I think is very much a important reality, but an important capability that we are living every day that was brought up by one of the audience members in that discussion or that fireside chat is the use of, you know, multiple languages and being able to leverage the Quarto documentation publishing system that to help users look at different code snippets between how you might accomplish something in R and how you might accomplish something in Python.

We both we all know that both R and Python as well as Julia and observable are all languages that are well supported in quarto. And so this, this blog post is talking about a couple solutions that you might have as you're crafting these resources together and you're putting these multiple snippets in place of letting the user really see kind of in a group fashion, these different snippets in real time as they're navigating through your interface. And within Quarto, there is the functionality to do tab sets in your page, to help organize content. This could be a web page. This could be a standard HTML document. It could be a cordial slide deck. We reveal JS. The tab sets are usable almost everywhere they have interactive HTML.

And what a lot of people will do is they'll have, say, a tab maybe for code, another tab for output. And like I said, in this context, they might have different tabs for, like, how you do certain function or certain analysis in r. And then maybe for teaching purposes or, you know, getting members of your team up to speed, you might have the equivalent, snippet in Python. There are two ways to accomplish this. One is, built in to Quarto itself, and another is a really awesome extension that does a similar thing.

So the first way to accomplish this is that when you're defining these tab panels, there is a panel kind of div operator that you put in your Chordal document to set up this group. You give that that group a name. You can call it, like, language or whatnot. In fact, that that could be any naming you want. But then within the subheadings of this tab set, you put the r code with an r heading and then say a Python block with a Python heading. And then you make sure that these are, again, tagged appropriately with the group amongst multiple tabs that so that they have the same group name. And then when you click, like, the r tab in one tab set, the other tab set will react accordingly.

Same with clicking the Python tab. And there are examples of this in action in the blog post, so you're invited to check that out after the, after listening to this. But if you go through that that example, there's, like, an r section where it's just doing a simple scatterplot and then a Python section doing a similar thing with both a scatter plot and, like, a data table. So that's one way to do it built in. But I mentioned there was a great extension to do this as well, and it's called Tabby. This is authored by one of our favorites to follow in the quartal space and amongst all data science, James Balutoma, who goes by the cultless professor, always one of my favorite handles.

And this extension, Mike, I think is just up my alley for some recent projects. Why don't you walk us through this? Yeah. Absolutely. It's an extension that

[00:07:12] Mike Thomas:

accomplishes much of, you know, what the first half of the blog post, talks about. But the first half of the blog post is a little bit more manual in that, you would need to sort of specify the tab sets explicitly as opposed to using this Tabby extension. You can have all of your different chunks, which I think could be dynamic. So, say, under one tab set, for example, you have r, Python, and Julia. And then under another tab set, you have, you know, r and Julia. And you don't actually need to change sort of the wrapper, the the quarto div, if you will, at all. You can just use this dot tabbing, call and then group equals language, and it will programmatically build out the tab set for you as I understand it, for one tab for each of the, chunks that you have underneath it based upon the particular language for each of those chunks. So I think it's a really nifty, smooth way to be able to go about, accomplishing, you know, what the first half of the blog post does a little bit more manually. If you want to install this Tabby extension, just open up a terminal, wherever you have quarto installed and run quarto add, colist dash quarto backslash tabby.

It's gonna install this extension under an underscore extension subdirectory, that you might wanna take a look at if you're using version control. And the setup and ability to get going is is really quick. They have some basic examples with Python, JavaScript, and R, all underneath this dot tabby div, if you will, in quarto. And it generates this beautiful three tabbed tabset with Python, JavaScript, and R code in it. There's a great I guess we'd call it a package down site. I'm not sure if it is a package down site because this isn't an R package. Right? It's a quarto extension.

But there's a great a great site that, that we have put together here for this extension. It's quarto.thecultlistprofessor.combackslashtabby. So maybe that makes me think that, is it James? Has even more, more quarto extensions coming our way. We'll have to see. But, great great blog post here, great extension, super useful. One that I have read about before, but admittedly have not tried and put into my workflow yet, and I need to because we have a billion use cases for this.

[00:09:40] Eric Nantz:

In fact, I have a use case I'll I'll share right now as, Mike knows from our preshow banter that I've been on a AWS, journey, if you will, with leveraging, how we deploy Shiny apps and how we deploy maybe custom APIs. Now most of you know, I'm I'm an R user. I I develop most of my everything in R. And while there are a lot of advancements in the R ecosystem respect to interacting with cloud, you know, mechanisms or cloud providers like AWS, like Azure, and whatnot. When you start to get in the weeds of, like, certain bits of these services and let's say for AWS, you're you know, it was object stores. There's, like, the secrets manager. There's I'm roles and all that jazz. Not important for what I'm talking about here. What is important is as you're thinking about, okay, what are the best ways I can call these APIs from our and you start to search and you get some hits here and there.

But then most of the time when you search for this, there are other languages are coming up at the top of this, especially in this case, Python with the Boto three library. So what I want to do with this kind of paradigm, especially with this Tabby extension, is I'm I right now, I'm writing notes for mainly myself as I'm navigating through all this, but I wanna empower other statisticians and data scientists in my team to be able to deploy these resources too. So I'm not, you know, what I like to call the bus factor for a lot of this. So I'm writing kind of this document, and it probably will be a portal site when I'm finished with it. And I wanna put in the equivalent of, like, the r snippet to tie into these APIs and the Python snippet with this boto three library.

Because a lot of times when IT asks, you know, oh, you're having trouble authenticating that, what are you using? Oh, you're not using Boto three? Like, oops. Well, I I do on the side to do my testing to make sure it's not me messing up the r side of things to make sure that the Python side is working first. So in this documentation, I can do tab sets. So when the user says, okay, here's the r way of connecting to this API or deploying this thing. And then they hit the Python tab, and they'll get the same thing in Python. That's where the majority of our internal documentation from the IT groups have written as is like, we assume we're using Boto three, so here you go. Well, I'm like the the first r user in this journey. So I'm thinking this might be a useful technique to follow, and and what a great way to put this all together with this Tabby extension in Corto. I'm I'm sold.



[00:12:14] Mike Thomas:

Definitely. And if you envision that the audience is either gonna be a Python person or an R person, for example. You can group all of these tab sets together with the Tabby extension so that if someone switches from R to Python in one tab set, all of the tab sets, that you've decided to should be grouped together will switch from r to Python as well.

[00:12:38] Eric Nantz:

That's a huge user experience enhancement and one that, like I said, this is gonna be a huge asset to me personally as I try to document this journey and notes in real time so that, a, I don't forget in a year from now, and b, I can empower other developers to join me on this. But we teased at the outset, everybody that, you know, it is in today's day and age. We usually have something to say about large language models on this podcast these days, but we're trying. One nice thing about the r weekly highlights is we definitely cut through the fluff and, the noise that can be out there. And we try to showcase real novel uses of the technology. And like we said the other week, Mike and I are always learning something new, and it looks like this blog post is gonna give us just that. We have a recent use case with the recently highlighted Elmer package, but this one is titled the Elmer package for using large language models and r is a game changer for scientists.

So this is very much geared towards the research side of things. But this is being authored this blog post has been authored by the seascapes models group, which I think we featured back in episode a 96. I believe the lead of this group is Chris Brown. But we don't exactly know who authored a blog post. So we're just gonna say it's from the seascapes team. And after the introduction where the author talks about, you know, why is the Elmer package in particular a game changer for scientists, well, a lot of the things that we talk about here as we're learning about this, Elmer is really helping you automate a lot of this setup to many of the l o m providers out there, as well as the capabilities of tool calling and some other nice enhancements. So I think we're gonna be getting to later on in this post, especially when dealing with textual data.

So the first part of the post, we'll we'll breeze through this, but Elmer also has great documentation on how you set up your authentication to these services. I believe in this example, they're using the anthropic API from Claude, which is something that Mike and I are using routinely in our projects now. So we're starting to get familiar with that. And then it walks through just the basic ways of setting up the chat object in Elmer based on your system prompt, the model. You can also specify the max number of tokens.

That can be quite important if you're working on costs too, to make sure you're not overcharging there. But there are some interesting nuggets as we get to the use case that this blog post is highlighting here. And the use case that they're talking about, A lot of times the data or the insights that you wanna summarize are contained in PDF format. If they're from a research paper, maybe some internal documentation that was written years ago, It's in some sort of PDF. So Elmer out of the box, and I believe this is new to the CRAN release that we touched on last week or a couple weeks ago.

There are functions that can aid with the processing of text from PDFs. Though, the author's first attempt at this is they have a manuscript on turtle fishing, which is interesting read. You can check that out in your leisure. They first tried to import this, paper online dynamically for the web link using the content PDF URL function that Elmer exposes. Well, that didn't work. There was a four zero three. That's a typical HTML errors means that you're unauthorized or there's a server error around there.

So he, the author speculates that this may have been a bot that's trying to prohibit, you know, some rogue processes from scraping web content. So the workaround is that you download the PDF locally on your system. Then they ran the same function, but now it's content PDF file. So that browses to the local copy of this, turtle manuscript, and that actually works. So you get an object. They call it my PDF, and that will be used later on in all this. But after setting up the clawed chat object and, again, using the the familiar system prompt, the specific model, and the max tokens, this is interesting.

Elmer has some built in functions to help you with conversion of different object types. And in this case, there is a type object function, which I have not used yet. But within this, you can have some arbitrary definitions of the information you're trying to look for in this paper here. In this case, they have some for sample size of the study and the manuscript, the year of the study and the method. And within these type objects, you can have ancillary functions called, like, type underscore number, type underscore string to kind of give it what it should get when it's scraping this PDF after running the model.

So if all that's set up, then the author calls all of this and the chat object with the extract data method. Again, feeding in that PDF object that they got from extracting from the file. And then the type is using this type object that was defined. They call it paper stats. So when you get that back, you start to see then in this object, the different slots for those three kind of data types that was looking for the sample size, the year of the study, and then the method, which was another string of like, what was the statistical method and a paragraph or less, and it works, but by cautions everywhere. Right? You do have to be cautious about, you know, what results you're getting back and the type of framing you're putting in with this.

And this, it looked like the sample size was the correct amount. But, again, you have to be cautious. And if you wanna run this in a batch setting, that's the next part of the post. What if it's not just one paper? It's like a hundred or so or such of these. So you could write a wrapper function like they do here to kinda automate this process of grabbing the text, extracting the data with that custom type object, you know, definition, and returning the result. Now the other risks that they talk about is that if the chat, you know, object or the chat service can't find the answer, it might try to make one up called hallucination and, and the common lingo.

There are some safeguards you can put in your Elmer calls to try to suppress that in these type, type function calls where there's a parameter called required. If you set that to false, typically speaking, if it doesn't find that particular set of data with that type you define, it should just leave it as a null. Not always though. They caution that they still might try to do something. So you've gotta, again, look at the results, in their experience when they set that required false, it was still hallucinating on some of these answers from these different types.

So again, your mileage may vary, but what I'm learning here is that a, this this type, object definition is hugely important when you're in this context of scraping information from these PDFs or other type of structured text. And so in some of the reflections that wrap up the post here, some things you wanna think about is, you know, these things aren't free. Right? I was just telling Mike, I just pony it up for paying for the professional cloud account. So I don't wanna burn through my usage in one month because I'm doing a bunch of repeated calls as I'm prototyping things. So you may want to, you know, keep your testing at a little minimal as you're iterating through this. Maybe set the token limit to less or whatnot.

And then dealing with structured text or unstructured text can be, can be problematic, especially in PDFs. That's why, if you have a resource in HTML, oh, you're golden, right? Because that's structured in a markup language, easy to scrape, but we don't always have that luxury. But if you have it, they always recommend grabbing HTML first. And like everything in these in these pipelines, the prompt is a huge part of this. Probably takes a few iterations to get this right. But if you make it as domain specific as possible, I think, and also maybe repeating it a couple times with some experiments, you can kinda see the variation that occurs wherever you do the same prompt or maybe tweak one or two sentences in that prompt. I think that's also a good practice to have.

But the potential is here, folks. The potential is here if you got this data trapped in these PDFs or other textual documents, looks like Elmer is trying to help you out to grab this data out, help you define how you wanna extract the different pieces out with these type objects. And lo and behold, you might have a great way to at least reduce what half your effort, maybe 80% of your effort of trying to do this all yourself. I still remember at the day job, there was a vendor we paid a lot of money for to do this quote unquote curation of these PDFs all manually. It would take them months upon months upon months to finish it. And even then, we get this big old data set out in Excel format. We don't know how heads or tails ought to make sense of it. So will this replace everything? No.

But I think this is a huge win for research out there, especially for for this type of information that we're seeing in this, highlight here. So learn something new with these, with the Elmer package as always, Mike. What do you think about all this?

[00:23:02] Mike Thomas:

Yeah. Absolutely. It's pretty crazy. And I think that this blog post and some of these highlights from the Elmer package, actually taught me some things about, what's going on in the, I guess, LLM ecosystem and some things that I hadn't realized yet. I think one thing that we touched on is there is a function in here that's called, what is it? Content PDF file or content PDF URL. And I was thinking initially that that, you know, maybe extracted the text from the PDF, stored it in a vector database, and this was sort of a rag type of a thing. But it sounds like from taking a look at some of the Elmer documentation is that it's actually sending the PDF file to the LLM service itself and then letting the LLM service, do that. And it's it seems like now some of these APIs, at least, and probably the front end interfaces and and, Claude and ChatGPT and and all of them actually allow for a PDF file as input and not just, you know, your your prompt text, if you will. So I think that's what's going on here. Obviously, if you have, you know, sensitive information in a a PDF at your company, you make sure that you wanna check with somebody before you send that to, one of these third party services because you never know exactly what they're going to do with it.

So, and then the this whole type object idea is really interesting too. And I I got to imagine that it's not it's still sort of imperfect a little bit, but it's nice that some of these LLM services are starting to provide some of these, you know, maybe guardrails or or whatever you wanna call them to better, you know, hopefully structure the result that you're going to get. So this this type object, and type number, type string, all of these different object formats that we have the ability to specify within Elmer, it looks like, you know, these are fairly recent updates as well to these LLM services out there that allow us to do that.

Yeah, we have plenty of use cases where we want to do this exact thing. Right? You have some sort of large PDF, and you have a couple data points that your analysis team is is looking to get out of those PDFs. And instead of having them control f through the whole document, it would be great to just throw it at an LLM and get those get get those answers right away, and stick it in an Excel workbook for them to analyze. But, you know, I think you still sort of run the risk depending on how, you know, high risk, of a situation, you know, how important the model's accuracy is of getting the wrong answer. Right? And you still need to sort of spot check, all of those answers if it's a situation where, you know, the results of this LLM are going to be used for downstream decision making. But again, it seems like we're getting some improvements here to make things a little bit more accurate, you know, provide a little bit more guardrails to try to, you know, better, ensure that what we want is what we're getting out of these models. So that's that's really, really interesting to me.

You know, pretty exciting stuff from what I can tell. And, again, sort of that object, this object paradigm is is a recent LLM feature according to, the vignette on structured data in the Elmer package down site is is that this structured data, a k a structured output, is is this recent LLM feature that we have the ability to leverage. And and, again, I'm not sure if this, you know, goes for all of the LLMs out there or only, you know, some of them. But it definitely is interesting because I think if it's not implemented in all of them, it it will be soon. And these new features definitely help us as as data scientists be able to ideally, you know, better serve our end users.



[00:27:01] Eric Nantz:

And I can see this also combine you really nicely with what I mentioned earlier, the fact that Elmer and is is a front end to using this tool calling paradigm, where we can let the r process in your local system help the the chat model get to an answer that may be more dynamically dependent on either, you know, say, the current time or the current situation or current data that you don't want exposed to the LOM, but you want it available to it, in a local sense. And I think imagine having a set of PDFs or other structure or unstructured text that is giving additional context to a data analysis that you're doing or or a summarization of a research process. And you've got some, you know, data already in house that has some of the information, but you wanna help it out with some of this other information, you combine this ability to getting the the PDF text out with some of the recent advancements that Elmer's bringing with tool calling. And then I also plug, I believe, just this week, I saw a post on deposit blog yesterday, sister package, the Elmer called Gander by Simon Couch just hit Cran as well, which will let Elmer be aware of the our environment of the objects that you have loaded in your session.

So I see lots of interesting ways this could all be melded together to help give additional power to the bots, if you will, the chat bots to get these insights more quickly. And hopefully, if you're sifting through, like, hundreds of these documents or hundreds of these documentation pages, it may be more of a screening thing where you're just like, okay. I only need a certain set of this for this decision. When you get to that set that you narrow down from this big fishing net over to, like, this maybe five or 10 set of key inputs, that's what you really wanna validate yourself on is just getting to that to that final answer. But like you said, Mike, everything you said I agree with. This I couldn't aid in decision making. You just gotta be cautious about it.

But it's just interesting to see how you can blend all this together, which I'm still

[00:29:15] Mike Thomas:

trying to get my wrap my head around, but I think it's a really promising start here. I I agree. Yeah. We have a pretty big project going on. Right now, we're we're doing something similar. We get these 200, three hundred page PDFs that we're trying to get 20 data points out of. And it's a it's a situation where accuracy is really important. So we're actually building a Shiny app front end, and our AI engineer has has leveraged some of these open weights models and and put together some pretty cool, you know, system prompts as well as, I think, just sort of the regular prompts that you you throw out these LLMs.

And what we're doing is we're returning all of the instances where we found in the PDF, snippets of text that could match the particular answer that the user is looking for, and then giving them radio buttons in the Shiny app to pick which one they think is the correct one, or manually override it in a a text input. So we're pretty excited about it. That's our sort of way to avoid, you know, all of the risk that comes with, you know, automating that process fully.

[00:30:19] Eric Nantz:

Yeah. I'm super excited to see where that effort goes. And and as always, I'm I'm a novice in this space. I'm learning a bit too in my in my local prototyping, but, lots of lots of novel use cases here, and we got the tooling to make it happen. And our last highlight today is, you know, showcasing some of the pioneering efforts that occurred from a long ago time. We're talking about over a hundred years ago to be exact on some novel uses of of visualization to help with a very important health issue that occurred back in 1854, folks. Yeah.

I'm I'm maybe an old timer, and I'm not that old. I I kid. I kid. So this this highlight here has been authored is actually a package that's just been released called snow data. It is authored by Neema Minnag. I know I didn't get that right at all, but it's she's a postdoctoral researcher at Maynooth University, and this package is actually exposing some of the datasets that she curated as part of her exploration from earlier this year in January, to be exact, to understand with modern tooling, how one could recreate this eighteen fifty four cholera outbreak that was visualized this time in R.

But to give you a very quick overview of this, in 1854, a very influential, physician, John Snow, used a data driven approach to trace what was becoming the source of a very, harsh, a very devastating cholera outbreak in London, and he was able to trace it back to a contaminated water pipe. But what he did was to actually start mapping out where the outbreaks were occurring and then trying to trace that back to where these water pipes were actually located in the city using cartography and literally writing on notebooks. You know, there are scans of this online that we can link in the show notes, to basically help help the government figure out, hey, we found the source. We gotta stop this now to stop this outbreak for even further.

So we invite you to check out the blog post that, that has been authored here, as a great accompanying part of the package. It was a package to be transparent, just as the datasets and not much else. So you're kind of left at your devices to figure out how to put all this together, but the blog post literally walks through her effort to, at the time, build this all up herself, which now you can have in the package for these, cholera cases, dataset with the locations, the x and y coordinates, and the observation IDs along with, the water pump location information.

And in the blog post, she shows some clever use of the raster vis package to take a TIFF object that is, again, available online through these different, publicly available domains and render in your graphic device what definitely looks like the scanned copy of this map of these streets in London where these outbreaks are taking place. So it's a it's a cool way to have a vintage looking representation, but annotated on top of the plot are these dots showing the outbreaks. And then she also was able to change the size of the dots based on the prevalence of the outbreaks in that particular region.

And there's a lot more that's going on with respect to how the the streets angles were calculated to get the location a little more precise. Really novel stuff that I wouldn't know heads or tails of how to do myself. So if you see a situation where you've seen maybe some pioneering effort in spatial visualization, whether it's health related or otherwise, I think what what we see in this in this highlight here from from Niam is a great showcase of what you can do with our a little a little getting in the weeds on a bit, but the snow data combined with these techniques for mapping is a great way to recreate that vintage visualization that that came from John Snow back in the day.



[00:34:59] Mike Thomas:

Yeah. Absolutely, Eric. This story was, I think, one of the most famous early uses of geospatial data analysis to solve what was really a public health crisis at the time. This is a such a nice little package sort of surrounding that story. And I think the package could be a great utility for, like, an undergrad data science course, like, mini project. If I'm thinking back to when I first learned R and stats one zero one, two zero one, whatever it was, we did not do any sort of fun interesting projects like this that were tied to. It was like our norm. Let's take a look at the normal distribution. You're soon straight, parameters. Yep. Change the parameters and see what happens. I feel like you could use this as, like, a real world use case that happened with this cholera outbreak, and and try to follow John Snow's steps, and play a little detective, and find the location of the source of the outbreak. I think that would be fun for students to get their hands on a little r programming, a little geospatial, data analysis, data vis type stuff with, you know, this these, I think they're terra datasets, or something like that. So, I think, yeah, a really interesting little package, a really nice, use case to sort of tell this this really awesome story of Jon Snow saving the day.



[00:36:16] Eric Nantz:

Yeah. Excellent. Yeah. Like I said, pioneering work and a major health crisis, and there are so many of these out in history. And if we can get our hands on these original data sources, yeah, we should be able to digitize this and be able to recreate these these novel visualizations. And I admit having a project like this sure would be, you know, the infamous you've got red and balls in an urn, figure out probabilities. Oh, this this one I could sink my teeth into even as a as a visualization novice with spatial visualization.

But there's a lot more you can sink your teeth into with this issue of our weekly. We got, of course, the link to the full issue itself. It's got its usual gamut of new packages, great tutorials, blog posts. We're running a bit short on time, so we won't do our additional fines. But, again, we have everything linked, in the show notes if you wanna check out the full issue. Rio, as always, does a fantastic job here. And, you know, it's all fantastic as well because of you and the community. So if you wanna help out the project, the best way to do that, if you find that great blog post, that great tutorial, is to send us a poll request with that link right at rweekly.0rg.

There's a link in the upper right corner, get you to the GitHub template for the poll request. Little easy to use, template there to follow and all marked down all the time. We'd love to contribute it to the next issue, whatever you find. And, also, we love hearing from you. And as as you saw on the episode number, next week's a big one, episode 200 folks. That's hard to believe we're getting there, but we are getting there. So if you have a favorite memory you wanna share with us, we'd love to read it on the show. You can get in touch with us multiple ways.

We have a contact page in the episode show notes. It's right there for you to send a little web form for that. You can also get in touch with us on social media as well. I am on blue sky these days with @rpodcastatbsky.social. Also, I'm Mastodon, where I'm at rpodcast@podcastindex.social. And I'm on LinkedIn. Just search my name, and you'll find me there. And, Mike, where can listeners find you?

[00:38:22] Mike Thomas:

You can find me, blue sky at mike dash thomas dot b s k y dot social, or you can find me on LinkedIn. If you search Catchbrook Analytics, k e t c h b r o o k, you can see what I'm up to lately.

[00:38:36] Eric Nantz:

Very good. And, yeah, I got a preview of what you're up to, and I am intrigued to say the least. So we hope to hear more about that soon. But like I said, next week's a big one, episode 200, and we'll see what actually happens there. But we're hoping you can join us for that next week as well. So until then, we'll close-up shop for episode a 99 of our weekly highlights, and we will indeed be back with episode 200 next week."
"20","issue_2025_w_11_highlights_638773461766959388",2025-03-12,41M 48S,"Giving your package documentation site a little personality and much more with {pkgdown} customization, plus a novel new package to bring the power of LaTeX to all of your plots! Episode Links This week's curator: Batool Almarzouq - @batoolmm.bsky.social (Bluesky) & @batool664 (X/Twitter) Customize your expedition: Create a unique documentation…","[00:00:03] Eric Nantz:

Hello, friends. We're back of episode 98 of the Our Wicked Highlights podcast. Boy, that magic number is getting closer, isn't it? But, nonetheless, this is the weekly show where we talk about the awesome highlights and other resources that are shared every single week at rweekly.0rg. My name is Eric Nance, and I'm delighted to join us from wherever you are around the world. Yes. I am very thankful. We're already in March, so I feel like my mood's lifting up. The sun's coming out more. The time zone chain or the time change a little bit in my neck of the woods, so it's lighter later in the day. Although it's darker in the morning, so give can't have everything at once. But I'm happy, and all things considered. And I'm also happy because returning to the show is my awesome co host, Mike Thomas.



[00:00:49] Mike Thomas:

Mike, we're so glad to have you back. I barely, flew the ship solo last time, so thank you for being here. Well, that's not true at all. I listened to the the great solo episode you recorded, and I appreciate you you doing that and carrying the water for us, for that week. Well, I was out for a couple weeks, unfortunately, but very happy to be back. Thank you to the OurWeekly

[00:01:11] Eric Nantz:

folks for starting me off on a lighter issue this week to ease me back into the OurWeekly groove. Yep. Sometimes we need that little bit of easing into it. But, yeah, let's dive right into it. And our curator this week is Matol Amrazak. And as always, she had tremendous help from our fellow, our weekly team members, and contributors like all of you around the world with your poll requests and other great suggestions. And we're gonna give a little personality to a great topic that, you know, has been one of my, big focuses in the last couple years of a lot of internal projects, but that, of course, is writing good and concise documentation for your packages.

And what better way to surface that information to your end users, whether you're doing something internally or you're releasing this to the the open source world, if you will, is to create a website for your package in r. And there is a package that's been kind of the front runner in this space for quite a few years now, and we are talking about the package down package or p k g d o w n. I always wonder how people say these things, but I'm gonna say package down and run with it. But I've been using it for years, and out of the box, you get a a nice utility type of experience, but there's a lot more you can do with it, folks. And that's where our first highlight comes in and is being authored by Muriel Del Mote. Hopefully, I've said that correctly.

They're from ThinkR. So this is from the ThinkR blog, which is titled customize your expedition, creating unique documentation for your r package. So we first start off in this post with how straightforward it is to get one of these up and running if you leverage one of my favorite utility packages for development in the r space is the use this package. It seems like there's almost nothing use this can't do other than literally just, you know, fix all my errors at once, but maybe that will come later on. But you can bootstrap a, package down site with use this colon colon use underscore package down.

When you have your package already configured with your r functions and their function documentation, This is gonna give you that basic structure right off the bat, and you can literally build your package site interactively right then and there with package down build underscore site. And there you go. You now have if you run a render that in a preview in your editor of choice, you've got a, very, again, utilitarian looking package site right off the bat, which will give you an index of your functions and the reference tab.

And if you had any vignettes that you wrote as part of your package, they will also be included in the article section of the package down site. And last but certainly not least right off the bat, if you are using a news file to keep track of your package updates and changes, that will be in the change log section of your package down site. I visit that section every single time I'm using a package because there's a lot of things the fast moving, packages I'm leveraging lately, and I always like to see just what happened in that update compared to our previous version, especially if you're dealing with, you know, more, how should I say it, like older r versions with older package repositories.

And then you start to leverage, like, a development project with RM, then you realize you've updated a new version of that package. So what changed between those two versions? You know, the change log can be helpful in that approach. So, again, right off the bat, you got a package down site ready to go. But as this blog post is saying, there's a lot more you can do here. Like a lot of things that are fundamentally based of our markdown, which package down is, I'm sure, very much inspired by, A lot of the configuration in your site is driven by a YAML file. The package down dot YAML file to be exact.

I swear, I now understand why some people keep saying YAML is almost like a programming language in and of itself because there's so many things that we can figure with it. Right? So we're gonna walk through, in the case of package down, some of the options that you have here, starting which is the navigation bar. I told you what happens by default, but you could organize this any way you see fit with the structure, element. You can determine what goes on the left, what goes on the right, let's say those icons to say the GitHub repository of your package. You can put any other icon or reference that you want in there. There's a lot more you can do with that.

And then to really look at how those navigation elements are constructed, you can have components within each of them. So in the example we see here, they've added a new section called expeditions. And within that expedition section, it's got its own submenu of different elements. This may look familiar because if you've been using portal lately, you can kinda take a similar approach when you make a website with portal and how you construct the navigation bar there as well. So there is interesting some synergies here. It's not always synergistic, but there's some interesting interesting things you can take from constructing, say, a portal site and whatnot. So there's a menu section where they have the hyperlinks and then the text for that hyperlink.

Again, once you grok it and look at it for a little bit, makes makes a lot of sense. And you can also, again, make sure that if you want to space things out, which is a nice navigation feature, there's a little tip that I saw. You can have arbitrary text in these menus and just put a bunch of hyphens in it. Now it gives you a nice little separator between the the different groups if you wanna organize it in that fashion. So little things along the way that I learn every time I read these posts. The other meat of a package down site is typically the function documentation. This is where literally, every day I'm developing or, an internal package or I'm leveraging an internal pipeline that's using a bunch of packages.

I have on the right side of my screen the package downside of that other package, and I'm looking at the documentation right off the bat. Typically, in our studio, you could look at the documentation in the editor itself. Lately, I've been using Positron, and they don't have that nice little help browsing panel just yet. You have to do question mark name of the function, and then it appears. So sometimes I just like having the web browser on the right side with the package documentation right there. That way I can just kind of navigate back and forth. So back to where we're talking about here, in package down site, there is a reference section.

And this is where you can kinda give categories or groupings of your functions without necessarily having to do that in your package documentation itself in the package. Case in point, in the example here, they've got a reference section and with different titles for these groups in the title argument and then a description that gives a more plain text kind of one or two sentence description of these contents. And then inside the contents section are the names of the functions that go into that section.

This is really, really important, especially to me as you're developing a package that may have twenty, thirty, sometimes 40 functions, but there there are inherent groupings that you want to use it to quickly understand what certain maybe they're pinpointing an output type of function or a function that's processing data or whatnot. So a lot of the package downsides that I've seen lately are using this grouping feature really effectively. I'm really appreciative that this is easy to do now in package down to give that more enhanced user experience, browsing the functions in your package. So it's really getting the title, the description, and then the contents being the names of the functions going into that group.

And then once you rerender that, you would see then in your reference section, you've got really nice easily, you know, viewable groups for these functions. All of this at this point has been the structure of your site and literally the elements inside of it. We've left out one important detail that, Mike, you're gonna rectify now. Let's bling this up a little bit, shall we? Let's make it pretty for sure. And that's where,

[00:10:18] Mike Thomas:

you know, CSS and and HTML and and Bootstrap can come into play here. And the folks who have authored the package down package for us, again, we can configure all of these things in this really nice YAML document, underscore package down YAML file as you mentioned. And there's a section that you can add, called template. And underneath that section, there's there's two immediate, subparameters, which are bootstrap, which, specifies the version of bootstrap that you want to use. Typically, nowadays, I think five is the the latest and greatest and has been for some time. Who knows when bootstrap six will drop?

And then you can specify, some parameters as well. And, a lot of folks, in my experience, find the boots watch, project to be very handy because it is a set of I don't know how many there are today, Eric. Maybe 30 ish or so, I feel like. Yeah. I gotta say it's at least 20 of these boots watch themes that I've seen in the wild. Yeah. Yeah. Sort of pre specified, different themes in terms of, you know, what color is is the nav bar, the background, the foreground, font, styling, all sorts of things like that. So you can play around with the the 20 or 30 sort of preset options that they provide you, which can be a nice starting point. Maybe that that works perfectly. You find one that that looks exactly like you want.

Or if you wanna go even a little bit further, you can add an extra dot CSS file, in your packages package down directory, and that can help update the CSS properties of your package down site, which is really cool. You can set things like the the navbar colored, you know, navbar, drop down links, you know, the font weight, all sorts of different types of things, any sort of CSS element on your package down site that you might wanna customize, you can handle with this extra dot CSS file, and that allows you to leverage whatever sort of branding you want to incorporate or or anything like that that you think is going to make your package down site pop or fit into sort of the theme and styling that you had in mind for your package.

Package down, you know, it's awesome. It's something that we have in the R ecosystem that I think has completely exploded. I think we've talked about this before, but when I go to a to look up in our package that doesn't have a package down site, it's really sad, because I have to maybe go through, you know, one of the old PDFs, that, you know, is published out there when a package gets published on. Korean, and that's that's certainly not as fun to explore as, you know, an interactive HTML package down site that has the searchability and all those types of cool things. And and maybe one of these days, package down and and web r will collide, and we can run some of these functions, from a package that we're exploring in the browser at the same place where we're looking at the documentation. That is is one of my dreams. We'll see. On a sort of similar tangent, I've been using a lot of, polars from the Python ecosystem lately.

It's I'd call it maybe a dplyr equivalent with a lot of speed improvements, but in terms of syntax, it's it's fairly similar to what you might be familiar with in the tidy verse. And, has a a great site that looks kind of like a package down site, maybe even a little bit more robust because of everything that's going on in that package. And I noticed this week that there's a new little new little icon in the bottom right corner that's in an AI chatbot that allows you to to ask a question of the documentation.

And my hope is that it's it's maybe rag based or, you know, fully up to date with the latest developments in the polars, package so that we are not doing the thing where I go to chat g p t, which was trained on polar's six months ago. And I am writing legacy code sometimes as opposed to, you know, writing the correct code based upon where the the package and the project stands at this point in time. So maybe we'll see something like that come to, the package down ecosystem as well, but I thought that was an interesting and relevant anecdote.



[00:14:41] Eric Nantz:

Oh, yeah. That that is intriguing for sure. And, you know, our listeners might be out there wondering, well, how do I actually know if this package I'm looking at or I'm using in my r session has one of these package downsides? So there's a few ways you could probably get to that. One thing I typically check first is I'll search for the package name, and then typically, it will search be in your search result. But sometimes if it doesn't quite appear obvious, you go if it's on CRAN, you go to the CRAN entry, and then there will be a link in the URLs to that package downside if it is on CRAN. Also, if the package is on GitHub, usually, the package author will put the package downside as, like, the website associated with that repository.

Not all the time. Sometimes you have to do a little smoothing to get to it, but that that for discoverability, those are the things that I've done as well. But going back to the styling stuff, Mike, I wonder if our listeners may be wondering, well, in this blog post, they kinda knew to style these, like, dot navbar elements and the drop down menu elements. And for someone new to this world of styling, they may be wondering, how did they know to write that type of syntax? What would be your preferred method to discover those elements?



[00:16:02] Mike Thomas:

To discover CSS elements that we want to? Yeah. Yeah. Like, how would they know what to target when they wanna style the nav bar? How do you think they would get there? Yeah. It's a it's a good question. So, you know, the workflow that I typically have and it it takes some getting used to, and I imagine that this may be your workflow as well. But you could preview the site in your browser, and then you can, depending on sort of what operating system you're on, you should be able to right click in your browser and and find a, navigation to something called developer tools or inspect this site.

And that typically brings up this right hand sidebar that will show you all the HTML and CSS properties of the the site, that you're looking at. And if you sort of hover over, you know, some of the the different elements, in that sidebar, it should show you on the left hand side of your screen, sort of highlight, the related area on the UI that that CSS element relates to. And you should be able to sort of expand and collapse, these different CSS properties to be able to take a look at, the components of them, the parameters of them that you can configure.

And once you are able to do that, you can get in a little bit of a workflow of, understanding which element you need to target and then naming that correctly in your custom CSS file and overriding, you know, or updating the particular property that you'd like to change.

[00:17:33] Eric Nantz:

Awesome tip there. And I I wanted to make sure that, you know, when I was getting into a lot of the shiny, my customization stuff many years ago, and I would see these fantastic looking apps, and this is well before b s web came in the picture. It just seems such magic to me. How did they do that? How did they know what to get to? And since then and since reading some great resources like, you know, the outstanding user interface that was Shiny by David Grange and others, there there was always one common theme that in in web resources, wherever it's shiny, wherever it's packaged down or just quarto in general or anything that you build of HTML, CSS is behind it one way or another. And one of the best ways to get there is to get into the system, as I say, with the dev tools or the dev console.

And once you get the hang of it, it does take a little gain used to buy. I just wanted to reassure our listeners out there that may be new to this that it may seem really cryptic at first. No. Just start simple. Just style one thing. Try it out. The best thing about those dev consoles is that you can do it all this interactively, it's not gonna break your actual code. Right? It's just literally in that preview. There's no there's no real cost. It's like a sandbox to try things out. It always seemed like, oh my gosh. I'm changing it here. How do I know what I do over there? So it it just takes some getting used to it. But I thought it was a nice touch in this post because I don't often see this with package down sites, this level of detail. So really great post here to to walk us through it.



[00:19:12] Mike Thomas:

Absolutely. No. That's a good that's a good tip. Definitely, don't be scared that anything that you change in your developer tools is going to be stuck forever.

[00:19:35] Eric Nantz:

So we were just talking about package documentation and a lot of times, especially those packages that are offering a statistical methodology, maybe a new modeling, you know, algorithm, you know, under the hood. Just like anything in stats, there's a lot of math behind it. And a lot of times, especially when you look at things like a package of vignette or research paper associated with it, You've got yourself some mathematical notation and that is usually written with LaTeX. Yes. I had to write my dissertation with LaTeX.

It was not fun. I have to be honest with it. I knew enough to be dangerous, but not much after that. And I've been away from writing LaTex for quite a while until literally about a few weeks ago when I was telling Mike in the preshow, I'm working on a shiny app that's, you know, surfacing some Bayesian methodologies under the hood. And I am a Bayesian novice. I know very just the very high level details of how things work. We were trying to solve a thorny issue with one of our distributions not getting the right, you know, probability of success where I was getting nonsensical results and the probability is going up out of the range of zero and one.

And I look at the code, and I'm realizing I really don't have an idea of how some of this works. I've gotta get I gotta write this out to see where the problem is. So I wrote LaTeX in a portal doc, and that brought back some memories, folks. And LaTeX, whether you like it or perhaps even don't like it, is the tried and true standard for mathematical notation. And I'm talking about right now, I've been talking about in the context of documentation, but there are a lot of times, especially in statistical teaching, whether it's in grad school or even with con maybe an application or a web page that is surfacing a visualization of, say, a model type.

You wanna actually inject, if you can, that mathematical notation of that distribution or that probability function into that visualization to help the reader, you know, maybe learn along the way. If you've ever wanted to do that in R itself, there may have been some approaches in the past, but this next highlight here has a pretty fascinating approach to make all this happen. And this is called the x d v I r package. Maybe it's pronounced x divir? No idea. But it is authored by Paul Merle, who I believe is a member of the r core team and has been one of the front runners, the trendsetters of leveraging the grid graphics system in r ever since its inception. So talk about, you know, the tried and true and and high quality package that that this is coming into play here.

And so we're gonna talk a little bit about what this package actually does, and we'll have a link to the vignette we're about to narrate here in the show notes if you wanna follow along afterwards. But the main purpose of x divr is to let you put latex snippets in your r plots. Notice I said plots. I didn't say specifically which type of plot because x divver actually supports all of them. That's an achievement in and of itself. So under the hood, what's really happening here? Well, LaTeX, as some of you may know, is a type of markup language for, again, mathematical notation and very precise way of of of defining all that.

So in order to put these equations on, you need three things to make this happen or this notation onto a plot. You need the language itself, LaTeX. You need the typesetter, which is gonna help determine how this LaTeX notation is gonna be translated into the the glyphs and the fonts that go into these plots and then to actually render it to the location that the user desires to put this into the plot itself. So the x divr package is automating a lot of that manual step in those three different stages. So first, you define a character vector.

You can call it anything you want, but inside that vector, you've got yourself the LaTeX notation right there. So once you define that and then you can, you know, probably do what I did. I had to look up a bunch of LaTeX cheat sheets a couple years ago to figure out how do I do the integral? How do I do the the sigma summation? It was, flashbacks. Flashbacks. Lots of flashbacks to that. So in this, example of the vignette here, he's got the notation for the standard. Looks like the normal distribution of my, spy correctly.

And then once you have that ready to go, you could then choose from whatever plot method you're using how, you know, with how to put it in. And the package comes with a function in the case of lattice graphics, a grid dot latex function where you give it that latex code and then the x and y coordinates of where you're putting it. And this is also leveraging some additional packages such as, of course, the grid package itself to help you with that positioning. And then once you have that, as you see here, there's a there's a plot in the in the vignette of the standard normal distribution with the CDF, I believe, or the PDF of the of the function in the upper left corner, and it looks really sharp. We're very legible.

Not like it's, a fuzzy copy pasted thing that you would do manually. It looks like the grid system should. And you can do this, of course, with other types of plots, such as a traditional plot. You can do this also with the grid dot latex function. If you just did a standard plot of, I say, y and x on the y and x axis, you can use the grid dot latex function. Again, very similar paradigm as we just saw with lattice. And last but not least, you can also do this with ggplot too. Many of our listeners love using ggplot too for their visualization.

So the x divr package comes with additional functions where you can put in a LaTeX GROB, into this, ggplot object. And, again, the parameters are the same, the LaTeX code, x and y coordinates, and the justification. Lots of interesting ways you can do this. There are even other helpers for the g g plot two users out there, such as element underscore latech, where if you wanna actually put this as labels such as in your title or whatnot, you can throw it in there too along with and, you can also do it for styling symbols in your plot such as the points on a scatter plot as an example of that as well. Putting, like, the notation for the sample average with subscripts on selected points.

I mean, that's awesome. It works. I get it works, and it looks like it's a pretty easy way to opt right into it. So there are some things you wanna know as if you wanna embark on this with your your next visualizations. We were just talking about the high level, but in order to pull this off, the machinery that need is needed for it is, of course, a Waytech distribution on your system. A lot of people like to use, the tiny tech distribution that Ewasea authored many years ago, as a very small fit for purpose, version of this, but you could use other ones like x e text. I'm not sure how to say it or Lua text. There's a bunch of other ones you could use with it too.

And then once you have that distribution, that might be the only real prerequisite you need. But knowing what's happening under the hood, the the link that we have in the in the show notes is the more verbose, manuscript that Paul has authored about these different stages of the processing going from the authoring step where it talks about what does it surround the LaTeX code with to make it a standalone LaTeX document that is ready for further processing, how it's converted into the DVI intermediate object that latex often does in that in between a going from the raw latex code to your output object, which may be a PDF or in this case, a snippet that will go into a grid or a graphic later on, how it's dealing with typesetting, how you might debug this when things don't work because oh, no. We did dive a lot of late nights trying to debug Latex when I was compiling that dissertation before the deadline, figuring out why the heck you know what? It's just like a shiny. Missed a bracket.

Missing brackets in latex is not fun because it's not as friendly as saying you miss a bracket. The the error messages are quite cryptic. So there's some narrative about that as well along with the different customizations that you can do with rendering, the the the the resolution and whatnot, doing multiple pages. There's there's a lot under the hood here. So, again, I only know know enough about latex can be dangerous, but next time I do in a static graph that has a statistical concept that's surfacing, I'm gonna give x divver a try and see see what I can do.



[00:29:21] Mike Thomas:

Absolutely. And I feel like these are the packages that don't get as much credit as they probably deserve. And it sounds like Paul has Paul Mural has really broken a ton of ground in this space over the years. So I think this is an appropriate time to give Paul a a huge shout out and a huge thank you for all the work that he's done in this space to get those of us who take it for granted, to get our plots to render the way that we want them to render. Right? Which is not a trivial task, the more and more that I read up about it. And, you're you're exactly right, Eric. You know, Paul talks about some of the related packages that are out there because we do have a lot of choice now in the open source software ecosystem. And that, X Divr, you know, provides a lot of similar functionality to grid text, g g text, the Marquee package, which I think is one of the more recent ones by Thomas Lynn Peterson, and some other folks.

But sort of the main difference between X Divor and those packages is that, the others are are built on markdown, while X Divor is built on LaTex, and in Paul's words, with all of the joy and pain that that brings. Definitely my favorite, yeah, excerpt from this this vignette. And it couldn't be more true because I think LaTeX allows us to do some of the customization that we we sometimes really need, and and have that ability to really, for the most part, in my experience, you know, fully customize whatever we want to show on screen in our PDFs, wah wah wah, as opposed to, other methodologies. But you're exactly right. It can be cryptic in terms of the error messages. The syntax is, I I think, objectively not the prettiest to look at.

And, you know, it's just one of those things. And I I really did also appreciate, as as you mentioned, the final section of the vignette around troubleshooting, where it's a little bit of a a deep dive into the weeds on how Xdivo relies on on the glyph rendering system that was added to our version 4 dot 3.o. So fairly recent developments, it it seems like made this possible, and a little bit of a discussion about the particular graphics devices that you will have to have available and set and ensure that you have a tech installation. Right? And a lot of us use tiny tech, for that purpose. That's one. I would recommend, I think that's an e way project, that is another unsung hero. Even if it is sung, it's not sung enough, in my opinion. Well, most said. Yeah. This this is a really good this vignette provides a really great walk through of the the more detailed, highlight. So feel free to check out both.



[00:32:05] Eric Nantz:

Yeah. And like I said, I've been in the wheeze a bit with this other app of surfacing this information, and we do have to, at some point, build in a feature to get more of the static outputs out of the app for, you know, further, you know, discussions of a team or the file as a record for, you know you know, prosperity, if you will. So I may end up actually using this in that app and when we do, like, a GGPOT two version of that distribution type. Because when we're mapping out these prior distributions that we're letting the user customize in this app. We indeed do have a section in the app that literally plots out that distribution function for the prior such as a normal distribution or other other ones for hazards and whatnot. So, yeah, I'm gonna I'm gonna give this a play and see see what happens.

And there's a lot more to play with in this issue as well. We got a lot of great, great additional resources that Batool has put together for us in this issue. So it'll take a couple minutes for our additional finds here. I'm gonna get into the architecture weeds a little bit on this, but it's a great achievement nonetheless. A a recent blog post from Dirk Edebutel has announced that the r two u repository now serves Arm 64 versions of our packages. Woo hoo. So you may be wondering why is this a big deal?

Well, as he points out in his blog, a lot of the major cloud infrastructure is starting to move their servers to arm to save on cost, save on power usage, and in some cases, especially in the Apple space, a lot of the newer our pet laptops are powered by arm processors, such as the m one chips and and the like. ARM hasn't been like a new thing for the our ecosystem, but yet this is a very new thing for the Linux users out there that are leveraging ARM processors, whether it's through their systems or through containers, and also through GitHub actions. That's how all this kinda came to be is, more recently, Microsoft, who, of course, owns GitHub, has now let users target a version of Ubuntu with ARM architecture and not just a typical x eighty six sixty four architecture that most Intel and AMD processors are using.

So he does mention that there is a lot of support right off the bat. I think more than three quarters of the packages are in binary format right off the bat because they didn't take compiled code. The ones that are compiled, I think he says about 4,500 of the 5,000 are supported in this repository, so there's still a handful to go. But he says that it it is it is on the way to to being available for everybody. So congratulations to Dirk and the team behind RTU. This is a massive achievement in the space of package, architecture in the R ecosystem.



[00:35:07] Mike Thomas:

Thank you for for calling that one out. If if you didn't, I was going to, Eric. Yeah. It's I can get to the punch. Yes. Big news for us. We're heavy, heavy users of the r two u project, albeit, you know, primarily, in Docker Linux environments. But I think this is still, you know, super important for some of those use cases that we do have that where this will show up. So awesome achievement, and and thanks to Dirk and everyone else for their work on that project in general. I did want to shout out a package that I admittedly have not used yet, but I acknowledge is absolutely awesome. And it's the tidy plots package, by Jan Engler, who's who's out of Germany.

And there is a new website, out there called tidy plots use cases that he's put together, with a bunch of beautiful charts and a little collapsible code snippet above it. And the conciseness of the code that we're able to write, the r code that we're able to write to generate these beautiful plots is pretty mind boggling. So I would recommend just checking out this site. It's it's very visual. It's really all charts with a little collapsible code snippet above it, and I am going to guess that, like me, it will it will blow you away.

So pretty pretty cool project, and I I think one to to check out. And maybe this is a very easy introduction into it. And I I really applaud him for taking the step of creating this particular site, this particular page, because I I think it, you know, really demonstrates this package very, very well and was a great sort of eye opening moment for me to then dive into the GitHub and the package down site and take a look a little deeper at the tidy plots package.

[00:36:59] Eric Nantz:

This is fantastic. Oh, so glad you called this out. I literally see things here that, a, would have been very useful for me about eight or nine years ago when I was doing a lot of bio biomarker analysis. There's a section for bioinformatics plots here, such as the the famous volcano plot for p values and fold change and correlation, like, in heat maps. Oh, my goodness. This tidy plots packages is really top notch to get you up and running really quickly. I'm gonna assume it's building upon ggplot too because it looks very, very similar, but yet look at the lines of code. It takes maybe just three or four lines to get a lot of these these these plots up and running. So, okay, this is bookmarked for sure. This is this is a great resource.

And, again, for any visualization type of package or utility that you put out there, this is a great template to draw upon. The code's available on demand, and you quickly grab to what visually pleases you the most, then grab that and run with it. Really good find here.

[00:38:05] Mike Thomas:

Yeah. I'm seeing in the description file, I'm seeing g g plot two, g g b swarm, g g pover, g g raster, g g repel. The HMISC package, I think that might be bioinformatics

[00:38:18] Eric Nantz:

related if I'm thinking of some Oh, that that's from Frank Harrell. Frank Harrell, has read an HMISC package for many years, which I remember using because I had one of his textbooks on, survival analysis. And so he had some his own functions in there. So he's been maintaining that for probably over twenty years at six.

[00:38:39] Mike Thomas:

I was trying to remember where I I had seen that one before. And the Patchworks in there as well. Oh, yeah. So yeah. We got all that the visualization heavy hitters.

[00:38:49] Eric Nantz:

Excellent. So again, these are the things that you see every week on our weekly. There's always something new that Mike and I will look at. We're like, did you know that was there? No. I didn't. Well, we do now. So

[00:39:01] Mike Thomas:

tidy plots is another one. We're gonna yeah. You deserve your flowers, whoever authored Tidy Plots. This is I'm gonna take the mask off and let the audience know that we do this as much for you as we do for ourselves to keep up to date with what is going on in the our ecosystem.

[00:39:17] Eric Nantz:

I tell everybody, whoever it's at the day job or people I talk to on social, you know, online, that our weekly is one of the best things I ever discovered because I've learned so much in these issues. So being able to talk about and then learn from it. Yeah. Can't have it any better. Can't have any better. Like I said, look at the back catalog of our weekly as well. There is a lot more to choose from. And, I know, Mike, you've been looking at the back catalog when you were timed off and saw some great things. So, yeah, there's a lot a lot that happens every week. So where is it, you ask? Is that r week without r r g? Of course.

You should have that in your bookmarks, and it's ready for you right there to, to read at your leisure. And, also, it is a community project, so we invite you to help out in whatever way you can. Our weekly has always been kind of under this value for value mindset. If you get value from it, we would love to have value back. If it is just your time and sharing it with your colleagues, sending us a poll request of that great new resource that you found online, we'd love to hear about it there too. And, also, in the case of this very podcast, we have a fun little boost mechanism. If you wanna send us a little fun along the way, we have links on how you can do that as well.

But, also, we'd love to hear from you on our social media channels. So if you wanna get in touch with us, you can find me on blue sky with at rpodcast@bsky.social. Also on Mastodon where I'm at rpodcast@podcastindex.social, and I'm on LinkedIn. You just search my name, and you'll find me causing all sorts of fun there. And, Mike, where can the listeners find you?

[00:40:55] Mike Thomas:

Primarily on Blue Sky these days at, Mike dash Thomas dot b s k y dot social or on LinkedIn. You can find me if you search Ketchbrooke Analytics, k e t c h b r o o k.

[00:41:09] Eric Nantz:

Yeah. Hard to believe. We just finished one ninety eight. We got two more to go before the big one. So as I keep saying, if you have a great memory of what we talked about in the past and what you find most enjoyable about our weekly, we love to hear about it. So definitely get in touch with us on social media. We also have a contact, page that's linked to in the show notes if you wanna share your feedback too. We'd love to read it on the air for episode 200. We'd love love to hear from you. With that, we're gonna close-up shop for episode 98.

Thank you so much for joining us today, and we'll be back with episode one ninety nine of our weekly highlights next week."
"21","issue_2025_w_10_highlights_638769103233118733",2025-03-07,41M 45S,"A major milestone for leveraging LLMs in R just landed with the new ellmer package, along with a terrific showcase of retrieval-augmented generation combining ellmer and DuckDB. Plus an inspiring roundup of the recent Closeread contest winners. Episode Links This week's curator: Sam Parmar - @parmsam@fosstodon.org (Mastodon) & @parmsam_…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 97 of the Our Wicked Highlights podcast. My name is Eric Nance, and thank you for joining us. And, unfortunately, we were off last week, but, at least I am back here this week. This is normally the point where I introduce my awesome co host, Mike Thomas, but, he and his family have been, the victim of a diabolical flu ongoing, which is affecting a lot of people, I would say. So he's off this week, but we wish him all the best, to get better soon. Nonetheless, I'll pilot the ship today if he will. And before we get to the me of the episode, I wanna give a couple, little plugs here.

I had the good fortune of joining the Coder radio program, episode 608. It has been recently revamped by the host, Michael Dominic. He's been really fun to talk to. It's ironically the third time I've been on one of his, podcasting adventures, but this time it was on the granddaddy of them all, so to speak, with the show that he's been involved with for many years. So we had a lot of fun talking about all things are and where it fits in in the world of software development, how it compares to other languages that we often hear about, and he seemed interested in getting his, data science action on down the road. So still use a lot of Python, so we'll have to work on that.

I I kid. I kid. But, no, it was a great episode. We'll have a link to that recording in the episode show notes if you wanna listen into that. Also, speaking of podcast, there will be a newer podcast that with an episode in April that I just finished recording an interview with, and I'll plug that when it gets released. But it was a lot of fun, and it definitely focuses in in how we're using our shiny life sciences, another topic that it's near to dear to my heart, so to speak. So look for an announcement of that, coming up next month.

Alright. Well, nonetheless, let's get the show on the road here, and our issue this week is curated by Sam Parmer. And as always, he had tremendous help from our fellow Rwicky team members and contributors like all of you around the world with your poll requests and suggestions. It is definitely the age of leveraging large language models. Right? And while there can be some, you know, a lot of fluff in the news about this, but I've recently been convinced, as I've mentioned on previous episodes of this very show, about when used in the right way, can really, enhance your productivity in various efforts.

And one of the engines that has been, you know, with a lot of rapid development in the last year just had a milestone release. We are speaking about the Elmer package, author by Posit, and in particular, Hadley Wickham, which had its first CRAN release that just landed last week, version o .1.one. It is now on CRAN. Before, it's been on GitHub, but there has been, as I said, a lot of rapid development, and this release definitely shows the fruits of that said development. So for those that are new to Elmer, just what is this? This is now becoming one of the de facto standards for interacting with large language models in your r session, and you get to choose from quite a few providers on this.

Some of the names you've definitely heard of if you've been working in this space a bit, such as OpenAI, Anthropic, Gemini as well. But this particular release has also support for, you might say, more of the industry or cloud based front ends to these services. I'm speaking about Azure, AWS Bedrock, Databricks, or Snowflake. That support is in this this release on CRAN. So if you're in an organization, and most of the time these organizations are putting some kind of layer in front via these aforementioned cloud services, Elmer should be able to talk to those as well. This is certainly something I'm paying very close attention to because my particular organization is leveraging AWS Bedrock, so I'm eager to try it out with that and just see how smooth it goes.

So once you've got your provider chosen, it has fit for purpose functions, Elmer does, to initialize the what is called the chat object that's gonna govern kind of your interaction with that LOM for that r session. They're named very intuitively, such as chat underscore Gemini, chat underscore OpenAI, etcetera, etcetera. And like many things with web services, you do have to set up an API key to authenticate with these services, but you can do that with the classical dot r environment approach for your project.

I've also recently started playing with, with the dot env package with a dot env file. It's all similar. Right? Just an environment variable. Again, full caveats because I can't say I can't ignore this whenever I tell about API keys. Never ever put this file in your version control repo. Trust me. There'll be dragons if you do. Ask me later. Nonetheless, once you get that, authentication squared away, it's time to actually interact with that that LOM. You've got a few different ways of doing that in Elmer.

You can give yourself that kind of more classical chat console experience that you might get when you go to, say, the web UI portal of chat g p t and whatnot via the live underscore console function or having a browser based version of that, a live underscore browser. Great way to start playing with it, just making sure things are working, and maybe that's good enough for you. I think the real meat of where Elmer comes into play comes in the other ways. You can interact with these models, such as a programmatic way with the chat function itself that you can put in your scripts.

You can also bring a pro programmatic way of interacting with a chat by returning the result of that as a string for further processing. Here is the part that has my attention the most, the part that was the eye opener for me when I first started learning about this, and this is called the tool or function calling. Because right now, the LOM, especially those off the shelf and these public services, have been trained on data that may date back a couple sometimes three or four years from now or from this point.

And there may be some things that have happened since those training dates have been concluded that the LM is not gonna be able to answer very well. Combined with any time it needs to get more real time access to information, you're kind of out of luck. That's where tool calling comes in. There are examples on the Elmer repo that take the this idea of getting, say, the current time or the current weather forecast. The idea being is that you help this chat interface that you're interacting with by giving it a fit for purpose function, registering that with the the chat object where you register on a score tool.

And then the bot can basically call your function as needed to help get the information that you're requesting. I for every reason, I never knew anything like this existed. This is not something that Elmer invented. Other, you know, services are offering this. I just never really dived into it until I've learned about Elmer back in the early days last year. There is also for those that are interacting with Shiny, putting Elmer in those capabilities, the idea of having streaming and asynchronous calls via the stream function, that's gonna be perfect if you wanna put this as an embedded chat like window or chat like tab into your Shiny app as they're exploring this.

You kinda see some of this in action with a Shiny assistant that Pause it made along with other, you know, bells and whistles. There are things to happen, you know, coming in because Elmer, I think, is definitely gonna be one of those that gets rapidly iterated on as our users are adopting this in their main workflows. Some of the what's coming up in the pipeline according to the blog post is that there will be better use of caching and parallel requests so that not only are they running things in in optimized performance, but making sure that they're minimizing the cost associated with surfacing these queries, which is translated into sometimes tokens, if you will, for these services. And you might have, you know, a batch of tokens that you've bought.

You don't wanna use those up right away if you can't be efficient. So that's in the works. And then the other part in the works is working on, more, standardized processing via a new package for retrieval augmented generation, basically a way to feed into your l o m, you know, interface, some either domain specific data, maybe some private data that you don't want being given to that service, but it can much like the tool calling, but the LOM tap into this resource in that particular session as a way to augment its information.

Again, a really, you know, interesting capability that we're gonna be keeping an eye on. Elmer has a lot going on. I I'm I'm very impressed with the engineering that Hadley and and the team have been putting into this effort. I hear big things are coming with this as the backbone and some of the newer efforts that Posit is creating. So I'm very eager to learn more about that as that gets revealed. I'd imagine we'll hear more about that deposit comp hashtag just saying. But I am starting to play with this a bit more and more. I think there are some great use cases.

Again, not for every use case, but the fact that I can use Elmer to, you know, define the chat interface the way I see fit, I think, is gonna be the start of something really great. We've already touched on some extension packages in the community. Some are authored by Simon Couch that we talked about a couple weeks ago that are wrapping Elmer functionality and more, you know, specific use cases. I think that's just a tip of the iceberg. I think we're gonna see a rapid growth in this field. And, again, as long as we're using this responsibly, we're always checking the outputs that are coming back to us.

I'm slowly but surely starting to come around to this, and I've always been one of the biggest skeptics of AI workflows. This this is definitely turning my attention a bit. So I'm eager to play with the new version of Elmer, and like I said, it's on Kran, so highly recommend you give it a shot if you're curious about how these services work that could help with your art productivity. Literally just mentioned some of the things in the works in the Elmer ecosystem in regards to the rag approach or, to more specifically, the retrieval augmented generation, a concept I'm learning a bit more about.

I've heard, you know, some mixed results on it, but I've always just wondered just how do you actually try this out in lieu of not having a a really, you know, standard approach to it, via, say, the Elmer ecosystem. Our next highlight is showing some intriguing use cases that you could do when you combine Elmer with, wait for it, DuckDV, one of my favorite new database packages, to to supercharge your rag prototyping. And this blog post for our second highlight comes from Christophe Schurk. Again, pronunciation is hard. Hope I got that close.

He is an independent data science consultant and BI consultant, and he introduces this post much like how I just mentioned in the previous highlight that when you have these LMS, it's only leveraging the training data that it was based upon for its model. When you need to give it some additional information, what are the ways of doing that? Certainly, you can try to beef up, if you will, the prompt that you're supplying to the LOM to have more context around it to give it a better framing perhaps, but that may not be the only way. We just mentioned the tool calling, I think, is another really innovative feature to give it some flexibility in how it gets this information.

But let's be real here, folks. A lot of the information that can help these LMS is, I'm gonna maybe be a little reaching here, somewhat trapped into these documents or other static type of information that an organization might have for a specific domain, such as saying the case of life sciences, maybe the documentation associated with a clinical trial, all the nuances of that trial, the operational aspects of it that aren't necessarily captured in the data itself. Maybe for a technical project, the documentation on how to use that product, such as, say, leveraging the quarto, you know, the quarto documentation to publishing system, There's a wealth of documentation online for it. Could you get an L one to tap into that information as a way to augment its responses?

So let's think about what happens when you try to get some current information without using the rag approach. And so what Christophe does in the first example is he leverages, a simple chat interface of OpenAI via Elmer and the GPT four o mini model of literally just asking the chatbot, who is Christophe Schuch, I e, asking about himself. It's revealing in the snippet of response that he shows in the post because the latest update that the l o m admits to having is from October 2021. Now that may be fine, you know, if if Christophe hasn't done a lot since then, but I'd imagine he's been doing some stuff since then.

So, you know, it's not a terrible response, but it illustrates the fact that it couldn't tap into what Kristoff has been up to lately. So how can Kristoff give it a helping hand, given us some more up to date descriptions of himself that could be fed into the ways that it responds to these type of queries. Now we're gonna get into the nuts and bolts of where RAG comes into play. But a fundamental block of RAG is having, you know, translating, say, this text, this information that you're supplying, and translate that into what are called embeddings that give it a more quantitative, you can say maybe say score that can help it determine if that particular text that's been quantified here is similar enough to what the user is asking about.

This is basically turning that text into some high dimensional vectors. This isn't too, you know, dissimilar from the deep learning paradigm that a lot of the LMS are based upon. And then being able to translate that, but also you need to be able to weigh to query this information very quickly if this is not part of that trained model right off the bat. So he experiments with a function that will take, and the model endpoint from, in this case, OpenAI called the text embedding three small model that will simply grab a string of text that the user supplies, feed it into this particular model to generate back this embedding vector of numbers, and they won't look like anything to us. They'll look like numbers that you might get from, like, a a call to our norm or something for a standard normal distribution.

But it's a sequence of of any of decimal type of numbers, but they are quantifying the similarity, that you get in certain pieces of that text. So if you do this for a lot of text, you'll get these different similarity scores of these embeddings that you can then use as part of the searching that the chatbot will do when it's feeding in the response. So you might think, well, I could just store that in a data frame and be done with it. Well, imagine the text gets larger and larger, much more than just a little embedding of who is Christophe Schuch. What if it's a full set of documents?

You need a way to query this really quickly, and that is where DuckDV comes in. Already making a lot of waves in terms of how much high performance DuckDV is offering, users that are leveraging its database functionality. It's not just how it's optimized right off the bat with these queries itself. DuckDB, much like the our ecosystem, has an extension system where people in the community can build upon the foundation of DuckDB in these specialized cases. One of those is this vector database type of extension called VSS to let the let but when feeding in this information in DuckDV to be able to search these similarity scores very efficiently, That is great for these kind of prototyping cases.

But to be honest, DuckDV, I think, is also extending quite nice of production too. So the next part of the post, Christophe walks us through establishing a DuckDV database and then installing the extension itself. You do this via queries. Admittedly, that seemed not intuitive, but I've done this a few times. So now it makes sense. You install the VSS extension, and then you load that in your session. From there, he does he establishes a table that's gonna have two columns in it. One of the raw text, which is the typical text format of a of a SQL column, but then this next one is called embedding, which is a float definition.

And he's already right off the bat giving it a fixed array size, which is coming back from what he looked at in terms of this API call and looking at the result the optimal result that it gets back for this dimensionality that, in this case, the OpenAI model is coming back, this text embedding three small. It returns a vector of length one thousand five thirty six. So that's what he's gonna define for this, length of the array in this database column. So with that in place, now it's time to okay. Let's start with that original question and feed into it a set of documents that will have imagine these are coming from, like, real documents, but he does a simple, set of vectors here of the different types of responses. One of which is a more accurate representation from his words on who he is.

And then once that text is defined, he defines a function to convert that text into the embedding numbers, again, using that OpenAI model for text embeddings. And then because of some limitations in duck duck DB, the r binding to duck DB, he does a custom query that's dynamically generated based on the text and the embedding results and results and feeds that into the table that he defined earlier. But he doesn't stop there. He does two other examples, one of which is a slightly less optimal description of himself.

And then the last one is a whole bunch of nonsense about him where he becomes this renowned intergalactic cartographer. Sounds kind of fun. Right? But that's definitely not him. So it's a good way to have a control, case compared to the two that are more accurate to who he is. So all those are fed into the database table. And then the other key point that he mentions here is that when you have these algorithms are gonna do, you know, very rigorous searching of these fields, it's always optimal for our database to have an index, which can be used to help speed up these queries.

You know, index being like a sequence of numbers. Right? But you can flag a particular column in a database to be that index. And then he notes that if you wanna keep this persistent, this high optimized DuckDB, this index needs to be persistent as well. So he has the code via the SQL queries on how to actually do this. And I don't have the expertise to translate what he's doing under the hood, but the code is in the blog post where he's flagging in, the type of indexing called hierarchical navigable small world.

I I apparently uses, an optimized nearest neighbor searching mechanism that, again, will be fit for purpose for these embedding type of columns. With all that in place, that's a lot of setup. Right? Now you can actually start to use this. So he tests this out by, again, the original question, who is Christophe Schruuck, And then he makes a function that's gonna quickly translate that into the embedding. And then in this the query deductDB, figure out where the similarity of this embedding, string, the embedded version of the string, how close it is to what's already in the database via a set of parameters that he defines here, such as the minimum amount of similarity and the number of results or documents to retrieve that will help in that answer.

He does one here, but you could have more than one if you have a a more volume of set of input and you wanna have a way to query this more, you know, with more rigorous information available. So of all that, it's got a function in place, and then he shows just how that can be used in the chatbot that was constructed by Elmer. But with this augmented version of the string that kind of embeds in, you might say, via a prompt like technique, the info and the relevant info that he gets from this prework he does of querying the database. So it's a way to kind of do the RAG approach, have it in your local session, grab the information from DuckDV, and then augment your existing question with that additional input.

And then at the end of this post, he shows exactly how that's illustrated, and it is definitely a more up to date description on who he is as compared to what we had in the original part of the blog post. Really, really fascinating. I've always wondered just what was the what are the techniques that rag involves? So because I've only heard it spoken about at a high level. Never really dived into it. This really gets my ideas flowing here that as long as I can get this information in a text format, whoever it comes from structure text or whatnot, and then do some work with DuckDB to kind of set up this, this database with this table optimized for these for this information, I could see prototyping this with some, like I said, maybe smaller documents that give more context to the type of question I'm gonna answer in hopes that it can leverage this information effectively.

Wow. I've got a lot of ideas for this. So I'm really thankful for Christophe's post here. And, again, this is all available off the shelf with the recent version of Elmer, with the DuckDVR package. You can get all this set up in your r session without even having to leave it. Really, really fascinating. I am definitely gonna play with this a bit. I have a project that I just started at the day job where I'm leveraging some techniques of Elmer. And I thought about how do I supplement my prompt with more, you know, fit for purpose information that's more current, this seems like, an approach to go with. So excited to see where this goes. And, yeah. Thank you, Kristoff, for writing this great highlight, and, I'll be, checking with you again in the future to see how much progress you make here.

Wrapping up our highlights today, we got a a fun one to to summarize here because I was speaking very enthusiastically about a newer development I learned from the recent Pasa conference back last year with respect to giving, a way to have a more dynamic way of of expressing a data driven story or some data insights in a way that beats the typical static type of document or even some of the more standard HTML layouts. And that, of course, is using this new close read quarto extension. For those aren't familiar, close read is an extension to give you that really nice and polished way to scroll HTML and have like dynamic visuals update as a user scrolls.

You can zoom in and out on things. You can really style it very effectively. And I've seen this technique used in practice by say five thirty eight or the New York Times, you know, data type of blogs that they'll put out there. They look fascinating. They take a lot of engineering to build, but I think closer he's gonna give all of us, you know, data scientists that like to use Quartle, a very minimal friction approach to creating these these resources for everybody. And one of the ways that kind of spur the community on to, you know, test their might, if he will, of using close read is posit ran the close read contest for users to give their submissions on how they're using close read to talk about an interesting topic, maybe serve as some interesting visualizations and leverage whatever additional tools that could be used along with close read to make that a real engaging experience.

And so this comes to us from the posit blog once again, and this is the results of the close read contest. There was an esteemed group of judges that included the close read authors, Andrew Bray and James Goldie, along with Curtis Kephart, you know, data scientists and community manager at Posit, along with Joshua Bird, Suzanne Lyons, and and they've been contributing from their experience as a data journalist and and scientist journalist. And so with that in place, let's talk about the winners here because there are some really fascinating things to look at.

The grand prize winner was the Euro twenty twenty four final scrolly towing analysis. Oh, if Mike was here, I'm sure he'd be geeked out about this. I know he's a huge football fan, the worldwide football I should say, and this this, this close read application as a as a sports nut maybe for hockey more than others, I could definitely see where I could use this. So this was talking specifically about, the the result of the tournament, the the game between Spain and and England in the final. And as you scroll through this after some setup at the top, you get a nice visual of the soccer field and the different positions and the numbers of the players involved. And when you hover over that, you can see their names as you go along. So you see the lineup as the game started.

And then as you scroll, it it it surfaces on, say, the formations and narrowing through the different players at each part of the formation. So this is the defense, the forwards, the middle forwards, etcetera. And it does this for each team. Again, as you scroll the mouse, it's changing the visual on the fly, changing the description of throughout, and then it gets really interesting when he talks about the game itself. So just what happened, say, midway through the game and and a goal scored, and it's actually zooming in on the plot at the parts where the the ball was kicked and then, hence, talking about the how the play started, who are the defenders around it. Oh my goodness. I mean, just scrolling it right now as I speak. This is just fascinating.

Even highlighting the regions near the net where it was going where the pass was targeted and how the the the receiving player kicked it into the net. Oh goodness. And then there's even more insights where it augments both the field display with, some line charts of the minute and and the, total expected goal percentage as it changes throughout the duration of the game. We see this a lot, like ESPN and other, sports sites as they look at the trends in the game and, like the probabilities of winning and things like that.

Oh goodness. This is just magical to me. I'm I'm I'm blown away by it if you couldn't tell. So that that is a well deserved grand prize winner, I must say. And that was that was authored by Oscar Bartolome Pato. I hope I said his name right. Fantastic. Oh, wow. What a deserving entry, and it uses g g flat two and g g I r f. Just imagine the things you can do with those packages. Just just amazing stuff. Of course, there are a lot more entries and some additional prizes that were awarded. Some of these include a really, really thoughtful demonstration of an an engaging story around the housing and neighborhood income inequality in Vienna authored by Matthias Schuttner.

Again you can look at the show notes that play with these and in the linked resource. We've got, you know, a great example of an engaging story on that front. Great technical achievements such as which way do you ski, which is really impressive visuals of, like, mapping and and other dynamic tables that are being surfaced as the user scrolls. Like, oh goodness. There's just there's just so much happening in this space. Again, the power of interactive elements, folks. This is why it's so hard for me to go to static formats when I document my results of an analysis.

There's just so much you can do with with this ecosystem. It just again, just fascinating to to see. I definitely don't have time to go through all the additional winners, but I definitely see things that you could be used in the educational space with mathematics, some great honorable mentions as well. I as I said, maybe it was a few weeks ago, I could see trying to embed some shiny components into these and seeing what happens, especially with web assembly. So I'm that that itch to try that just got a lot bigger, to say the least with looking at these winners. So I'm I'm blown away. Absolutely blown away.

Let's put this in perspective, folks. Close read has only been in existence for maybe half a year at the most or somewhere around there. And look at what people are doing here already with it. Like that is it goes to show you and there's admittedly there's some parallels in my opinion to shiny itself. Little deposit know at the time when shiny was put out there just how embraced how it would be embraced so quickly by the art community. And lo and behold, we start to see these really novel uses of shiny that would blow everybody away.

Close read seems to be following a similar trend here. I definitely have ideas with this, and now you've got at your available to you now over 40 of these submissions, if my my math is correct here, that you can look at all of these on deposit community, blog as well as the GitHub discussions that talk about, you know, extensions or use cases of close read. This is terrific. Absolutely terrific. I invite you to check out these submissions. No way does an audio podcast do this justice, but you can see them all, like I said, on deposit forum and and see what inspires you. As a sports fan, as a a fan of engaging storytelling, yeah, I've got I've got me some ideas. We'll see what happens.

And there's a lot more that you can see happen in the art community when you look at the rest of the issue of our weekly. As always, our curators, in this case, Sam did a tremendous job with this issue. And since we are off last week, my additional finds are actually gonna go back to the last issue that we would have talked about if we hadn't been off last week, an issue that Colin Faye curated and one of the highlights there that is an area that I played with and I've had admittedly very mixed results with, but looks like someone's trying it again.

We are talking about the way to distribute a shiny app in a more self contained manner. And in particular, what I tried a few years ago when I had a project where I tried to share a shiny app that could be installed on somebody's laptop or computer in a training session, I had tried wrapping a Shiny app into the Electron framework. And if you're not familiar with Electron, you can think of that as a way to wrap what amounts to an embeddable Chrome browser with JavaScript and and a web based application. So if you use things like Slack, Discord, countless others that they may look like they're native, but they're not really. They're cross platform because Electron is compiling them into that operating system's preferred, you know, format, such as, I guess, cutables for Windows, DMGs for Linux, or may I should say Mac OS 10, and then other formats for Linux. So it it it it runs the gamut.

Nonetheless, it looks like there is some attempts to make this a bit more robust for the shiny side of things once again. In particular, Jinhwan Kim has a blog post talking about a minimal framework called Nyriss, which is an an ag an anagram of shiny to transform a shiny application into a standalone application. There is a lot to this. He does link to a video that demonstrates this in action, but it is indeed based on Electron. He's tried to make it easy so you can kind of bootstrap all this once you clone the repository that that he has set up for you.

And doing some shell scripting that is tapping into Node. Js, which you have to have installed since it is all JavaScript under the hood. And another shell script called project that will help you include the R installation that you're using on your system, the R packages, the electron bits, the Node. Js packages. And then out of that you get a Shiny application that you can execute as an executable. I don't believe there's support for this for all the operating systems from what I can read yet. So it looks like there is more to come in this.

I'm I'm impressed with how far he's gotten. I will say though, I think just enough bad things can happen when you've got something that's not so trivial. I feel like the wave of the future is still the web assembly route. Maybe I'm wrong, but I think the web assembly route has a lot of momentum behind it. I think we're gonna get to a point where we're much closer to that vision of I compile a Shiny app and web assembly. I give one file to somebody. They just execute it in their web browser, and they're done because everybody's got a web browser. Right? So Shiny is, of course, browser based, you know, type of app. Long as you can execute it there, you're good to go.

It's not quite there yet because you still have to run some kind of web server process, whether it's an r or Python or something else to host that. That's still a part I'm still struggling with, but we're getting much closer folks. We're getting much closer to support for WebAssembly is just getting better and better every week. Electron, if this can be, you know, used in many different types of applications, I could see this as a good alternative. I've just been jaded by past experience. So maybe Jin Hwan has found a nugget to crack here. I will keep watching this space.

I'm still still a bit more biased towards web assembly. Nonetheless, if you are gonna play with the electron and you were struggling with some of the previous presentations you saw, let's say, a previous shiny conference or a previous Rstudio conference, or I've seen this mentioned once or twice before, you may give this a play. See what happens, and nonetheless, try a Vatoy project and and hope for the best. Perhaps this could be wrapped with a Knicks bootstrapping. Who knows? I don't know. I'm just spitballing here.

Okay. We're gonna wrap up this episode of r w hives. But before I go, I wanna make sure I tell you all how to get in touch with us. We are a community project through and through. There is no corporate sponsor to r weekly. We don't get donations coming. It is all through the hours of us on the our weekly team and hopefully from you and the community help us out with curating these issues. One of the best ways to help is to send your suggestions for our upcoming issue, a great blog post, a great new package, a great tutorial, maybe an upcoming event. We wanna hear about it. You can send that to us by going to rweekly.0rg.

There is a link in the upper right corner, a little banner for you to fill out a poll request that will, you know, have a template for you to fill out quickly. And once you submit that, our curator for the upcoming issue will indeed be able to merge that into the upcoming issue. All marked down all the time, folks. That's scrolling, telling, close read extension, all marked down. Combined with our code, you can do it. And, also, we love to hear from you as well directly. We have a feedback, you know, portal that you can link link to in the episode show notes. Go ahead and click on that and send us a little message along the way.

And if you're on a modern podcast app, like Podverse, Fountain, Cast O Matic, CurioCaster, You can send us a fun little boost along the way. I would say in particular, fountain makes this the easiest. So if you want it, you're interested in playing with that, I would get in touch with fountain. Get that on your system, and it'll have we'll walk you through right through all the steps you need to get a wallet established and send us a fun little boost. Lots of fun to be had there. Trust me on that. And, also, you can get in touch with me on social media these days.

I am on blue sky with at rpodcast@bsky.social. Also on Mastodon, where I am at rpodcast@podcastindexonsocial, as well as LinkedIn. Just search my name and you'll find me there. Alright. We're gonna put a ball on episode a 97. Again, I may have mentioned this a couple weeks ago. If you have a favorite memory, a favorite segment, or just, you know, what's your favorite part of our weekly, we'd love to hear about it, and we'd be glad to read it on the episode 200. Share your feedback. I have no idea what else I'll do for that episode. One way or another, we'll make it fun.

Alright. I'm gonna close-up shop here for episode 97. Thank you so much for listening. We'll see you back for episode 98 of our weekly highlights next week."
"22","issue_2025_w_08_highlights_638757139532492773",2025-02-21,52M 18S,"Our candid takes on the state of CRAN's role in light of recent package archival events, how creative use of LLMs could greatly streamline your next literature review, and a few great illustrations of lazy being a good thing in your R session. Episode Links This week's curator: Eric Nantz - @rpodcast@podcastindex.social (Mastodon) &…","[00:00:03] Eric Nantz:

Hello, friends. We're back of episode a 96 of the Our Weekly Highlights podcast. Oh my goodness. That's four away from the big 200. It's sneaking up on us folks, but in any event, this is the weekly show where we talk about the awesome highlights and additional resources that are shared every single week at ourweekly dot o r g. My name is Eric Nance, and, I'm I'm feeling a little decent now, but I will admit I've been under the weather lately. So we'll do my best to get through it, but I'm feeling good enough right now. But this is definitely one of those times where more than ever, I am so glad I don't do this alone because I got my awesome cohost,

[00:00:40] Mike Thomas:

Mike Thomas, to pick up the pieces when I fall apart. Mike, how are you doing? I'm doing pretty good, Eric. I feel about 50% healthy. I'm fighting it too. So, hopefully, you're 50% and my 50% can can add up to a hundred if that math checks out. I do want a shameless plug, and we didn't even talk about this pre show, but were you on another podcast recently that anybody can check out?

[00:01:03] Eric Nantz:

I will be. Unfortunately, that post got sick too. So we had the postcode now. So talked about a preshow. No. No. It's all good. It's all good. I will definitely link that out when it's published, but we're that'll be a a fun episode that will be Dakota radio program. So stay tuned on my social feeds for when that gets out. But, yeah. Apparently, it's going everywhere even across the country too. So, nonetheless, we are we're we're good enough for today. And, let me you know? Gosh. It's been such a whirlwind week. I gotta figure out who curated this issue. Oh, wait. Oh, wait. Look in the camera, Eric. Yeah. It is me. It was me that curated this week's issue.

As always, great to give back to the project, and this was a very tidy selection. And as always, I had tremendous help from the fellow, our rookie team members, and all of you out there with your great poll requests and suggestions. We got those merged in, and some of those ended up becoming a highlight this week. So without further ado, we're gonna lead off today with arguably one of the more spicy takes we've had on the show this year at least, and maybe even maybe even the past year as well. And this is authored by, I'll just say, a good friend of mine from the community, Ari Lamstein, who I've met at previous positive conferences. And in fact, I have fond memories. He was actually one of the students at one of my previous shiny production workshops. So he's been always, you know, trying to be on the cutting edge of what he does, and he does consulting. He's done all sorts of things.

But this came into my feed, around the time of the curation this week, and he has this provocative title, is CRAN, CRAN being the comprehensive r archive network, holding our back. So let's give a little context here, and then Mike and I are gonna have a little banter here about the the pros and cons of all this. So Ari has authored a very successful package in the spatial, you know, mapping space called coral plethora. I hope I'm saying that right. But he's maintained that for years and years. He would say that it is in a stable state. He did rapidly iterate on it over the years in the initial development, especially when it was literally part of his job at the time to work on this package.

But, yeah, it's been in a stable state, you know, along those things where if it ain't broke, don't fix it. Well, it was about a month or so ago. He was informed that it was his package was going to be archived. Meaning, and for those aren't aware, when CRAN archives a package, they will not support the binary installation of a package with the typical install dot packages function out of the box because of some issue that has been raised by an r command check or maybe another issue that the maintainers thing should have been resolved and end up not being resolved.

Now you may ask, well, what was the issue with the package? It wasn't with Ari's package. It was of a dependency package called ACS, which I've not heard about until now. Apparently, that got a warning. And what was the warning about? Well, if you go in the digging of the ACS packages, post on CRAN where it gives a link to the command check warnings, Try this one for for size, folks. The note, it was not a warning. It was a note.

[00:04:38] Mike Thomas:

Right.

[00:04:39] Eric Nantz:

And it says, configure slash bin slash bash is not portable. Mike, when did r include a bash shell? Do you know? Did I miss something here?

[00:04:52] Mike Thomas:

I must have missed it too. I mean, I have to imagine that this is when the right. This is when the the checks run, so it probably has something to do with the runner itself and the the Linux box where the this is being executed on. This may be above my pay grade, but that is not an informative message.

[00:05:10] Eric Nantz:

And it literally has nothing to do with r itself or even, another language that a package could be basing off of such as c plus plus or Fortran or the like. So Ari, you know, he did not like this, and, frankly, I can understand where he's coming from here. And he decided that, well, chloropleather is not at fault here. And you know what? I'll let the chips follow where they may, and it is now archived. So his package is now archived because of the ACS package being archived. We I personally don't know who authored it, not that I really care to for the purpose of this discussion.

It has been archived, and now Choroplethor is now archived as a result. This, so Ari and the rest of the post has some pretty candid viewpoints on the state of CRAN at this time. I will admit he's not alone in some of the recent discourse I've seen online. I've heard, community members like Josiah Perry, who I have great respect. He's had some qualms about some recent notes he's received from crayon as he's been trying to, I mean, either add a new package or update an existing package. And Ari, you know, really is being provocative here about what impact is Kran having now on the future growth of r itself.

And then Ari has lately in the last, I would say, a few years, been working on additional projects in Python. So he does compare and contrast how CRAN compares to Python's arguably default package repository called PyPy. Now let's, let's have a little fun with this, Mike. I'm going to play, hopefully, the role of a good cop, supposedly. YCran, even with these issues, is still a valued piece of the our ecosystem and that maybe this is just a very small blip in the overall

[00:07:20] Mike Thomas:

success of it. And then your job is gonna be able to talk me down on this. So let me you ready? Buckle up. Yeah. We we can do that. And I think as a disclaimer, I would say that I don't wanna speak for you, but both of us have mixed feelings on both sides of the fence. Yes. But I think we'll we'll go through the exercise of, good cop, bad cop here. I think that's a great idea. Yep. So

[00:07:43] Eric Nantz:

I've been a long time R user since 02/2006. So I've I've I've I've known the ecosystem for a while. I dare say that as somebody new to the language, as I was getting familiar with Baysar and then suddenly I would have courses in grad school that talked about some novel statistical methods and also my dissertation on top of that. Without the CRAN ecosystem of a way to easily once I did my, say, WIP review or other research about the pack the methodology I needed, and then finding that package and then be able to install that right away in my r session.

But that package being authored by leading researchers in those methodologies and the fact that this was a curated set that I could have full confidence that the package I'm installing is indeed going to work on my system, and it has been has been approved from a curated, you know, curator type of group that the CRAN team is and to be able to use that in my research. Other programming languages do have a more automated system where, yeah, you can throw any package on there, but you have no idea about the quality of it. You have no idea if it's going to destroy your system sometimes. You have no idea if it's even doing the right thing, statistically speaking.

In my opinion, one of the great advantages of r is indeed that CRAN is this targeted stable set that's very akin to, say, the the Debian type philosophy in the Linux world. It's stable. You can rely on it, will not break on you, and they are making a concerted effort to make sure that this is a reliable network for you to install packages on. And I don't think ours is where it is today without CRAN having that that curated group behind the scenes to make all this happen. What do you think?

[00:09:51] Mike Thomas:

Yeah. So I think that maybe the times have changed a little bit. And I think that maybe five plus years ago, people were developing packages not via GitHub, this GitHub package development workflow that really exists now, I think, across the space. You know, sometimes you'll go to an old package and you'll search for it on Google or your your favorite LLM, I guess, these days. And you'll try to find, you know, the the GitHub repository that that sits behind that package. So you can take a look through the code and it doesn't exist. You're just taken to, like, the the PDF, right, of the the package itself. And that's that's very frustrating nowadays, but I guess that's probably reflective of maybe the workflow that used to exist, you know, five or ten years ago where we didn't have really this GitHub package, development driven workflow. And this is something that was raised on on Blue Sky by Yoni Sidi. And they said, you know, back in 2016 that we they thought the R community was sort of setting itself up for problems by not building CRAN not building sort of this infrastructure and software development life cycle, geared towards really a focused GitHub package development where sort of most of, packages are are developed right now, and CICD can be set up and all sorts of things like that. So I I think, unfortunately, in a lot of ways, CRAN has not caught up with the times.

And then just some of the rigor and inflexibility that they have, I think can be construed as over the top, for lack of a better word. I think archiving a package because of a note, seems absurd. And I think that the time windows for folks to fix these things are unnecessarily short. I remember g g plot two, it was a year or two ago, was almost archived due to a dependency, you know, that was that had been archived. And ggplot two has, like, a whole entire company behind it that can work on trying to rescue that. They have relationships with the CRAN maintainers, you know, the volunteers that that work on Krayaan that the rest of us do not have. Right? We're limited to emailing back and forth, and I'm sure anybody that's done that before, you know, has has struggled with that, for lack of a better word, in in some sense.

So I think we're we have this dichotomy where if you are an R user, yes, CRAN can be very beneficial. But if you are an R developer of packages, it can potentially cause, you know, more headaches than it solves.

[00:12:37] Eric Nantz:

I definitely resonate with that. So let's let's put away our cap and and whatever uniforms here. Let's let's, let's be real here. I definitely think that this was a heavy handed instance. I think that it's one thing to ensure broad compatibility across different architectures that are supports. But for something like a bin bash note that admittedly was not affecting users up to this point anyway, Ari has always been very responsive to user feedback on his packages. He has not heard one iota in both his testing and others about colorectal being being affected by this.

I think this is an artifact of a a system that has gotten the our, you know, community at large to where it is. I mean, like like, without CRAN in the very beginning, I still stand by our was not would not be as successful as it is now. But as you said, Mike, this is a different time now. This is a different time where there are multiple personas, so to speak, leveraging our going from more of an academic type of, you know, statistical environment for research. Now it's being used across industries. My industry in particular is really doubling down on it, and it is being relied on in production way more than maybe someone might have thought five or six years ago. So I do think that there needs to be, a healthy assessment on where things can be improved upon.

I think transparency is one of them. I think leveraging newer technology for automation is in another because let's face it. Another key project that we covered on this show many, many times now is the growth of our universe where, no, it's not a human curated effort per se, but it is also leveraging a lot of automation to give, you know, confident, reliable package binary installations across different operating systems as well as new technology, hint hint, web assembly, to make this even more future state proof.

I think there is somewhere in the middle that maybe eventually either cran or another effort that is slowly coming up into, into the discussion, the multiverse project, which we'll be hearing, I think, more about this year, where maybe there is a best of both for us. They still have a human in the loop, but yet take advantage of modern technology that say our universe has pioneered to make this hopefully a better experience for everybody involved, for the consumer, I e the user of the package, but also the maintainer and developers of the package.

And and Ari's post, he does throw some statistics about how, you know, Python itself has a lot more, you know, metrics of their packages being downloaded and whatnot. Well, admittedly, Ari, that can be a loaded statistics, so to speak, because I think and this is actually captured in a in a LinkedIn post that Ari put up his blog post with some great comments as well. I agree with people like Joe Chang who commented on this. The PyPI, yeah, maybe it's an automated thing, but it is a wild west And dependency management in Python, I'll stand on this soapbox.

Still leaves a lot to be desired, and hence, there are people that are very are are are destroying and upping their environments left and right for a single project. Joe in particular had a comment about he must have installed pandas, like, over 50 or 60 times when he was testing various things with with his efforts in Python. So it yeah. It's great that PyPI has this less friction approach to get something online for a Python package, but you can go extreme in another direction, and it can cause havoc in that regard. So I still think there's a middle ground to be had here.

But in any event, do I I still think that this was a very heavy handed, heavy handed approach here taken with with with archiving chloroprether and and ACS as a result of this kind of note. Because in the end, did it affect the users? No. Maybe it affected one esoteric build target that Cranston uses. I won't say Solaris even though I do wanna say it because that's usually the butt of many jokes for antiquated architecture. But I I I think it it's good to at least have these discussions and hopefully with the efforts that are happening with with our universe and in the future, not so distant future, the multiverse project that we'll we'll get to a middle ground somewhere.



[00:17:35] Mike Thomas:

Yeah. Yeah. I have to say, you know, the experience of installing in our package is, I think, one of it. The big benefits of our over Python, you can install packages from within R Right. Without a second piece of software like Pip, which is, you know, incredibly frustrating for Python newbies, especially. And I guess my other my only last other point would be and I'm not saying that this is the case with ACS. I don't know much about the package. But if you are an R package developer, one thing that you can do to try to mitigate risk, in my opinion, is to take a look and see how actively maintained the dependencies of your package are. And if you're going to add a new package, make sure you you take a look and see if there is somebody, who's, you know, contributing to that package recently. It looks like they're maintaining it, such that they would be responsive if an issue like this did happen. There's a lot of R packages out there that haven't been touched in, you know, four plus years. If you you go back and take a look at the code base, and, it's probably only a matter of time until something happens. And if nobody's there to respond to it, that's going to be an issue for all of the packages that depend on it. So in my opinion, that's that's an additional way that you can try to mitigate some risk.



[00:18:54] Eric Nantz:

Yeah. Very valid point, Mike. And and my other key, you know, thought I had is that a maintainer should not be penalized for having a stable package where maybe a check does arise, but has nothing to do with, like, how r itself works or anything like that. If it's having to do with this kind of arbitrary Linux shell prompt that the package has nothing to do with, that should be treated differently than say, oh, wait a minute. Your use of, like, a a an object class system just completely broke or or a test completely broke, you know, whatever. That that's a different story. I don't think these notes are all created equal here.

And that that that nuance is not lost on me. That was a note, not a warning, not an error, a note, which yes. I mean, you you obviously strive to minimize those, but, again, those are not all created equal.

[00:19:52] Mike Thomas:

Likewise. And I guess maybe the last thing I would say is if you are somebody who's developed our packages and you're looking to dabble into potentially developing Python packages, Ari's also, drafted a a blog post on his blog called a guide to contributing to open source Python packages, which,

[00:20:08] Eric Nantz:

you might find very interesting. Very good, Mike. We will link to that in the show notes. So up next in the highlights, well, it is 2025. You're usually gonna hear something about large language models and how they're, you know, helping productivity or helping a data science type of pipeline. And so our next high, we do have an interesting use case that admittedly I even mentioned when I was doing using r for my dissertation probably would have been really helpful in my lit review because we are literally gonna talk about how large language models could help you in a more systematic approach of literature review.

And this post is coming to us from the seascape models group at the University of Tasmania. Shout out to Tasmania. That's a first on the highlights in the duration of this box. That is awesome. Yes. If you ever had any any doubt about how international r is in this footprint, you know, this is this is a proof right there. Nonetheless, this is the first time I heard about their research group, but none but they I don't know who exactly authored it, but I'm gonna say from their research group, they talk about, you know, a very practical approach that in their research or how they've looked at, you know, it's one thing to assemble all the resources or manuscripts or papers that comprise a lit review, but actually, So in particular So in particular, what they walk through in this post is how you can leverage an off the shelf, you know, large language model, you know, chat g b t like service.

And then to be able to take a set of PDFs and extract the information from them. And to take that text, try to clean that up as well. And then once you get that going for, like, a single type of manuscript, how you batch all this together. So first, like I said, this is using the approach here is using one of the off the shelf providers for generative, you know, AI, large language models. They are using anthropic in this post, and I've heard good things about that. So, of course, there's no such thing as a free lunch here. You're gonna need an API key because that will be leveraged as you interact with the service to grab the information back from their chat like bot interface.

And so they got some nice call outs of packages that help you with managing environment variables. You know, typically, I'm a I'm an old school guy. I've always done the dot r environment file in my working directory. There's this great package that I admittedly need to use more often called dot env that will help you do this in a slightly more agnostic way. But you just set up a dot e m v file in your home directory, put in your anthrop anthropic key, and you're off to the races because dot m has a little low dot e n v function to import that in. And then you can use that as your environment variable, and you're you're good good to go there. So great great little package right off the bat for that side of it.

And then the packages that they're using to interact with the anthropic models is called the tidy chat models package, which I am not familiar with as well. To do some research on this, to where this this package comes from, but looks pretty straightforward here. You create a chat object defining the name of your service, your API key, and the version of the API. But you could use other APIs as well. So I have to I have to look at this, this package in more detail later on. Looks pretty nifty here. Once you get all that set up, now just like with any of these services, you gotta figure out what you want to use for your prompt and how to perform that. So they have little, you know, basic example for adding a message based on the role that you supply and the prompt text.

Role is a key concept here because there's typically two roles here. There's a user role and a system role where you may get more precise control over, say, the system role, but they give you some links to, to determine which is best suited for you. In this case, they're gonna look at a more system role here to give a little more granular control over the type of model that they're gonna use in the l o m, you know, interrogation. And you can add certain parameters such as temperature or max tokens, which they have lots of links of documentation on where you can find more information here. But this kinda checks out with my explorations of the Elmer package where I learned very quickly the prompt is the key here along with some other interesting things you can augment with that chat or with that interface to that chat in, that chat service.

So once you have all that, they've got them so they got it ready to go to look at, you know, getting getting the text, you know, summarized, but you gotta get the text in there itself. So in this case, they've leveraged the PDF tools package for a convenient way to grab the text from a PDF that you've downloaded on your computer with the PDF underscore text function. But you also have to make sure that you are able to authenticate directly to your API service because you may get, on top of the authentication, you may have to format the text effectively. So he he or she notes that the first time they ran this, they got a 400 error from the API service because the formatting wasn't correct. Because when you extract text from PDF, there could be some strange symbols in there, some strange artifacts, and you gotta you gotta clean that up a little bit. So that's a good walk through on the practical ways of leveraging this workflow. But yeah. And the rest of the post, Mike, why don't you talk us through some of the challenges I'm learning that the authors had of this workflow here?



[00:26:35] Mike Thomas:

Yeah. You know, it's really nice that we have these APIs that allow us to do things programmatically. I think, like you mentioned, it wasn't quite as straightforward as, extracting the the text from the PDF using that PDF tools package. There there was some cleanup for those characters. But once that took place, it was pretty straightforward to be able to interact with this this LLM, and I believe that's much thanks to this tidy chat models package that exists, that allows you to, it has functions in it like add params, where you can set what's called, like, the temperature, and the the max number of tokens that, you expect to interact with, with respect to the LLM. You can add a particular message, in this case, a system prompt. And, the the system prompt here was, you are a research assistant who has been asked to summarize the methods section of a paper on turtle mortality.

You will extract key statistics on sample size and and year of study and do not extract any more information beyond this point. So those system prompts are intended to sort of fix the response or the way that the LLM will respond to you, prior to even providing your prompt, your user prompt. And then finally, there's a function called addMessage, which allows you to submit that user prompt. And in this case, it'll be the the text, that was extracted from the PDF. In in order to, send this to the LLM, there's a final function called perform underscore chat, that will send that. You can save that as at least save the output of that to an object. In this case, the author used an object named new chat.

And then to take a look at what the LLM gave you back, you can use the extract underscore chat function, from the same package against that object that you saved. And it's it's pretty cool here to take a look at the results. I will give one hot tip. So the results come back, as text where the LLM says, you know, based on the method section, here are the key statistics. Your sample size of 357 sets, for large scale long line fishery, and the year of the study was 2018. So it it is text that you would then have to parse, and there's a little bit of code that leverages a lot of functions from stringer and I think dplyr as well in order to actually extract, you know, just the numeric values for the sample size and the year.

I will give a hot tip here just to based upon our experience in the past. We have done things like tell this system prompt to or use the system prompt to tell the LLM to only return, data in JSON format with the following elements, like sample size and year of study. And that will actually spit back out JSON, that you can, you know, then consume much more easily without having to do any string data wrangling, if you will. So that that may be, helpful to to some of those folks out there. But if you can ensure that the output that you're getting is is fairly standardized, like it seems to be the case here, then, you know, hopefully, we can program the whole solution. Right? And we don't have to, do different parsing logic based upon, you know, different, prompts that we are essentially querying.

And and so the code here is a fantastic walk through, pretty lightweight in terms of the number of packages that are being used, which is is awesome. There's some considerations here and some some great discussion at the end around cost uncertainty. If you're doing these things programmatically, right, you need to make sure that you're calculating, estimating your cost. You know, most of these models out there have the ability to set or most of these third party providers have the ability to set budgets, I think so that, you don't go over, you know, $10 or a hundred dollars, whatever it is that you, you know, have linked in your account. I know at least the Claude models have that, which is really nice.

And maybe the maybe the last thing that I will say just on a related topic while we're talking about LLMs is a totally, separate shout out, but there's this project out there called Continue. I don't know if you've heard of it, Eric. No, I'm not. It's a v s v s code extension, and I heard about it on Hugo Bowne Anderson's recent podcast. I think the, the Vanishing Gradients podcast, which, interviewed the one of the, I believe, chief developers of this continue project. And it's an open source coding assistant.

And you know me, an open source, you know, go away, Microsoft. It it it does have the ability so you can select sort of your back end model that you want to use, and that could be OpenAI, it could be Claude, but it could also be a local model, like one of those from the Ollama project, which is what I've been using, one of the smaller ones. And it has the ability to just chat with your code. It has the ability to bring in particular pieces of context, Like, you can use a a GitHub repository just specifying the URL, and it will crawl that GitHub repository, you know, using your your GitHub p a t, if it's a private repository that you can supply it with. And, it will essentially do all the text embedding work for you to, you know, put that in a vector database or whatever it's called, to be able to utilize that as context that you can chat with. You can put multiple sources of documentation like that if you want.

And, you can also highlight pieces of of code and, you know, chat with the context being that particular highlighted piece of code. Does auto complete, all those really good things. And it's a really, really cool user experience. And I would encourage anyone out there who is looking for, you know, the the Copilot experience without necessarily wanting to, interact with a third party and continue to leverage open source software to take a look at this continue project. I think it's maybe continue.dev,

[00:32:58] Eric Nantz:

but we we can link to it in the show notes. Yeah. We're gonna link to it. And while you were talking, I wanted to see, hey. Wait a minute. I wonder if I could plug this into Positron in my my bleeding edge evaluation of positron. The good news is this extension is on the open VSX registry, meaning it's not locked into Versus code. You could use it on the variant. So I think I'm doing that later today, Mike. I'm gonna give this a shot because I have been hesitant to get on the copilot train for even my open source stuff that I've been leveraging, some other alternatives, and this may be one of them. So just goes to show you there's a lot advancement here. And by the way, yeah, plus 100 to your tip about giving the prompt some detailed information on how to get results back. Getting results back in a structured way like Jason just opens up so many possibilities, and I leverage that technique extensively when I was making this, fun little haunted places shiny app that leveraged an LOM for our pharma last year. I made sure that when I made it randomly generate the quiz questions, give it back to me in JSON so that I could present it in shiny very easily with a with dynamic input input. So there's a lot a lot at your fingertips here. I think this post does highlight.

Do some quick tests first in a specific use case, and you will have a lot a lot that you'll learn along the way. But we're only just scratching the tip of the iceberg with this, so to speak. There's a lot more to come, come in this space. And last, but certainly not least, Mike, I will admit on a day like this when I'm not feeling like myself, I do, want to feel, and I do feel a little lazy about certain tasks. But we're not gonna talk about lazy and a negative connotation here because our neck our last highlight today is literally giving us a very practical take with the many different ways that lazy and laziness is actually available to you as an R user depending on your context, depending on your workflow and the packages that you're utilizing.

And so this last highlight comes to us from the, our hub blog and from a very brilliant group of authors. If I do say so myself, we got Mel Salman, Athanasia, my Winkle, and Hannah Frick. So that's a, that's a a trio of trios right there to start off with. And so I'm gonna introduce certain pieces of this, and I'll turn over the mic for the rest. But if you're an r user the last few years, there's probably one interpretation that of lazy that you've heard throughout your journey with our, and that is a concept of lazy evaluation.

That is arguably one of our biggest selling points is the idea of lazy evaluation. And what this really means is that if you have a function with arguments, that those arguments are only going to be evaluated in the runtime, so to speak when they are accessed. So you may be able to pass like a huge value for an argument. Maybe that's like a data frame or some other large vector. If it's not used, it's not gonna really matter. It's just gonna be there just in the in the definition. Only until you actually call something that leverages it will actually be used.

And, there is a counter concept to that called eager evaluation. But in typical default behavior behavior for r, It is a lazy evaluation, and they have a link to a great chapter from the advance r book that's authored by Hadley Wickham with another more, you know, thorough introduction to lazy evaluation. And in fact, in base r itself, another concept that you may be familiar with is the idea of a promise where it may be something that is on tap to be evaluated, but it's in essence more of a recipe to get to that value.

They call that an expression most of the time or it could be from an environment as well. And again, only when you need it will be evaluated in your memory. So that can be very important depending on your workflow. I mentioned promise. Right? Well, there is a very important part of the our ecosystem that leverages a different take on promises with respect to, high performance computing and async processing. And that comes from the future package. The future package is authored by Henrik Benson, another really brilliant researcher in the art community that I had the pleasure to meet at Paz at Confuc a few years ago.

And in the future package, this promise is more thought of as a placeholder of a value And that and, again, there are different ways to configure this with the future package. You can have what's called a lazy future or an eager future. And in essence, when you define this future, the default behavior is actually be eager, meaning that when you define the function that has that future encapsulated in the moment you define that and it runs something right off the spot, it's gonna wait until that task is done.

However, you can also feed in an argument of lazy equal true, meaning that that future will just be set to run-in the background. It will not hog your our session, and you could do other things in the council and and do whatever you want, but only until you want that future value will it actually do it. So that could be important if you're new to the future package to figure out, well, wait a minute. I thought the whole point was that it could run-in the background. You gotta be careful in how you define how that future, is is spelled out or initialized.

So, again, their version of lazy is not quite the same as the definition we heard in base are and whatnot, but that is important if you're into into that space. And now we're gonna shift context to data itself because a very key concept in database operations within the R language is the idea of lazy operations. What does this mean? Well, in a nutshell, with these database back ends, such as, say, MySQL or SQLite or others, these queries that you might define with the with the help of, say, the DB plier package that accompanies a lot of d plier like syntax. But for databases, you may have a pipeline of an analytical pipeline where you're gonna take data, maybe mutate a few things, summarize with group processing, but only until you quote, unquote collect that result will it actually be evaluated. So it's a way to efficiently run SQL queries instead of in a typical data frame. It's gonna run all those steps one by one in memory right away.

So that is a really important concept that the DB plier package, that DB plier package surfaces. And also more recently, the DT plier package does a similar thing with data dot table as the back end for managing that data. And again, much like the database, you know, paradigm we mentioned earlier, the lazy way of doing it is gonna capture the intent of those data processing steps, but not actually do anything until that result is requested, I e collected with a collect function. Again, really important selling point.

But there is in this in the realm of databases, another newer contender that has even more of a Nuance take on this, and that is the duck plier package because Mike and I are big cheerleaders for the duck DB back end. I absolutely love it. So with duck plier, again, this is using duck DB on the back end, so to speak. Now there can be a little bit of a problem here with respect to traditional d plier type usage. Usually, things are eager by default in dplyr, like I mentioned, with a typical data frame.

But when the concept of DuckDV, one of the reasons we wanna use it as a whole is that we can optimize the queries, optimize the computations before they're actually run. So Duckplier does need this same concept of laziness as those traditional packages like DBplier actually need. Now this is what's interesting here. The way Duckplier is pulling this off, we're getting a little in the weeds here, is that it is leveraging alt rep, which is one of the more fantastic contributions of ASR of over the last two years where it's more power power behind vectorized operations, but it supports what's called deferred evaluation.

The more specifically, and I quote from the post here, alt rep allows our objects to have different in memory representations and for custom code to be executed whenever those objects are accessed. So that means for ductplier that they can have a special version of these callbacks and other functions to interrogate whatever is the root of that operation, say the query, an analytical summarize, or whatever have you. So then duct plier by proxy is actually lazy in terms of how it itself runs as operations, but it seems eager to you as the end user when you're running like a duct plier based pipeline.

So they got they got examples here where there could be cases where this is very important to utilize this functionality and cases where it might be might be, more more, applicable to add a little more control to it or add a safeguard to it. I've never played with this before, but there's a concept called prudence to control just how automatic this evaluation of this laziness vault rep is is done here. There's stingy, and then there's thrifty. I love these names, by the way. Those are really creative, but they got examples in the post with the NT cars set of the differences between how these are these are, approached here.

So these this is something that you probably wanna look at with the recent version of duct plier. It it had an upgrade, I think, within the last few weeks or the last year. There's a lot of rapid development on it, and I think it's got tons of potential for leveraging high performer workflows with database at the back end. And, again, a clever use of laziness with respect to alt rep. So I am I'm eager to to try that out. But, of course, there's way more ways of laziness and, you know, lazy evaluation play a role in the rest of the our kinda typical workflows that you might have. So, Mike, why don't you take us through those?



[00:44:33] Mike Thomas:

Yes. A few more quick hitters for us in this blog post. When we talk about lazy loading of data in packages, I think a lot of us have experienced this before. When you you're in R. Right? You can quickly access, like, the iris and the mtcars datasets which are built into your installation of R. I'd I'm not sure if they're loaded Eric, you probably have to help me with this a little bit. If they are loaded into memory prior to calling them, prior to actually evaluating them. But that's sort of this concept here where if you have an R package that does have a package dataset in it and and sets the lazy data field in the description file to true, then the exported datasets are are lazily loaded and they're they're available without having to call the data function, right, for for those particular datasets.

But they're not actually taking up memory until they are accessed. So that's something interesting there. It's something that we've run into a few times actually. We have some some functions in some of our packages that programmatically, sort of, you know, with the use of, like, regular expressions and stringer, try to decide which internal package dataset you want to leverage in that function and unfortunately you have to call library on the package first in order for that function to work you can't just name space it or else it will fail. And I'm not sure if we've solved that yet. It's it's a bit of a workaround.



[00:46:07] Eric Nantz:

Is that something you've run into before, Eric? Yeah. The hard way quite a bit even with my goal in power, Shiny apps are on include an internal dataset as, like, a way to have, like, me my colleague test or an example set that the app would use. I I I've I've had to I've had to do some, you know, very weird hacks of, like, just doing an arbitrary command on that data frame to trick it to load in the memory before the function completes. I don't really have a great solution for that. So, hey, Colin, if you're listening, maybe you could help me out with that, by the way. But, nonetheless, that's where I've encountered that bugaboo the most.



[00:46:46] Mike Thomas:

Yes. Yes. No. That's that's a great point. And there's a couple of links here, I think, that may help discuss this concept further of lazy data. There's the r packages book by Hadley Wickham and Jenny Bryan, then there's the also also the Writing R Extensions book, which I think is is more sort of authored by some of the core R developers or, you know, so I think from that perspective. So those might be two good resources if you're interested in learning a little bit more about lazily loading data in packages.

I love lazy logic that checks to see if something ever needs to be rerun, and that's sort of the concept of caching, right, in a in a broad sense. And the authors here give the example of the lazy argument in the package down build site function, which if that argument is set to true, it will only rebuild articles and reference pages if the source is newer than the destination, which makes a whole lot of sense and can save a whole lot of time depending on how big your project is. And that's something that I have to talk to a client about today because we have a GitHub action that is taking way too much, way more time than it needs to take.



[00:47:58] Eric Nantz:

I feel seen about that. Absolutely. Yep.

[00:48:01] Mike Thomas:

I digress. Similar concept with the lazy test package that helps you only rerun tests that failed during the last run. And the last example here is regarding regular expressions. I had never heard of the terminology lazy being applied to regular expressions, but if your regular expression is finding all matches, of of whatever pattern you're looking for, that's considered eager. And if it's only finding the first match or the fewest number of repetitions as the authors define it here as possible, then it's considered to be lazy.

And in the example that they provide, the question mark character in the regular expression is what adds this laziness. So ton of examples here, really, really interesting blog posts. I think it's it's always interesting, you know, whatever these authors put out. It's some neat perspectives that maybe we don't think about or or have on a day to day basis. And, I would say that if you you didn't get it already, there are a lot of different definitions around laziness when it comes to programming and and our programming, especially.

They did omit one definition of laziness, which is the one that that takes place when people just copy and paste code from ChatGPT and don't even look at it before incorporating it into their project or repository or even worse, pushing it to production. That's bad laziness as opposed to a lot of good laziness that we were talking about today. But Context is king as I say. And, yes,

[00:49:32] Eric Nantz:

we've we we both have had experiences where that's happened, and we're like, oh, boy. Is this what we're in for now? Just my 2¢. Yeah. Yeah. Yeah. But, I I think it's a it's a viewpoint that's shared with a lot of people. But, yeah, lots of lots of great additional, you know, links in this post to dive into each of these in greater detail. As I said, I'm really intrigued by the duck pliers approach to this because I've never seen it kinda try to total lines. See both eagerness and laziness depending on the on the way you're interrogating that. So I'm gonna do some homework after the the show about that because I'm trying to up my duck DB, power here, so to speak, after that great workshop I took back at Pazitconf last year. I'm all in on that train. And and, yeah, in this case, lazy is definitely not a bad thing in many of the many of the approaches here.

And what else is not bad is our weekly itself. I would dare say we're not lazy in terms of how we curate the issue. That is very much, an eager evaluation in a good way. Normally, we do our additional fines. We are running a bit low on time, so we're gonna close-up shop here and, again, invite you if you wanna help contribute to the project. The best way to do that is with a poll request through our weekly itself and the upcoming issue. If you found that great blog post that maybe spurs up a lot of discussion in the community like we had in Ari's post or a great technical deep dive or a great way to use a new r package out there. We're just a poll request away. All marked down all the time. The template's already there. Head to rweekly.0rg for complete details on that. And we love hearing from you on the social medias. Great shout out to those that have gotten in touch and send us some good things on on social media.

But you can find me. I'm now on Blue Sky, where I'm at rpodcast@bsky.social. I'm also on Mastodon where I'm at rpodcast@podcastindex.social. And I'm on LinkedIn. You can search my name, and you'll find me there. And, Mike, where can the listeners find you?

[00:51:35] Mike Thomas:

Sure. You can find me on blue sky at mike dash thomas dot b s k y dot social or on LinkedIn, if you search Ketchbrook Analytics, k e t c h b r o o k, you can see what I'm up to lately. Very good stuff. And, thank you again. We made it. In our 50%

[00:51:54] Eric Nantz:

workflow, we somehow made it. So that's why having a co host is a really good idea in these times. So nonetheless, we will close-up shop here for our weekly highlights rep. So hun nine 96. Yeah. We're far away from 200 folks. It's coming up soon. And we'll be back with episode a 97 of our weekly highlights next week."
"23","issue_2025_w_07_highlights_638751064633861944",2025-02-14,53M 56S,"An illuminating set of tips for making the best out of the phrase ""fifty shades of grey"" in your next monochrome visualisation, how the unique formatting features of the Scotland census data were tamed with the power of R, and how the first-ever native mobile application powered by R has opened the doors wide open for innovation across many parts…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 95 of the Our Weekly Highlights podcast. This is the weekly show where we talk about the great highlights and additional resources that are shared every single week at ourweekly.0rg. My name is Eric Nance, and thank you for being patient with us. We are a couple days away from our usual release because, you know, as much as we would like to control our schedules, sometimes things just fall in our laps and and certain groups just need our attention those days. So that happened on me on our usual recording day, but we're back here. And I say we, of course, because I am joined by my awesome cohost, Mike Thomas. Mike, how are you doing this fine morning? Doing well, Eric. Yeah. We're a little later in the week this week. And for once, it's not me, but I you're helping to helping to balance it out. So I appreciate that. Yes. The the yin and yang are are starting to balance or the force, if you will. So that is it'll happen, and it may even happen next week too. We'll figure it out. But nonetheless, we are we're happy to be back here on on this day, and our issue this week has been curated by another one of our OG curators on the team, Jonathan Carroll.

As always, he had tremendous help from our fellow, our weekly team members, and contributors like all of you around the world with your poll request and other suggestions. And we're gonna visit the, visualization corner right away on this episode of our weekly highlights. And in particular, type of type of technique for visualization that may seem retro to you, especially when you think about how you used to print paper documents back in the in the days of dot matrix printers and not so much colors available in those printers.

So this post is coming to us from Nicola Rennie who is talking to us today about alternatives to the typical color palettes that you might be using in your visualizations in R. Now the first question is, why would you not want to take advantage of colors in your visualizations when you wanna share that with the world or share that in other means? Well, there may be cases where you don't really have a choice. One of those being that in the world of academia or even just other academia in general, you may have requirements from the, public publisher that nope. No color plots. Gotta be monochrome.

Black and white, basically, or what monochrome is technically speaking is it's different shades of a single color. So typically, we do think of that as gray. The different spectrums are gray. Right? And so there are a lot of publications that require that right off the bat. Although there is another side benefit to this that you may not realize when you're going down this route is that this could be a win for accessibility as well if you structure it the right way. And I think what Nicola's, advice here will get you along the way to do that.

Now you might be thinking, okay. Now I have to do the monochrome plot. You know, there is an easy way to do this. Right? You print the PDF and you could choose either color or black or white. Right? And that will just convert everything to that that black and white spectrum. Well, in the first part of the post, Nicola talks about that's probably not a great idea because you're losing a lot of the nuance in these different colors especially if they're closer together via what's called saturation. And she has a nice visual, again, we're audio here. We'll we'll speak of the visualization, horizontal bar chart. And so in this legend of the visualization, she colors the bars by by transmission type, But then when you look at the the black and white converted version of it, you cannot tell really at all the difference between at least two of these lead these items in the legend, which of course is really bad if you can't figure out which color is going with which. So that means yes. You're gonna have to, you know, code this up like you would with any typical visualization.

So your next bet is to look at the different palettes available and of course one of these packages that you know is one of the mainstays for looking at palettes is the color brewer and by proxy the our color brewer palettes and you might be looking at those And then she links to a website, for color brewer where you can look at which palettes are what they call photocopy friendly, meaning that if you were gonna scan or literally copy this this document that has a plot in and it was gonna convert the black and white, which palettes are actually more amenable to that. So she does show in the next example using, the color brewer package, which, pallet, it's called set three, where at least in this time, you're getting a closer to the right direction with legend colors that do look somewhat different. Although I'll be honest, the two grayish ones are so hard to tell on my screen which ones are a movie different, but it's it's it's a small step to get there.

But you may wanna take it a step further and really think about you're in this monochrome paradigm. Maybe there are palette types that are more better suited for this world that you're you're about to embark into. And taking a step back, what are the types of color palettes that we typically use? One is sequential where you have more of a gradient or a gradual decrease or increase in the shade of the color. So in this case, thinking of the a darker color to a lighter color. You also could add diverging palettes where it's like the extremes of those scales are very different, but yet the middle, you know, is kind of in the middle so to speak, kind of blending them together.

And then you have the discrete type of scale where it's just there's no real ordering to it. It's just different colors for each category. Now which one of these is better suited for the monochrome type of visualization? Well, when you look at the sequential palette, it's actually not too bad because you can see, you know, the lighter color being at the lower end of a of a of a continuous type of legend all the way to the darker color. It it actually does translate pretty well. So she's got an example in this case of using, I believe, the highway mileage on the scale there, and the lighter color is the less miles and the darker color is the higher miles on a dot plot. And at least you can see the darker dots compared to the the lighter dots. Is it perfect?

Yeah. Your results may vary there. The ones that definitely does not translate well was divergent palettes because when you think about those different extremes in the middle, how do you really convert that well because the direction might get lost in translation in terms of those those different conversions. But also another difficult one is the discreet palette because you never know based on the choices you chose for those colors which ones are gonna translate well to that conversion. And in fact, that little resource we just told you about the color brewer palette that look up, they said, Nicholas says there's really only one that's photocopy friendly. So you're kinda stuck if you're still searching for that. I'm maybe trying to guess which ones are are really best suited for it.

So she recommends that maybe starting your design of that plot with the monochrome world in mind you might then start to choose different colors that are visually different enough than when you do that monochrome type conversion, which in g g plot two, there is a scale gray variant of functions that will give you that gray, palette converting that from that color palette. And she does another example where you can see the three different legend, items for, in this case, transmission again is like going from almost black all the way to very light gray with a darker gray in the middle and you can see that really pop out in that visualization.

But again, it definitely takes attention to detail to making sure you're picking those colors that are distinguishable enough for that conversion to really have a have a good play there. But you also have to keep in mind the type of chart you're making. This works really well for a bar chart where, obviously, the bars themselves have a lot of area that are being spent or being taken up in that overall canvas. Whereas for the dot plots, obviously, your eyes are at the squint a little bit to look at the size of these dots, and it may not be so obvious to see these distinguished colors even if they look really good in the bar chart too. So you've got to think about the type of plot you're doing that should inform you alongside just having, you know, the monochrome world in mind or the framework in mind which type of colors you want to use for that conversion.

But one thing to open your eyes about is that you don't always have to stick with just colors. When you're in the monochrome mindset, you can take advantage of other features in the visualization to really make your visualizations pop. And I think, Mike, she has some really great advice on the different types of patterns you can use in these visualizations.

[00:09:56] Mike Thomas:

Right? She does. And this took me back a little bit to r one zero one. I think probably using the base plot package. And I don't know if this is the case for everyone or for a lot of maybe the the older folks listening, but I feel like my introductory or plotting, knowledge or classes were filled with using shapes as points. And I remember that quite fondly, and I have honestly, Eric, forgotten that that's even possible. I don't think I've used a shape, and and this is terrible. I'm so sorry, Nicola.

I I don't think I've used a shape in a plot in a long, long time in r. But it's useful to remember that that's even something that's possible to do. And and Nicola mentions two different packages here, and I think that they can be really handy, especially as we think about the monochrome world. The g g pattern package and then the fill pattern package. I think the g g pattern package allows you to leverage, like, texture and shapes, within your g g plots specifically. And again, this is audio, so it's it's hard to do. But it looks like, on on a bar chart example, you can have, you know, one section of the bar have sort of polka dots in it, another section of the bar representing a different discrete, class in a discrete variable has, diagonal lines through it. The other one looks like plaid as well. So it's an interesting approach to getting away from leveraging color to represent, different classes in a discrete variable to actually representing patterns. And it's, it's really, I don't know, really creative to me, to see and and really interesting. And honestly, it does make it very easy to discern the different categories.

I think maybe for folks who aren't traditionally used to living in the monochrome world, just one thing that you may have to to watch out for. If I was giving this to, you know, as a deliverable to a client or something like that, I think that they might be distracted a little bit the patterns, just because it it does look a little bit old school. But I I think if you work with them to communicate the reasons why you did so, and, understanding that, you know, the deliverable should be monochrome for reasons x, y, and z, I think that this is a fantastic approach to consider as well just to make it really easy, for the eye to be able to tease apart the different categories that are at play here.

And, I think the fill pattern package allows you to do the same thing, but it it will work with base r graphics as opposed to the g g pattern package is very specific to g g plot. You know, one of the other considerations, that Nicola sort of has as a theme throughout this blog post is, as you work with monochrome palettes, it may sort of come to light that the plot that you're using, whether it be a scatter plot or a bar chart or or whatnot, may actually not be the best way to represent that data in the monochrome world. And you might actually want to consider, you know, switching to a different type of plot. I I think in the the diverging palette example, right, where it's it's very difficult to represent a diverging, you know, gradient in a monochrome world, she recommended potentially switching to a lollipop chart so that you can actually, you know, measure the magnitude, in terms of change in a different way that would allow for, you know, a monochrome legend to make a whole lot more sense to the end user. Another gotcha that she mentions to to watch out for is in the case of missing data. And she uses the example of choropleth maps, where I think, Eric, I'm sure you've seen this before, but perhaps in a in a state in The US, if if we have a choropleth map, where each state sort of has its own color, sometimes if a state doesn't report data, we'll show that state as being gray as opposed to all of the other states having a particular color. And if you're trying to represent missing data, obviously, in the the monochrome world, gray is not going to work. Right? Because your Exactly. Your color scales are all different shades of gray. And I think that's another case where you're going to have to think creatively about different ways to potentially, showcase the data, that you're or make the point that you're trying to make, with maybe a different chart where it makes more sense to to leverage, you know, your monochrome color palette in a more effective way for others. So a lot of fantastic tips here. Some links at the end of the blog post as well. And I I appreciate, Nicole, not just walking through, you know, the examples of how to apply this, but also really taking the time to to critically think about, you know, all the considerations that you have to take into account when you leverage these types of approaches, really thinking about it holistically.



[00:15:02] Eric Nantz:

Yeah. And, as always, I like to or maybe don't like to date myself so much on this very podcast. So last time I did patterns, like, we're seeing in the in the bar chart example here, was for my dissertation way back in 02/2008, folks, because we've thought, well, okay. It's gonna be published at some point. Swagger I never did. But, nonetheless, for the requirements for the grad school, we did have to monochrome prints. And I remember for these bars, I was like, oh, no. How the heck am I gonna distinguish these different, you know, disease types with the competing risk, you know, output?

Oh, patterns. So, yeah, I got the retrovise when I looked at, the example of Nikola put together here. And overall, yeah, excellent advice here. You you go through the full spectrum of what you might try first all the way to, like, the real principles that she feels we we should all have in mind when we're in this in this, you know, tunnel vision approach to monochrome. Other techniques that she mentions here that can help is if you have the ability to facet your plots, that can help pop out the differences even further in the case of, say, the dot plots that we were seeing in the examples and annotations as well. If you got room to put labels above, like, the bars as well to distinguish those different categories, go for it. Obviously, you have to balance between how busy it's gonna look versus how presentable it is, you know, without it. But in the end, you've got you've got ways to account for these potential ambiguities.

And she does link to more additional resources that she's used to, build this blog post. So lots of great reading if you're if you're find yourself in this space. But I think overall, it's another great testament to the power that we have in visualizations and are to think of multiple perspectives and also think about accessibility at the same time. So really valuable post that's going in my bookmarks for visualization to to take a look at in the future, because you never know if if I ever get back in the publishing world again. Who the heck knows what that publisher is gonna require for us?



[00:17:11] Mike Thomas:

Totally. And I feel like, Nicole is always covering almost forgotten topics like this. So huge props for for tackling this one.

[00:17:34] Eric Nantz:

So as we were just talking about, things can get a little messy when you're dealing with certain types of visualization palettes unless you have the right frame of mind. Well, you know what else can be messy? Data. Because in the real world, we don't get those great textbook examples, right, where everything's just neatly tidied and, like, 20 observations, 20 rows, and no missing values, and everything's rectangular annotated correctly. No. No. No. In the real world, we have to deal with a lot of interesting, sometimes downright confusing formats of how these data are populated.

Our next highlight here is taking us through a journey on how we can do this processing applied to real world type of data coming from, census data at Scotland to be exact. So this post is authored by John Macintosh. John is, has been around the art community, I think, for many, many years. I've seen his name quite a bit. He is currently a data specialist and package developer at the NHS in Highland in Europe, And he talks about motive. The motivation for this post is that he and his team were working with census data from 2022 from Scotland.

And they do to their credit, give you a way to download the data. Again, package it up as CSV files and a large zip archive so you can grab that. He says that once you, you know, extract that out, you got 71 files with around 46,000 rows and, you know, a highly variable number of columns. So already, these files may have some inconsistencies off the bat. But wait, there's more in terms of how these are formatted. So buckle up because this this may, may give you some flashbacks to any, messy pre processing days that you've had in your data science journey.

First, there are three rows at the top of these CSVs that have some you might call metadata about these data files, but they're useless for your analysis. So a, they don't want to be imported. You wanna ignore those. But then at the end of the files, there's about eight or so rows that contain additional text or metadata that could also be discarded. So your data is kind of sandwiched in these these, messy rows. But then once you, you know, wipe those out, you may have file main files will have headers in multiple places.

Yikes. So then they have to be combined somehow because it might be telling different information about the columns. And then wait for it. There may be different types of delimiters being used in these first few rows. Oh goodness. Oh, the I I'm already getting triggered just reading this. But as John says, if you only had a couple of files here, you know, sure, you can manually account for that. But when you're talking about the volume of files that you have in this download here, of course, you're gonna have to take a programmatic approach and cross your fingers that you're able to figure this out.

So he walks us through his journey and trying to import these in. His first step is something that I would have tried as well. They have two reads or imports of the of the data per file itself. You first read it in as a as like a temp, you know, say data frame or whatnot or a temp file, And then he was going really low level with this on the second read and using the scan function to try and isolate that output area between those junky rows at the top and the bottom. And then that that way he would track where the actual data begins and then try to figure out where the actual header is in that in that chunk.

He leveraged the Vroom package, v r o o m, that I've seen, I think is authored by Jim Hester years ago that gives you great performance in importing textual data files because that does have a parameter for skipping you know certain rows and then, you know, suppressing the need to get column names so he thought well there but there was unfortunately on top of those parameters he couldn't figure out a good way to know how to eventually skip without doing that prior scan. So he was hoping Vroom could do it all.

Not quite. But then he went back to data dot table which is again has been highlighted quite strongly in in previous episodes which comes with a function called f read which already gets a lot of praise from the community for being a highly performant way to import large textual data files like CSVs and whatnot and sure enough you can, set header to false and then data dot table was, you know, in his opinion, intelligent enough to snap out those junk rows or exclude those junk rows from the beginning, but still was left in the multiple header issue. And then he's got some code snippets where he tries to figure out again that actual data area that rectangular area in between the the junk headers a little bit of grep magic inside, you know, data dot table calls And then to account for the number of header rows, he's got a use of the tail function to try and strip out those extra rows.

And then to be able to combine the header rows together, but then make a intelligent vector that actually consolidates their information that could then be used as the column names. But then it gets even more interesting here. He has to pivot it from the wide format to the long format, grab the values themselves, and then making sure that if there were any messy values or say hyphens or other weird characters that there were they were stripped out, and then it would become a numeric result because I believe it was mostly numeric data coming from this census.

And then he had to, again, make great use of data dot table set functions, other snippets of code that are in this post. And, yes, there is a GitHub repo that has all disassembled, which I ended up having to read before the recording here because I couldn't believe just how complex this was. Like, this is not for the faint of heart, yet data dot table was quite valuable, to do this. But he wanted to take this even more and optimize, format away from the CSV once all this messy stuff was accounted for. And this is gonna, you know, make my data processing, mine light up here. I want to convert this to parquet files and throw these into DuckDb because why not? This is a perfect use case or something like that so you can take advantage of a more efficient file format and to be able to process just what you need.

So there is a lots of benefits of that approach, but in the end, he wraps the post up with some practical tips if you find yourself in this situation. One of which is don't do what I just did when I first read these data files and I was dreading any code I'd have to write. Take a step by step folks. It may seem insurmountable when you look at one of these as a whole, but if you take the approach of, okay, how do I deal with these headers? How do I strip out the junk at the end and then figure out that data area?

Then you're you're kinda breaking up into components. Right? You can't just boil the ocean all at once as I say. And that's what leads them in in a second point. You may think that if you just want to throw all this in a map call where it does each data file in an iterative fashion, may not always get working at that time. So again, really isolate on a few different use cases and then scale up after that. Speaking of per itself, an underrated function that is great in these situations when you can't fully expect reliability when you're using that function is the safely function.

You can use safely to wrap that utility function to do your processing or do your importing. And in that way, it doesn't, like, crash the rest of the script. You can then parse after the fact what the error was or just ignore that that stuff altogether if you have other means to account for it. And then also, the base r, you know, installation itself has a lot of great string functions. I admit I grab string r and string I every time I do string processing. But if you're really thinking about performance or minimizing your dependency footprint, it may be worth the investment and time in learning about the grep, g sub calls that come in the base r. Yeah, the syntax can be a little, little hard to get at first. But if you practice enough, I think you'll get the hang of it. But, again, John's got a great, you know, repository setup where you can see just how he uses those, those base string functions in his call.

And, apparently, he says that after the fact of writing this that there is a way I believe this comes in either Excel or other Microsoft products called Power Query. I've never heard of this before, but, apparently, it can help with these messy imports. Do you know anything about this, Mike? Because I haven't heard about this. You're lucky if you've never heard of Power Query, Eric. Okay. Great. I'll tell you. Keep it out. That. Okay. I will keep it that way. The day job has not forced me to use it, and, I don't plan to. But, nonetheless, he says that could have made things easier.

I admit, I don't know if I wanna rely on a proprietary tool to make that easier. I think if you can script it out, future you and future reproducibility will thank you for it, but it's good to know that there are alternatives in this space. All in all, a good reminder that, a, the real world is never as perfect as the textbooks would like you to believe with these data formats. And b, you can really augment a lot of great functionality from data.table along with some of the base r string processing functions.

And if you take a step by step, you can get to where you need to take advantage of the fancy stuff, like ducted b and parquet down the road. So very enlightening post, and, hopefully, I never have to encounter anything as messy as what these census data files presented to John here.

[00:28:16] Mike Thomas:

No. It's a big effort to undertake something like this. I know because I've done it before, and I'm sure a lot of folks listening have done the same thing before as well because the way that data is published sometimes, publicly available data specifically, can be crazy. And this is not an exception, and I I'm sure that all of the folks who are interested in working with the Scotland Census data, hopefully, they see this blog post. Hopefully, they catch wind of the location where, John has landed the data in DuckDB, and it's all clean and easy to pick up.

What a great what a great use case. And, yeah. Unfortunately, it seems like this particular project itself had a little bit of everything in it in terms of everything that he he was up against. I really appreciate that the top tips there, the use of PRRS safely to make sure that, you know, if you are looping through something that you can, your your loop can continue or take an action if it runs into an error. I'll be honest, I'm I'm fairly guilty of using per safely inside, like, an if statement, as opposed to using probably the more appropriate try catch type of function. But it's it's just so easy, unfortunately. I I was really actually just doing this earlier this week on doing a little data archiving, data rescue for some US, government program data for a client that we weren't sure if it's gonna continue to be around or not. And I was, downloading it and sticking it in an s three bucket. And some of these datasets, that in the data dictionary said were available when you went to the URL, they were actually not there at all. So purr safely, you know, saved me quite a bit and allowed me to to loop through everything without having to change, my my iterator, if you will.

And, you know, the last tip there that that base string functions are are very useful and overlooked. I I couldn't agree with that more. I think, you know, for those of us that have to clean up messy data, string manipulation, you know, like grep and, you know, regular expressions and, you know, the what we get from the the string r and the the string I package. Although I can't say I use string I too much. I think a lot of that functionality has been mapped into to string r, are are absolute lifesavers.

Sometimes it's tricky to to get that regular expression pattern just right. I'll be honest, ChatGPT has helped me expedite that process quite a bit. So that would be my tip if you're struggling with regular expressions at all. Try to look there first because it might it might take care of everything that you need for you without having to figure out wild cards and placeholders and, you know, length of characters and all sorts of different crazy stuff like that that happen in regular expressions where the syntax looks really weird, but the power is absolutely incredible. And when it all works, it is so satisfying.

So, I think a big thanks to John for his efforts under here and documenting his efforts on this particular project and a a great blog post because I think it's something we can all relate to.

[00:31:26] Eric Nantz:

Yeah. That reminds me my very first use ever of Chad GPT was indeed for regex help because I was like, I could do the whole stack overflow thing, but wait a minute. All these people are talking about that. Let's give it a shot. And, yes, in that case, it worked immensely well. But, yeah, I think, you know, in the current climate, you may find yourself in a situation where you have to grab this data sooner than later from sources. So never never hurts to have these techniques available to you. Now getting back to some of the work I'm doing at the day job, we're working with a vendor who is giving us CSV files of certain event type data.

We've given them the requirements of how we want this data to be formatted. Guess what? They don't always follow that, so we've had to build an internal package to account for those things. But we are hoping that they give us API access to the raw data so then we can have a more consistent, you know, might say reliable pattern of what the data is gonna be represented at because I I I feel more comfortable handling some JSON coming back from an API of certain datasets than a cryptic CSV that may or may not have a header, and then it may or may not have the right columns even spelled correctly.

Yes. This happens when you have manual effort from people copying from one system to a CSV that goes through some stupid web portal, and then we have to be the ones who consume it. I'm not bitter at all, but it but it happens, folks. So if you get that chance of leverage an API, take advantage of it if you can.

[00:33:03] Mike Thomas:

Absolutely.

[00:33:15] Eric Nantz:

Well we are gonna in our last highlight here call back and an initiative and a huge development in the world of shiny that have both Mike and I absolutely giddy with what's possible in this new world we find ourselves in of taking advantage of web assembly with R and Shiny itself. And that this last post comes to us from a fellow curator of our weekly, Colin Fay, who, of course, is a brilliant developer and data scientist. I think our author of one of our favorite bar packages in the entire world, Golem, as well as other great innovations in the world of shiny.

He wrote a post on the think our blog about talking about the recent mobile app that they released late last year or early this year called Rlingual, that took the shiny community by storm in a in a great way and this post is taking a step back about why did they actually do this. So to give a recap, first check out the back how long we when we talked about this, this great effort in detail but in a in a very quick recap here, Rlingual is a, an actual app that you can install on your mobile device via the Play Store or the Apple App Store, for iOS.

And you can, in essence, take a little quiz about your knowledge about R in this very responsive greatly themed, again, installable application that runs completely self contained on your mobile device that wraps R under the hood via web r and web assembly. I am super excited about this. Those of you who listen to this show for a bit know that I've been on a journey with web assembly and some very important external collaboration. So anytime I get to see WebAssembly in the wild, I am all for it. But Colin talks about is, again, why what is the big picture here?

Why is this so important to the community at large that want to take advantage of R on a mobile device. And the the big takeaway here is that having that capability to be mobile and have the power of R at your disposal is a huge benefit across many different situations. So he walks through a few of these, in each case I can relate to in different aspects of it. One of which is what if you are we were talking about data earlier. What if you're in the field? What if you're in the trenches to grab this data and you need a way to record it on the spot? Maybe you're on location somewhere.

Who knows? You may be in a in a mountain somewhere. You may be in the rainforest. Who knows? But you probably will not have a reliable Wi Fi or Internet access in these remote locations. Right? But yet having a self contained app on your device who can help you track that data and maybe leveraging our to do some processing or some other storage of it absolutely is a massive benefit so that, again, you can run this in a completely offline kind of mode for that. You improve your efficiency, bring the data closer to your actual end product, really really helpful.

Number two, a great way to learn, again, in an offline fashion. Think of when you and I were in school, Mike. Wouldn't it have been great if we are ins if we had the technology now that back then, you know, we had to read textbooks. Right? We had to take notes and hope that we could run it on a maybe a an old Windows installation or something like that and hope that everything just works. Or in the case of my grad school, SSH to a server without knowing what the heck r was at the time. But imagine having this on a mobile device where you can learn about a key concept, about maybe the central limit theorem or maybe some other, you know, very important statistical concept.

And you can learn this wherever you are and and explore it, But, again, not have to be at your computer or have a textbook open to do it. So it can be an interactive learning device, which, again, was very similar to what they did with this quiz app that they worked on. It was completely interactive, but completely offline as well. It had everything self contained. Really, really novel use case. I think education is gonna education is already taken advantage of web assembly, already have a lot of resources.

We speak highly about the quartal live extension where you can embed web assembly powered apps into a quartal document. George Stagg and and the quartal team are doing immense work in this space to have this reimagination of the learn our package with a new way to leverage Quartle. Lots of great potential here on the mobile space as well with WebR and WebAssembly. Then when you think about other industries where you need real time feedback really quickly, he Colin calls out a couple other use cases. One of which is having real time quality control when you're in the manufacturing space. Maybe you need to inspect something that's going through a production line. You need to enter some feedback. You need to check everything's working.

You can look at quality control metrics on the spot based on some inputs you give it, Very quickly detect that something's going wrong because those are hugely important in manufacturing to see when things are going wrong, when things are being produced at scale. And then also, rounding out the posts are other considerations such as managing your supply chain and making sure you're optimizing that based on the logistics that you're dealing with. Maybe you have to run some models on the spot based on certain parameters, and that might change the way you send products out. May you distribute to warehouses or whatnot.

That could be quite helpful and going back to being on the field so to speak in the healthcare industry maybe you're again helping out you know with a huge unmet medical need in, like, a clinic, say, somewhere remote in Africa or some other region, and you're gonna need to have to look at maybe inputting patient characteristics and getting some kind of score out of it to determine their next treatment plan. So I'm seeing in the world of diagnostic type of evaluation, this could be a huge role. Who knows how long that will take because, obviously, I come from life sciences. Things don't exactly move at a breakneck pace, but I do see this as a potential for those in the field to take advantage of r to help with some of that evaluation and some of that processing.

So you now have at your disposal a way to run r on your mobile device and all sorts of different use cases. I don't think Colin, gaming begins to cover all the different possibilities here. But what think r has proven is that with the right initiative and with the tools available, you can enter this space and probably cause some really innovative breakthroughs with r at the back end. Am I here for it? Oh, abso freaking lutely I am. This is awesome.

[00:40:45] Mike Thomas:

It is awesome. It's this feels like this Rlingua project has a lot of gasoline on it. And, I think as soon as somebody drops a match, this is going to catch fire throughout the whole our ecosystem. I I think it's it's just new at this point. And as it continues to gain more exposure, I see it really exponentially taking off pretty quickly. It's it's absolutely incredible breakthrough. As you were talking about thinking, about back in, you know, when we were in school having to read textbooks and things like that, I know that I don't think it's a big secret here, but maybe at least in The US, I know a lot of teachers in, you know, high school and things like that, and and probably college as well, struggle with kids being on their phone all the time, right, and being distracted by that. So if you can actually bring the learning to the phone, then they can sit there with their phones and and learn at the same time. And you're, I guess, putting the education in the place where most of their attention is. So maybe this is practically helpful in that respect. I think that's that's pretty interesting. I also know that in, you know, more professional context in manufacturing settings and health care, as you were were saying, that there are actually, you know, companies out there that that make these almost cell phone looking devices to monitor sensor data and to monitor, you know, you know, just just really like data analysis, little handheld tools that are either, you know, smaller versions of an iPad, but they're they're these customized pieces of hardware.

And I think we might just be able to take, you know, the functionality of those pieces of hardware and leverage, you know, your smartphone, by, you know, using this Rlingua project to to visualize this data. You know, we have the power of WebAssembly here. We have power of DuckDV Wasm. And, I think probably all of that number crunching as it gets better and better and better, and it already is fantastic, can probably perform just as well on a mobile device, a smartphone, if you will, as opposed to, you know, some of these customized pieces of technology that I I think are probably getting a little obsolete and a little outdated, compared to what we have right now. And you don't need to find somebody who's a Swift developer to do it. This is my, like, favorite thing of all time because we have clients who ask about, hey, could this be on a mobile app? I was like, yeah, well, we're gonna have to go hire somebody who knows how to write iOS, you know. I think the language is called Swift, if I'm I'm not mistaken.

Yep. And, oh, and then Android too. Who knows what what language that is? Good luck with that SDK folks. I'm just saying. Exactly. So if we can avoid that, it's absolutely incredible and use a programming language. I think that's a little more literate like our to do our work. I think it's gonna be absolutely, you know, incredible. So a lot of potential benefits here. I see this as being a huge breakthrough that is really only a matter of time before it absolutely takes off.

[00:43:51] Eric Nantz:

Yeah. Again, when I installed our lingua even in its preview mode when when Colin gave me the heads up that this was coming out, That was my first takeaway was that I could not tell that this was an our thing. Right? And not just the looking of it, which, again, we have great tools in Shiny in general to make things not look like a Shiny app. But the performance was no lag whatsoever. It literally looked like or performed like it was built by a professional vendor that's been doing mobile development for who knows how many years. Who knows? It could have been in Swift or, you know, Objective C or whatever the heck ever languages are being used in that or Java itself in the SDK.

So this just enables so many possibilities here. I'm still wrapping my head around it. I mean, I'm taking baby steps of WebAssembly. The fact that we're able to submit a WebAssembly power shine up to the FDA is still mind boggling to me that we've pulled that off, but that's just a tip of the iceberg. I think where this can take us, in the world of life sciences, but also as as we talk about here, many, many different industries too. The one thing I just wanna make sure people realize when they're exploring WebAssembly for the first time, especially with the power of WebR and Shiny Live.

And again, it's more for awareness. I'm not trying to be a Debbie downer here, but if you do interact with online resources in a API call, probably not the best idea because in theory, when you bring all this stuff down, all that stuff, all that stuff for authentication is in the manifest, if you will, the bundle of code that is being translated. So that's just something to consider if you're gonna build something like this that does in fact interact with an API layer. But like I said, with Collins use cases here, these being built in a self contained way that have no reliance on off on online access to actually power the machinery behind it, you're good to go there. I just wanted to make sure that I throw that out there because I often get people asking me, why isn't that thing in WebAssembly that I'm building?

I would love to, but if I have to interact with an online service, yeah, that that that's one little gotcha. But maybe that gets better someday. Who knows?

[00:46:05] Mike Thomas:

Yeah. It's a great consideration.

[00:46:07] Eric Nantz:

But but all in all, still very inspiring. And also, you know, to be to be honest, you should be inspired by almost everything in this issue here. John is, curated an excellent issue here. He he always does a fantastic job. So we'll take a minute for a cup or a couple minutes, I should say, for our additional finds here. And speaking of huge improvements in the ecosystem, I do have a soft spot because of my day job and my research areas and my career of high performance computing with our and one of the very, solid foundations that has come to light in the last few years.

The Mirai package, author by Charlie Gao, has had a substantial update in the recent version two dot o upgrade. This is a here's your least for for Charlie and and his, team of developers, And he has a great blog post. I'll mention a couple of great additional enhancements here, especially if you find yourself leveraging cloud resources as your high performance computing backend, talking like AWS batch or other resources. He's got a much easier way to launch all those background processes via a more clever use of SSH under the hood to do that. And by proxy changing the protocol for bringing the connection information back and forth from WebSocket layer. Now it's using TCP level, which basically is a lower level way to communicate with these online resources. It's faster and in his words even more reliable.

And the other one that caught my eye too is we are leveraging this within a Shiny app, whereas what Charlie's done in collaboration with Joe Chang brought Mirai integration with the new extended task paradigm and Shiny itself, you now have a much more elegant way to cancel that workflow in a Shiny context if you decide that, oh, wait a minute. That's not what I wanted to run. Get me out of here. There's a great way to implement that canceling functionality, and he's got linked to, I believe a vignette article where it talks about that integration in great detail along with some great interaction with the per package as well because now per is in the development version providing more parallel mapping capabilities, and not Mirai can be a great back end to power that and not just say the future package like it was before then. So great updates of version two, Amirai, and I'm super excited for what Charlie has in store for future releases. It's a wonderful package here. Yes. Absolutely. All that

[00:48:52] Mike Thomas:

asynchronous possibility, I think, is is really incredible in terms of what we're able to to push the envelope on. So my takeaway here is a, blog on the epiverse site in the it's authored by James Imba Azam, Hugo Gruesome, and Sebastian Funk. And the title is Key Considerations for Retiring or Superseding an Art Package. And I believe the epiverse probably has a a suite of different art packages, that they they help work with. And I also saw a a recent, I think, post by Hadley Wickham, maybe on Blue Sky or somewhere like that, where he is authoring sort of a history of the Tidyverse.

And, I'm sure part of that history will include things like ggplot versus ggplot two. Maybe, you know, the the Reshape two package moving to dplyr and tidyr, and, you know, the sort of life cycles that these packages take when maybe, you know, you're you're hoping that users move to the new version, but you don't want to fully take away the old version, you know, even though it's it's pretty legacy and it's not maintained. But perhaps folks have, you know, legacy workflows that leverage your your old versions of packages. So there's gotta be a lot of decision points that you need to make, in order to try to accommodate as many people as possible. And I think this blog post is a really nice reflection on that that I wanted to highlight.



[00:50:21] Eric Nantz:

Yeah. This is a very, very important post especially if you find yourself developing a lot of packages, but yet you learn so much along the way that you wanna make sure that, a, you do have a way to take advantage of what you learn, but also not forget those that were early adopters of your previous packages. I remember my colleague, Will Landau, was wrestling through a lot of this too as he transitioned from his Drake package over to Target, because Targets, in his opinion, contains a lot of what he learned in the process of developing Drake and what he made better for for the Target's ecosystem. So I know he's thought about a lot of these principles too. So highly recommended reading, and I'll be I'll be watching, Hadley's, blue sky post on that too. And hopefully, he comes out with a interesting, article or whatever whatever knowledge is is shared there alongside this great post from the Epiverse team. Really, really solid find. And, again, lots of solid finds in the rest of the issue too, so we invite you to check it out. It is linked directly in the show notes as always.

And, also, you can find the full gamut of updated packages, new blog posts, new tutorials, and upcoming events, and whatnot. So lots of lots of great things to sync your reading, chops into. And, also, we love, hearing from you, as far as helping us with the project as a whole. Our weekly is a cure is a community project through and through. No corporate sponsor overloads here. We are just driven by the the efforts and even yours truly will have an issue with a cure rate next week. And I'm building a completely over engineered shiny. I have to manage our scheduling paradigm that I hope to talk about in the near future, but I'm doing it all late at night hacking if you will because it's not my day job, folks. But where we can have your help is finding those great resources. If you've seen it online, wherever you authored it or you found someone else that did, we're just a poll request away because everything's open on GitHub, folks. Just head to r0e.0rg.

Open the poll request tab in the upper right corner. You'll get taken to the template where you can simply fill that out in our Curator of the Week, which if you do this now, it'll be yours truly. We'll be glad to get you in that in that next issue, that resource. But we also love hearing from you, online as well. We are on the social medias. I am on blue sky as well at rpodcast@bsky.app or social, something like that. Also, I'm Mastodon with at rpodcast@podcastindex.social, and I'm on LinkedIn, causing all sorts of fun stuff there. Search my name and you'll find me there. Mike, where can they find you? Primarily on Blue Sky these days, in terms of social media,

[00:53:03] Mike Thomas:

non LinkedIn, at mike dash thomas dot b s k y dot social, or on LinkedIn if you search Ketchbrooke Analytics, k e t c h b r o o k, You can find out what we are up to, and we are still on the hunt for a DevOps engineer, for anybody interested out there who knows a little bit of Terraform, Docker, Kubernetes, and Azure. That's the stack.

[00:53:28] Eric Nantz:

Yeah. I know there's a lot of great people out there that are working with that stack and even now I'm trying to educate myself on some of those and it is a brand new world to me. So having that kind of expertise is always helpful. So if you're interested, get a hold of Mike. He'll be glad to talk to you. But with that, we will close-up shop with episode one ninety five of our weekly highlights. We thank you so much for joining us, and we'll be back with another episode of our weekly highlights next week.

"
"24","issue_2025_w_06_highlights_638743320595082837",2025-02-05,46M 47S,"Context is king in a trifecta of R packages harnessing LLMs to be your virtual assistant in package development and data science, plus the world (of data) is at your fingertips for data exploration and sharing your insights using the innovative closeread Quarto extension. Episode Links This week's curator: Jonathan Kitt -…","[00:00:03] Eric Nantz:

Hello, friends. We are back of episode 94 of the Our Weekly Highlights podcast. If you're new, this is the weekly podcast where we talk about the excellent highlights and additional resources that are shared every single week at rweekly.0rg. My name is Eric Nantz, and I'm delighted you join us from wherever you are around the world. And I'm always joined in this February, but it's still my same cohost, and that's my choice here, Mike Thomas. Mike, how are you doing today?

[00:00:29] Mike Thomas:

Doing pretty well, Eric. It was kind of a long January here in The US, and it seems like we're in for an even longer February. But happy to be on the highlights today, and, may your datasets continue to be available.

[00:00:44] Eric Nantz:

Let's certainly hope so. I will say on Saturday, I had a good little diversion from all this, stuff happening. I was with about 70,000 very enthusiastic fans at WWE's Royal Rumble right here in the Midwest, and that was a fun time. My voice has finally come back. Lots of fun surprises, some not so fun surprises, but that's why we go to these things so we can voice our pleasure or displeasure depending on the on the storyline. But it was a awesome time. I've never been to what the WWE has as their, quote unquote, premium events. It used to be called pay per views at a stadium as big as our Lucas Oil Stadium here in Indianapolis. So I I had a great time and, yeah. I'm I'm slowly coming back to the the real world now, but it it was it was it was well worth the price of admission.



[00:01:39] Mike Thomas:

That is super cool. That must have been an awesome experience. Luca Oils Lucas Oil Stadium, a dome?

[00:01:45] Eric Nantz:

It is. Yep. That's the home of the Indianapolis Colts. It's been around for about fifteen years, I believe now. Last time I was there, I was at a final four, our NCAA basketball tournament way way back when where we saw my, one of my favorite college basketball teams, Michigan State. Unfortunately, we lose the butler that year, but it was a good time nonetheless. So, we won't be weighing the the smack down on r for this. We are gonna we're gonna put over r as they say in the business, and that is an emphatic yeet, if you know what I mean.

Yeet. But speaking of enthusiastic, I am very excited that this week's issue is the very first issue curated by our newest member of the our weekly team, Jonathan Kidd. Welcome, Jonathan. We are so happy to have you on board the team. And as always, just like all of us, our first time of curation, it's a lot to learn, but he had tremendous help from our fellow r Wiki team members and contributors like all of you around the world with your poll request and suggestions. And Jonathan did a spectacular job with his first issue, so we're gonna dive right into it with arguably still one of the hottest topics in the world of data science and elsewhere in in tech, and that is how in the world can we leverage the newer large language models, especially in our r and data science and development workflows.

We have sung the praises of recent advancements on the r side of things, I e with Hadley Wickham's Elmer package, which I've had some experience with. And now we're starting to see kind of an ecosystem start to spin up around this foundational package for setting up those connections to hosted or even self hosted large language models and APIs. And in particular, one of his, fellow posit software engineers, Simon Couch from the Tidymodels team. He, had the pleasure of, of enrolling in one of Posit's internal AI hackathons that were being held last year.

And he learned about some, you know, the Elmer package and Shiny chat along with others for the first time. And he saw tremendous potential on how this can be used across different realms of his workflows. Case in point, it is about twice a year that the Posit team or the Tidyverse team, I should say, undergoes spring cleaning of their code base. Now what does this really mean? Well, you can think of it a lot of ways, but in short, it may be updating some code that the packages is using. Maybe it's using an outdated dependency or a deprecated function from another package, and here comes the exercise of making sure that's up to date with, say, the newest, you know, blessed versions of that said function and whatnot, such as the CLI package having a more robust version of the abort functionality when you wanna throw an error in your function as opposed to what our lang was exposing in years before.

It's one thing if you only have a few files to replace, right, with that stop syntax or our or abort syntax from our lang. Imagine if you have hundreds of those instances. And imagine if it's not always so straightforward as that find and replace that you might do in a in an IDE such as r studio or positron. Well, that's where Simon in as a result of in participating in this hackathon, he created a prototype package called CLIPAL, which will let you highlight certain sections of your Rscript in RStudio and then run an add in function call to convert that to, like, a newer syntax, and in this case, that abort syntax going from r lang to the CLI packages version of that.

The proof of concept worked great, but it was obviously a very specific case. Yet, he saw tremendous potential here so much so that he has spun up not one, not two, but three new packages all wrapping the Elmer functionality combined with some interesting integrations with the RStudio API package to give that within editor type of context to these different packages. So I'll lead off, Simon's summary here on where the state is on each of these packages with the first true successor to COIPow, which was called pal. This is comes with built in prompts, if you will, that are tailored to the developer, I e the package developer.

If you think of the more common things that we do in package development, it's, you know, building unit tests or doing our oxygen documentation or having robust messaging via the COI package. Those are just a few things. But pow was constructed to have that additional context of the package and I e the functions you're developing in that package. So you could say, highlight a snippet and say give me the r oxygen documentation already filled out with that that function that you're highlighting. That's just one example.

You could also, like I said, build in, like, CLI calls or convert those CLI calls from other calls if you wanna do aborts or messages or warnings and whatnot. And that already has saved him immense amount of time with his package development, especially in the spring cleaning exercise. He does have plans to put Pal on CRAN in the coming weeks, but he saw tremendous potential here. That's not all. Mike, he he didn't wanna stop there with Pal because there are some other interesting use cases that may not always fit in that specific package development workflow or the type of assumptions

[00:07:59] Mike Thomas:

that Pal gives us. So why don't you walk us through those? Yeah. So I'll there's two more that I'll walk through, and one is more on the package development side, and then one is more on the analysis side for, you know, day to day our users and not necessarily package developers. So the first of which is called ensure, e n s u r e. And, one of the interesting things about ensure is that it it actually does a couple of different things that PAL does not do. And PAL sort of assumes that all of the context that you need is in the selection and the prompt that you you provide it. But, when we think about in the example that Simon gives here writing unit tests, it's actually really important to have additional pieces of context that may not be in just the single file that you're looking at, the prompt that you're writing, or, you know, the highlighted selection, that you've chosen.

You may actually need to have access to package datasets. Right? That you'll need to to, you know, include in that unit test that maybe aren't necessarily in the script or the the snippet of code that you're focusing on, at the moment. So ensure, you know, goes beyond the context that you have highlighted or or is showing on screen and actually sort of looks at the larger universe, I believe, of, all of these scripts and items that are included in your package. And it looks like, you know, unit testing here is probably the the biggest use case for ensure in that you can, leverage a particular function, like a dot r function within your r directory.

And if you want to scaffold or or really create, I guess, a a unit test for that, it's as easy, I believe, as, you know, highlighting the text that you or highlighting the lines of code that you're looking to write a unit test for. And, just a hot key shortcut that will actually spin up a brand new test dash whatever, test that file. It'll stick that file in the appropriate location under, you know, test test that for those of us that are our package developers out there. And it will start to write, the those unit tests on screen for you in that test that file. And there's a nice little GIF here that, shows sort of the user experience. And it's it's pretty incredible, that we have the ability to do that, and it looks really really cool. So I think that's, you know, really the main goal of the insure package.

Then the last one I wanna touch on is called Gander. And again, I think this one is a little bit more, you know, day to day data analysis friendly. The, functionality here is that you are able to highlight a specific, snippet of text or it also looks like Simon mentions that, you know, you can also not highlight anything and it'll actually take a look at, you know, all of the code that, is in the script that you currently have open. And you by pressing, you know, a quick keyboard shortcut, it looks like, you can leverage this add in, which will pop up sort of like a modal that will allow you to enter a prompt.

And in this example, you know, there's a dataset on screen. Simon just highlights the the name of that dataset. I think it's the Stack Overflow dataset, but it's just like Iris or Gapminder. And he highlights it, you know, the modal pops up and he says, you know, create a scatter plot. Right? And all of a sudden, the selection on screen is replaced by a g g plot code that's going to create this scatter plot. And he can continue to do that and iterate on the code by saying, you know, jitter the points or, you know, make the x axis formatted in dollars, things like that. And it's it's really, really cool how quickly he is able to, really create this customized g g plot with formatting, with fastening, all sorts of types of different things, in a way that is is obviously much quicker and and more efficient even if you are having to do some minor tweaks, to what the LLM is going to return at the end of the day than if you were going to just, you know, completely write it from scratch. So, pretty incredible here. There's another GIF that goes along with it demonstrating this. It looks like not only in this pop up window is there an input for the prompts that you wanna give it, but there is also another option called interface, which I believe allows you to control whether you wanna replace the code that you've highlighted, or I would imagine whether you wanna add on to the code that you've highlighted instead of just replacing it, you know, if you wanna create sort of a a new line, with the output of the LLM. So really cool couple of packages here that are definitely creative ways to leverage this new large language model technology to try to use, you know, provide us with some AI assisted coding tools. So big thanks to to Simon for and the team for developing these and sort of the creativity that they're having around, leveraging these LLMs to help us in our day to day workflows.



[00:13:18] Eric Nantz:

Yeah. I see immense potential here and the fact that, you know, with these being native R solutions inside our R sessions, grabbing the context not just potentially from that snippet highlighted, but the other files in that given project, whatever a package or a data science type project with the information on the datasets themselves. Like, that is immense value without you having the really in a separate window or browser and say chat g p t or whatnot, trying to give it all the context you can shake a stick at and hope that it gets it. Not always the case. So there is a lot of interesting extensions here that, again, are made possible by Elmer.

And, you know, like like I said, immense potential here. Simon is quick to stress that these are still experimental. He he sees, obviously, some great advantages to this paradigm of literally the the the bot, if you will, that you're interacting with is injecting directly into your r file that you're you're writing at the time. Again, that's a good thing. It may sometimes be not so a good thing if it is going on a hallucination or something, perhaps, who we don't know. So my my tip here is if you're gonna do this day to day, if you're not using person control, you really should. In case it goes completely nuts on you, You don't want that in your commit history and and somebody asking you to code review. What on earth are you thinking there? Oh, it wasn't me. Really? Well, well, it kinda was when you're using the bot anyway. But nonetheless, having version control, I think, is a must here. But I do see Simon's point that other frameworks in this space, he mentions it kind of in the middle of the post, are leveraging more general kinda back ends to interacting with your IDE or your Git like, functionality of showing you the difference between what you had before and what you have now after the after the AI injection. So you could review that kind of quick before you say, I like it. Yep. Let's get it in, or not so much. I wanna get that stuff out of here and try again. So I would imagine again, this is Eric putting on speculation hat here. With the advancements in Positron and leveraging more general, you know, Versus code ecosystem, extension ecosystem, that there might be even a more robust way to do this down the road on that side of it. But the advantage of the RStudio API package is leveraging is that thanks to some shims that have been created on the Positron side, this works in both the classic RStudio and in Positron. And I think that's, again, tremendous value at this early stage for those that are, you know, preferring, say, R Studio as of now over Positron, but still giving flexibility for the those who wanna stay on the bleeding edge to leverage this tech as well. So I think there's a lot to watch in this space, and and Simon, definitely does a tremendous job with these packages at this early stage.



[00:16:31] Mike Thomas:

That's for sure. Yeah. I appreciate sort of the deposit team's attention to UX because, again, I think that's sort of the most important thing here as we bring in, you know, tools that create very different workflows than maybe what we're necessarily used to. I think it's important that, we meet developers and and data analysts and data scientists, you know, in the best place possible.

[00:16:58] Eric Nantz:

And I mentioned at the outset that Simon is part of the Tidymodels, ecosystem team. I will put some quick, plugs in the show notes because he is leveraging on he's I should say, writing a new book called Efficient Machine Learning with R that he first announced at the R pharma conference last year with a excellent talk. So he's been really knee deep into figuring out the best ways to optimize his development both from a code writing perspective and from an execution perspective in the tiny models ecosystem. So, Simon, I hope you get some sleep, man, because you're doing a lot of awesome work in this space.



[00:17:34] Mike Thomas:

I was thinking the same thing. I don't know how he does it.

[00:17:45] Eric Nantz:

And speaking of someone else that we wonder how on earth did they pull this off at the time they have, our next highlight is, you might say, revisiting a very influential dataset that made a tremendous waves in the data storytelling and visualization space, but with one of the new quartal tools to make it happen. And long time contributor to our weekly highlights, Nicola Rinne, is back again, on the highlights as she has drafted her first use of the close read quarto extension applied to the Hans Rosling famous Gapminder dataset visualization.

If you didn't see or or I should say, if you didn't hear our our previous year's highlights, we did cover the close read quarto extension that was released by Andrew Bray and James Goldie. In fact, there was a talk about this at the aforementioned PasaConf last year, which we'll link to in the show notes. But close read in a nutshell gives you a way to have that interactive, what you might call scrolly telling kind of enhanced web based reading of a report, visualizations, interactive visualizations. You've seen this crop up from time to time from, say, the New York Times blog that relates to data science.

Other, other, reporting companies out there or or startups out there have leveraged similar interactive visualizations. Like, I even had an article on ESPN of all things that was using this kind of approach. So it's used everywhere now. But now us, adopters of Quarto, we can leverage this without having to reinvent the wheel in terms of all the HTML styling and other fancy enhancements we have to make. This close read extension makes all of it happen free of charge. So what exactly is this tremendous, report that Nicole has drafted here? She calls a gapminder, how the world has changed.

And right off the bat, on the cover of this report is basically a replica of the famous animation visualization, plotting the GDP or gross domestic product per capita with life expectancy on the y axis with the size of the bubbles, pertaining to the area around those, and for each country, represented there. So once you start scrolling the page, and again, we're an audio podcast. Right? We're gonna do the best we can with this. She walks through those different components. First, with the GDP with some nice, looks like a line plots that are faceted by the different regions of the world, getting more details on on gross domestic product, and then getting to how that also is affected by population growth.

Again, another key key parameter in this life expectancy, calculation, which gets to the life expectancy side of it. And as you're scrolling through it, the plot neatly transitions as you navigate to that new text on the left sidebar. It is silky smooth, just really, really top notch, user experience here. And then she isolates what one of these years looks like. She calls it the world in 02/2007, showing that kind of quadrant, that four quadrant section you have when you look at low versus high GDP and low versus high life expectancy.

And as she's walking through this plot, she's able to zoom in on each of these quadrants as you're scrolling through it to look at, like I said, these four different areas, and that's leveraging the same visualization. It's just using these clever tricks to isolate these different parts of the plot. Again, silky smooth. This is really, really interesting to see how she walks through those four different areas and then closing out with the animation once again that goes through each year from the '25 nineteen fifties all the way to the early two thousands with, again, links to all of her code, GitHub repository, and whatnot.

But for a first, first pass at Close Read, this is a top notch product if I dare say so myself. And boy, oh, boy, I am really interested in trying this out. There was actually a close read contest that was, put together by Posit late last year, and I believe the submissions closed in January this, this past month. But if you want to see how others are doing in this space, addition to Nicola's, visualization here, we'll have a link in the show notes to, pause a community all the posts that are tagged with this close read contest so you can kinda see what other people are doing in this space. And maybe we'll hear about the winners later on. But this one this one will have a good chance of winning if I dare say so myself. So I am super impressed with close read here, and Nikola's

[00:22:52] Mike Thomas:

very quick learning of it. Yeah. It's it's pretty incredible. And I was going through the Quarto document that sort of lives behind this, and it actually seems pretty easy to get up to speed with this scrollytelling concept. It's pretty incredible. I think there's a a couple different specific tags, that, you know, allow you to do this, or maybe to to do it easy. It looks like, there is a dot scale to fill tag, that I believe probably handles a lot of the, zoom in zoom out sort of the aspect ratio of the plots or GIFs in Nicola's case that are being put on screen. Because in her visualization, it's almost like there's this whole left hand side bar, right, that has a lot of the context and the narrative text, that goes along with the visuals on the right side of the screen.

You know, some of the pretty incredible things that I thought were interesting here is, you know, not only was she able to, you know, fit a lot of these plots in a nice aspect ratio on the right side of the screen, but there's also actually a section of the scrolly telling visualization where she zooms in across four different slides, if you will, on four different quadrants of the same plot, to tell the story of these four different quadrants, you know, one being low GDP per capita and low life expectancy, low GDP per capita and high life expectancy, and, you know, the other two as well, vice versa.

And it's pretty easy, it's pretty awesome, I guess, how the visualization sort of nicely slides from one quadrant to the other as you scroll to the next slide, if you will. So this is for for any of the data vis folks out there, data journalism folks out there, I imagine that in order to accomplish something like this in the past, it was probably a lot of d three, JS type of work, and the end product here compared to the quarto code that I'm looking at is it's pretty incredible. And it just sort of gives me the idea that it's a lot of the heavy lifting has been done for us, in in the ability to create these quarto based scrolly telling types of visualizations.

So I'm super excited about this.

[00:25:26] Eric Nantz:

You know, it made me go then the way back machine a little bit on this. I'm gonna bring Shani in this because I love to bring Shani in almost all my conversations. But back in 2020, of all things, I remember I had the good fortune of presenting, I at the poster session at the conference, and I had my topic was kinda highlighting the latest innovations in the shiny community, and I was, trying to push for what could we could we ever have something like a shiny verse or whatnot of these community extensions.

And to do this poster, I didn't wanna just do, you know, PowerPoint over anything. Come on now. You know me. But I leverage, our good friend, John Coons. He had a a development package way back in the day called Fullpage, which was a way to create kind of a shiny app that had these scrolly telling like elements. But I will say he was probably too far ahead of his time on that. I won't say it was that easy to use. And, frankly, he would probably acknowledge that too. Here's my idea. I still have the GitHub repo of this, you know, poster I did.

I would love to have my hand at converting that to close read and wait for it, somehow embedding a Shiny Live app inside of it. Can it be done?

[00:26:42] Mike Thomas:

I think it can too. I think you'd be breaking some new ground, Eric. But,

[00:26:47] Eric Nantz:

if if anybody's up for that challenge, I know it's you. How did I just nurse night myself? Like, how does that happen, Mike? What you must be hypnotizing me or something without even saying anything. I have no idea.

[00:26:59] Mike Thomas:

Peer pressure.

[00:27:13] Eric Nantz:

Now you may be wondering out there yeah. The Gapminder data, we we are fortunate that we have a great r package that literally gives us this kind of data. So once Nicola has this package loaded, she's able to, you know, create this awesome close read, you know, scrolly telling type of report. Well, there are many, many other sources of data that can surface this very similar important domain such as what we saw in the Gapminder set. And you may be wondering, where can I get my hands on some additional data like this so I can do my own, you know, reporting? Maybe with Close Read or Shiny or Quordle, whatever have you. Our last highlight is giving you another terrific resource of data for these kind of situations.

This last highlight comes to us from Kenneth Tay, who is a applied researcher at LinkedIn, and he has a blog that he is, his latest post is talking about some recent advancements in this portal called Our World in Data, which I have not seen before this highlight, but it is a, I believe, a nonprofit organization whose mission is to create accessible research data to make progress against the world's largest problems. So you might think of, say, poverty, life expectancy, some of the other issues that, say, the Gapminder said highlighted.

But they wanna make sure that anybody that has the desire and the skill set to use, say, a language like R or whatever else to produce visualizations to really start to summarize and explore these data, that there is as less friction as possible to access these. And, yes, you could access their portal. You could download the data manually on their website, but it was earlier in 2024 that this group had exposed an API to access these data. So Kenneth, in his blog post here, walks through what it's like to use this, this, new API, particularly to call it a public chart API because it is the basis for, I believe, some interactive visualizations that their web portal is exposing here.

But because there is a an API now, he brings back a little bit of old school flavor here, the h t t r or the hitter package. That was one of those cases where I've been spelling it out all this time, but on on hitter two, the read me, Hadley literally says how it's pronounced. So thank you, Hadley. I wish I wish all our package authors would do that.

[00:29:45] Mike Thomas:

In case the baseball player didn't give it away.

[00:29:48] Eric Nantz:

Exactly. So great great hacks on the new on the new package itself. So back to Kenneth's expiration here, he shows us how with the old school hitter along with a little tidy verse magic and JSON light under, loaded into the session. He needs all three of those because, first, it's one thing to access the data itself, which apparently are exposed as CSV files on the back end, but the API lets you grab these directly. But the metadata comes to that to that in JSON format. So he wants to use JSON like to help massage some of that too.

So the first exploration of this and the snippet on the blog post is looking at the average monthly surface temperature around the world. So once he's get the he's got the URL of the dataset, then he assembles the query parameters, which, again, in the role of APIs, you might have some really, really robust documentation. Maybe some other times you have to kind of guess along the way. It's kind of roll of the dice, isn't it?

[00:30:52] Mike Thomas:

Yeah. I find the the latter to be the case more often, especially in professional settings, unfortunately, which seems to make no sense.

[00:31:01] Eric Nantz:

Who would ever think that? But yet, I feel seen when you say that. Yes. Even as of this past week. My goodness. Don't get me started. So luckily for this, there is a a healthy mix here, I would say. So he's got some query parameters. So look at the version of the API, the type of CSV, the return, which can be the full set or a filter set, which I'll get to in a little bit, and whoever to use long or short column names in the dataset that's returned back. And then, also, he does a similar thing for the metadata.

That's another get request, as well, and then he brings that through that content directly, with JSON format. So the metadata comes back as a list because most of the time when you return JSON back, it is bake basically a big nested list, and that gives some high level information on the dataset that is returning. So you get, basically a list of each character string of the variable name and the description of that variable. So that's great. Now the data directly, again, setting up similar, setting up the query parameters.

This time, he's gonna demonstrate what it's like to bring a filtered version of that data right off the bat. And that is where there's a little guessing on that because he went through the web portal of this, played with the interactive filters that this web portal gives gives him, and looked at the end of the URL. So if you're new to the way requests are made for API, you might say get requests where you're running to grab something from the API. More often than not, you'll attach different flags or different variables at the end of the URL often in, like, key value type pairs with an ampersand separating the different parameters. So once he explored this web portal, he was able to grok that, oh, yeah. There is a parameter for selecting the country.

So I'm gonna, you know, put that in the query parameter and feed it in the direct value. And then once he does the get request on that, this is important here, the contact the content, I should say, that's coming back can usually be three different types of flavors. The raw might say binary representation of that value, the textual value of it, or the format of JSON or XML version of it. In this case, the it was a text value coming back because it's literally the CSV content as if you just had the CSV open on a new file in your computer.

That's how the text is coming back. So he feeds that into a read underscore CSV directly. And lo and behold, you got yourself a tidy dataset as a result of that. So and then with that, he just said a simple plot of the time in year versus the temperature of the surface across The USA just to show that that's exactly how you would bring that data in. And there's a lot more you can do with this type of data. But, again, it's a good example of, first, going to the documentation where it's available. But then when things maybe aren't as well documented, yeah, nothing beats a little trial and error. Right? Sometimes that's the best bet we get, and that's how he was able to do that filtered dataset pull. But, nonetheless, if you're looking for inspiration, I'm looking at similar data as we covered in the second highlight, but across a wide range of world specific type of data. I think this portal has a lot of potential.

And, yes, r is your friend. Again, we're grabbing these data from almost any source you can imagine. So really great blog post straight to the point. You could take this code and run with it today. And, in fact, a good exercise would be what would you do to convert that to the hitter two syntax, which shouldn't be too much trouble. But, nonetheless, you've got a great example to base your explorations off of here.

[00:34:53] Mike Thomas:

Yeah. I I think it's just a good reminder in general, especially for, you know, junior data science folks who are just starting out that your data isn't always going to be in a CSV format. Yes. I know that our world in data allows you to export that. But a question that you should be asking, you know, in order to try to automate things as much as possible for yourself is often, you know, is there an API, right, for this this dataset or is there an underlying database that we can connect to so that I can just run my code directly against that, run my script with one click as opposed to having to go someplace and download the data to a CSV first, before I do my analysis. So, you know, if you can sort of automate a recurring script that you have against data that that might be just updating but in the same schema on some particular basis.

I think, yeah, this is a fantastic example of leveraging our world and data's API to do that, some really nice, base plotting, some really nice g g plotting as well, a pretty cool mix here That's been put together. And like you said, Eric, a great example of dealing with what's called a get request, which is where you're actually just modifying the suffix of the URL, in order to filter the dataset that's going to get returned here. So it's a really great example of doing that with a couple of different parameters that are being managed. I guess one parameter being, tab equals chart, another one specifying the time or the date range, that we're looking to get data back within. And then the last one being the the two countries here in the case of this last example where we're plotting the average monthly temperature for the entire world and then, for Thailand as well. So, you know, two items in the legend here. As you said, great great walk through blog post of using a publicly available, API to wrangle some data and and make it pretty.



[00:36:54] Eric Nantz:

Yeah. The the the limit's only your imagination at this point. So like I said earlier, you could take what Nicola made with her close read example, apply it to this kind of data, and and go to town with a a great learning journey. Great for a blog post such as this, you know. All if again, maybe, like you said, speaking to the the data scientists out there that are looking to get into an industry or or an a data science type of role, it never hurts. Well, if you've got the time and the energy to build a portfolio of things like this because you never know just how useful that will be as you're trying to showcase what you find and what what skill set you have to generate insights from data like this. Because not to not to pull the old, back in my day, syntax here, but we didn't have access to these type of data when I was looking for a job earlier. So take advantage of it, folks. It is here for the taking.

Speaking of what else you need to take advantage of, you need to take advantage of our weekly folks because if this isn't bookmarked for reading every single week, you are missing out because this issue has well more than what we just talked about here in these highlights. We got a great batch of additional tutorials, new packages that have been released, new events coming up. It's the full gamut. So we'll take a couple minutes for our additional fines here. And, leveraging what we talked about at the outset of the show with Simon's explorations of interacting with Elmer, a very common problem across many, many different industries and organizations is dealing with data that I'm gonna go on a limb here is kind of trapped in image or PDF format.

Because wherever you like it or not, there's gonna be some team out there that said, you know what? We have this great set of data here, and they act like everything is perfect access. And then you as a data scientist says, oh, yeah. Where are the CSVs? Where where are the parquet files if they're really up to date? Oh, no. No. They're in these PDFs. Oh, gosh. Okay. What do I do now? Yes. There are things like OCR that can help you to an extent, but with the advent of AI, there might be an even easier way to do that. So frequent contributor to the our weekly, highlights and elsewhere, Albert Rapp has another post in his three minute Wednesday series on how he was able to leverage Elmer to extract, text from both an image file of an invoice as well as a PDF version of that image and to be able to grab, you know, certain numeric quantities like number of billable hours, time period, and whatnot.

I think this is a very relatable issue that, again, many organizations, big or small, are gonna have to deal with at some point. And I've seen projects being spun up at the hashtag day job where they're looking at ways of building this from scratch. Well, if you're an R user, maybe Elmer with its image extraction functionality might get you 90% on the way there. Hashtag just saying. So excellent post, Albert and I may be leveraging this sooner than

[00:40:07] Mike Thomas:

later. No. That's awesome. We have some some projects that are doing the same thing with some of these self hosted open weights models to be able to take a look at a PDF and extract very particular pieces of information that we want from it, and we can tell it, you know, give us that back in JSON form, and it allows us to, you know, leverage it downstream. Of course, you have to build a bunch of guardrails around that to make sure it's not hallucinating because it's a Absolutely. Box. Yep. But it's it's pretty powerful stuff, and the accuracy that we're seeing is is pretty shocking, pretty awesome.

But what I want to call out is an article by the USGS, which is the US Geological Survey on mapping water insecurity in R with TidyCensus. They just always do an absolutely beautiful job, with data visualization. All the code is here for a lot of these visuals actually deal with, households that were were lacking plumbing in 2022 in The US, and then changes, via, I guess, barbell plots, they're called. I don't know if there's any other names for them. Lollipop plots?

[00:41:13] Eric Nantz:

Yeah. I've seen them thrown around interchangeably.

[00:41:15] Mike Thomas:

Yep. Yep. To take a look at, improvements in plumbing facilities, particularly in, New Mexico and Arizona, which were the two states in based upon the 2022 census, that I think had the the lowest rates, of of household plumbing. So it's, you know, it may be a a niche topic for some, for lack of a better word. But the the data visualizations that they have here on these choropleth maps are really, really nice. I I love the color palettes that they use. I I really love the walk through that they provide on the website in terms of the code and the narrative around how they made the decisions that they made to go from dataset to visuals. I think it's a great job. You know, on the the East Coast here, water scarcity is not something that we really are concerned about. But I know on the West Coast, because we do a lot of our work in agriculture, it's it's quite a big deal in terms of water rights and water access and things like that.

So I really appreciate the work that the USGS is doing on this particular, you know, niche.

[00:42:27] Eric Nantz:

Yeah. And I have a soft spot for the the great work they're doing. My wife actually was fortunate early in her career to have an internship at the USGS and, albeit this was a day where r wasn't quite as as readily used as it is now, but it's great to see this group in particular being really modern with their approaches. And, again, top notch narrative, top notch visualization, so really exciting to see. And I believe we featured this group on previous highlights, so you wanna check out the back catalog for some of the excellent work they've been doing in this space, previously. So excellent excellent find, Mike, and there are a lot more finds than just those. So, again, we invite you to check out the rest of the r weekly issue at rweekly.0rg.

We, of course, have a direct link to this particular issue in the show notes, but, also, you wanna check the back catalog about both the issue as well as this humble podcast itself because we got so many great things to talk about here, so many great things to learn. As you heard, I've I've basically nurse signed myself for a new project, hopefully, this year that I can work on with Shiny and Close Reads. So we'll see what happens there. But, yeah, if you wanna see what else is happening and if you want to be a part of what's happening here in terms of what the readers are gonna see every week, We value your contributions, and the best way to do that is, again, head to rweekly.0rg.

You'll see in the top right corner a link to this upcoming issues draft where you can send a poll request to tell tell us about that great new package, that great new visualization, that great new use case of shiny or AI or other technologies that you can see in this data science community. We'd love to hear it. Again, all marked down all the time. I I would stress again when told me years ago, if you can't wear an r markdown in five minutes, he would give you $5, and he didn't have to give any money for it. So there you go, folks.

And, also, we love hearing from you. You can get in touch with us via the contact page in the episode show notes. You can also send us a fun little boost with the modern podcast app. Those details are in the show notes as well. And you can also get in touch with us on the social medias. I am now on Mastodon these days with @rpodcastatpodcastindex.social. I am also on Blue Sky as well where I am at rpodcast.bsky.social. I believe that's how to say. Mike, where can the listeners find you?

[00:44:51] Mike Thomas:

Yes. I am on blue sky for the most part these days at mike dash thomas dot b s k y dot social. Also on fa Mastodon a little bit, mike_thomas@faustodon.org. And you can check out what I'm up to on LinkedIn if you search Catch Broke Analytics, k e t c h b r o o k. And a bit of a shout out here, self plug that we are looking for a DevOps expert. If you are somebody who has expertise in Docker, little Kubernetes, Azure preferred, but it doesn't really matter because we're all spinning up Linux servers at the end of the day, we could use some help managing ours and our clients' ShinyProxy environment. So any DevOps folks out there, please feel free to reach out.



[00:45:40] Eric Nantz:

I'm sure there are many of you out there. So, yeah, take up Mike on this tremendous opportunity. I'm still learning the DevOps ropes. We share many stories about that in our adventures there. So that's that's a great great plug, Mike. And I'm also on LinkedIn as well. But, yeah, we'll, we'll add that little, call out to the show notes as well if you're interested in pursuing that. Nonetheless, we're gonna close-up shop here for episode 94 of Haruki highlights. Before I go, I wanna send a very hearty congratulations to Chris Fisher and the team at Jupiter Broadcasting. You recently had episode 600 of Linux Unplugged.

Tremendous achievement, folks. You'll be seeing a boost from me in the coming days. I don't know if we'll ever get there, Mike, but, nonetheless, that's a huge number for a podcast of that size. So congrats to them. Well, we'll we'll get to 200 at least, and we'll see what happens after that. Alright. We got no place to stop. Well, yeah. Me either. Yep. We'll see what happens, buddy. But, nonetheless, we hope you enjoyed episode 94 of our week highlights, and we'll be back with another episode for one ninety five next week.

"
"25","issue_2025_w_05_highlights_638737249045054386",2025-01-29,34M 29S,"Ready to bring your next presentation slides to the world of Quarto? Our first highlight has a batch of power tips you can use for your next slide deck. Plus terrific insights from first-time contributors to open-source software. Episode Links This week's curator: Jon Calder - @jonmcalder@fosstodon.org (Mastodon) & @jonmcalder (X/Twitter) Seven…","[00:00:03] Eric Nantz:

Hello, friends. We're back at episode 193 of the Our Weekly Highlights podcast. This is the weekly podcast where we talk about the great highlights and additional resources that are shared every single week at rweekly.org. My name is Eric Nanson. Yeah. I'd hardly believe January is almost over. And, I'm I'm not sure if y'all listening or kinda like this in the wintertime, but this is not my favorite season. So I'm kinda hoping they get back in the spring very soon. But nonetheless, I'm talking about happy stuff today, but I'm not doing it alone, of course. I'm joined by my awesome co host, Mike Thomas. Mike, how are you doing this morning?



[00:00:39] Mike Thomas:

Doing pretty well, Eric. Yeah. It's it's quiet, this time of year with the the weather here in New England, and, I think we'll talk about it later on in the episode, but I'm already excited for summer and and conference season specifically.

[00:00:52] Eric Nantz:

Absolutely. Me too. Me too. Yeah. We will get to that in a little bit. A little, admin note here as we're recording this. The issue as a recording is not quite out yet. It's baking in the oven, so to speak. So we're gonna talk about a couple of the highlights that we know for sure will make it, but we always invite you. Check out the episode show notes for a direct link to the issue so that when this episode is published, you'll definitely see it then. But nonetheless, our curator this week is John Calder, another one of our longtime curators. And as always, he had tremendous help from our fellow Rwicky team members and contributors like you around the world with your poll request and other suggestions.

I was just talking to Mike and the pre show how I'm preparing for a a big work set of work meetings this week, and I've had to make slides and probably my not so favorite tool to do it. But luckily, in this first high, we're gonna talk about one of my favorite tools to make slides going forward for future thinking, you might say. And that, of course, is reveal JS format, a web based format, which is now being, you know, becoming more of a mainstream capability, thanks to the Quartle documentation system.

So I have been using Quartle a bit more, especially for my open source presentations. And I always feel like I'm on the cusp of learning a lot, but there's always things I feel like I don't know, especially in the day to day of building, you know, effective the fact of presentations, not just on the content, but some of the mechanics of reveal JS themselves. So I am always eager to see new resources being shared by the community on more practical kind of day to day life, so to speak, of creating revealed JS based slides.

And our first highlight is addressing that such item here. It has been brought to us by doctor Tom Palmer, who is a senior lecturer in biostatistics at the University of Bristol over in the UK. And he's got this great blog post on 7 tips that he has learned for creating quartile revealed JS presentations. And as a as a teacher or lecturer himself, yeah, it's definitely I can tell he has a lot of great experience of it, so we'll kinda tag team here on a few of the the tips we're seeing here. One of which, and often I hear this when I prepare for an external presentation that may be at a conference venue such as like PASIconf or some of these other statistics conferences I've been to.

It's one thing when you develop the slides on your setup, whether it's your laptop or you got a nice monitor that you're, you know, looking at the the fancy fonts and your images. But you probably wanna test your slides on a display that does mimic where it's actually going to be presented, whether that's trying to optimize for, like, remote viewers and their, say, laptop resolutions or, like I said, an aforementioned, like, conference room venue that your presentation might be in. So it's one thing when you have a WYSIWYG type tool like Microsoft PowerPoint or LibreOffice Impress or whatnot, where you can kind of resize the window and everything just magically, you know, become smaller as you're resizing it.

Not always that way when you look at web based slides. So taking a page from mobile development, one of the great tools that you can utilize directly in your browser, this is independent of quarto itself, is to look at changing the resolution in your browser, whether you do it for the console tools or developer tools that you see in all the modern web browsers. There are also extensions that will let you do this too so you can see what your your content looks like, say, on a mobile resolution or a more standard 10 80p resolution.

So it's just one of those nice things that as you're getting ready to do that presentation, it's always good to kinda do that double check. Did your did your fancy, you know, graphics or your nice plot look good on those other, you know, partitions or those other areas of this place? So I'm gonna take that to note especially as I prepare for, presentations hopefully later in the year. Then we get to looking at putting code chunks in your presentations, which again is always what I like to do when I do a more educational, you know, slash training type presentation.

You can you already have a lot of great tools available for you via the code chunk options in quarto or r markdown for that matter. Because, again, as we mentioned many times on this very show, our markdown without our markdown, Quartal, in my opinion, doesn't exist. It pioneered everything here. So there are some interesting options that you can use in these code chunks that, you know, bring to the lineage of our markdown that I did not know about. One of which is direct interaction with the format r package, and in particular, using a couple options called tidy and tidy ops, which I've not seen these before.

But this is great when you wanna control how wide the code is that you're putting into that chunk, so you don't have to use, say, a scroll bar if you wanna display a little more text, and the user doesn't have to scroll right or left. So with these options, you can either set them on a per chunk basis, or you can set them globally in the YAML front matter of the quartal slide deck, via these aforementioned tidy and tidy ops options. And, again, that is really really neat so that you can make sure that, you know, a lot of people do have wider resolutions. But if you're on a constrained, you know, resolution, you may not want users to have to scroll right or left just to see the output of that variable or whatnot. So I thought that was extremely nice for that more horizontal optimal viewing experience.

But there's a lot more to this. So, Mike, why don't you walk us through the rest of Tom's great tips here?

[00:07:01] Mike Thomas:

Yeah. Absolutely. There's not really an easy way to make code output chunks taller in a quartile reveal JS presentation. So we have to dive into the HTML and the CSS, and and doctor Palmer shows us how to navigate to the developer tools in Chrome to see that HTML source code. So if you've never done that before, this is a great blog post to, introduce yourself to that concept. I know, you know, if you're in the Shiny world at all, this is probably something that you wrestle with on a a fairly regular basis to make your, you know, Shiny app look the way that you want it to. But whenever we are delivering something involving HTML, you know, this is potentially something that we have to wrestle with, in order to make it look the way that our audience wants it to look. So once we figure out the element specifically that we're trying to modify, we can create a custom CSS file that alters the max height CSS property for that element in order to make it taller.

And then we can make sure that our presentation leverages our new custom CSS file by using the CSS YAML header property at the top of our quarto, file, our QMD file. So that was a nice little trick as well as a couple more I'll touch on here. There's a lot. Embedding interactive Mentimeter presentations. So this is something I had not come across before.

[00:08:27] Eric Nantz:

So, yeah, Mike, I have not heard of Mentimeter either. I did a quick search, and it looks like this is a way to embed interactive polling into your presentation slides, not too dissimilar to what you might do with another framework I've seen called Slido or whatnot, where you can embed these HTML representations of what looks like these interactive polling, into your other HTML documents. So this is news to me.

[00:08:57] Mike Thomas:

Interesting. Yeah. It it sort of reminded me of our Shiny in production workshop that we did at Pawsit Conf 2023 where I know, you know, you, Eric, embedded a bunch of quartile presentations that we created, one for each module in our training into the overall workshop website. So, you know, I think it can be a a pretty useful thing, right, in order to have these iframes that we can embed in other places. So it sounds like this Mentimeter, tool is is something to check out if, you know, that's of interest to you. And a nice walk through here about how to, leverage and and create those iframes and get the code and that you need to copy and paste the HTML, from one place into another in order to embed, that.

And then, 2 more little ones. Sometimes, you know, Doctor. Palmer notes that, you know, HTML tables may not look the way that you want them to look, and it's possible to sort of avoid quarto's inherent HTML table processing and there is a parameter, you know, aptly named HTML dash table dash processing that you can set to none if you'd like to. And there's a nice example code chunk in there that Doctor. Palmer provides of how to go about doing that. And there's, you know, a couple more nuggets in here, but it's a a great walkthrough of 7 different tips that Doctor. Palmer has on, you know, enhancing your quarto presentations and and little tips and tricks. Because I think, at least in my experience, Eric, you know, there's always something that you have to wrestle with in in your quarto presentations, to make it look that the way that you want to. And there's a lot of different rabbit holes that you could potentially go down. And I know the the quarto.org site is fantastic in terms of the documentation and the the searchability, but there are I have been down many, many Internet rabbit holes trying to chase down little tiny quarto things that are just quality of life enhancements. And it's it's really nice to see doctor Palmer's blog taking a stab at a few of these.



[00:11:03] Eric Nantz:

Yeah. I I think this is a a welcome time as as I'm gonna I've been on a, you might say, a crusade at the day job. But trying to evangelize the use of web based formats is something that Mike and I definitely sympathize with each other about because we both have our own internal struggles. We have our clients or other customers at our various day, you know, day job duties where it's still either a PowerPoint or a PDF world. Right? It's just sometimes you can't escape it. So having having, like, a consolidated bank of day to day tips such as what what doctor, Palmer has put together here as well as, I'll plug once again, Emil's, slide craft series of blog posts that that he has on his blog. Those are all top notch resources.

Do you have to learn every option? No. Absolutely not. I mean, as you said in the Cornell docs, there is a multitude of options and tweaks you can make. But even in this post here, knowing that little trick around the HTML tables, this is all news to me, as well as some of those other code chunk options that I mentioned at the beginning of the segment. So there's when you need it, there, I hope that posts like this combined with the quartile docs, you can have a great bank at your disposal that you can draw upon when you kinda need a certain technique, more often than not. But going back to that, no idea of getting to web based formats versus static outputs.

It did, it was both amusing and and frankly, a little sad that, yeah, there is another tip here about exporting the PDF, which comes up no matter what. Right? There are some especially when you go to some external conferences who insist on having a static format of a presentation even if you made it in a web based format. So if you are in that world, that that tip using the page down package to help print this via Chrome or Chrome browser's kind of printing utility, that might be your best bet. Just be warned, there are some gotchas when you export these, the PDF, especially you have content that is a little more interactive. Obviously, it's gonna be a snapshot of that. But, again, maybe that's fodder to if you're on a similar crusade at your organization to show the power of web based formats, you can say, well, this is what you're missing when you export the PDF. So who knows? Maybe it'll be helpful in that in that sense. But, yeah, this is definitely being bookmarked for a future reference here.



[00:13:32] Mike Thomas:

Yeah. I'm right there with you trying to do all the convincing as possible to have our deliverables deliverables be HTML because I think that that is the ideal state for us as developers, but it's not always the case. You're right.

[00:13:50] Eric Nantz:

Maybe in the future for those that are still, you know, admittedly way more comfortable than me in those more WYSIWYG type, you know, presentation software where it's Keynote or PowerPoint or LibreOffice Impress or whatnot. Imagine a world where there's been kind of some progress with the whole interactive quartile or markdown editing features and say our studio ID or posit work or positron, I should say. Imagine that with slides. Like, if we can get there eventually where if you're able to with some clever CSS tricks under the hood still kind of move things around in a more design principle, but yet it all just magically compiles that correct CSS behind the scenes.

Somebody is going to make a huge, huge impact if they can make that happen. But for now, I'll still whip up the code for it.

[00:14:57] Mike Thomas:

So our last highlight here is from our OpenSci it's a community call I think and the title of the call is from novice to contributor making and supporting first time contributions to fully open source software or FOSS for short. I think the community call was moderated by, Shanina Bishini Saibaneh, who is, you know, very prominent in the rOpenSci community. And there are, I think, 4 other folks who participated on this call speaking about their journey to their first contributions in open source.

[00:15:34] Eric Nantz:

Yes. Indeed, Mike. And I'll give, my take on a couple of those, presenters and their findings because as someone who is admittedly I want to contribute even more to open source, but also been in the trenches of whether it's open source contributions or day job contributions with concepts like version control, there are a lot of interesting nuggets that resonated with me. I'm gonna first talk about, Sunny Singh's, presentation where she gave a walk through for her experience in creating a new R package basically from the ground up called BBS Taiwan.

And the BBS stands for breeding birds survey. And this package, she wanted to fill an unmet need of an easy way for our users to to download and import these data into our r into their r session and to be able to perform summaries and visualizations on the fly with very helpful functions that would be, you know, fit right at home for a lot of the tidyverse type of work flows. So she talks about, you know, some great kind of tips that she learned. And, honestly, the ones that resonated most with me about her motivation to do it and then to be able to connect with future users in a really neat setting where she actually helped with a of another colleague organize a workshop to, undergraduate students over in Taiwan to learn this package kind of, you know, from the from the very beginnings in the early stages of it release.

And she says she got wonderful feedback from it and that it really motivated her to see, the positive reception to this package. And and, of course, many package authors help the cause a little bit more. She even came with stickers, buddy. Like, she she came she came running to to make sure that she could get the word out of this package. But, nonetheless, a lot of positive reception, and it was a lot of learning for her on the technical sense as well. Another nugget that she shares is that creating a package is a very fluid process. You may come in with one set of ideas for the way you wanna construct your functions for the package. But as you're actually developing it and you start to try and put on that hat or that persona of a would be user, you realize, you know, maybe there's a better way to expose that function out here. Maybe I can consolidate these into more utility type functions. So she went through that in this initial release of the BBS Taiwan package.



[00:18:17] Mike Thomas:

One thing that I I love about Sonny's presentation, you know, besides the fact that I I think it's an awesome initiative, I really like the slides that she put together. But the BBS Taiwan package is a great HEX logo featuring her own art, a bird that I believe that she drew and was able to incorporate into this hex logo. So shout out Sonny for that.

[00:18:40] Eric Nantz:

Absolutely. A great sticker goes a long way sometimes, but, yeah, a lot of that talk resonated with me. And then the other one I wanna summarize here comes from Pascal Bookard, who was, a longtime user of Git, but yet still in his early journey with contributing to open source software. And so he talks about a way that he got into this more fully by, finding an issue with another package that's authored by our our OpenSci, in particular one of our longtime contributors and fellow curator, in the past, Ma'al Saman, Pascal was leveraging the Baboquarto package because he wanted to develop his teaching materials for his day job in multiple languages, another great win for accessibility there.

And he noticed a small issue with the babel portal, and so he opened up an issue on the repository the GitHub repository. Nile got back to him saying, hey. I I I recognize this. I won't have time to address it now. But the poll request is welcome. And that was a a great, you know, kick start for Pascal to not just, you know, identify an issue, but to actually help fix it as well. And so he learned a lot in that process. And that's that small collaboration on solving this issue in Bevav portal. He learned so much in those 2 months of that collaboration, much more than he would have learned just trying to learn about the ideas of collaboration on his own.

A key nugget for me is finding that motivation and finding that really important use case to you that will let you push through some inevitable hurdles you might have as you think about contributing to open source. So that was a great great, great insight by Pascal as well. And then he talks about some of the Git specific tips that I think are very much in my wheelhouse as well. And I definitely try to keep myself accountable to following these, one of which is the use of branches. Branches are your friend. They're nothing to be scared of. You could have as many of these as you want. The best part about this is if you mess something up or if you had an idea and ends up being a complete dead end, you can just throw that branch away. It's as if it never happened. Go back to your main branch or however you like to call your upstream, you know, type of branch, and then you can start fresh. You don't have to worry about renaming files of, like, underscore attempt 1 or underscore I hope this works. It all is all consolidated in Git branches. So if you learn nothing else from my summary of this, please use Git branches. You will thank future you will thank you later.

The other parts

[00:21:39] Mike Thomas:

that Amen. I see. Amen. Amen. Exactly. Exactly.

[00:21:43] Eric Nantz:

This is the part, Mike, where you wanna make sure to keep me honest throughout this year because I really wanna be better about this. It really is helpful to commit early and to commit often. You it's very easy for me, at least, to be in the flow of, like, really trying to solve that esoteric issue. I finally got it working, but I find myself I have edited about 5 or 6 files to solve that issue. Then I have to try to almost go back in time and be like, well, had I followed this advice earlier, what would I have committed first?

Save yourself some pain. And as you get close to that fix, don't be afraid to commit often because there are ways you can kinda consolidate this after the fact. But if you just get it done, you can massage it a bit later. And and also be nice to future you. Don't just say, I fixed it in your commit message. Have a little more verbiage of, like, what you actually fixed because you can use this as, like, a diary of sorts or your development diary of your project, and you can figure out from, like, 3 months ago, how did I fix that esoteric issue with this shiny reactive not firing when I wanted it to. What did I have to do with that observed trick? This is literally fresh in my mind from yesterday.

And then I put in that commit. I'm like, okay. That's a kind of a way to reinforce that a bit. So he also talks about ways you can undo things and get. There are commands to do it as well as, like I mentioned, maybe consolidating those messages in a more succinct manner via a technique that I know I don't do enough of, and that's called rebasing. It sounds kind of frightening when you hear it. But if you just practice it a bit, I think rebasing can be very helpful as you think about how you want this to look a month from now, a year from now as you look back at this, development history. So there are some real good nuggets from Pascal and and his talk there.



[00:23:51] Mike Thomas:

Yeah. Eric, those are all really good points, and I I think it's really specifically important to practice these, you know, good git concepts of committing early, committing often, being really intentional about your commits that are especially important when you're working with other people. Right? And, you know, especially as we talk about the concept of contributing to open source and projects other folks are on. But it has to start with when no one else is looking, you know, when you're doing it yourself to to build those muscles such that when you do get into collaborative situations, you know, it's it's second nature for you and you're you're not having to sort of change your workflow.

I, you know, I I have in the past been just as guilty of, you know, getting in that flow state and changing too many things before I go to actually make that commit. And tools like GitHub Desktop, and I'm sure there are many other, you know, tools, out there that can help you sort of visually maintain your git environment, have been game changing for me because if I do change a file in multiple places, I can actually, in within GitHub Desktop, select specific lines of that file that specific changes that I want to commit and not necessarily the whole entire file. So I can break it down and make it look in my git, commit history like I committed early and often, and was very intentional, you know, and very specific about what I changed and and why. So that's been a game changer for me in terms of helping me have really good Git hygiene. So a little tip for anyone out there. And I know, Eric, you you might use some other GUI based Git tools that perhaps do the same thing. So I'd be interested to hear your experience there as well.



[00:25:41] Eric Nantz:

Yep. One that's on each of my machines here at home is called GitKraken, and it's a cross platform GUI editor. It's not open source. So if that's a thing to concern you, it'd be warned. But nonetheless, it it does exactly as as you said in your GitHub desktop workflow. I was literally using that feature the other day with some of my other open source work of, well, I don't wanna commit the whole file for this commit because there's only a certain section that's relevant to this message. So I simply staged those those chunks, if you will, did the commit, and then did the other part of the file and the remaining commit.

And little tip for those of you that are operating on the terminal side of things, whether it's by choice or by or not by choice, and you don't have access to GitHub Desktop or GitKraken on that remote environment, I'm gonna plug a project called lazy git. There was nothing lazy about this, folks. This is a terminal based git client that will look as if you try to put what you would see in GitHub Desktop or GitKraken in a terminal like workflow. So I can have a pane on the left that says here are all my unstaged, you know, files.

And then I can have on the right for each file what are those actual chunks, and I can do exactly as I just said. I can select via keyboard, you know, that particular line, that particular chunk. I can do a little hit the the c button, write the commit, hit enter. It's in. And shortcuts for pushing and pulling, new branches. It took a little getting used to, but as, like I said, maybe not necessary by choice all the time that I'm on a terminal interacting with good operations. But, if you're in that space, lazy git is worth a look. And there's, like I said, nothing lazy about it.



[00:27:34] Mike Thomas:

Sounds awesome. And that's what I'm gonna have to check out. That's a great shout out.

[00:27:39] Eric Nantz:

Yep. That's in a running TMUX session on our HPC environment for each project that has its own lazy git window and one window for for each of them. So sometimes I I I sometimes I made this mistake of doing the wrong repo for a message. Wait. Why isn't that file there? Oh, yep. Silly me. I have too many projects open at once. Yeah. First world problems, I guess. But there is a lot more to this talk. You're invited to check out the recording. There was another presenter that talked a lot about their experiences with incorporating automated testing and their contribution. So I think that's a great a great one to learn about as well.

And then, also, if you're listening to this and you you watch this recording, you're still wanting to maybe learn a bit more but in a more proactive fashion, we're gonna have linked in the show notes, a call for a coworking hackathon that are open side is going to lead. One of which is happening on February 4th and another one on March 4th where they'll let you kinda partner with somebody on that call. And as a as a title suggests, you both could be working on either the same project or even different projects and just share notes with each other as you're working on that. So it's a great way to learn with an expert on that call as you're doing your day to day development And maybe you are trying to get into, contributing to open source software, learning Git effectively. Those co working sessions have gotten rave reviews from the people I've talked to with attending them. So credit to rOpenSci for putting that effort together to make those happen.



[00:29:20] Mike Thomas:

Yeah. It's awesome that they offer, you know, those office hour sessions, especially for folks, you know, like myself, especially when I was first starting out. And I think in the age of remote work nowadays, it can sometimes feel a little lonely. So if you're looking to to hop on and be surrounded by other folks out there who are writing R the same as you, on just a collaborative call, I think it's an awesome, awesome initiative.

[00:29:44] Eric Nantz:

Absolutely. And and speaking of awesome, you know, the rest of the issue is just as awesome. We were able to cover what we think are guaranteed highlights for the issue, but we'll take a couple minutes for our additional fines here. And this is more of a, you know, a not I don't know, a reminder of sorts, but, very, in my wheelhouse, as they say, with my love for shiny. As Mike and I alluded to, it's already a lot of preparations are underway for conference season here in 2025. And one of those that I'm always a big fan of for, I believe, will be the 3rd or 4th year running, Epsilon's shiny comp is happening this April, and they still have registration open as well as the call for talk proposals.

So, well, I believe the deadline to submit for the talk proposal is February 2nd, so you wanna get those in. If you wanna contribute your your excellent idea or excellent project idea about Shiny or use of Shiny, Absalon has always been very receptive to a lot of these ideas. Full disclosure, I am on the program committee, so I'm really looking forward to being a part of that review process. And I remember narrowing it down from last year. There was a lot of great content out there there and a lot of great ideas. So definitely have that, in your in your bank if you're a shiny enthusiast.

And more generally, while I'm on this topic of conferences, I believe also the talk submission portal for POSITCONF 2025 is still open, but that will be closing, I believe, around February 2nd or 3rd as well. So if you have an idea for a talk for POSITCONF, you wanna get those, proposals in there as well. And we'll have a link to both of those conferences in the show notes. And, Mike, what did you find?

[00:31:28] Mike Thomas:

Nothing because the issue is not out yet. But I am just going to double down on both of those things because I think it can't be said enough. Shiny conf fantastic conference. Is it free? It's virtually, and I believe it's free. Yep. I believe so too. No excuse not to be there. Yes. Excellent conference. I know, Eric, you'll be involved heavily as always. So, that's when I look forward to to attending every year and grateful for Absalon for putting that on.

[00:31:57] Eric Nantz:

Absolutely. And a quick little nugget to my, fellow, compatriots in life sciences. We are also in the plannings of the R pharma conference already. We we start early because we do a lot for that. So if you're listening, you wanna get involved with that, definitely give me a shout, and I can get you lined up for our various committees on that as well. But yeah. So, again, contributing yeah. We we talk about first time contributors to open source. Well, a great way to ease your way into contributing to open source in general is with this very project itself of our weekly. Right?

We curate from the community. So if you find a great resource and you want this to be front and center in next week's issue, we are just a pull request away. Everything's on GitHub. Everything's in markdown. The you don't need a fancy wizzywig editor for this one, folks. You can just type in plain text to your heart's content on these pull request templates that we'll provide to you free of charge, of course. So head to rweekly.org to find all the great links and detours or details, I should say, for contributing that great resource that you found, wherever you authored it or you even found it from someone else in the community.

And, also, we love hearing from you online, wherever you're keeping me honest with my pronunciation skills or other great tidbits. We have a contact page in the episode show notes. You can, send a note to Mike and I directly on that. And, also, you can get in touch with us on social media. We are prime we as I am primarily now on both Mastodon, where I'm at our podcast at podcast index dot social as well as bluesky, where I am at our podcast, dotblusky.social or something like that. Anyway, it's in the show notes. It's still not muscle memory for me to recite it yet. Mike, where can they find you? Yeah. Probably mostly on Blue Sky these days. If you're looking for, you know, social

[00:33:48] Mike Thomas:

media, mikedashthomas.bsky.social. Or on LinkedIn, if you search Catchbrook Analytics, k e t c h b r o o k, you can see what I'm up to.

[00:34:01] Eric Nantz:

Very good. Very good. I'm knee deep in some interesting issues I hope to post more about. But, my journey with Knicks continues, and I'm I'm leveling up even more on that. So I hope to share more in the, coming months. That in container fun, it never stops there. Alright. Nonetheless, we are gonna close-up shop for upsell a 193 of our weekly highlights. We thank you so much for listening, and we hope to have you listening to next week's episode of our weekly highlights."
"26","issue_2025_w_04_highlights_638731142645681748",2025-01-22,50M 34S,"Bringing a little tidy magic to creating flowcharts in R, how data.table is addressing recent shifts in R's C APIs, and another showcase of R's visualization prowess in the realm of brain imaging. Episode Links This week's curator: Ryo Nakagawara - @R_by_Ryo@mstdn.social (Mastodon) & @rbyryo.bsky.social (Bluesky) & @R_by_Ryo)…","[00:00:03] Eric Nantz:

Hello, friends. We are back with a 292 of the Our Weekly Highlights podcast. This is the weekly show where we talk about the awesome highlights and additional resources that are shared at this week's Our Weekly Issue. My name is Eric Nantz, and I'm happy you join us from wherever you are around the world. Hopefully, staying warm depending on where you are in the world because it is frigid over here in my humbly abode here. But I'm warming up with this recording, and, of course, keeping me all warm and fuzzy in terms of, you know, cohosting is my awesome cohost, Mike Thomas. Mike, how are you doing today? Doing well, Eric. Yeah. Thankfully, these highlights are hot,

[00:00:37] Mike Thomas:

because in Connecticut, it is just as cold as I'm sure it is in Michigan, right now. So it's pretty out, though, here. We got some we got some nice snow.

[00:00:46] Eric Nantz:

Yeah. That's true. It hasn't all melted yet here. And when the kids see the sun, like, I don't want the snow to melt. They're like, it's not gonna melt at 0 degrees, buddy. It's not. No. Not here anyway. So but as you said, we got some fun, hot topics to talk about in the highlights this week. And, of course, this is a community project. Right? So we've got our curator of the week. This time was Ryo Nakakawara, who is one of our OGs in the curator space of our weekly. And as always, he had tremendous help from our fellow our weekly team members and contributors like all of you around the world with your poll requests and suggestions.

And we lead off with a visualization style of a package that definitely has a lot of utility in terms of the scope of it, especially in my industry. And we're gonna dive into it here. And this post is actually coming to us from the RPost blog, and it's a guest post by Paul Satorra. Hopefully, I said that right. He is a bio statistician. And in this post, he talks about introducing a package that he's created for the art community, now on CRAN, called flowchart. The name should make it pretty intuitive of what it does, and it helps you create flowcharts of R.

Now you may be thinking, and this show and other, you know, other, presentations or resources, creating flowcharts, there's a lot of different ways of doing this. Right? Especially in the in the realm of HTML style outputs. We we've been using I know myself and Mike, I believe, have been using frameworks like Mermaid. Js within our quarter or our markdown documents. So there's definitely ways of creating flowcharts there. I also was a heavy user of the diagrammer package from long ago. That was, helping me out quite a bit with creating may not be necessary flowcharts, but definitely things like decision tree outputs and and, you know, choose your own adventure kind of layouts.

But what flowchart brings differently than the rest of those is, in essence, a very tidy interface to make all this happen. So let's dive into this a little bit from the post. So first, we will have, of course, a link to the the package and the episode show notes here. But as I mentioned, it is on CRAN, so it's all just an install dot packages flowchart away. And it actually, you know, requires you to bring your own data. So for the case study in this example, there is a built in dataset from a publicly available clinical trial set of results called saffo, and it is actually about the journey of patients throughout the life cycle of a trial. When I say journey, I'm thinking in terms of what is the result of their status, and this could be that they are randomized to the trial, I. E. They get one of the treatment assignments, or they discontinue after the randomization for a various reason, or they end up completing it. There are many other nuances in this, but this isn't a clinical trial podcast, so I'll I'll stop there. But the the data is built into the package.

And, basically, in order to register this data to be available for a flowchart, you start off with feeding this dataset name into a function called as underscore fc. And this is basically gonna turn your data frame into a list object with 2 components here. One of which is the original data going into it, and you may be wondering what does the data format look like. In the case of this example, each observe each row is what looks to be a patient with a rant with a unique ID, and then the columns are the different kind of flags in terms of where they're at in the in the trial and what the statuses is. So, again, there is a vignette that describes this data set in more detail, but it's basically a bunch of yes or no type variables for what happened to that patient in the trial, whether they they were, you know, in they met the inclusion criteria, whether they had chronic heart failure or whatnot. Again, you can take a look at the data in in the episode show notes.

So once you feed this in that he has underscore fc, now you may be wondering what do we do with this? Well, you can just simply draw a very bare bones flowchart or one cell with one function called fc_draw, where if you feed in that original data or that original object, you're just gonna get a box with, an optional label of your choosing if you want. And this time, it has, like, all 925 records in one box saying that these are all the the patients inside. Well, that's that's boring. Right? Well, let's start actually having some flow in this flowchart. Right? So that's where the tidy interface kinda comes in here where you can feed in this dataset.

Again, make it a flowchart object with as underscore fc, and then pipe that further to an fc_filter object. And this is where you can perform what looks to be like dplyr manipulation with its filter statement. And in this case, in this first example, we want a simple filter to determine if the patients were randomized or not. Now there is no column for whether they are randomized or not, but there is a grouping column, which in essence acts like that because it determines what treatment group they were assigned to.

If that column is missing, it means they weren't randomized in it. So in the case of this example, the filter for rather that group variable is not missing. So an exclamation point is dotna group, and then you can give it a label. And then you can also show who were not meeting that filter. And that's gonna be automatically labeled in a box called excluded. Then when you draw that, then you get that original box of the 925 patients, but then there are 2 arrows going away. One arrow goes to the right, and it has this excluded box. And then the arrow going down has another box that, the author's label is randomized, which has now 215 patients. So, obviously, not many patients made to that randomization step, but this is a very similar format that we do in a lot of our clinical trial reports, and we get to what's called the disposition section where it shows the flow of the patients that meet certain criteria and who end up actually completing the trial.

So this look quite familiar to me, but you can do a lot more than just that single filter. Right? You can also, at that next step where it show those randomized patients, you can now split that into different boxes as well. You might call parallel boxes, and you can use a function called fc_split. You give it the group, the variable that determines the grouping of that split. In this case, it is simply group, and that's now gonna partition that randomized group into 2 boxes of the 2 different treatment groups. Again, pretty straightforward, pretty neat tidy interface here, and you can do even more manipulations with that fc filter applied to those, you know, middle boxes that we just created at the treatment groups. And you can read the example more in the post here, but, again, it's really just using the fc_filterfunction.

And then you'll see these boxes kind of in parallel chains or or or, you might say trails going down, but the boxes are all parallel next to each other for the equivalent kind of steps. So in essence, the flowchart looks pretty darn polished already. And again, with a tidy interface, I think it is a this is a great package to put in your toolbox if you just want something quick to the point of a familiar tidy verse kind of piping syntax, and you could feed this into whatever document you choose. I could see this going into a quartile document or markdown, you know, whatever have you, whether it's HTML or PDF format.

It looks like it's gonna output these image these, flowcharts as image files, perhaps, although I haven't tested that myself. But it is definitely an interesting paradigm if you want in if you know what your data going into is fairly straightforward, which it is in this example. And you may not necessarily need the additional customization that you get with frameworks like Mermaid. Js and whatnot or diagrammer or some of these other packages past. So, again, you might find a great use case for this. As me as usual, I like having choice in the way I I, perform these flow or I construct these flowcharts.

There could be cases where this might not quite fit your needs. If you have more, you know, customized kind of directions of the flow, maybe things kind of feedback to an above step. In that case, maybe mermaid. Js is is a better fit for you. But like I said, for this kind of flowchart where it's a pretty predefined start and stopping points or might say finishes and kinda the trail of where this flowchart goes, I think, Paul's package could be a could be a great fit for your toolbox.

[00:10:09] Mike Thomas:

I agree, Eric. Yeah. I make flowcharts literally every day. They're the way that I communicate with both my team and our clients about sort of the the end to end process that we're going to undergo to to get them to the solution, because that's that's how we bridge the gap. And there's you know, a few years ago, there weren't a lot of great flowcharting tools that integrated well with version control. You know, we used some like Lucidchart and Visio, But you really had to, like, export those as as PDFs or maybe host them somewhere that folks could go take a look at, but not scriptable, not easily versionable.

And nowadays, you know, as you said, there are better options, mermaid. Js being one of them, the diagrammer package being another one. But I'm really impressed with this flowchart package here. You know, it's it's really easy and simple syntax to get started, very tidy friendly for developing these flowcharts. And when you look at it on the surface, especially in some of these examples, a lot of these functions are really just taking one argument. And there, you know, isn't doesn't appear to be a lot of customization, but that's actually because there are a ton of other arguments that have default parameters that can be changed if you want to. You know, originally, I thought that this was a package that, you know, was just very simple syntax, you know, made a lot of decisions for you. And with the default parameters, they do, but you still have a lot of control over all sorts of different types of things, like the direction of the line, within that fc filter function, whether or not you wanna kick those filtered observations out to a flowchart node on the right or on the left, you know, font styling, font size, font color, things like that, rounding for the number of digits that are gonna get displayed, the background color of the node itself. So if you actually take a look at, you know, some of the the the reference, the reference page of the package down site, which is what I'm taking a look at, and click into some of these functions, you'll realize that there are a ton of arguments behind the scenes that are almost ggplot like in terms of the amount of control that you have over each element, in your flowchart. So I'm I'm pretty impressed pretty impressed with some of the new, features and, you know, some of the the interesting functions that they have in here that I'm not sure I've seen anywhere else like fcmerge and fcstack, which allow you to actually combine 2 different flowcharts either horizontally or vertically. I thought that's pretty interesting. Maybe could help in your workflows depending on how you're sort of modularizing your code. So really impressed.

Honestly, just on sort of a side note, I really like the the HEX logo as well. I think it's really cool. And I'm excited to start to play around with flowchart as well because I had not come across it until today. So great way to start off the highlights this week.

[00:13:05] Eric Nantz:

Yeah. I can see a lot of convenience here. A lot of ways just cut that chart done, like I said, for pretty straightforward datasets. And, yeah, I'm gonna show this to a couple of colleagues here as we're thinking about ways of using R and more of the document generation space, especially for these more, I'll be I'll try to be polite here, rigid set of documents that we have to do in my industry. We're we're slowly trying to feed R into these things. And in fact, there is a section that we often have in what's called our analysis data reviewer's guide where we talk about kind of the flow of how the programming works, wherever you go from dataset to program and then to output.

Perhaps flowchart could be useful in that too. So I've got some, I got something to share to some colleagues, I think, later on today. So, yeah. Credit to Paul for sharing this package with us, and, yeah, choice is good as they say. Pray for the day when our audience

[00:14:01] Mike Thomas:

finally lets us do dynamic documentation.

[00:14:03] Eric Nantz:

You can't have everything, Mike. Alright. And our next highlight here, we're gonna shift gears quite a bit because we're gonna get really in the weeds technically here, but with a pretty fundamental issue that I think has affected quite a few package authors in recent months and maybe even the recent year, year and a half, having to do with best practices and recommendations for authoring packages that have to do with more than just, you know, new r code in the package itself. In particular, we're gonna talk about what you wanna do when you extend a package with another language, mainly the c language in your next r package, and some of the learnings that have been that have been shared from a very influential package in this space.

So we are talking about the latest blog post that comes on the data dot table community blog, which has been featured, quite a bit in last year's, section of the highlights. This post comes to us from Ivan Krivlov, and he leads off with the tagline about using non API entry points in data dot table. Now it's amazing. In 2025, I think when most people think of APIs, they're thinking of those web APIs. Right? No. No. No. We're not talking about that here. API actually is a historical term in software development. We are talking about ways you can interface with the language in different constructs or different perspectives.

And in particular, we are talking about the API that the R language itself exposes to package authors via its integrations with the c language. So really getting, you know, setting the stage here from since the beginning of r itself, there has been, you know, the canonical reference if you wanna build something on top of r, which ideally is a package or perhaps are even gonna contribute to the language itself, there is the writing r extensions reference. This can be found directly on the r project homepage, and this is what the CRAN maintainers are using as reference for any new package that's coming into the our ecosystem.

Certainly, there are a lot of automated checks in place to make sure a lot of the principles in these in the extension manual is met. But in particular, why we're talking about this in this post is that within this manual, there are, in essence, entry points that are defined by the R maintainers to interface with the C API of R itself. And in particular, there are 4 categories that you'll find in this manual. 1st is literally called API, and you can think of these as the ones that are documented, they're declared for use, and that they will only be changed if the CRAN maintain or if the r, you know, maintainers end up deprecating that particular API call.

Then you get to the next 3. There is the public designation, which these are exposed for use, you know, by our package developers, although they aren't really documented and they could change without you knowing it. So you could think of this as like maybe in a package you have a function that is technically there, but you don't export it to the user with user facing documentation. But like any package in r, you can look at or use any function in the package with the namespace, you know, prefix between 3colons and the function name.

So again, some call that off label use. You your your terminology may be different. There is another category called private. These are used for building R, and, yes, they are exported, but they're not declared in the header files of R itself. And they say point blank. Do not use these in any package. Do not. No. None at all. And then you get to hidden. This one really is peculiar to me. They are entry points to the API that are sometimes possible to use, but they're not exported. But I think it kinda goes by the name of it. You probably don't wanna touch those.

So, historically, there's been no consternation from the R maintainers or the R package authors that the the the header the entry points designated as API, all good. Right? Should be able to use those. However, there has been a bit of discourse around the use of the public ones because they're not documented. They're not forbidden by our command check, and they've been there for a while. However, there has been a little bit of, you know, modification to the language itself where some of these, to be able to use these, there may have been either somewhat we call escape patches put in by, you know, having a header called define use our internals that was used by package authors in the past to kind of get around maybe some potential issues.

Well, that you might call escape hatch or loophole was kind of closed, in recent versions of r. And then the number of non API blessed calls grew a little bit in between package or between R versions. And, also, another, you know, discussion on the R development list is where is the framework or the header of the library called alt rep fit into this, which got a lot of great press in recent years in the r community about being a more optimized way of operating on vectors. And, in fact, I I was had the pleasure of speaking with, Gabe Becker numerous times who was influential in getting alt rep into the language itself, although it was certainly labeled experimental in those times.

So fast forward a little bit, but there's been a little confusion into, like, which of these API calls, you know, are really ready for package you package authors and whatnot. Luke Tierney on the on the our team, he's actually worked program to try programmatically describing these exported headers, these exported symbols, and to be able to, you know, give a little more clarity into what package authors can can can use. And he's, you know, found 2 additional categories as a result of this. Experimental, which I think sounds a little more intuitive. These are header, you know, pointers that are there. They're in the early stages, so there might be some caution to use them because they could change that NER version.

So be prepared to adapt, basically. And then there's one called embedding, and this is meant for those who wanna create what are called new front ends to the language itself. Itself. But for now, they're keeping it separate. There isn't a lot of traction on whoever to use those or not. And then now our command check has been beefed up a little bit to make sure that it is checking for any calls by that package that are using these non API entry points, I. E. Those that move from the API designation to some of these other ones.

And it looks like data dot table was on the recipient of some of these checks and recent upgrades. And so the next part of this post dives into, as a result of these checks, what the data dot table authors are doing to be compliant with kind of this reorganization of the c, you know, API entry points that data dot table has been relying on for years years. Again, some of these escape patches are being patched up, and I've actually seen discussion on the Mastodon, you know, in the r group from people like cool but useless, Mike FC, you might know him as about some of the adventures he's had with trying to use c API endpoints and some of those packages he's been dealing with and our command check issues and whatnot, but it looks like data dot table has been, looking at this quite a bit.

So I'm not gonna read all these verbatim because there are a lot of corrections that are being made in data dot table to use some of these more either updated or or newer API c endpoints or entry points. There are some that are quite interesting where they've got solutions in place and they link to every poll request that fixes these issues in each of these sections. Some of which is looking at, you know, comparison of calls and pair lists, I call it, and which entry point they're using in the past, what entry point they're using now, looking how strings can be composed as c arrays, refactoring certain reference counts, dealing with encoding and string variables, growing vectors so that doesn't destroy your memory. There are some new entry points for that as well that you can read about.

And then it gets pretty interesting because there is more and especially getting back into the alt rep framework. Apparently, there are ways or there are some might say confusion into where alt rep fits in all this and which parts of alt rep should be exposed in the in the way that a package author is not gonna get dinged in our command check. There is a lot of narrative in this, and this actually does speak to how you grow vectors and do some other type checking. So I thought alt rep was kind of all ready to go. I'm not saying it's not ready to go, but apparently there are some refactoring that needs to be done with starting with r4.3 in terms of how you grow these vectors with the alt rep framework.

So this post talks about the common rep methods in alt rep and other common, you know, you know, interactions with this and the c libraries. And there will have to be some refactoring in data dot table to use some of these newer recommendations for alt rep. And like I said, growing these vectors, growing these table sizes, doing things like fast matching of strings. And this is the one section where things are not fixed yet. There is a lot of refactoring. It needs to be done by the data dot table authors that comply with some of these new endpoints and some of these newer, you know, recommended approaches of using alt rep.

And there is even more going on here with some other attribute setting dealing with missing values where they are very transparent. They're not sure how to fix some of these yet in light of these new API calls or these API calls being shifted. Again, this is an extremely technical deep dive into it. I, for 1, have never authored a package that deals with c, so I don't have a lot of firsthand experience with dealing with these checks. Although I've seen, again, some conversation about this on social media and the rdeveloped channels and whatnot.

But if you ever wanna know how a very important large scale set of package package like data dot table and the authors of that package are dealing with some of these newer approaches that the R team is recommending for dealing with these API entry points, boy, this post is for you. There is a lot to digest here. Again, I can't possibly do it justice in this particular highlight, But I think it's important to have things like this as a reference so that it's not just so mysterious to you as a package author if you get dinged by an r command check about these API calls. I'm wondering how would another team approach this. This this is a very technical deep dive in how you can approach it. And as I said, some of these are not fixed yet. There is obviously still time in between releases to get compliant with these newer calls, so I'm sure data dot table is gonna find a way.

But we're all humans after all. Right? It's not always a snap of a finger to get into these newer these newer ways of calling these entry points. So getting into the internals of data dot table quite a bit, but more importantly, also looking at how they're dealing with this new world, if you will, of using c with a new package in the R community. Yeah. That's a lot. But, again, really recommended reading if you find yourself in this space.

[00:27:13] Mike Thomas:

Yeah, Eric. This one is is very technical as you mentioned, but I think it's it's great to have a really technical blog post like this. And it it may seem really niche, but I guarantee you it's going to help someone else out there who's probably going to run into the same situation with their R package where they leveraged, you know, this kind of API interface into sort of the underpinnings of of the c code behind r, to accomplish something and and realizing maybe now that, you know, CRAN is going to start to complain about that. And, you know, as much as we might have mixed feelings about CRAN and the the checks that they enforce can be stressful to us sometimes. Like, I did see a a blue sky post recently. I don't know what they're they're called, toots tweets.

But somebody had, you know, passed 6 checks, I guess, on the different types of operating systems that get checked on CRAN, and then the the 7th was Windows, and it failed. Like, that hasn't happened before. Right. My goodness. And, obviously, that's the worst feeling in the world. But if we really take the time to step back and think about how open source software, and I guess most software in general, is just, you know, software stacked on top of one another over and over and over. And if we're going far enough down the r rabbit hole, right, at c, And not to throw stones, but it's a little scary to me that, you know, something like CRAN doesn't exist in other languages. You know, I'm thinking about the Python ecosystem, and I think it's pretty easy to submit a package to PyPI. And I don't know if they require you to have, you know, any unit tests at all. Not not that R necessarily requires you to have any unit tests, but at at least they're going to try to build your package, right, and let you know if, anything is is breaking. And it's, you know, as you make changes and updates to that package, it'll rerun it and, you know, rerun a lot of those tests, and those tests are getting updated for things like this. Right? Newer versions of r and newer guide lines and guardrails that we have to adhere to to make sure that your package has the best chance of working on everyone's computer. Right? And I think that goes a long way to, you know, at least provide some infrastructure that's going to appease, you know, auditors.

You know, I don't think the SAS community is ever go SAS community is ever going to be happy with us, and they'll point to to situations like this about why their software is is more stable or or better, than open source. But I think you and I could talk for about 10 hours about why that's not the case. You know, but it's it's really interesting, and I'm very appreciative of blogs like this that really take the time to walk through all the decision points, you know, sort of everything that was laid out in front of them and and what they were up against and and why they made the decisions that they did to try to, troubleshoot this particular issue.

And, I'm also grateful to not have to understand any of this. You know, I'm being a little facetious, and I certainly understand that it's it's all c under the hood, but the folks that have really taken the time to understand, you know, the bridge between these two different languages to to build these higher level, right, programming interfaces, for folks like us that that make it easier to work with, you know, it's it's incredible. You know, I think it's why the R language and the the Python language as well, you know, are as popular as they are because the the syntax and the APIs, not to use a buzzword here, that have been developed, you know, make it very user accessible to a wide audience. And, you know, one last note here. I guess it's pretty crazy to think about how old Data. Table is.

2006 was the 1st CRAN release. The oldest version of dplyr released on CRAN, at least from what I can see on the package downside, is 2014. So 8 years later, still a decade old, but we're going on 2 decades of data dot table. And it's definitely, been a package that was transformative for the R community. So great to see it still still thriving, and, you know, the folks that work on that project are are at the cutting edge, you know, of a lot of what's going on, in the open source data science ecosystem. So hats off to them and great blog post.



[00:31:46] Eric Nantz:

Yep. It stands the test of time as as an understatement to say the least that it has that history and it's been that influential in this community. And and, again, not all of this was despair. Right? I mean, there were many of those points that, that are mentioned early in the post. It was simply changing the name of, an API header call or whatnot. And it was straightforward in the documentation of which to change it to. And again, credit in the post, having all the links to various poll requests that fix these. So Ivan did a tremendous job of being transparent of like showing the fix at a high level and then pointing to the actual code that does the fixing. I love that. I can't wait to dive into that a bit further.

But again, it calls out that like anything in open source, it's not always a quick fix to everything. So I will be keeping an eye on what's happening with those alt rep style header calls where there are new wrappers that need to be made in this in between world of the current version of r and an r version 4.5 or later, which is due out, I believe, this year. So, as usual, if anything developing a a highly influential production grade package or app, you gotta think about backward compatibility. Right? So that's what their their journey is on, and, yeah, we'll be very interested to see where it goes. And in the cases where they don't know the best fix yet, I hope that the community can help them out too and that there will be a transparent, dialogue for that. But data dot tables, group of authors have been on the cutting edge for many, many years.

I'm so thankful that he got that recent grant to put resources like this blog together and their various presentations that they've had at the conferences. So it's great to kinda get a lens into all the innovations they've been thinking about, you know, now in in the public domain like we get to see here on our very well humble little our weekly project. So we're not gonna talk about see you again for this podcast. Enough and again see, of course. We're gonna go back to some visualization with a very important type of visualization in the in the world of health, especially of a very important organ in our bodies that we're relying on every single day for obvious reasons.

So it's one thing to talk about, you know, how your brains work. Right? But when anytime we're trying to diagnose issues with our humble little organs inside our craniums up in our skull, you often turn to, you know, visualizations, I. E. Scans, of your brain tissue to perhaps diagnose issues or find ways that maybe a treatment is affecting certain, you know, or certain parts of your brain, if you will. Typically, this is done via MRI scans. And just like anything, the art community has stepped up for ways you can bring these visualizations into R itself for further analysis.

And our last highlight for today is a great tutorial on some of the issues and ways that you can import and analyze these type of highly complex visualized data here. This post is coming to us from Joe Edsall, who is a staff scientist at the cognitive cognitive control and psychopathology laboratory at Washington University in Saint Louis. That's a mouthful, but she definitely is a subject matter expert in this field from what I can tell here. And she has written multiple tutorials in the past. In fact, she's, constructed these with Knitter, which is a great great way to use, again, reproducible analysis for tutorials.

And she's addressing some of the points that she had talked about and working with 2 different types of quantities in these brain images. One is the volume and the other is surface area or surface of the of the brain visualizations. So first, she talks about the volumes of this. And, just like anything in in the real world in physics, we are, you know, 3 we have the three-dimensional, you know, perspective here. Right? And when you get these MRI scans, you get three-dimensional coordinates of these if you feed this into some of the more standard software to to actually visualize the readings from these MRI scanners.

And you see some example images here looking at, some off the shelf software where you look at on the right side the three-dimensional layout of the of the brain itself, and and then you get more of a 2 dimensional representation via the different perspectives. So all this data is readily available from these image formats once you import it via this great package called rnifty, r n I f t I, if you wanna look that up after, well, the link in the show notes. But there is, you know, very handy ways to import that image file.

I believe these are actually, zipped archives of these of these, images, and you'll get a lot of different attributes of the different pixel dimensions, especially in the three-dimensional space where you can use to help visualize this and perform additional processing. So that can be very important if you're looking at different areas of the brain and trying to see the coordinates and the different representations of those. So you this package can help you figure out all those different orientations, all those sizes of those areas.

And, again, off the shelf software that can be used to visualize this, is readily available, but r itself, again, gives you a nice way to plot this in your in your r session as well. But, again, it's not just the volume perspective. It's also the surface perspective, and this is where you can do some really handy things like looking at within your brain the cortex. Kind of this almost like a winding pipe inside your brain in different regions to see maybe where some areas are maybe are getting a little more, you know, condensed. Maybe they're getting plugged. Maybe there's an anomaly in the in the image there.

But these type of surface visualizations, they require a different type of format for visualization. It is called Giftee. Never seen this in my day to day work, but that is helping consolidate the image data into what's called pairs, kind of representing both the left and the right side of the brain in those corners. And she links again to some previous tutorials that she's authored to import these files into R as well via a function called Gifty, another or a package, I should say, called Gifty. Again, freely available. We'll have links to that in show notes as well, where you can then interrogate this, surface imaging, you know, data and be able to get different dimensional representations via, like, the locations, the maybe triangle type dimensions.

And again, you can plot these as well so you can get a visualization of the different hemispheres of the brain, not too like the hemispheres of a globe. Right? You have the left and the right, and then you can flip that around, do different color ranging depending on the intensity or the different areas of these images. So you get kind of that heat map like structure for the left and the right. Maybe some areas are having an issue, maybe more brightly colored than others. And, again, you get the code right here in this post for how you can define these regions and define the different visualization for how you can distinguish those from the other areas and maybe more of the normal representation.

So it is great in the world of bioinformatics, in the world of other, you know, health data, when we're working on treatments that are trying to help deficiencies or maybe areas in the brain that are getting, you know, affected by diseases. The one that comes to mind immediately is all the research that's being done in Alzheimer's disease where they're looking at things like the plaque, amount of plaque in the brain that's impacting tissue as a hypothesis to try to slow the cognitive decline of patients as they're as they're dealing with that debilitating disease.

But the first step, right, is to see what you got. So this great post by Joe, the look at the different packages that you can import this data in and be able to quantify these different regions and maybe point those out via an additional visualization. It looks really top notch. So if you're in the space of visualizing these readings such as MRIs, this is a wonderful post to kind of show you what is possible here. And again, with links to really dive into it further with these great packages, like I mentioned, Rnifty, as well as the GIFSKI package. Yeah. Really great stuff here.



[00:41:01] Mike Thomas:

Yeah, Eric. And this is just super super cool, and it shows us just how fantastic the graphics capabilities are in R. And there were a few publications that were referenced in Joe's blog post that makes me think about doing reproducible science, and how just impactful this type of work is. And we can create these publication ready visualizations programmatically based upon the data. And not only can we, but in my opinion, I think we have to. We must. My only other takeaway here is that I need to see this somehow integrated with the Ray render package for interactive 3 d visualizations of the brain and the different hemispheres.

So shout out to Tyler Morgan Wall, the author of the Ray render package. If you're listening, you know, no pressure, but it would be pretty cool. We don't nerd snipe on this show, do we? Never. It's usually me putting the pressure on on myself or you doing the same for yourself. So it's about time that we just start calling some other people out.

[00:42:05] Eric Nantz:

Alright. Well, if you wanna see more material like that and more, well, guess what? There is a lot more to this particular issue. As always, our weekly is jam packed with additional packages, great tutorials, great resources. We'll take a couple of minutes for our additional finds here. And we are talking about those that are contributing via add on packages to the r community and our data dot table discussion. Well, there is, in terms of contributing to the language itself, there's we have covered a lot of great initiatives to bring developers that are wanting to contribute to R itself in a friendly, you know, open way, whether it's these meetups or these hackathon type dev sessions with the r forwards group and whatnot.

Well, another great resource that's being developed as we speak and really taking, you know, it to the next level is what's called the CRAN cookbook. We'll have a link to this, from the rconsortium blog in the show notes, of course, and this is meant to be a more user friendly yet technical, you know, recipe type book, which is gonna help those new, you know, those new to the R language in terms of wanting to contribute to the language itself. And it really is great for those that are dealing with issues submitting their packages to CRAN and the different issues that they can come across.

There could be just about, you know, formatting your package as metadata with a description file. Could be about your documentation itself of your functions and, of course, within the code itself. So I don't think it's gonna get into all the weeds of those c header issues that we talked about. But, nonetheless, I think this is a great companion to have with, say, the R packages reference of an offer by Hadley Wickham and Jenny Brian as you're thinking about, you know, getting that submission to CRAN and some of the things that might happen that might blindside you if you're not careful, but a great way and accessible way to look at how you might, you know, get around those issues and how to solve them in a way to get your package on CRAN. So I know this effort has been in the works for quite a while. It's great to see this really taking mature and how it's being used by the Grand team itself and where they're going forward with it. So, yeah, credit to the team, the Jasmine Daley, Benny Ultimate, and others, involved with that project.



[00:44:29] Mike Thomas:

And, Mike, what did you find? Shout out Jasmine Daley, Shiny developer in Connecticut. Heck yeah. Yeah. Yeah. Gotta love that. A a bunch of great stuff. You know, one blog that I I found, which was just sort of really nice to reflect on was from Isabel Velasquez over on the POSIT team. It's the 2024 POSIT year and review. A little trip down memory lane of of all that POSIT worked on, in the last year. And, you know, a lot, obviously, around their R packages for interfacing with LLMs, like Elmer, you know, Shiny Assistant, Shiny Chat, Pal, you know, as well as things out of the Quarto ecosystem, including Quarto dash boards being a big one.

Obviously, all sorts of stuff coming out of the Python ecosystem on both the R and, Python or excuse me, out of the Shiny, world in both the R and Python side of the equation there. Some great advancements from tidy models and survival analysis that were really impactful to our team as well as a bunch of others across, you know, WebR. I know that's one that, you know, impacted you quite a bit in 2024. So it was just nice taking some time to do that reflection on, you know, all of the work and investment that Posit and the other folks that, contributed to projects that Posit maintains.

Shout out myself with one small, contribution to HTTR 2 in the latest release, just yesterday. So thank you. It's, I think, 2 words in the function documentation for Oxygen comments, but we'll take what we can get. I was I was on the list. So, thanks, Hadley, for including me among, I guess, 70 other folks who contributed to that latest release of HTTR 2. But it's it's cool to all collaborate together in the open, and I think that's all I'm trying to say here. And it was nice to to walk through a lot of these projects that have impacted me and my team, you know, in 2024 and beyond. Yeah.



[00:46:32] Eric Nantz:

Excellent. And you're on the score sheet as they say. They can't take that away from you. That is awesome stuff. I I congratulations on that. Yeah. It's amazing the breadth of contributions in this space. And, certainly, you know, AI was a focus for them with their with their awesome innovations of Elmer and Maul and the shiny assistant, which I'm really a big fan of now. I was one of the skeptics on that, so it's great to see him doing it and doing it responsibly. So credit to the the team on that. But, no, they're not just resting on those, innovations. As you said, the WebR stuff really is is jiving. It's really, getting a lot of traction, and I can't wait to see where we take that effort in 2025.

And when I say we more like what Jor Stag comes up with, and I'm just a very, shameless, consumer of it, but I love the stuff that he comes up with. So lots of lots of great stuff here. There's never a dull moment in the in deposit team here. And and never a dull moment. The rest of the issues we say, lots of great resources that Rio has put together for us. But, of course, as I said, this is a community effort, and we cannot do this alone. So one of the ways that we keep this project going is for your contributions.

You're just a pull request away from getting your name as a future contributor to our weekly itself. The great blog post, maybe a new package that you authored or you discovered, there's so many opportunities for it. Head to our weekly.org. You're gonna find a link to get your poll request up there right in the top right corner. We have a a handy draft for template for you to follow. Again, leveraging GitHub for the win on that. And, our curator of the week will be glad to get it in for you. And, of course, we love hearing from you. And, we did hear from one of our more devoted listeners about, apparently, I do not pronounce names well. And even though I practice it, I got, called out for it. So, I'm gonna get it right this time.

Nicola Rennie. Sorry for butchering her name. All these months in the previous highlight podcast. Thank you, Mike. Thank you, Mike, for calling not you, Mike. Mike Smith for calling me out on that. I need to be honest with it. So feedback warranted, and, I I may have to have a little cookie jar of, like, funding I send a nickel every time I butcher her name in the future. Hopefully, never again. Nonetheless. Okay. We love hearing from you, and the ways you can do that are through the contact page and the episode show notes as well as on social media as well.

I am atrpodcast@bluesky.social, I believe, is how to call it. Again, this is still not natural to me yet. I'll get there. I'm also on Mastodon with atrpodcast@podcast.social, and I'm on LinkedIn. Search my name, and you'll find me there. And, Mike, hopefully, you don't have a hard time butchering names, so we're we're we're gonna find you.

[00:49:18] Mike Thomas:

You can find me, I think, primarily on on blue sky nowadays at mikedashthomas dotbsky.social, or on mastodon, atmike_thomas@phostodon.org. Or, probably even better on LinkedIn, if you search Ketchbrook Analytics, k e t c h b r o o k, you can see what I'm up to lately.

[00:49:42] Eric Nantz:

Awesome stuff. And a little quick shout out to, good friends of mine, from the art community, John Harmon and and Yani City, because I've been using in some of this r wiki infrastructure I'm building, some of the packages they've created to interact with interface with the Slack API of all things. So it's been pretty fun learning there. And, again, h t t r two is involved in some of that as well. So it all comes full circle in this fancy schmancy calendar thing I'm I'm making. So always learning all the time. So shout out to those 2 for making some really elegant packages to interface with an API of a framework that seemed really cryptic to me at the time. But now now it's starting to demystify a little bit.

Alright. Well, we'll close-up shop here for episode 192 of our weekly highlights, and we'll be back with another episode of our weekly highlights next week."
"27","issue_2025_w_01_highlights_638725121417384171",2025-01-15,48M 7S,"The R Weekly Highlights podcast returns for Our first episode of 2025! We learn how hosted and self-hostem LLM's perform in the summarization of Bluesky R posts, as well as how models hosted on Azure infrastructure summarize cycling destinations. Lastly, we visit the visualization corner once again to bring a little (or a lot) of color in your next…","[00:00:03] Eric Nantz:

Hello, friends. Did you miss us? Oh, we missed you. We are back with episode 191 of the Our Weekly Highlights podcast, our first episode for the year 2025. I was just telling my awesome cohost here I'll introduce shortly. It feels normal again. But, yes, my name is Eric Nance, and I am delighted that you're joining us from wherever you are around the world. This is the weekly podcast, unless we had an extended break like we just had, where we talk about the great happenings and resources that have been shared in this week's our weekly issue.

And, yes, I was solo for a bit there at the end, but no. Not this time. I was right with the world because my awesome co host, Mike Thomas, is back here joining me on the mic. So how are you doing today, my friend? I'm doing great, Eric. It feels good to be back. I apologize,

[00:00:53] Mike Thomas:

to have you have been solo there towards the end of 2024, but I am committed and determined, in 2025 here to get us at least to our our 200th episode. We are close to that milestone, and I'm confident we're gonna go well beyond that.

[00:01:10] Eric Nantz:

Yes. Every there's always hurdles in the podcasting world, but 200 is a nice one to achieve. So we'll we will get there, and then the chips are far where they may, so to speak. We will we will find out. But but as always, it's always so much fun when I don't do this alone. So I'm glad you could join me today. And, also, the Art Wiki project is possible because of the community, and our curator team curation team is back at it after some well deserved rest. And we got a fancy, updated calendar from yours truly, Debu, to keep us honest with it. But this week's issue was curated by Batoa Amrazak, and she had a whopper of resources to look at because this built up over 3 weeks, I believe, since our last issue. So she had a lot to pick from our team that are awesome voting as usual. But as always, she had tremendous help from our Rwicky team members and, of course, can the community like you around the world with your poll request and suggestions.

It is 2025. You know that this year, just like last year and parts of 2023, there is gonna be a common theme in many of the advancements in the tech sector. So we do have some very interesting uses of AI to talk about, and you you're talking to 2 co hosts here that can be a bit skeptical on certain pieces of it. But I will say I learned something from each of these that we're gonna talk about here. And the first one we're gonna talk about is a very extensive and and frankly, enlightening analysis of conversation sentiment and summarization on the new Blue Sky, social platform that we've been talking highly about. And in particular, this post comes from Stephen Turner, who has been on the f on the highlights feature previously. He is the head of genomic strategy at Colossal Biosciences, and he is no stranger to investigating AI models. And in fact, some of his posts from last year were concentrated on using the open source models. That's something I've been very curious about, particularly the llama models coming from Facebook and meta.

And he does mention, you know, they are getting better. You know, there's been a lot of hype around them. He does mention in the post, it's not quite there yet matching the hype, which, again, in this space, you gotta be careful. There's always a lot of hype going around this this area. But with the recent development as of last year of having Wickham's Elmer package, which I've I've had a bit of time to try and we've covered on the highlights before, it opens this new framework of being able to try not just open source models, but also, as Steven calls, the frontier or you might say host of the base models. We're talking about, of course, like OpenAI's chat GPT and others, including one that I don't have a lot of experience with, but the anthropic anthropic, easy for me to say, clogged models, which do get talked about quite highly in the world of development.

So he thought what a great you know, what a use what use case could be that he could compare how the models are doing, particularly this anthrotic, clogged model with the llama models. So he thought, well, blue sky has now got a lot of momentum, a lot of great conversations to be had about our favorite language, the our language, as well as other domain specific science groups that are coming there. So he thought, well, you know what? I am going to do an analysis of these blue sky posts comparing how well these different models actually summarize a large number of these posts over a given duration of, like, months or even a year.

So what's the tooling that's gonna be involved here? Well, with the momentum Blue Sky has had, there is a very awesome package. We may have mentioned it last year called atrrr or attr. I don't know how to say it. That is authored by Johannes, Gruber that basically talks to the API behind the scenes that Blue Sky is using in their operations of how they kind of federate all the posts, and he and Steven was able to grab a 1,000 of these posts that have been tagged with the r stats hashtag, which is again very familiar back in the, you might call the glory days of the Twitter days where we all use the r sas hashtag to share our our happenings and our questions and our our finds and whatnot.

So as always, there's always a little bit of cleaning to do after you get those posts. So use a little dplyr, to clean up the the text of those posts and get it ready to be imp to be supplied to an AI model. And by the way, all the code that Steven has produced here is shared via GIST, that we'll have linked to in the show notes as well. Very, well structured. Steven is is definitely a veteran of, robust r code, and he shows that in his, code here. So once the blue sky posts are prepared, the text behind those, then the next step is to feed that into the models.

So he's got again, he's gonna treat this like an experiment, which I would expect someone like Steven to do, where he's leveraging 4 models. There there's a Claude model, the llama 3.3, gemma 2, and mistral. So and there may be yeah. We'll have to look in the results of this as we get there, but he's going to run the experiments on on each of these. How well does it summarize? And then he's being transparent about this too. He want he will share the results, the summarized results of these posts via a GitHub gist and markdown, but he's not just manually copying all those however many summaries are. No. He's using the gister package offered by Scott Chamberlain, previously from his are open side days, to automatically put that on as a gist. So that that's some pretty slick stuff. So I'll have to take tabs on that for future reference.

So the results. This will either be surprising to you or not surprising to you. I will admit I'm not too surprised based on what I heard. He does say that the Claude model, the Sonnet model in particular, was by leaps and bounds much better at summarizing the content of these blue sky posts than the open source versions of these. Well, you know, and when you run these hosted models, there's no such thing as a free lunch. Right? You have to buy the tokens, if you will, to leverage these API services, so you are gonna pay a little bit depending on how much you use that. Now I I he does say the cost of it, which, again, luckily in this case is not breaking the bank. It's basically pennies on the dollar for this particular case. But let's say you scale this up at a at a company or whatnot. You've gotta you gotta watch this this sort of thing. But he does have an excerpt of the summary produced by Sonnet in this post, and it is very you know, he's not wrong. This is a very concise summary that looks to be pretty on target for the time period that he analyzed in these blue sky posts.

And, Mike and Mike and I were just talking about the use our conference that's about to happen as part of the summary here that's gonna be held in Duke University in in 2024. Talking about some of the notable themes, such as visualization, tutorials, generative art, mobile development. I think we know where the mobile development one came from. That was when Colin Faye made the splash late last year on the mobile app powered by randshiny that's now available, which is included in this summary, points about community engagement, the the hashtags that were used in these posts, notable authors.

I mean, that that's top notch stuff, as well as the top posts, the top ten, if they if you will, that were that were, authored here and such as Hadley's post about posit joinopensourcepledge.com, as well as others from Daniel Navarro, and others included. So you'll have to go to the gist that he made to look at the rest of the model summary the summaries from the other models. He does give a preview that they are quite different than what Blue Sky authored, going from just, you know, coming looks like something that anybody could summarize just in high general, high level overview, or just listing summaries from each post individually instead of, like, the whole collection of them.

So, unfortunately, this is consistent with what I'm hearing from others I trust in the community that the open source models are gaining traction, but there's still a gap in terms of certain domains and what they can analyze or summarize well and what they can produce well in these models. There is some bonus material in this post, but there is another framework for running LOMs in R that I'm not as familiar with. It's called MALL from the ML verse. An interesting way to get predictions powered by large language models, not just, like, summarizing what's there, but actually predicting content as well. And he shows excerpts based on what he did to predict the top 10 most liked posts from the past week and then also translated into another language on top of it. So, pretty slick stuff, I must say. I never tried anything like that, but that's a good, that's a good way to do the translation and also do a sentiment analysis on top of that, which, spoiler alert, a lot of positive vibes on blue sky. Now that let's hope that continues because I I like to have a little positivity in my life.

Nonetheless, another applied type problem that was tackled here by Steven, really starting to see where are the trade offs between the host of the models, the open source models. It's certainly my hope that the open source models do catch up because there are certain things I want to leverage AI for that I don't really feel comfortable throwing over to a hosted provider, and who knows what they're gonna do with that information. I want to be in control of that. I'm starting some self hosted things where I am in control of that. But, you know, as of anything in open source, you know, innovation can take time. The iterations are you're probably gonna have to be a little bit more before they can be on par with these solutions. But Steven does a great job, again, summarizing his experiment very well. The code is well well documented, well commented, and, yeah. Some food for thought for sure. But, at least the positive vibes have been, you know, proven, if you will, in this experiment that they're they're there and some great content that that we're seeing here.

So mixed reaction for me in terms of the endgame of it, but, nonetheless, I think, a a great analysis to kick off the highlights here.

[00:12:36] Mike Thomas:

Yeah. Absolutely, Eric. And I think you can probably hear the skepticism in both of our voices around this type of, content for a while now, but I think we'd also be remiss to, you know, not admit that there are definitely some use cases here where there's value. I've seen people like, Sharon Machlis on Blue Sky talk a lot about how much better they feel Claude does at generating accurate R code compared to chatgpt. I have not really tried that out yet. I've, you know, used chatgpt. To be honest with you, sort of where I found the most value is in generating code for other languages that aren't my first language, but I have enough programming experience to be able to sniff out the issues, if that makes sense.



[00:13:26] Eric Nantz:

Absolutely.

[00:13:27] Mike Thomas:

Yeah. So that's where I'm finding the the biggest value, but if I was just to try to blindly use it to generate our code, I from my experience in the past, and I probably haven't tried in the last last month or 2, but it wasn't it wasn't code that I wanted to to use or, you know, I could have written it just as fast as having the model generated for us. But I'm assuming that that these leaders, you know, Claude and Chad, GPT are probably fairly equivalent as you you mentioned in their ability to summarize information. There is this gap, right, between the paid, hosted models versus the, open source that we have. But my hope is that the open source models continue to to plug along. Right? They're they're gonna lag a little bit, but hopefully, you know, in 6 months from now or 12 months from now, they'll sort of catch up to where the the Claudes and the chat GPTs are of the world are currently. So we'll see.

And I can't help but notice in the Claude Sonnets sort of summarization of all the blue sky posts and how Stephen's able to spit that out in the markdown, I think, that he puts in his blog. It looks a whole lot like the our weekly homepage in terms of the latest. It's all the latest news from the R ecosystem. So I don't know if, the R weekly team can use something like this to make their their lives easier in terms of content curation on a week to week basis or pull requests or god knows what these days, but I don't know. It got my wheels wheels turning potentially.

The one small hallucination that Steven points out, which was pretty interesting, it's it's that USAR conference on August 8th through 10th, and the the post itself on Blue Sky doesn't have a year. So the model stuck 2024

[00:15:20] Eric Nantz:

on there, but, obviously You're right. I didn't even catch that. Thanks for keeping me honest. Yeah. No problem.

[00:15:26] Mike Thomas:

Steven has a, a little tiny footnote at the bottom of the blog post that that points that out, and the the year should actually be 2025, obviously, because it's an upcoming conference. But it's an interesting little gotcha and, you know, as I I look into the sentiment analysis that was done as well that's driven by a large language model, You know, I'm trying to wrap my head around the utility there as well, because, you know, the weird thing about these generative a AI models, as opposed to, you know, something that's that's more deterministic, your traditional machine learning model, is you could put the same inputs in and you may not get the same output every time. Exactly.

So if you're trying to leverage an LLM for sentiment analysis, you know, how do you deal with the idea that, you know, that analysis may not be reproducible. Right? Because the the sentiment may be slightly different, the sentiment output may be slightly different, the second time that you put the same inputs in. So to me, I I have a little bit of a hard time wrestling with that. I mean, I think for, you know, exploratory work and just creative work and things like like that for this the purposes of this blog post, just a one time little sentiment analysis didn't have to be reproducible or anything like that. I think, you know, absolutely. It's totally fine. But if you are, you know, considering trying to do more robust, build more of a robust framework and process around doing sentiment analysis, you know, in your organization or something like that. I think that's probably one thing that you should consider. Right? If you're deciding whether to, use, you know, a machine learning approach versus the this generative AI sentiment analysis approach. But it was interesting.

Love the fact that most of the sentiment was very positive. That is at extreme odds with what I have seen on the the other app, the old app, where the sentiment seems to be quite different. And, I can't say enough about Blue Sky and what's going on there and how empowering and exciting it is to be in that community and how much it feels like like the old world of community, social media, data science, Twitter.

[00:17:41] Eric Nantz:

Yep. And the and the fact that we can we can look at, you know, the sentiment and many other factors of this in many dynamic ways, thanks to the power of APIs, the power of our just our imaginations really. So I'm, yeah, I'm intrigued by where the community goes on that space of it. And and, yeah, you won't find me first in line on that prediction approach, like, what you saw here, especially in my industry. That is, oh, boy. Yeah. That that's that's that's a tough sell for us. So we will find out. Yeah. Predicting with LLMs that

[00:18:16] Mike Thomas:

turned my stomach a little bit when you said that.

[00:18:29] Eric Nantz:

If you are in an industry or, you know, whoever, a large, you know, midsize company, at at this point, you, I'm sure, have some kind of internal towing that's been set up at your organization and are surfacing some of the more low hanging fruits, so to speak, of what AI can do in terms of integrating with other platforms. And I don't know how much common knowledge it is to the data science community as a whole, but I do, you know, in my spare time, of course, listen to other podcasts, and it's been very apparent just how important Microsoft has been in OpenAI's, life, if you will, with the funding that they receive.

And, of course, as you would imagine, Microsoft does have a lot of integrations with OpenAI. I literally deal with this on a daily basis, wherein my meeting calls at the day job, about 5 seconds after the call ends, there's a, summary generated by Copilot just right there. It's happening, folks. So if you are in this ecosystem and you're looking at ways of how you can leverage AI models and maybe integrate with other services in a specific Microsoft type setup, our next highlight does show just the what you can do in that space, and this post is coming to us from Martin Chan, who is a data scientist at Microsoft, where, as I said, if you in your organization are invested in the Azure ecosystem, and, yeah, Mike and I have had our adventures of Azure as well, they do offer an open AI, integration.

Now the the goal in this, demonstration post that Martin has put together is first part of the post is how you actually get set up for this, and there's no way I'm gonna audio and narrate that. That's all very straightforward with the screenshots and whatnot, but he wanted to use, like, you know, a practical way to demonstrate how all this works. He was able to find a dataset called the 100 climbs dataset, which is about different locations around the world. My guess is this is somewhere in Europe if I had to had to had to guess about the different attributes of these locations that would be relevant to a cyclist, including having a URL associated with each location that you can boot up in your browser and read about that location, one of which is called the the Cheddar George or Gorg or I'm probably not saying that right. Nonetheless, it's got some interesting information, and so what what Martin wanted to do is take this dataset, take the URLs associated with each of these locations, which, again, is just a field in the dataset, leverage the Arvest package to extract just the content of that summary of that location or that description of that location, I should say, using, you know, a clever use of CSS div ID so you get the fluff of the whole web page, just the content of that description location, and then leverage an OpenAI model, in this case, a chat gpt4 minutei, within the Azure implementation of OpenAI.

But much like other posts that we saw from Anastasia, I believe, last year, he kind of built this up himself as a way to kind of learn how all this kind of works under the hood, leveraging the previous version of h t t r package to write a custom function to supply a call to the API using a prompt that he supplies a function parameter and then returning the results back in JSON format and then munging that for the rest of the analysis. And so definitely a, you know, pretty pretty straightforward look at all how all this works.

But the results that he got from this, again, were were quite promising, albeit didn't ask you to do a whole lot. It was a summarization of that location, not too dissimilar to what we saw in Steven's post earlier. And it take it, in this case, with this, this location that I mentioned, this Cheddar Gorge location in England, it kinda had the top takeaways of this of this article, such as the climbing details, how popular the location is, significance, you know, the experience of it, and any local attractions around it.

Again, does the job. Right? But it was a it was a neat demonstration of how you could leverage, you know, building your own functions to do this. If you have to integrate with an AI model, but it's not as convenient as, say, what Elmer brings to you, if you are leveraging one of these hosted platforms that may or may not be offered by Elmer, then what, Martin has done here can be an interesting way to prove that out, especially as in the context of this of this highlight here, you want to integrate this pipeline as part of others in that major ecosystem. So here we're talking about Azure. I could see a very similar thing with AWS and what they do with model hosting that you might wanna integrate with, say, optic storage or databases or whatnot.

These are the kind of things that I know are happening more routinely in many industries. I'm still pretty new at it. I'm not as in-depth as many others like Martin is here, but it is interesting to see all the tech giants out there are trying to put services in front of these models to, you know, make it, in their words, easier to run, but also integrating with their other pipelines. Again, just like we talked about in the last highlight, there's no such thing as a free lunch, so you will be charged for these API requests when you do these hosted model approaches.

But I do know some of these platforms are also gonna give their customers a way to not just run a off the shelf model like chat GPT or whatnot. I know some of these vendors are also letting the the company bring the open source models to that platform, and they just provide a front end to it. So that might be helpful in certain situations as well. That's not the topic here, but that's an area I'm looking at, I think, more closely this year when I look at how I integrate this in my daily work. Nonetheless, a great summary here. The code is linked in the post, and and all in all, if you're in the Azure ecosystem, you've got got a lot of options available and, more power to you for what you can do there.



[00:25:05] Mike Thomas:

Yeah. Fantastic blog post, walking through sort of this end to end process, you know, including the custom call, if you will, to the API via h t t r. So and it looks like I used the the GPT 4 o mini model from Azure's OpenAI integration. So, it's nice for us in the highlights to be able to get to see a variety of these different types of, LLMs. And one thing that that he did, which I'm curious if Elmer allows for, is not only provide, you know, what's called a user prompt, but also a system prompt, which is sort of before you enter your user prompt, you're you're instructing the model what you want it to return and and how you want it to behave. I think in the last example where he is, you know, asking for some information about the yellow spotted lizard, he provides the system prompt that says, describe the following animal in terms of its taxonomic rank, habitat, and diet. And then his user prompt is is yellow spotted lizard.

And the structured the structure of the output is exactly what he had defined in his system prompt. So it's it's interesting how those two different things can work together. I know people do silly things with that system prompt that says, you know, responds like you're Darth Vader or something like that. Right? Or write me a write me a poem, about our stats in the the style of Snoop Dogg or something crazy like that in those system prompts. But I think, you know, they're potentially very powerful in helping you somewhat fine tune, and I know that's a loaded word here in this this context, but, fine tune the the output that you're going to get. And, you know, to to me, I think that the future here in terms of applicability at, you know, your organization and and, you know, really squeezing as much value out of these tools as possible has to be creating an easier path towards fine tuning or, you know, retrieval augmented generation, which is is still pretty heavy at this point, because, you know, you're still leveraging sort of this this full LLM file, which, you know, has all sorts of context in it about things that you probably don't care about. Right? And you have a specific task maybe that you're you're interested in leveraging generative AI to assist with. So I'm interested in in projects like, that I'll just shout out here, the the Rasa project, which I think is a Python framework. But it allows you to sort of combine, you know, rules based, I think, you know, if statements, if you will, for natural language processing, natural language generation with large language models to be able to help build some guardrails around, you know, the the chat interface system that you are providing for, your end users.

So, you know, my hope is that as we take a look at sort of more of these general use cases for LLMs, that we'll start to see more targeted use cases, you know, that maybe are more deployable on a a larger scope of problems at your organization as opposed to, you know, a lot of creative use cases at this point. At Catchbook, we're knee deep right now in in some r and d work in trying to build really the best text to SQL LLM model that we possibly can on a specific database that we have, that we expose for our end users so that we can leverage, you know, Shiny chat essentially, which is the the work that Joe Chang and his team have done to be able to build chat interfaces into our our Shiny apps that are tailored specifically and perform really accurately, against the the database, and they write, you know, correct SQL. So we're still, we we found sort of more hurdles than we expected. I think in Joe's demonstration at at Shiny Conf last year, it seemed like it it worked fantastically well. And I think that that was a small data set with really obvious column names, that that sort of helped. And I think in a lot of real world use cases, and I apologize if I'm going off on a tangent now. But that's not necessarily the case, and these things aren't aren't as accurate, as you may expect on larger types of of context windows. So that's stuff that we're working on. I I'm excited, hopefully, to see this whole entire space start to, you know, move, I guess, towards easier paths for developers to to fine tune, these generative AI models.



[00:29:57] Eric Nantz:

Yeah. I see that as the future as well where you want a targeted model or or targeted setup of a model for a specific case and that it is first in class in that specific case. I remember Joe telling me that in the rag piece of it when they were trying to summarize their quartal documentation, he shared this in the AR Pharma talk as well. It was completely off the mark compared to what they were expecting, which doesn't make a lot of sense when you think about it. You would think that just like we're talking about in these two highlights here that we have a given set of text and we know how to summarize that quite well.

But it really depends on what's being fed into it and what the back end model is actually doing. So part of me hopes that as these open source models get more mature, that there are ways for us to kinda tweak pieces of those and then make a different version of a model that's more tailored to, like, a maybe, like I said, a data context. Maybe it's more tailored to documentation context or, you know, a more of a development context. So we're seeing a lot of these being an attack right now, and there's no real clear winner here yet. I think it's a lot of wait and see to see where a lot of this shakes out. So I'm I'm I'm curious where it goes as well.

We're we're really trying to do the low hanging fruit currently, but that it can't stop there. And we're hearing all talks about different advancements and and the level of AI that's being prototyped right now. Sam Waltman's had some very, provocative remarks about where that goes down the road, wherever it gets there this year. Call me skeptical. I'm not sure. But I think the key part for us is to kinda slow down a little bit, do these more practical cases first where we try to to to build the Skynet already, not to be too, Debbie Downer

[00:31:51] Mike Thomas:

on that. No. Absolutely. And I think, we've made a lot of progress on our text to sequel, work. I'm pretty excited about what we've been able to do in the last week or so, and I'm gonna promise a blog post around that because I won't be the one writing it. So it'll actually happen in the next couple months.

[00:32:09] Eric Nantz:

Okay. Well, well, a human writer or is AI gonna write it? No. I'm kidding. No. No. Human writer. Okay. That's great. You got you got the Are We Can Eat videos on YouTube. Yes. That's good.

[00:32:22] Mike Thomas:

But

[00:32:23] Eric Nantz:

it agentic AI. Right? Agents are gonna solve the solve all of our problems. That's what LinkedIn tells me at least. Oh, yeah. LinkedIn will tell you everything. I just want a a robot that washes my dishes. Is that too much to ask? And our last highlight today, well, we're not gonna tell an AI for this one because there's very much a human element that is used when we look at the best practices of visualization. And one area that you do have to address one way or another is your use of effective use of colors in your visualization.

And luckily, the R ecosystem, R itself, has always been, in my mind, 1st in class in visualization since practically day 1 even with base R itself. But if you're thinking about how can I get my head around, what are some ways I can leverage colors in a better way, or how can I make it more routine in my visualization workflow? Well, no stranger to the highlights. Nicole Arrini is back on the highlights for 2025 with a very comprehensive overview on how you can work with colors in R across many facets. So we'll we'll tag team on this one. I will lead off with first of all, you know, you wanna use a color.

How do you actually define how to use it? Well, there's always more than one way to do things in R. And one way that's been supported since day 1, the first way I learned, they have built in names for colors. Obviously, the the standards like blue, yellow, red, whatever, and many more, but you can just specify that when you wanna define a palette for your visualization. She has tomato, sky blue, yellow too, and it's gonna show up right there. Here's where it gets interesting. Maybe you wanna be a little more low level with it. Maybe you wanna integrate it with other parts of your visualization pipeline.

That's where probably my favorite of the next approach is the hex codes because I use those so much in my CSS styling of Shiny apps. It's kinda like that unit language that I'm used to now when I have a browser in my tab of these different, like, color pickers. I type in a name of a color, and then I get the hex code. I might get other values, but then I can copy that, put it into my app as, like, part of a, a class or whatnot. HEX codes are the way to go for that, and they're what's nice is recent versions of Visual Studio Code and Positron.

When you put this hex code as a character string with a pound sign in front, it shows you the color right next to it. I love that feature in my editor. So I've been I've been a hex code, you know you know, addict because of that. So I I I love that. If you wanna get even more low level, RGB, the red, green, and and blue mixture value might be very helpful as well as things like h HSV and HCL, which again kind of putting more numbers on the different hues or the, you know, the different intensities of this.

HEX codes are kind of my bread and butter, but, again, depending on where you are in your visualization journey, one of these is probably gonna be what you end up using. So once you know how to use the colors, now what are the best ones to actually use? This is where she gives a shout out to one of my favorite pack visualization packages, and that's Palleteer by, Emo over at Posit, where it there is, like, I think over a 100 of these specific pallet packages in your ecosystem. Palleteer will let you use all of them and just choose which one you wanna use. So she's got an example here looking at, the Met Brewer palette, which is inspired by the Met Museum in New York, the Terra palette, for example, and and you can just call it with a plot palette function right away and be able to use it.

But you may find a set of color choices that you think look great, keyword you. But, like I often tell my kids, it's not always just about you. Right? Because there's a very important domain, Mike, that we really hope that many get more into their into their workflow and that is accessibility. But Nicola talks to us a lot about new ways that we can think about accessibility when we choose these colors.

[00:37:00] Mike Thomas:

Yeah. I think accessibility around, visualizations is something that we've talked quite a bit about on this this podcast before, and I think the reason being is that it's it's really important to consider. And especially when you think about color palettes. Right? There's a lot of folks who are red, green color blind, and it's very easy with all of the tools that we have at our disposal to accommodate that. Right? And avoid using your traditional red is bad, green is good color palette. Even though that's what a lot of my clients want. Sorry.

Unfortunately, we gotta break them out of that cycle. I try really hard. I promise. I get it. And and there's a couple of great packages that Nicola shouts out here. There's one called the color blinder package. No e there at the end, b l I n d r. And that I guess, it has a function called cbd grid, which simulates how a plot that you've created may look to people with color blindness. That to me is absolutely incredible. I think that's awesome. In in terms of sort of generating colors as well, you know, once you have gone through and created a have a couple of different palettes that are color blind friendly or at least have a workflow to assess the accessibility of the color palette that you have selected so that you can do that iteration.

There's some additional tools and one that I thought was super, super cool because I've had a different workflow for doing this that is a little bit more manual, and I'm excited to switch over. There is an R package I had no idea about called eyedropper with a capital r at the end, and have you ever seen this package, Eric?

[00:38:46] Eric Nantz:

Long ago and I almost forgot about it. So I'm glad Nicole reminded me here. So I had always leveraged like a a Chrome extension

[00:38:55] Mike Thomas:

to be able to, you know, be on a web page and click on a particular essentially pixel on that web page and get the hex code, right, that's behind that pixel, representing the color that I want for brand styling. And this eyedropper package allows you to specify a a path to a particular image, which I think could be a local image and and maybe a URL as well. I'm not a 100% sure. You can do both. Yeah. You can do both. That's great. It looks like, and you can specify the number of colors that you want to extract from that image, and it'll create a color palette by, you know, picking out the top, I would imagine, you know, n number of colors that you specify in the the end argument, colors from that particular image and and give you those hex codes that you can leverage in your own color palette. And I think that's really, really incredible. And and as always, you know, I love Nicola's blog posts. They're they're littered with code. They're really well articulated. The visuals in here are beautiful. I think it's a master class in palette generation and, choosing colors for, you know, not only accessibility, but also, you know, visual styling and, you know, creating these workflows. I mean, she leads off the the blog post by creating this small little custom function for quickly visualizing a color palette, you know, right in the plot viewer in your IDE. So simple, something that I should have, you know, done a long time ago, but I am excited to mooch that code, from the blog post and and leverage that in my workflow, because I think utility here is is fantastic. And maybe a, related shout out that I think is is probably appropriate to give right now is a recent project I think from the Posite team, where they're allowing you to create this brand dot yml file, which can be used across all sorts of different deliverables, that you may have including, you know, your quarto or or Rmarkdown maybe even reports.

I think, your your dashboards, your flex dashboards, your quarto dashboards, your Shiny apps as well. And you can just continue to sort of reuse this, this same YAML file with all of your, you know, corporate brand styling that you have across your different deliverables. So I think that's really, really cool. It allows you to define colors, logos, different fonts that you want to leverage. So I think that's, you know, a nice shout out at the intersection of of this kind of topic here. But fantastic blog post by Nicola as always, and I learn a lot when she writes something.



[00:41:40] Eric Nantz:

Yep. And thanks to her wisdom here and many others in this visualization space, I feel like I don't necessarily need, the pace, extremely high overpriced vendor to teach me about all this all the time. I can I can read this? I can play with it. And for my apps or for my given analysis, I have enough tooling at my disposal to at least get, you know, a lot farther than I would be just on my own for sure. I am keeping an eye on brand. Yammo, like you said. They are still working on the shiny for our support for that, but once that lands, yeah, I'm I'm taking that and running with it. It does work with quartal as you said, so I'm thinking about that for my quartal content. But but, yeah, when you think about what style do you wanna put for those brands or maybe, another type of analysis, having, having knowledge about how palettes work and the best ways to choose and optimize them for for accessibility.

Yeah. This is must reading for any visualization analysis in my humble opinion. Just like our weekly itself, I think that's must reading for everybody too in the art community. But, again, I'm biased, and I'm proud of it. So, Batool has put together an excellent issue here, and we'll give a couple of minutes here for additional fines. Well, Mike was the, I guess, the unwilling victim in the preshow of hearing me speak once again about my enthusiasm for Knicks these days, the, packaging framework that's a slightly different take than containers.

And thanks to Bruno Rodriguez and Philip, they have put together the Rick's package for us to use Nick's principles in the R side of things, and they just had a new version released as of about a week ago. And this version has a couple really important things that I've been waiting for that have landed the first support of. One of which is the ability when you bootstrap these next environments for your r analysis, you can now specify a date for what you wanna associate that analysis with or the packages, I should say, for that analysis.

And then Rix will take care of finding at that time the package versions that correspond to that date, not too dissimilar to what we may have seen from the MRAN project that Microsoft ran many years ago with a way to snapshot your packages based on a date. Now we kind of get that flavor in RICs as well with this new parameter. Also, for the RM users out there, and I am one of them, you now have a function to take that RM block file and convert it to a Nix default profile that will, at in different aspects, either try to get as perfect for the version of that package as possible or get close to it. There are trade offs. It's not gonna be quite one to 1, but boy oh boy, this is a huge advancement for me as I think about legacy projects and adopting them to a RIC's workflow. So I'm super excited.

I've already tried it out multiple times. It's working great, and I'm now using it with Positron no less to have a custom R environment fronted by Positron. I dictate the version of R, the version of packages. Positron just grabs it, and I am off to the races. I feel like a kid again, Mike. I can't get enough of it.

[00:45:03] Mike Thomas:

I love it. I love it. I'll be there someday. I'm not there yet, but I will be there someday, and you know it. I just lag a little bit behind you. I found a blog post, in the highlights as well from Stephen Paul Sanderson, and it is in sort of my world of of finance, that we do a lot of. And it is an analysis of NVIDIA's recent earnings call, that leverages, I guess, data in this this platform called Dotada, Dotata's knowledge platform, which is essentially a database designed for investment professionals.

So if you're in this space, I think this is a really interesting way that it looks like now we're able to access, financial statement, financial earnings call content, in a from a sort of database like structure instead of having to do all sorts of crazy sentiment analysis and natural language type of stuff, to be able to make sense of this data. So I thought it was a really interesting, blog post that I want to shout out.

[00:46:02] Eric Nantz:

Love it. Yeah. I'm gonna be taking a look at that too. I, this machine I have here right next to me has an Nvidia card that's over 5 years old. I may be upgrading soon, but not sure how much I wanna spend on the bank on that because, graphic cards prices are not exactly cheap these days. You can guess why. Yeah. Not just gaming anymore, folks. None nonetheless. Yeah. It's a fantastic issue that Betola has put together here. But, the Art Weekly project is, again, driven by the community here, and one of the best ways to help out is to send that great new resource at, via a poll request, which is linked at the top of the page of each, issue.

You can fill out a quick template. We'll get the curator of that week. We'll get it in for you. And, also, we love to hear from you. We did hear from a few of you, you know, over the break. We'd love to keep that dialogue going. We have a contact page in the episode show notes that you can fill out. You can also get in touch with us on social medias. I am on Blue Sky as as I've been speaking highly about. I am atrpo our podcast dotbsky.social. It's still not natural for me to talk about yet, but I'm getting there. I'm also on Mastodon with atrpodcast@podcastindex.social, and I am sporackly on the x thing with at the r cast. And, Mike, where can listeners get a hold of you?



[00:47:23] Mike Thomas:

You can find me on bluesky@mikedashthomas.bsky.social or on LinkedIn, if you look up Ketchbrooke Analytics, ketchbrooke, you can take a look at what we're up to.

[00:47:36] Eric Nantz:

Awesome stuff. I love to see where you're up to on there. And, yeah. Let's keep the positive vibes going for 2025. That's our plans anyway. So thank you so much, for being patient with us while we took a a a break at the end of last year. We are back in the swing of things of our weekly, and we're happy to share more great content on our road to episode 200 when we actually get there. So, again, thank you so much for joining us, and we will be back with another edition of our wiki highlights next week.

"
"28","issue_2024_w_51_highlights",2024-12-18,39M 40S,"An evolution of asynchronous programming techniques to boost your Shiny apps, how a puzzle from over a century ago could tip the scales for mathematics algorithms solvable in R, and solving the recent 2024 Advent of Code puzzles with data.table. Episode Links This week's curator: Colin Fay - @colinfay@fosstodon.org…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 190 of the Our Weekly Highlights podcast. This is the weekly podcast where we talk about the terrific highlights that have been shared in this week's our weekly issue available at ourweekly.org. My name is Eric Nantz, and, unfortunately, I am fine solo again this week because my awesome co host, Mike Thomas, is currently a knee deep and finishing up some really high priority work projects. I know from experience that the end of the year can be quite intensive to get these things wrapped up because anything can happen. Right? So I'll be holding the fort down today, but as usual, I never do this alone in terms of the project itself because this week's issue was curated by the esteemed Colin Faye, who had tremendous help from our fellow Aroki team members and contributors like you around the world with your awesome pull requests and suggestions.

And one little bit of admin note before we go on further, I am now recording this on my media box that my media server, I should say, that is just as of yesterday of this recording, been refreshed from the ground up to be powered by Nix OS instead of the standard Ubuntu Linux distribution. So, hopefully, you won't notice a thing in terms of listening to this, but I have been knee deep in getting things reconfigured. And I think everything's working, but, hopefully, this episode goes without a hitch. Speaking of our curator of the week, the esteemed Colin Faye himself is leading off our highlights because he has authored a terrific blog post on the think our blog on a very, very important concept to think about with not only within the computation of various data analysis task in R itself, but how it relates to you as a Shiny developer to give your users the best experience and the best response times.

This is a very difficult task, and he leads off the post with, you know, an a very humorous saying about the the 2 or 3 things that are hardest to do in computer science. And, yes, a couple of them do relate to performance, and the one we're gonna be focusing on today is asynchronous computing. And, yes, there is a lot of ways we can tackle this. But first, Colin sets the stage, if you will, to get us all, you know, in the right mindset of the different types of what I'll call performance enhancements or high performance computing aspects that you can give to your computer science type programming task.

One of those is parallel computing. This is tailored more for when you have, say, a group of things that maybe it's, you know, a set of countries in the world with a data set. Maybe it's the states in the United States in a data set. Some kind of grouping where you could do an analysis on one particular group, but that other group doesn't have any impact on it. In other words, while you could do an analysis sequentially across these different groups, it may be more advantageous to lurk look at all these groups in parallel and do separate, you know, analysis tasks for each of them.

So very much the split, apply, combine kind of mapping, reduce out, you know, paradigm that he has some nice illustrations to show how that works. And there is a very important R ecosystem that helps with this framework immensely, and that is the future set of packages. You might call it a future verse, if you will. That's, originated by Henrik Bengtsson, who I've had the great pleasure of meeting at previous RStudio Conferences. But there are many additional packages in the future ecosystem on top of the future package itself that kind of tailor to different back ends or different paradigms that you can do parallel computing.

And, yes, with certain parallel tasks, you can get massive improvements in execution time versus doing the sequential approach. But, like everything, there's trade offs. Right? So maybe parallel computing may not be the best when you have to deal with kind of the upfront cost of what Colin calls the cost of transportation, meaning that you have to somehow allocate to these different processes that are being launched kind of the setup to get things going. 1st, analyzing the code itself to figure out what are the functions that it depends on, what are packages that depends on, any data objects or other R objects that it depends on, and then writing or sending out those prerequisite objects to disk into so it can be loaded into a new R session.

And then when everything's done, you've gotta assemble everything back together. This is pretty much negligible when it comes to, like, more simpler computation task. Not so much for a task that maybe has that upfront cost that maybe takes a bit of time to transfer, and it may end up being faster if you just did it all yourself anyway. I know how that goes in real life too, and Colin's got a great example to illustrate that. So, again, the other key part to keep in mind for parallel computing is even though you are launching these different, you know, group processing on different sessions, whether it's multiple CPU cores or multiple servers, in the end, you're still waiting for that result. You just hope that you don't have to wait much longer.

The other paradigm, especially the the meat of this post, is the concept of asynchronous programming. This is a little different because it's not so much dividing an overall, you know, dataset or or or other object into groups and then launching those in parallel, you have maybe various tasks that are, you know, maybe they're related, maybe they're not related. But you wanna make it so that you can launch those in the background and then at some point, get a check to see if they're readily available to you.

That's the concept of asynchronous programming. And, yes, the the future package can help with this too, and he's got examples in the code to show what this might look like with some contrived sleeping and and messaging. Now, another thing to keep in mind is that this is not blocking the r session. You could type over r code in the console, and then when that task is finished that you you launch in the background, a message will pop up depending on how you configure the function to do this. Now, this could be a bit of a challenge to figure out how do you manage a lot of this together, how do you track when a task is done in this asynchronous approach, and then what do you do when you actually get those results?

That's where the promise package comes in and promises, I should say, which is very much inspired by, you know, JavaScript, utilities such as promises that come fundamental of modern JavaScript languages. The promises package kind of gives you that same paradigm where you can define then what happens for a given function that when the result is done or when things go wrong, I e, what should what should it output or what should it return when there's an error in that execution? And then what do you want the user to see or return from when everything is done, whether it worked or not? So there's a good example here about how to tie the 2 together, future and promises in an arbitrary type of function setup.

And you can see that you can, you know, define these 3, you know, these 3, scenarios. What happens when it works? What happens when it doesn't work? And no matter what, what happens when everything is done? This, you may be wondering, okay. That's great, Eric. What does this have to do with Shiny? Right? Well, in Shiny, you definitely want to think about asynchronous programming, especially in the scenario that Colin outlines here when you have multiple users, you know, re using your app at the same time, the traditional paradigm in r, which is affected by Shiny as well, is it's single threaded, I e one overall process running that execution.

Hence, if you don't build asynchronous programming in your Shiny app, if you have 2 users and 1 user starts to launch a task, it maybe takes, you know, a handful of seconds or even a minute to complete. That other user, when they launch that same Shiny app and they hit that button, they have to wait to even start their back their analysis until that first user has completed their task because there's one session per app in a typical execution in R itself. So the good news is everything we just talked about of asynchronous programming can be used in Shiny as well.

The previous generation of this that he starts off with was this combination of future and promises, and it was many years ago at a Rstudio conference that Joe Cheng, the author of shiny himself, had this interesting example of what he called the cranwhales app to show how he could optimize a shiny app in production, and it was using the same integration of future and promises. Now the way they configure this is, again, building upon the the more, streamlined example of the r process itself. But Colin has an example here where it's using what he calls a multi session future plan, which means that asynchronous task is gonna be spun on multiple r processes with 3 workers. We'll keep that in mind for later.

And then based on hitting a button that will basically, format the the current date and time, he defines then the function wrapped in or the set of code wrapped in future to generate a random normal distribution. With a set of numbers and then defining what happens when the result works, when the result fails, and then what happens when everything is done. This takes a bit of bookkeeping to set up, but there is one important assumption to make here. If there are more than 3 users, let's say a 4th user goes to this app, the first three users are able to launch their task when they hit that button in parallel.

But that 4th user, oh, no. Sad panda face. They have to wait because everything is blocked because that meant there were 4 future sessions launched in 3 sessions, and when those sessions are busy that other users gotta wait. So there could be improvements in this, and yes, there are. There is a wrapper combining these two concepts of future and promise called future underscore promise is that you can now launch this asynchronous type processing, get a queue going, and then as these, sessions finish, it can then launch new ones that come as soon as one of those workers become available instead of all the workers being available.

That's a nuance there, and he's got a nice illustration to show this in action along with the revised code to to launch this. So, again, the setup looks pretty similar. You're just swapping in, future underscore promise instead of future. This is working better. But there but, again, that 4th user will still have to wait a little bit until at least one of those three tasks finishes from the previous three users. But there is a new paradigm, something we've covered on our wiki highlights a while back this year, the extended task function that now comes native since shiny version 1.8.1, which is giving you an r six class to provide native asynchronous support without a lot of the what I'll affectionately call somewhat odd setup that the future and promises, package and a shiny or a combination and a shiny app required you to do. You will see the example when you read the post, but it was never quite intuitive to me.

Extended task is in the right direction. I would say to make things a bit easier because you're creating a new task with an r six object, putting in your future promise, where, again, some of that boilerplate still in there. But this is going to help the user experience quite a bit because you get some nice helpers with this as well, such as having a button in your Shiny app that can be automatically relabeled on the spot when the user clicks it to give them an indication that something is happening. So you get a little processing message on that button when they hit the button so this is again a great improvement to help unify things a bit more and extended tasks has some really nice documentation that we'll link in the show notes as well.

So when I saw this post, of course, I love seeing, you know, very, you know, detailed, you know, overviews of asynchronous programming because it is such an important concept in Shiny development. But I did have another thing in mind that could be put in this same paradigm, but I'm gonna flip the script a little bit. Colin is coming at this from multiple users, accessing an application at the same time. That's not always the case. I have situations, and I know many others in it in the in the industry do, where it's not so much many users hitting the app at once, but one user, their experience of running the app, they may want to launch something that's gonna take a while, but yet do other things in the app without the whole session being blocked. So what's I call within session, asynchronous programming is desired.

And for this, yes, extended tasks can be used in this as well, but I do dare say that the combination of Will Landau's crew package with Charlie Gao's MirrorEye package can be a really attractive paradigm for you as a shiny developer. If you know there are gonna be tasks launched in your shiny app that could be as simple or extremely complex, crew with MirrorEye is gonna give you an elegant way to launch these workers without really knowing ahead of time how many to launch per se. It's gonna kind of adapt to it. It's got some defaults that you can tweak, but the example that I'll link to in the show notes is from the vignette of KRU where they have a shiny app where you can hit a button and and simulate basically 1,000 coin flips.

The default version of this, the output of, like, the heads, tails, and the number of flips is very laggy, the update, because everything's kind of being done in one overall process. But with crew and MirrorEye, when you hit that button, it's like looking at a a speedometer, and it just continuously increases faster, and you could be able to type the the numbers. It is amazing to watch. So I would say if you're in this paradigm of within session trying to optimize the performance as best as possible, KRU and Mirai is a combination you definitely want to look at.

And the good news is that Will and Charlie have also talked to Joe Cheng about this, and he's been playing with it too. So there may be more integrations between these two paradigms of shiny's extended task and Mirai and Krue in the near future. Nonetheless, a terrific post by Colin on what can be an extremely difficult concept to master as a shiny developer for many many different reasons, but this is a great way to get started and I invite you to check out these terrific resources in the blog post as well as Will's and Charlie's crew and Mirai package respectively.

Oh, there's that time of year wherever you're, you know, getting some much needed time off from the day job, spending time with family or friends or whatnot. You might get a little downtime once in a while, and you wanna, you know, maybe do something interesting and challenging for yourself. There's definitely an appetite to look at different, you know, various puzzles, especially with coding, which we'll get to in a little bit. But there are some really interesting ways to challenge yourself yourself from very, you know, historical yet very interesting publications from centuries ago.

And our next highlight here is giving you a little brain teaser to work on, and, yes, you can use R to help out with it. Data scientist Nina Zumo, who is a consultant for the WinVector consultancy firm, she has a new post on the Rworks blog, which, by the way, I wanna give kudos to R Works because I don't think we've talked about very specifically on this on the show. This is kind of the spiritual successor to Joe Rickert's immensely successful and highly acclaimed our views blog posts that you would have from his days of revolutions computing in that posit as well. But this is kind of like the next generation of that, and Joe is one of the editors of this along with Isabel Velasquez, but they give you the opportunity to put posts on their blog if you if you wish to contribute. It looks like Nina took them up on this offer multiple times.

Nonetheless, what Nina's talking about here is a a great brain cheeser with mathematics that you can, you know, you actually use r to help solve, and this is rooted into the 4 weights problem, which has a long history in it itself. This was from a article author by Henry Dunanese, from The Strand Magazine back in December of 1908. That's over a 100 years ago, folks. But, yes, this has all been digitally archived, so we can learn from this too. And the basic premise of what it's called the 4 weights problem, think of having a scale that can be bound a weighing scale that has, you know, two sides left and right, and the object is to find 4 weights. They all can't be the same of 4, say, weighted objects that in the integer range of 1 to 40 can be put on these two levers of the weighted scale and still be balanced together.

That sounds it already sounds complicated to me. I mean, this would take me a few maybe a few hours, if not longer, to sit down and do. But you want to be able to find what are the best ways to get to this solution. So you one thing is just find the values that could work that could, you know, add up to the value of 40 without going over. But then how do you balance that together? So so the fundamental paradigm behind a teaser, a brain teaser like this, if you will, is that this is kind of based on a a different type of arithmetic, maybe, than what you may be doing in your day to day because, you know, that she she introduces that there are weights of 1, 3, 9, and 27 that could satisfy this, but you have to figure out then what this unknown quantity x to make sure that on one side of the scale that we can distribute these weights so that the scale actually balances. And, yes, that is somewhat contrived because you wouldn't have to weigh everything if you already know it ahead of time, but this is rooted into modular arithmetic for how you can solve this.

So she starts with kinda, you know, figuring out the best way to kind of change our thinking of this with this unknown quantity x, and how do we shift our lens of this to a base n type representation. This is where it'll get, you know, a little complicated to summarize in audio. But the idea is that you have this you want to represent a non negative integer with up to m digits and binary representation. And if you recall, binary digits can be represented by any number in a range from 0 to some arbitrary m of 2 to the power n minus 1.

So there's some pseudo code here to look at the different ways we can transform this x into base 2 notation, and in the end get to a trinary representation of this that can satisfy these requirements. And, yes, there is some neat r code. To do this, she's got a handy function called basen that will convert that x to it's a base end representation. And it's got a, you know, a simple while loop that based on certain conditions trying to figure out the the best way to represent that. And we can see then that when we want to convert 13 to binary, we can use this function feed in 13 as a verse parameter, and then the base of this notation is usually 2 by default and the number of digits for maximum.

And in the end for this example of 13 we get the result of 1, 1, 9, and 1 and then that can be converted to what's called a ternary representation with another function that she has constructed called 2 decimal, where we're gonna feed in the the number of digits to put in that, and hence we get 9,298 as the result to convert that to 18. So now that that gives us ground in for it to come into base functions. Going back to that 4 weights problem, we've got to now figure out with an equation that she writes here these for the for the x these different kind of coefficients, if you will, in front of these 4 weights and figuring out what are the the the coefficients that we can put in front of that. So, again, some nice r code to kind of loop through this with a a function that she wraps. She calls it way to take this x.

And, again, we got, you know, bounded by 40 or or or less. Otherwise, it'll be out of the range. She's got some arithmetic to figure this out, as well as scaling the notation as well, which can be helpful to get that trinary representation in that. And then she gives some examples of trying these functions out, and you can see that we've got a few few different options here. And in the end, the answer that she comes up with is on one side of the weight scale. You put 1 and 3, weights of 1 and 3, and then the other side is just gonna be the the leftover of x.

So there's a well, a few different ways that they carry this out. She's got multiple examples here. But, in the end, I think this is something you're gonna have to sit down with it a bit and maybe, again, try to challenge yourself in some of these other puzzles that could be found in publications like this. But it goes to show you that with basic arithmetic, yeah, R can do just as well as any other programming language to convert, you know, the the pseudo code that you're kind of going through in your head as you come up with an algorithm to fit for purpose functions that can tease this out. And we'll have a link in the show notes as well to Nino's, first Nino's, first blog post about this. It gives more background on kind of the the other ways you could tackle this.

But, yeah, very good brain teaser. I didn't do it justice in audio, but you're invited to check it out if you wanna challenge yourself a bit when you get some downtime this, holiday season. And speaking of challenging yourself, I did allude to a bit earlier, this is also a very popular time of year to challenge yourself with some coding puzzles, too. And the one, you know, area that gets, gets spun up almost every holiday season for the past many years has been the advent of code, where for every day in this advent calendar, a new coding puzzle is released, and you're invited to use whatever language suits your fancy to solve these these challenging problems.

I admit it's on my bucket list to be able to do this someday. I just haven't had as much time yet to do it, but maybe someday I will, maybe in retirement or whatever. But that's not that's not soon for sure. Nonetheless, there is there is some great opportunities to leverage your favorite data science language, such as R, to handle this. And, of course, within R itself, there are many different ways to tackle this too with various packages and whatnot. And so our our last highlight today is authored by Kelly Baldwin, who is a assistant professor of statistics at the California Polytechnic Unif State University, and she's actually authoring this on the data dot table blog, the our data table community blog, to be specific, because she wanted to challenge herself.

She's, again, always been, you know, very happy about the Advent or Code series of puzzles, and she wanted to see, you know what? Just see what it takes to use data dot table and some of these puzzles. So that is really awesome. This blog post has actually been updated with with a couple solutions here based on the puzzles that started on December 1st. And so the first puzzle in this advent of code for this year is a bit of a warm up if you will where you have 2 lists. You might say 2 columns of numbers, and the goal is to compute the distance between the adjacent elements of these lists. But you want to do it from the least, you know, the smallest number first and one list and then the smallest number on the right list. And then kind of keep, you know, sorting accordingly to figure this out in the most elegant way that you can. And so with data. Table, Kelly shows that this can be a pretty straightforward operation where you're just simply using its, sort function to basically mutate in place, if you will, or change the variable in place after reading this text file, and then create a new variable called diff, which is again the absolute value between those two numbers, and then computing the sum of that at the end.

That was pretty straightforward. Basically, four lines of code with data dot table, you know, not too challenging there. And then part 2 is a a little bit different because you can do things a little because in part 2 in part 2, she is now looking at the kind of distance or similarity, if you will, between these these two these different types of distances that have been computed. And sure enough, that's yet another variable that you can assign in data dot table as the sum of the previous of the variable where the or the sum when the value in the first column is equal to the the the various values in the second column only when they're equal, multiply that by the other variable variable one value itself, but then grouping it by those unique values in the first list and then summing up that similarity calculation.

Again, two lines of code for that one. So, again, pretty pretty straightforward once you open that text file to solve that with data dot table. And then there is yet another puzzle that has multiple parts to this as well from December 2nd, where the background of this this puzzle is there are certain reports that basically can be represented, by one line in a in a matrix, and that the report is a list of numbers called levels, which are basically the columns. So in the example that I will link to in the in the show notes or, I should say, the problem in the show notes has 5 levels, 5 columns for each report list, but the goal is to, quote unquote, find when these variable reports are safe, which basically means that the levels are either all increasing or all decreasing, and any 2 adjacent levels differ by at least 1 and at most 3.

So those are 2 constraints to keep in mind. So this this could be a bit of a challenge. Right? So again, what can data dot table do here? So after importing this, this, set of data, Kelly defines a function called check report, where it's gonna determine if these this vector of numbers, in this case kind of like the list in that report vector, if they have negative or positive integers, and then be able to figure out then for whether it's too big or whether it's increasing or decreasing, an indicator for that, and then returning whether that was increasing or decreasing, and if it's not too big. So basically, the report will the check report function will return true if those conditions are met and false otherwise.

So she takes this report, the dataset, transposes it first, and then with data dot table basically computes then for the different, for the different deltas between these different columns in in the dataset applying wherever it works in a check report fashion. So it's kind of doing column wise, but applying that with the apply function on each of these and sure enough she was able to determine that those are those are working well. And then in part 2, she has a function of testing all this to make sure that this is working as well, and sure enough it it is as well.

So she wasn't very happy about using the apply function for some of this, so she wanted to, you know, make see if there's another way to do it. And, yes, in data dot table, there is yet another way, another way to do it with data dot table itself where she's using the call sums function. You know, I should say the call sums function wrapping those, those various calls or those indicators that you can use after you compute those deltas. And, again, very very similar. And she hasn't done anything with data dot table yet for December 3rd or 7th, but we'll check back on the blog as as we progress in Advent of Code to see where this shakes out. So, again, Advent of Code, fun little puzzles to entertain yourself, you know, with some fun challenging exercises.

I heard I believe they get harder as as the as the time goes on. But again, there's no nothing's too easy here, so to speak. So great way to challenge your challenge your, skills here and use a little r and your favorite package like data dot table in the process. And, of course, there's a lot more in this terrific issue of rweekly that Colin Faye has curated for us this week. So I'll take another minute here for my additional find that I wanna talk about here in this issue. And you know me, I always like to see a really great showcase of data science in action with analysis and visualization, and there is a wonderful dashboard that has been contributed to this issue called the Lime bike analysis.

This has been authored by Nils Indetrin. And if I had to guess, yes, indeed, my spider senses are correct. This is actually actually a flex dashboard. Retro, so to speak, with our markdown. But when you look at this, it looks absolutely terrific. And that what a Lime bike actually is, It's an electric bike and scooter sharing company, and they have, you know, they are in various cities around the world. And so they're look this dashboard is looking at from a single Lime bike account in London the different total distances by month and day of week that have been traveled, a nice little set of dashboard icons at the top, and an interactive map looking at the concentration of the trip start and end locations, and another tab for estimated costs.

And boy oh boy, these charts are powered by e charts for r and high charter and leaflet for the math visualization. Really elegant looking dashboard. Lots of great ideas that you could take everyone is stick with flex dashboard or even use this with quartile dashboards as well. Just love the interactivities. These plots just look amazing here, like, interactive clicking. I'm playing with it right now as I speak. Really fun to explore here. So credit to Niles for creating this terrific dashboard. And, again, there's no shiny involved here. This is rendered, if I had to guess, via either GitHub actions or just rendered on the spot when he committed this on on the repo. But lots of lots of great ideas to take from the source code here, so I invite you to check that out as well as the rest of the r weekly issue. We've got a lot of terrific content here for you to look at, but also it is getting near the end of the year, and we may have one issue left for the rest of the year, but we could always use your help for helping with the project, and that can be through your great blog post that you've either authored or you found online, maybe a great new package that has been very helpful to your workflows, or even other great content out there in the art community.

We'd love to hear about it, so you can send us a poll request at r wiki.org. There's a little Octocat icon at the top that'll take you to this week's current issue draft where you can get a template going when you submit a poll request. We have a nice set of pre filled, you know, notes that you can use to figure out which category your contribution goes in, but our curator, Faric, will be glad to merge that in for you. And, also, we love hearing from you on this podcast. We have a lot of a few different ways to do that.

You can get in touch with us via the contact page and the episode show notes on your favorite podcast player here. It'll be directly linked there. You can also send us a fun little boost along the way if you're listening on a modern podcast app like Podverse or Fountain, Cast O Matic. There's a whole bunch out there. You have all details in in the show notes as well. And you can find me on social media these days. I am mostly on Mastodon. I am with the handle at our podcast at podcast index.social.

You can also find me on Blue Sky recently, or I'm at our podcast dot bsky dot social, I believe. Either way, it's in the show notes. I just haven't memorized it yet. You can also find me on LinkedIn. You can search my name, and you'll find me there. And just a little programming note, this may be the last episode of 2024 because my, my 2 little ones are not so little anymore, but their time off of school is starting later this week, and we want to spend some good quality time together. So I'll probably be not doing as much technology things and won't have as much focus time to do an episode. But, nonetheless, I definitely thank each and every one of you that have stuck with us throughout the year and have enjoyed our weekly and contributed to the project and contributed your kudos to this podcast.

Again, we love hearing from you. It always is a a great pick me up when we see these great comments, whether it's Blue Sky or her Mastodon or LinkedIn or whatever. We we enjoy the feedback. So with that, I'm gonna close-up shop here. I I fly solo today, and it probably showed, but I did the best I could nonetheless. But, again, hope you all have a wonderful rest of your year in 2024 with however you wanna celebrate holidays, time with family and friends. Enjoy it. Hopefully, you get some good rest and recharge with next year.

But, nonetheless, that's gonna close-up shop here for episode 190 of our weekly highlights, and we'll be back with a new episode of our weekly highlights in 2025."
"29","issue_2024_w_50_highlights",2024-12-11,1H 1M 28S,"The future of R-Universe looks even brighter for 2025 and beyond, revisiting the key factors for possibly switching to the Positron IDE, and why there is more than meets the eyes when it comes to the potential of LLMs and AI (even in highly-regulated industries). Episode Links This week's curator: Eric Nantz - @rpodcast@podcastindex.social…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 189 of the Our Weekly Highlights podcast. This is the weekly show where we talk about the terrific highlights that are shared on this week's Our Weekly Issue. My name is Eric Nantz, and yes, we are now getting towards the middle of December. And it is the season of giving in many, many were parts of the world. And, of course, over the weekend, my, kiddo was nice and gave to his entire family a nice little nasty cold that I'm just recovering from. Hopefully, the voice is up to snuff for this episode.

But, yes, I am here. But, luckily, I'm not here alone this time because fresh from his travels across the country is my awesome cohost, Mike Thomas. Mike, how are you doing today?

[00:00:41] Mike Thomas:

travels across the country is my awesome cohost, Mike Thomas. Mike, how are you doing today? travels across the country is my awesome cohost, Mike Thomas. Mike, how are you doing today?

[00:00:55] Eric Nantz:

you. And luckily, no Wicked Witch is, delaying your flights or anything, so that's terrific. Not this time. Thank goodness. Yes. Yes. It is a busy travel season, so we're always, like, you know, consider ourselves lucky when things go smoothly. And and, honestly, we are also very lucky that we have, on my opinion, a fantastic issue. And I'm not just saying that because, check check notes here. Oh, yeah. It was me curating it this time, but, no, this is awesome because I have all of you in the community to thank for your awesome resources. I was able to merge into this issue, and and I dare say, I think this was a human smoother process than the last time I curated it. And we had some nice poll requests that I merged in and a lot of content that we can talk about. But we'll be focused on the highlights this this time around. But, of course, since this will be the last issue I curate in 2024, just wanted to do a dramatic pause there for a fact, I definitely wanna thank the rest of the team throughout the year. It's been terrific fun to work with you all, and I'm looking forward to having more great great adventures of our weekly in 2025.

But without further ado, let's get right to it. And we are talking about now for our first highlight, a very key piece in the world of the R ecosystem, in particular with package repositories that has had already a wonderful effect on many parts of the r community. And with this recent news that we're about to share, we think there is even bigger things to come for this project. What are we speaking about? This is the R Universe project that has been led by R OpenSci and their, chief engineer, Yaron Ohmes, who has shared with us and as long with the R Consortium blog that the R Universe has been named the newest top level project under the R Consortium umbrella.

Your first question probably is, what does a top level project actually mean here, Overnon sounding really important? Well, and technically speaking in the blog post, it is a particular project that was now going to get 3 years of funding from the ARC consortium, and this is a recognition of this being recognized along with other projects that are currently designated top level which include DBI, the database, you know, common interface that many of the R Packages involving with databases utilize, as a key dependency.

The R Ladies initiative and the R user group support program or the RUGS program helping to bring, you know, infrastructure around having all sorts of R type meetups and community resources. Our OpenSize R Universe is now joining that effort. And there's a great quote from the executive director of the R Consortium, Teri Christiani. She says and I quote, the ability to find and evaluate high quality R packages is important to the R community, and the R Consortium is pleased to support the R Universe project with a long term commitment to help strengthen the foundation of the r package ecosystem.

We are pleased to be working more closely together with our rOpenSci on this effort. And there's also a great quote quote from the new executive director of rOpenSci, Noam Ross. He also touches on, you know, the importance of our universe and their excitement to work with the our consortium to strengthen the infrastructure even more. And we'll also have linked in the show notes. The the you might say the companion blog post to this from our open side directly, from Noam Ross and Yaron who co authored it. It highlights another point that I do want to emphasize is that our universe, yes, on its own is already bringing this revolution of a, you know, a package repository that's powered by a lot of DevOps principles, a lot of automation, and very very intricate infrastructure to help you as a package author give your users a way to have not just a source of a package available, but binaries that are compiled, and more recently even WebAssembly compliant version of your package. That's a huge win right there.

But our universe is now serving as the foundation of newer efforts in the r package management ecosystems. There is a new effort and I must stress it's early days, but there is some excitement around the R multiuse prod multiverse project, I should say. This is being headed by a team of people including Will Landau, the author of Targets, and this is building upon our universe's infrastructure on their automation, their compilation of binaries to help bring a transparent kind of governance that combines with our our universe's infrastructure for another type of package repository.

Certainly, industries are paying a very close attention to this. It is early days, but it just goes to show that the Our Universe platform could be used for more things than just Our Universe itself. And that's what we're probably gonna see in 2025 and beyond. So I am thrilled to see this, you know, more, you can say, rigorous or just more robust backing of the project. We know that times can be tough financially for certain vendors. So the fact that now, our open side can count on this additional funding for this particular project, I think it's gonna be a huge win for your own and and the rest of the our open side team as they make, our our universe even more of a awesome platform for a new and and older package authors alike in the our ecosystem.



[00:06:55] Mike Thomas:

Yeah. Eric, one thing that I will say is a quick call to action as you get toward the end of the year and this isn't to to my organization's own horn or anything like that but we donate every year as a charitable contribution to the our consortium into our open side if you work at a company and I think that it would make sense and you use our there and it benefits your company, I think it would make a whole lot of sense to ask them to do the same thing. This is the season, of a lot of corporate charitable donations, so it might be an easy yes and I think it's certainly worth asking the question considering how much some of these projects have given back to a lot of us. Our universe is incredible. If we think about what we sort of had to do before our universe, there was really no searchable way to figure out what sort of packages are on Korean. I mean, you can do it through, I think some of the API tools that'll give you a list of all of the packages on Korean right in your console.

And then you could do some filtering, but there's no interactive GUI, very visual, hyperlinks to all of the vignettes, with a hyperlink to the GitHub repository, with graphs that show when the most recent contributions were and who the contributors were it's just this fantastic visual medium uh-uh to be able to take a look at all of the different our packages that are out there that at least have been picked up by the our universe project and the scale of it still blows me away. This this whole infrastructure that your own and others has created is it's mind blowing that you know the amount of automation that's involved here, you know, the the way that packages are are being built as binaries instead of users needing to, you know, install from from GitHub and actually build that package themselves.

It's it's absolutely incredible. And to think that there's a a multiverse project out there and more coming on top of this is really really exciting to hear. A question for you, does does multiverse mean multilingual or are we still just talking r?

[00:09:06] Eric Nantz:

It is still focused on r, but, I actually don't have the full genesis on how the name came to be, but I think it is trying to bridge, as I said earlier, these, you know, concepts that can be derived from certain aspects of, say, CRAN versus certain aspects of our universe, and we're blending them together in a way that kinda takes the best of both worlds, as I say. And, again, this is all very early days. You'll hear more about this in 2025, but it is, you know, taking the the best of additional frameworks here and and certainly our universe.

Without our universe, our concept, like, our multiverse is simply not possible because can you imagine, Mike, you and I, even if we had a lot of funding spending this up on our own, like, the amount of engineering it takes to build this in a way that now, like I said, can be built upon with its API infrastructure, with its automation, whether it's powered by GitHub actions or other slick services, just the amount of attention to detail that's been laid here. Yeah. We are we are very thankful that this exists at all.



[00:10:16] Mike Thomas:

Yeah. I can't imagine how much the government would have paid a big four consultant to put together a project like this. I think what your own has done, government dollars is probably probably 1,000,000. But

[00:10:32] Eric Nantz:

Sort of looks that way when you look at the UI, doesn't it? Because everything is so polished and, even looking at you know, you can have we we often say in the art community, we have these, you know, verses of packages such as, of course, the tidy verse. And in my industry, a a suite of packages called the pharmaverse. I have an entry to the pharmaverse right on this page. And when I click it, I immediately see all the packages inside that. It's very searchable, very you know, like I say, you can put an API in front of this should you wish. There's so many different ways to get to the interesting parts of a package, including the documentation, which is rendered on the spot in this platform and is absolutely the the attention to detail cannot be understated. So I I expect there's gonna be more really big things coming to this platform that, again, our universe itself will will be a front runner to this, and then other projects like multiverse are gonna hugely benefit from this too.



[00:11:33] Mike Thomas:

This is so stinking cool. I'm looking at the the pharma verse sort of landing page in the our universe under underneath our universe, and it's it's really really cool that the visuals that we have access to just the amount of of automation and ease of use for, working with our tooling and our packages out there and that the way that things are really beautifully organized and documented here is is fantastic. And I guess just to go back to to an earlier comment, and please don't fire me from the R weekly highlights podcast, but if I could speak it into existence, it would be pretty cool to have something like this that could service both R and Python packages.

You know, if we think of we're doing a lot of work building both types of packages for the same sort of function. If you think about what POSIT's doing with, like, both GT and great tables, it's pretty much the same functionality just serving both user bases and we've been doing a lot of the the same lately, and to have sort of one place where maybe the binaries are already built as well to install those things would be be pretty cool. That's why I was thinking multiverse might be multilingual, but but we'll see. Just throwing that out there.



[00:12:47] Eric Nantz:

Yeah. Who knows? We'll take it. And, admittedly, I may have stepped into it a bit on on Mastodon a little bit when I couldn't resist taking the bait from, Bruno Rodriguez when he was talking about his, disdain for managing Python dependencies and educational projects. And I had a, you know, rather snarky comment about this is one of the reasons why I try to avoid Python. It's going through those nightmares. But hey. You know what? They could learn a thing or 2 from our universe. I'm just saying. Because I don't say anything like this with Pypie or anything of that sort, but I think a lot of people, you and me included, I do a little share of Python on the side here and there, would love to have this kind of curated resource, for multiple languages. I I think who knows? Maybe you heard it here first. Maybe we'll look back on this a few years later, and we'll say, hey. We're the ones that spoken in existence. Who knows?



[00:13:42] Mike Thomas:

We can hope.

[00:13:59] Eric Nantz:

Well, Mike, as usual, you kind of have a crystal ball. We were talking about multi language type, situations. Well, I think our next highlight is very much about a new product, an IDE, that is very much trying to be a multilingual data science, IDE powered by the latest innovations in software engineering, and we are speaking about Posit's new IDE called positron, which had its beta earlier this year. And then, of course, there's a huge focus in in quite a few talks at the recent positconf. But that, of course, posit has made the RStudio ID for many years, and it's got a lot of engineering behind it, a lot of commits, a lot of features behind that.

So you as the user who may have been using our studio for many years or even a few years, and you hear about positron, you may be thinking, I wonder if it's time for me to take a serious look at this. Well, there have been a few, few posts that have addressed this, but the latest post I think is, got some real nice insights here. This is coming to us from the jumping rivers blog, in particular, offer by Theo Roe. And the post is titled positron versus our studio. Is it time to switch? Now, of course, we always throw the caveats. This is certainly a subjective decision, but I like what this post is doing. It's kind of laying the facts down on what's currently available in between both environments and kinda comparing and contrasting different aspects of what you, as an R programmer, expect to see or have to utilize in IDs of the past.

So the first thing right off the bat we wanna mention is that Positron is not solely focused on r. It also has support for Python and Julia, but also additional languages that wanna come to play, so to speak. There are ways through its APIs under the hood for a new language to talk to its engine, so to speak. And we'll talk a little bit about how it does it with the r language in a little bit, But this has been built unlike with RStudio, which, let's be frank here, is predominantly an r based IDE with a little bit of Python here and there with Reticulate, but you don't need that with Positron.

If you have Python available, if you have R available, Positron is gonna pick it right up. And there have been many, many people, you included Mike, who just mentioned you may be switching between the frameworks for a given project. In positron is just a toggle in the upper right corner away from switching from an R environment and R interpreter to a Python interpreter. So that is already, I think, for those that are operating in multi language projects, a huge win in the positron favor. There are other things that you might have to get used to in positron that maybe we're kind of bolted on to our studio later in the game, so to speak.

Such of one example is the command palette. If you use IDEs like visual studio code before or Adam before that, you may be used to the command palette as a way to quickly bring up a well, it looks like a little search box. You You start typing a few keywords and it will auto complete to a particular command based on what you're searching for, such as maybe adding a new git commit or bringing up a new file, rendering a new app, or things like that. RStudio never came with a command palette until probably about a couple years ago or so.

Now there there may be, you know, those that might say it doesn't quite feel as native in RStudio as it does in Positron. But in Positron, you kind of have to get used to it because that's the way to really unlock most of the functionality in an IDE of a Positron is to interrogate that command palette to get to what you need to run or what you need to open and things like that. So there are ways you can bind additional shortcuts to that, but it is something to get used to alongside the way that it handles settings.

Those are a bit different too. Rstudio, there was a way to find them in, like, your in a config file in your home directory or somewhere else. Positron is similar, but it's kind of agnostic to, you might say the language being used inside. There are plugins or extensions based on languages from time to time, but it's basically a JSON file. You can get to it either using the interactive setting toggle in the editor or just editing that JSON file directly. You get the kind of choose your own adventure with that. It would trip people up in the RStudio world sometimes to figure out exactly where that file is stored and trying to edit it outside of, like, the GUI elements of the settings. So that might be advantageous to you if you wanna really customize your experience with Positron but do it through, like, a file based way.

Other things to watch out for or there may be a benefit to you depending on your perspective is that in positron, you're not necessarily gonna have to spin up what are called those r project files. So the file is starting in or ending in dotrproj. Our studio used that extensively for things like setting up a git repository in a project, setting up a package, other other uses like that. In positron, It's just the folder. You can have what's called a workspace, set of settings in that folder, which basically are a way to customize settings per folder if you wish, but you're not gonna need that dot rproj file to tell positron that you're working in an r project when you work with that.

There have been some people that really like that file, and there have been an equal number of people that really don't like that file in the repo. So that might be helpful to you if you've, you know, had some angst on that in the past that Positron is kinda doing away with because, again, they're building upon an open source clone of Visual Studio Code. They didn't they're they're piggybacking off of another effort with a lot of toying on top of it. Whereas our studio was built literally from the ground up to be a first class r based, data science editor.

A couple other things I'll mention before I turn it over to Mike here is that the layout will look a bit different. I've been used to the layout in Positron because it does have a lot of similarities to the Visual Studio Code layout that I've been using a lot of my open source projects, but you will often see in a default layout in Positron, the file pane is on the left, and this file pane has a lot more going for it than I think the one in RStudio does, and I don't think I'm upsetting people when I say that. There's a lot more you can do in the file pane and positron versus what you could do in the RStudio file pane, such as even just expanding folders with, like, the toggles to expand the nesting or whatnot.

Little things like this can add up for a bigger project. Trust trust me on that. But then you also see on the left side your extensions in another tab, your, Git, integrated Git console in the other tab. There are people that like the the visual visual studio code or the positron way of doing Git versus the way our studio does Git. Your results may vary depending on where you fall on that fence, but it's all right there. There isn't anything special you have to do for it. It'll pick up Git right away. But with that, there might be some things that aren't as intuitive in the beginning. You just have to play a bit a bit.

But with the extension ecosystem, you can supercharge your Git experience with extensions like Git Graph. There are other ones called Git Lens that can really do some slick things for your command pallet and Git operations. There's there's a lot going on here. The other thing I'll come in on that I think is something you wanna look into is the data explorer and positron, which is something the visual studio code does not have. This is something new that posit created in this version of positron. You get a much more richer experience for when you're looking at your data frames to sort the columns by multiple columns if you wish.

The filters are gonna be much more intuitive to work with because they're gonna be across the top of your of your data frame and instead of above the actual column itself. So that way you can navigate them much more quickly. And it can handle larger data sets about, you know, causing your IDE they'll wait by 5 or 10 seconds although the the snapshot arose. So there's a lot of engineering behind that I've heard from the previous talk, so it might be worth a look. If you really find yourself using that data viewer in RStudio a lot, I think the positron data explorer is something you where you wanna take a look at. There are some things to be aware of that just aren't gonna feel as native right now, such as the use of add ins that RStudio used to kind of give you that additional functionality in the editor without building it into the editor itself.

I'm hearing there isn't like a direct one to one to use those just yet. Although it makes me wonder because with the our extension to visual studio code that I use for many years, there were, features that were added to extension a couple years ago to leverage add ins in Visual Studio Code. So I know it's possible, but we'll have to see if positron adopts that or posit adopts that for positron in the future. So with that, any additional functions that use that Rstudio API package that was often used on the back end of Rstudio itself to kind of interrogate features of the ID, that's not going to work well either.

And Rmarkdown, you can do Rmarkdown in positron but it won't feel quite as native as it will in, the Rstudio ID. But if you moved on to quartile that may not be an issue for you because quartile, however, has very first class support in positron. So the question might be have I switched? I am not fully switched yet, and I'm not saying that because I'm using RStudio a lot. It's because I'm still using Visual Studio Code a lot because I have so many workflows that have built upon it. However, now that Bruno and company have figured out a way to get positron installed on Nix systems yes. I have positron on my Nix system. So I am trying to use it. I'm trying to adapt my visual studio code workflows into that.

Mostly going well. The thing I miss the most is the dev container stuff. I live off the dev container feature that visual studio code has. Unfortunately, there's no easy way to get that in the positron because that's a Microsoft specific extension. It's not an open source extension. That's another thing to keep in mind. There may be a few extensions in Visual Studio Code that do not work in Positron because it's using the open source extension registry, not the Microsoft specific one. So with those caveats in mind, I think it has tremendous potential. It is still in beta.

So your results may vary depending on the project you like or your entire use of it. But the foundation is there to carry out positron, to really carry out its vision, to be in a first class multilingual environment. So I'll be watching it closely, and I'll see where it breaks and share when it does.

[00:26:02] Mike Thomas:

I'm gonna continue to watch it too, Eric. I have not made the jump yet, and I need to. I need to start exploring it a little bit more. I am very locked into Versus Code and and dev containers and I guess I'm gonna blame you, as opposed to myself. I'm gonna deflect here and say that, you were the one that started me out down that journey that has, locked me into that tooling for right now. I'm just kidding. It's hugely hugely helpful. But, the command palette, you know, we should talk about the the fact that positron so closely mimics Visual Studio Code. And I think for a huge proportion of data scientists out there that are more comfortable in probably our studio than in a, you know, more developer type environment like Versus Code, Positron is a perfect bridge between those two things, in my opinion.

I think it starts to bring in some of the best elements from something like a a v s code or, you know, like a full stack developer platform, into a an environment that RStudio native users might be a little bit more familiar with. You know, the command palette being one of them. I I know that there was a command palette that existed and it and exists now in our studio, as you mentioned, for the last couple of years, but, it's not quite as obvious, if you will, as the command palette that exists in in Positron. That's sort of, you know, a little bit more obviously put in front of you and drives a lot of the functionality, of of the IDE itself. You know, another thing that I think is a benefit and one thing that I love about Versus Code as opposed to RStudio to be able to sort of search all of the files in your projects, there is a Find in Files button in RStudio underneath edit and it it's handy. Works very very well. It takes a couple clicks, you know, or if you know the keyboard shortcut, you'd get to it a little bit quicker. But there's a giant magnifying glass in the left hand sidebar in Positron and Versus Code that allows you to immediately do that. I think it's it's much quicker, and these differences are subtle because I I think the functionality still exists in both places, but the UX is just a wee bit better, in in Positron than in RStudio that I I think, you know, that slight difference, makes all the difference to some extent, if you will.

I think a really interesting thing is, you know, the lack of a need for dotrproj files or our projects you know this is again sort of moving folks away from our specific workflows into you know slightly more developer specific workflows and understanding how to interact with working directories without running set working directory if you can help it right so I don't know how this impacts the the here are package that was developed by by our studio and now pause it I think it'll still work fine because it'll look at your working directory as well and I think it'll create relative links to that, but I know that some of the functionality of the here package, actually looked for that dot our project file if I'm not mistaken, and sort of recursively search to be able to find that to figure out, you know where the the working directory was that needs to be set in order in with respect to all the other files that you want to work with so I'm not sure how that plays into this whole positron IDE e in the event that you know you you have a lot of workflows that depend on here and maybe it's a little less stroke straightforward to create a new our project although I do see in a screen shot here under the workspaces our project section it's very faint but if my eyes aren't deceiving me there's actually a button that says new project under our so maybe that takes care of that that for you I know that you know you may not necessarily need to do that, but if you want to do that and you have a lot of, you know, workflows within your organization and your team that leverage our projects and all of the different functionality that that comes along with that and you wanna continue to do so, it looks like the functionality is still there for you. So that's just something to something to watch out for as you move from RStudio to to Positron and sort of decide on which pieces of of functionality in your current workflows you wanna continue to to leverage in which you you may wanna change, to evolve, you know, your team's practices, if it makes sense to do so. But I thought that this was a really nicely comprehensive blog. I I do think that one of the strengths here that we're going to get is the vast, vast ecosystem, of extensions within that open VSX repository or community of extensions, whatever you wanna call it. I know that there's a lot of RStudio add ins but I can pretty much definitively promise you, that the number of extensions in the OpenVSX ecosystem probably is is quite quite larger than that.

One thing that I absolutely am envious of for the Positron users who are already using it and and something that'll probably push me over the edge here to start using it is the data viewer. There is one in Versus Code. It leaves a lot to be desired. Obviously, the put it nicely.

[00:31:37] Eric Nantz:

I know.

[00:31:38] Mike Thomas:

Obviously, the data viewer in RStudio is is is great. You know, it's it's geared towards data scientists and exploratory data analysis. But this is what we have in positron is is the Rstudio data viewer on steroids I would say you have you know column level summaries summary statistics including missing values in the left hand sidebar while you're viewing your data, in the majority right side of the screen as well as the filters that have all been applied that can easily be, you know, added to or removed, through a click of a button kind of along this nav bar at the top. I think it's a fantastic UI.

I think it's really super powered data viewer compared to anything else that I've seen today. I have seen some products like this that are just, just, you know, standalone SaaS platform data viewers, if you will. But to have this in our IDE, in the same place that we're doing our development work is really really exciting. So, you know, excellent job by jumping rivers. I think to summarize, you know, the trade offs here and the benefits that we have from the Positron IDE. And I'd encourage anyone that hasn't had a chance to check it out yet, self included, to to check it out as soon as you possibly can.



[00:33:00] Eric Nantz:

Yeah. I mean, certainly, it's becoming easier to install as I suffer even, you know, major geeks like me and others. Now we can install it on next. I have put it through the paces a little bit. I started theming a little bit. You're right about that extension ecosystem. There is something for everything. And one of my favorite extensions I was using in my live streams way back when, turns out it's available in the open source one, they call it power mode, where when I type, I get these nice little, like, explosion sparks happening next to the words as I type just to give a little flair. And I was like, there's no way that one's on there. Oh, sure enough. It is. So I can replicate some of my live streaming experience in positron that I had for my shiny dev series stuff from a while back.

I will also have links to a couple additional posts, some of which have been covering highlights before. One of those was some Athanasia Mowinkle, and she talked about her experience of Positron. I hate to say it looks like that here package or project stuff isn't working as nicely in Positron as we're hoping for according to hers. So I'll have to see if that gets better over time. And as well as a post by Andrew Huysse who also has used Positron a bit and gives his 2¢ on his favorite extensions and the customizations that he's done to make it, you know, the experience more fit for his workflow. That's a key right here. Right? Is that Positron is already giving you a lot of nice, you know, all out of the box configurations but there's nothing that's locked so to speak. You can tailor that to whatever you see fit and with the power of the vscode you know open source code I should say foundation you can do all sorts of things. You can get your VIM key bindings. You can do all sorts of interesting ways to make that your own experience.

So I will admit at the day job, we can't really use, positron yet because it's not part of the, posit workbench enterprise product just yet. They're obviously gonna posit's not gonna put that in until it's production ready, so I still have to wait a little bit for my work projects. But sorry for my open source stuff, I'm gonna give it a go and report back on what what breaks and, hopefully, what works even better. You know, Mike, we are in the you might say now we're starting to get into the doldrums of winter, but our next highlight has a very interesting title to it because it speaks on a few different levels.

This is talking about one of our keynotes that was given at the recent R pharma 2024 conference by Opposits CTO, Joe Chang himself, on the new tooling that's coming to the our ecosystem with the realm of artificial intelligence and interacting with large language models. The talk was affectionately to titled Summer is Coming AI for R Shiny and Pharma. So we have talked about some of the new tooling already in previous highlights of this show when we spoke very highly about the Elmer package, which is a key focus of this talk, which is giving you in the our ecosystem, very, you know, robust compliant way to call different LLMs both in, you know, third party services like chatgpt or Claude or our others, as well as self hosted versions of it, as well as the accompanying package ShinyChat, which gives you a way to bring that LOM kind of console experience into your Shiny applications and build upon Elmer to do that.

This talk was a tour de force of a few different concepts, but I wanna set a little bit of context here because, first, I have shared on this show. I've been a skeptic as anybody about kind of how AI can be put in directions that shouldn't be put into. It can be almost nauseating scene. Some of the fluff that's put out there on cough cough LinkedIn about some of the weird uses of it. But guess what? I wasn't alone in that skepticism. Joe Chenk himself was very skeptical of this. It took him a while to warm up to this.

He had some epiphanies earlier in the year and combined with, you know, getting to know that the AI tooling as as, as you see it, there's more than meets the eye to steal the frames from transformers because you can build on top of these services. And that wasn't something that came obvious to him right away. But this talk is first introducing again, the aforementioned new tooling in our, the Elmer package and the shiny chat package with the demonstration that was lifted from deposit conf talk he gave where we have what's called the restaurant tipping Shiny app, where instead of the app developer having to build a whole bunch of sliders, select inputs, toggles to try and be proactive, so to speak, on what the user wants to do to explore the restaurant data.

There's, like, a chatbot on the sidebar where the user can type in a key question like, you know, what is the average tip rating for males in this year or whatever? That is a prompt going to an LLM to translate that prompt into a SQL query and update the Shiny app on the spot. That was an eye opener for me when I first saw that. And then when Joe had mentioned that he was going to give this keynote at our pharma, he, you know, had had a quick call with me to ask, you know, what what can we do to make this a little more relatable to the life sciences folks because, yeah, we all love restaurant data. But this audience in particular, we can be pretty skeptical of things. Let's put it that way, and we we often have to be right. It's a very high regulated industry.

So I gave him a little seed of what if we take part of the shiny app that we did for this, our consortium submissions working group, where we sent a traditional Shiny application with a few different summaries to our health regulators as a as a way to prove it out that we could we could send a Shiny app for a submission. There's a portion in that app, I'll have a link to in the in the show notes, where we have a survival type plot of time to event, so to speak. And we had a couple sliders and toggles that were built by the teal package to explore that data going into the plot. I thought, why not have that chatbot in this display?

So Joe, to his credit, it only took him about a week to do this, but he spun up another demo for this presentation to take that Kaplan Meier interactive visualization built with ggplot and put a chatbot into the left of it so that we could ask similar questions on different partitions of the data, and the plot would update on the spot. New sample sizes, new distribution curves, or survival curves. Amazing. This to me, to be, you know, putting my head my spin on this, is a very intriguing feature when we start looking at data reviews and hopefully finding ways to get to insights more quickly, but in a controlled way.

When I say controlled way, that's another part that Joe emphasizes here is that the way this is all built is a very intelligent yet diligently structured prompt that's going to the chat server or the LOM when the app is launched so it has a context set correctly. Now correctly may be a a strong word here because nothing's ever absolutely perfect in this realm of LMs, but it's trying to control the possibility to its best extent of the l m giving complete nonsense to the result coming out and telling it to do a SQL type query that is getting gonna be used to filter the data going into that plot, kind of a translation layer on top of it.

The other key concept is that these packages are leveraging another functionality that you might need if the LOM can't do everything on its own. That's a concept of tool calling. Another eureka moment in my mind where maybe in the example you gave in a talk, you ask an LOM what's the what's the current weather in California. It may not be able to do that on its own because it kind of needs to have an interactive way to look up that weather at a given resource. So Joe's example was giving it access to an r function that calls an API for weather data.

Having the function be documented with a parameter like the city name or whatever have you, the LOM calls that function and then takes its result and it gives it back to the user. But it's kinda like that assistant to the LOM to get the job done. That was the overall moment to me is that we don't necessarily have to be limited by just what the l m can do on its own. We can augment it with other services, other ways that if you can code it up in an r function, you might be able to use it as this tool paradigm with what Elmer can do to call to these LOMs for you on your behalf.

And then the last part of the talk was talking about some of the practical considerations, and there are quite a few. I think the the parts that show me that he is still grounded in this. It's okay to say no, folks. If they've given you results that don't make sense, it just may be time to move on to a different solution. But putting in use cases where the answer is not always so black and white, it may be more of a layer to get to a final answer where you have a little more flexibility, but also keeping a human in the loop, which, again, in my industry, you better believe we better keep humans in the loop when we look at look at these results, there's still a lot of productivity gains to be had if you can harness this the right way. But, again, a very realistic talk. Again, he's excited about the tooling, but he is being realistic too. This is not gonna solve all the world's problems. It's not gonna magically put our drugs on the market, like, half the time as it currently takes.

But I think this can greatly help certain aspects of development such as the way we produce either applications or produce our tooling to interpret this data, get us the insights more quickly. There is a really robust q and a after the talk. I had the pleasure of moderating that, but then we'll also have linked another dedicated talk from our APAC track of our farmer where Daniel Beauvais led a q and a with Joe Chang himself who actually called in later that night around midnight his time to join that call just because he was so passionate about connecting with the Asia Pacific colleagues on it, and there's some great q and a in that in that session too. So am I still skeptical?

I won't lie. I'm still kind of skeptical of certain things. But what Joe gave me in this talk was a way to show that, like I said, there's more than meets the eye for how he can leverage these l o m's and the tooling in front of them to craft a solution that I think is more fit for purpose to your particular needs and cut out all the noise you see in the various social media or other, tech spheres.

[00:45:00] Mike Thomas:

I'm I'm very aligned with you, Eric. I think that the way that Joe is approaching these concepts and the way that Pauseit, in general, is building this tooling out, I think is is fantastic, and it and it aligns with sort of what I would hope for. I was I was quite skeptical, and then, Eric, I know we were both at Positconf this past year, and I watched that keynote talk, that hour long talk from Melissa Van Bussel on practical tips for using generative AI in your data science workflows, and it changed things for me a little bit.

It was very applied. It was very geared toward the audience. And, just to be honest, there were a lot of things that she had presented that I didn't know were possible. And I thought that I knew everything that there was to know about about AI and I I thought that the cons outweigh the pros, but that talk in particular sort of brought the the pros up to to maybe as even with the cons, maybe if not if not more, and maybe want to start to look into things a little bit more in this space. Try some things out and tune out, like you said, maybe some of the LinkedIn narrative marketing hype BS that's that's out there right now. I don't seems like AI agents is is all I'm hearing about these days. I don't even know what an AI agent is. I don't really care to know either. Agentic or or whatever. But, yeah, another mind blowing thing. And, you know, hats off to you. Did a fantastic job moderating this talk.

It's well, well, well worth your time, if you're in the R or data science space and and trying to make heads or tails of these l l m's. And I think that the tooling again that Joe and the the team have put together for us is really really cool. I mean you can't watch it and and say that it's not super cool or super interesting. Whether or not you wanna leverage it is totally up to you and your use case. But some of the possibilities that we have here are really really cool. And thinking about these large language models is maybe a step in the workflow, and their ability to call, you know, another process like an an additional API is really really interesting and work with them. I know that recently, the open a I, if it can't find the answer to your question and it's training data, I believe it can like execute a Google search, or a web based search and then sort of fairly quickly execute that search, process the results that are coming back, you know, maybe just looking at the first few links and trying to crawl over them and and leverage those as the context that it's using to try to answer your question, which is is pretty incredible.

I know that that, you know, just as a tangent here, while we're sort of still on the AI topic, the SoRA model was released from OpenAI yesterday, I believe, which was a long time coming and that is supposed to be text to video. So, you know, check that out. I would recommend, even if you're a skeptic and you you you're really really against this stuff, I think it's it's worth watching just to educate yourself and understand what the art of the possible is because the art of the possible is changing every day.

And we're trying to do a lot of thinking at Catchbook about how we are going to integrate these into the Shiny apps that we we develop for our clients in a way that that makes the most sense isn't going to just, you know, involve our team going crazy with all of this stuff to the point where we're just, you know, strapped for resources because everybody wants this. We're we're really trying to work hand in hand with our clients to figure out, you know, how and where it makes the most sense to leverage this type of technology.

So the videos like this and the tutorials, that really take a practical approach and hands on demonstration about how to go about injecting this functionality into your your Shiny apps, are invaluable for us. So a big thank you to you, Eric, and and Joe, for all of the time and effort that you've put into trying to do that for those of us on the ground.

[00:49:16] Eric Nantz:

Oh, I felt he he did all the the hard work. I'm just, like, are you are you kidding me? Is that even possible? I mean, it it is it is amazing to see what we can do. And and, honestly, yeah, I I definitely had almost like a closed eye perspective on this. I just got I got, you might say, perturbed too much by all the noise out there before really giving it a fair shake. But, like you said, we were sitting next to each other at Pazacom. That was step 1. And then step 2 was this talk because now it wasn't just a quote, unquote, you know, fun toy example. Now it's like, okay. What can we do in life sciences that will open some eyes? And there are so many other areas we're pursuing too. It's not just, you know, quote, unquote, the interactive data reviewing. There are many other realms of automation that we wanna use to make the mundane more done more quickly and hopefully find advancements in training these models or giving it the prompt to train itself kind of on the fly.

But that that there is another part of that talk that you all should see. Again, being realistic here. There's some areas that surprised them as well at POSIT, such as when they try to use LOMs to help ingest their documentation, their technical documentation, and then putting, like, maybe a bot in front of that. They had very mixed results on that, which kinda surprised them. Right? Because technical documentation, that's literally the source right there. Right? You would think an LOM can ingest that and then immediately when they get a question about it be able to surface that more effectively. And I know others are looking into that as well. I thought about that area for some of my internal documentation because I write a great website on using r and HPC environments.

It'd be great to have a little bot next to it that people can type their question on, and it's gonna use that doc to kinda help point them in the right direction without, always emailing yours truly when something goes crazy. Not that I'm not that I don't like helping people, but, there's a there's a balance. There's a balance there. So I'm I'm intrigued to see where that goes for sure. Me too. Yep. No. Boundaries are important. There are no boundaries, so to speak, when you see just the breadth of what's possible in this ecosystem these days, and I dare say they are with the issue.

Not to sound biased here, but I think we gotta build something for everybody here and all the the full gamut of new packages, and there's a a good chunk of them in this issue. I'm I wasn't I wasn't, I wasn't shy about putting all these great new packages in here as well as updated packages. Some really interesting tutorials, so we'll take a couple of minutes for our additional finds here. And it's December. If you like me like music and you're on social media, you're on. Lyc. These the the Spotify wrapped post that often people are showing about their favorite tracks that they've listened to in 2024.

It's always, entertaining thing to look at. Well, Andrew Heiss, we mentioned him earlier, kinda took matters in his own hands because, a, he doesn't listen to Spotify, and frankly, neither do I, but he listens to Apple Music. So he has this great post called Apple Music wrapped with r, and he leveraged a way of exporting out the metadata associated with his listens from Apple Music and Itunes because there's some somewhat interesting XML based files that you can correct from this. Built some code via tidyverse type packages to process that XML data with a little bit of intelligent, like, time lapse, you know, summarizations.

And he was able to derive basically those key metrics that we often see in the Spotify rap and, you know, got some interesting results on what he's listening to and, to the surprise of no one potentially. I don't know Andrew personally, but apparently, Taylor Swift is in his, his top, tracks to listen to, which I think many in the world would have that same thing. This inspired me and not just on first Andrews as usual. Don't know how he finds the time to do all this and plus he he, wrangled some gnarly XML to make it happen.

But I've recently spun up some really intricate self hosted version of my music listening. I took a day during my break for Thanksgiving, and I ended up ripping a whole bunch of my CDs that I bought when I was a teenager, music CDs, onto my beefy little server here in the basement because I thought these CDs aren't gonna last forever. I might as well rip them up and put them on the MP Threes on my or FLAX, I should say, on my server. But then I thought, well, it's great that I have them out of these files. That's no way to listen to it. There's gotta be a better way to have, like, a Spotify like experience.

So I found this program called Navidrome, which basically I can put in a Docker Compose and my Docker container, serve up the MP threes from a directory, finds the album art, finds the metadata for the artists and, you know, the track and whatnot. And I can basically listen to my songs even in the web player, which doesn't look like much to shout about, but it's an API under the hood much like our universe has an API under the hood. And if you heard about a framework called subsonic, if you have a Subsonic compliant player, you can basically tap into that service and put it on your phone, put it on your computer, whatever have you.

So that's great. But then I thought, well, it's not really keeping track of what I'm listening to. Sure enough. It has a plug in for that too, combined with another project called Melaja. Don't don't ask me how do you name these things, but it basically gives me a way to track every time I play a song. But I keep that data in house folks. I ain't going to Spotify. They ain't going to, what's it called last FM or anything. So next year, I'm gonna speak this in existence. And Mike, you're my accountability buddy here. I'm gonna make a version of Spotify wrapped but completely self hosted with my geeky taste in music. So you heard it here first. So, Andrew, thanks a lot. You may have just nurse sniped me into another project.

How do you top that? I have no idea.

[00:55:33] Mike Thomas:

I don't know. And if for the folks that have listened long enough or or know us, there's nothing more we love than than music and nerding out. And when you can combine the 2 of those, it's bad news for everybody else. But that's a great that's a great find. I wanna call out a blog from, for any of the cinephiles out there. A blog from Mark h White the second who is a PhD, and it is about, predicting best picture at the 2025 Academy Awards. As of he's updating this weekly with his predictions on the win probability, that he's seeing based upon, the sort of critical reviews that are posted online.

And it looks like, the brutalist is just ahead of Wicked, in the ranking of which, movie is is most likely to win best picture at the 2025 Academy Awards. So really really neat little blog post. Nice little interactive, visualization here at the top of it. And check-in each week on the blog to see who's winning.

[00:56:36] Eric Nantz:

That's awesome. Yeah. I know a lot of people like to do those predictions and it's always, you know, somewhat fun, sometimes scary trying to predict what ends up being a very subjective voting as the Oscars are or Academy Awards are. So, yeah, I'll be interested to see how that shakes out. And, like you said, very interactive, plotly visualization, Plotly, another package. I should give thanks to Carson Seifert every time I see him for this because I use it in all my apps usually for shining, but, but there are other ones too. As we all know, we can't say Plotly without giving good kudos to echarts from our good friend, Jon Coon. So we're fair and balanced here on this podcast.



[00:57:19] Mike Thomas:

Fair and balanced, and I will extend an olive branch as well to, Kelly Baldwin who wrote a fantastic article on her adventures with Advent of Code using data dot table solutions.

[00:57:33] Eric Nantz:

That was awesome. I I'd love to see that see that. And, I even saw this wasn't necessarily our specific on on Blue Sky, a post about somebody using DuckDV to do the app and a code. Yes. DuckDV with SQL queries, and it's special. I don't know how all of you are able to do this. I was actually talking to a few people earlier today about it. I'm someday, I will do Advent of Code, but my goodness, I feel like I am so far behind. It's almost like imposter syndrome just thinking about it. So I love living vicariously through Kelly and offers as they do this.



[00:58:10] Mike Thomas:

I feel you as well. Someday, we'll get there, Eric. Yep. We can be accountability

[00:58:15] Eric Nantz:

buddies on that one too. But what we are accountable for, hopefully, is sharing, you know, what we find so exciting about the our weekly project and this particular issue. But, of course, you can find this and all the other issues at rweekly.org, as well as how you can give back to the project. And the best way to give back is to share that great resource you found. Or maybe you created that great new package and you want the art community to know about it. We are a poll request away to use the GitHub language. You just find that little poll request, the Octocad icon in the upper right corner. You'll be taken directly to the template.

You can fill out the poll request right there. We got nice little template text and navigate with sections your resource should go in. But, again, that curator for that week will be able to merge that into the upcoming issue, and we love it when we get your contributions. It's always a a smile to my face whenever I get the curation, and I don't see a 0 for poll request. This is one time I want the poll request. There's several times that you're dreading it. Not that I would know anything about that. Nonetheless, other ways to get in touch is with us specifically.

We have a contact page that you can find in the episode show notes. You can send us a quick note there. Also, you can send us a fun little boost in Podverse or Fountain or Cast O Matic if you're on a modern podcast app. We have details for that in the show notes as well. And we are on these social medias when we're not being drowned out by AI noise that you might see on various spheres. You can find me on Mastodon where I'm at our podcast at podcast index dot social. I am now more recently on bluesky.

I am at our podcast dotbsky.social. That's a little addendum I should make. I have seen people put custom domains on that, and I need to figure out how they do that. I may be tempted to do that in the future. But, nonetheless, that's where you can find me currently. And I'm also on LinkedIn. Just search my name, and you'll find me there, and I promise I won't send out garbage posts about AI on there. But, Mike, where can the listeners find you? You can find me on mastodon@mike_thomas@phostodon.org.

[01:00:26] Mike Thomas:

You can also find me on blue sky atmikedashthomas atbskor.bsky.social, or on LinkedIn if you search Catch Brook Analytics, ketchb r o o k. You can find out what I'm up to.

[01:00:42] Eric Nantz:

Excellent. Excellent. Always great to see what you're up to. And, you know, I I considered a badge of honor that I tune you in on to the death container round. I have no regrets about that in the least, buddy.

[01:00:53] Mike Thomas:

I am so grateful for it.

[01:00:55] Eric Nantz:

Yes. If I can get my day job and do more of that, but let's not let's say that on a positive note. This was a a great episode, I dare say, and, we hope that you enjoy listening wherever you are. Again, we love to hear from you, especially as the year wraps up. It's always great to hear how your year has been in the our community and your journey with our end data science. We always love hearing your stories. That'll put a wrap on episode 189. That means we're 11 away from 200. And one way or another, we'll get there. But we will be back with another episode of our weekly highlights next week.

"
"30","issue_2024_w_49_highlights",2024-12-04,43M 36S,"As the holiday season enters the picture, learn how a humble R package helps you to give thanks to the contributors of your open-source package. Plus a practical introduction to missing value interpolation with a tried-and-true R package with a rich history, and a comprehensive analysis to predict an NBA superstar's next shot result (who has made a…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode a 188 of the R weekly highlights podcast. This is the weekly show where we talk about the excellent highlights that are shared in this week's our weekly issue, all available at ourweekly.org. My name is Eric Nantz, and I'm delighted you join us from wherever you are around the world. And I'm actually fresh off of a very short, yet very fun vacation during the Thanksgiving holiday break here in the United States where I was able to go back to my old, hometown in Michigan. And, of course, I go there and sure enough, Lake Effect snow decides to pay a visit as well while I'm up there to remind me of all those times I had to travel through pretty ugly snow, especially in graduate school. But, nonetheless, we had a great time. Like, one of my highlights is actually taking the kids to a local hockey hockey game up in the in the city. They had a lot of fun with that, and our team won, so that made it even better.

Nonetheless, I'm back here at the, virtual studio here if you wanna call it that. And I am flying solo today as my co host, Mike Thomas, is out on business. But, nonetheless, I'll hold the fort down as best I can, and we have a terrific issue to share with all of you today. This week's issue was curated by another longtime contributor and curator to Art Weekly, Jonathan Caroll, who apparently never never stops to work on both dev tasks and curating issues. But nonetheless, he had tremendous help from our fellow our weekly team members and contributors like all of you around the world with your poll requests and other suggestions.

As I said, we I just finished celebrating here in the United States, the Thanksgiving week last week, which means that there's always a time, especially this in in the December month, to start reflecting a bit on the year and especially, you know, showing some gratitude and giving some thanks to those that have been helping me throughout my professional and personal life. And alongside that, we have, of course, in the world of open source, many many different ways people can contribute to a given project but also various ways of acknowledging them as well.

However, that's a nice little shout out on social media, maybe a great blog post of acknowledgments and and etcetera. But there are other ways to acknowledge these great contributions as well. We're going to talk about a very innovative way of doing this, just that, within the R ecosystem. This comes to us from the rOpenSci blog and has been authored by Mark Padgham, who is a research software scientist over at rOpenSci, where he puts a spotlight on a package he's created to help you in this process of, in a more automated way, giving thanks to contributors to a given package.

And this has been motivated by a service that is called all contributors.org, which kind of gives a both a automated bot as well as kind of a general guidance to how as a given, say, a software project can acknowledge the different contributors to their particular project. And they do this through what happens to be, you know, commit messages in the git repository for that project by having special, you might say, kind of tags used in the commit message. In fact, they look kind of similar to an r oxygen tag with the at sign in front of it.

And this bot will actually parse the commit messages and start to put them in a more streamlined format in, say, a project's readme or other documentation. Now this is great and it looks like a lot of projects do use this service, but Mark highlights a couple of disadvantages to this particular workflow, especially in the realm of data science and in the realm of R Packages. And as I said, one of the mechanisms that this works with is putting those contribution type tags in a commit message. I don't know about you. Sometimes it's easy to forget these things when you're committing and you're all in the dev mode, so to speak.

In fact, it reminds me, slight tangent here, I've been really trying to opt in to the world of conventional commits, where I kind of give a prefix to the commit message about the type of commit that's all about, whether it's a new feature, a bug report, etcetera. And I've been wanting to do this for years, but it's only until a recent project that I really literally forced myself to do it. And it hasn't been easy folks. It does take a lot of discipline to do that. So this could be viewed in a similar light.

And then the other disadvantage that Mark highlights here is that these acknowledgments of these contributors are packed in a single contributor section. And there isn't really a neat way to customize that appearance. It's kind of what you see is what you get from that bot itself. And also those contributors are listed in that section. It doesn't actually link out to what contributions they actually did. So that brings Mark to introducing us to the all contributors R package, which as you might guess is also part of the ROpenSci umbrella.

But this is meant to give a package author a very nice and semi automated way to help acknowledge contributors to their particular project. And this is one caveat that Mark mentions here in the post is that this is acknowledging contributions that are through the git log of a repository or through GitHub interactions. That means that other types of contributions that are listed in that that other service we just mentioned may not be picked up here such as like organizational contributions, you know, organizing documents or organizing a community around a project.

You know, those kind of things will not be tracked here with the all contributors package. So if you are looking to acknowledge a more broad spectrum of contributions you still might, you know, be recommended to go to that, all contributors.orgserver spot instead. Nonetheless, if you are content with acknowledging the code based contributions, this package really gives you one function to get this all going and that is the addContributors function. It does have a lot of parameters that you can customize to your needs, but the goal of this is just running this function at the root of your projects repository locally.

It will automatically add or update your list of contributors that you can put in to, say, a readme and whatnot. The other way Mark recommends to leverage is instead of us running that function from time to time to get this update. He includes a way to copy a template GitHub actions workflow, which would help you for when you commit this workflow file for the first time that in every push to your repository, it's gonna run the same all contributors function. And you can also define this regular timed intervals as well so that no matter what is being contributed to, as you make that push, whether you merge in pull requests from every contributors, maybe they solved issues or whatnot, you will get that into this list automatically through that GitHub Action.

The other nice benefit of this package as well is that there is going to be a direct link for each contributor to their contribution itself. He gives an example that we'll put in the show notes of, somewhat ironically enough, the read me of the all contributors package. Where if you look at this as I'm talking through this or after listening to this, you'll see at the bottom there is a Contributor section where it's got a nice little introductory sentence that this was automated by the All Contributors package.

And then it's got a, basically the avatars of each contributor, but it's broken down by different categories. And in the case of this package there's a category for code contributions as well as issues contributions. And under their avatar is a hyperlink that has their GitHub user ID, ID. But when you click that link it's going to basically, for this particular packages repository, show you the commits where that particular contributor was directly involved. I even, just as I'm recording, just clicked on the link under the avatar of my Elle Salmond, who, of course, has been a very frequent contributor to highlights in the past. And sure enough, it took me right to the 4 commits that she was involved with with this package.

That is really really neat. You can also do the same thing with the other contribution types as well, such as the issue contribution types. So clicking on, say, an avatar or link under the avatar of a contributor on the issues section, you will see take you'll be taken directly to a filter view of the issue tracker in GitHub where that that issue was authored by that particular user. So that is terrific. That is absolutely terrific. So and and this is a wonderful time to think about a package like this because for getting involved in open source, it can sometimes other than hearing from, say, the maintainer of a project themselves from time to time, it can sometimes feel like you're sending that to the void a little bit. Yeah. You may be scratching your own inch potentially, but admittedly it is a nice pick me up, especially mentally, where in open source it's rarely compensated with, say, financial compensation or whatnot or other things to compensate for the time spent for these contributions.

Every little bit of kind of nice, you know, kudos or pick me ups or whatever you want to call it, It goes a long way, especially for those that are new to the world of open source contributions. So I did not know about this package before the highlights so that's why I'm always thankful for our weekly itself to bring this to my attention. And with this new, ShinyStay package I'm making that hopefully will get contributions from others in the R and Shiny communities, I will be very glad to leverage in all contributors to that workflow to be able to make that, you know, an easy way for me to give those proper acknowledgments without, you know, using myself as the excuse for forgetting to do it in a manual way.

So really slick looking package here by Mark. I'm very interested in trying this out. And, definitely, if you want if you've been trying this out in your various package development in the community, feel free to send me a shout out, or contact us. I'll I'll put all that information at the end of the episode. I'd love to hear how you all are using all contributors or other ways you've been sending the kudos and thanks to your contributors to your open source projects. Our next highlight here gets into a very realistic issue that anybody in a data analysis is likely to encounter from time to time, especially when you get away from those cookie cutter textbook examples that you might have had in those statistics or math textbooks back in the day, and you start dealing with real data collected from humans.

And, hence, there are times where in the ideal situation, you would have all the available data for that particular variable or that particular record. Folks, it doesn't happen that way no matter which industry you're in. I can definitely speak in life sciences. It definitely happens, but many of our industries have to deal with how we account for missing observations, missing values in our datasets. And there are a multitude of ways to look at this. And one of the fundamental techniques that are often used, especially when you get to modeling and predictions, is interpolation of missing data.

Going over all the ways of doing this would be entirely another podcast or 2 dedicated to that alone. But our next highlight here is a very fit for purpose fundamental principle that has been available in R for many years. And this comes to us from the latest one of the latest blog posts from Steven Sanderson who has an amazing blog. If you haven't subscribed to it before, it's called Steve's data tips and tricks, and he turns this content out regularly. In fact, when I set up my little self hosted RSS feed on my beefy server here in the basement, I'm using a service called comma feed, and I this is one of the first blogs I put in there because there is always something great to learn from Steven about both R and even some of his recent tutorials on Linux itself, which have been entertaining read.

So in his blog post here, he talks about how to interpolate missing values in R and using at the back end of this a package called zoo. And this brings back a lot of memories to yours truly here from his very early days in the R ecosystem and learning R for the first time in graduate school. I may have shared the story before, but the very first time I saw R was when my one of my graduate school professors in statistics taught us a time series course. And on top of just never seeing R for the first time, I'm trying to get a hang of the language, and many examples had library statements at the top and one of them was indeed library zoo.

So the zoo package has been around, forget this, at the time of this recording for 20 years. Now that is some serious longevity folks. I'd I'd checked the CRAN archives just to be sure and sure enough that is legit. Zoo is tailored for a lot of very powerful functions dealing with time series data, appropriate enough given I saw that in the time series course. There is one function though that is used throughout many analyses and not just in time series analyses that this package surfaces to you and that is called naapprox And this has quite a few parameters to do interpolation, but Steven talks to us about what are some of the use cases for this.

So this na. Approx function, by default, will let you be able to interpolate missing values, numeric values, in a given vector. So in his first example he starts off with a simple vector of a few missing values. And sure enough when he runs na. Approx on that vector those NAs are filled in with missing values. And the way it's in doing the interpolation it's using the surrounding values to do, in essence, an averaging using those values around and then breaking that up as appropriate depending on the observation. So in this very basic example with a vector from 1 to 8 but then missing values were say a 3 or 4 or an 8 should be this na. Approx function is going to indeed put in 3, 4, and 8, taking in account the number of missing values and the boundaries around them.

Now a more realistic example is looking at, in this case, a simple time series type data set, where for a series of 4 dates there are, or 5 dates I should say, there are 2 missing values between the first and the 4th, 4th observation. And with that it's basically taking, you know, 2 approximate values, and they are they are not neatly rounded values. They are, in essence, averages of those boundaries at the different slots. And that, again, can be great for small records, but there may be situations where it's not just like a few missing values here and there sporadically placed.

There could be large gaps of missing data in that given variable. And the na. Approx function gives you an argument called maxGap, which will tell the function don't fill in missing values if there is a continuous sequence of it that exceeds this maximum gap amount. So in the next example, he has a, set of 5 missing values consecutively between a couple of non missing values. And when you feed in the na. Approx function a max gap value 2, guess what? Those missing values are not touched. So that can be very important if there is a very valid reason for that gap and missing data. You want to make sure that you're, you know, tailoring that rule appropriately.

So that gives you just a scratch at the surface of what interpolation can do. But again, in the realm of data science, sure enough, you probably will be tasked with doing an imputation in one way, shape, or form. So the zoo package is giving you one of those fundamental techniques, fundamental solutions to tackle that problem. And I will mention in the show notes of this episode we also, there is a huge domain of how we can deal with missing values in R. And a couple packages come to mind to accomplish this.

One of which, if you want to really get into the different ways that this can occur, is the R package I've heard about for quite some time called MICE. That stands for multivariate imputation by chained equations. That can be really important if you really need to customize the different interpolation methods and take advantage of some really novel statistical methodologies to do this. So I'll have a link to that in the show notes because I see that used quite a bit, especially in my industry of life sciences. There are many many ways you can deal with that here. And another R package that I have had great success with is called the Narnier package.

That is a terrific package offered by Nick Tierney. And this is also a wonderful package to look at missing data, as well as visualizing missing data and exploring the relationships that are as a result of the missing data. So you'll probably want to look at that as well if you're gonna do a more rigorous analysis of this. So I have a link to Nick's, Narnier package in the show notes as well. So I hope, Steven keeps this great content going. It's another short to the point tutorial, but again a very important issue that anybody involved with data analysis is going to have to deal with at some way shape or form.

Rounding out our highlights today, there is, of course, an explosion of data available to us in the world of sports these days as both the professional leagues and other endeavors have, you know, exposed a multitude of new ways of collecting data, whether it's sensors on say players jerseys, whether it's custom cameras in the arenas, or other types of advanced tracking. There is just a wealth of opportunity to leverage some really really nice techniques in the, say, prediction side of things, inference side of things, and whatnot.

And so our last highlight today is looking at the realm of basketball and looking at a novel way of predicting, and in this case one of the most prolific shooters in NBA history, who is no, who is in no doubt going to the Hall of Fame when he retires. And this blog post is looking at can we accurately predict whether Steph Curry, who is the very famous superstar on the Golden State Warriors NBA basketball team, a winner of multiple MVPs and multiple championships, what techniques we can use to predict if he's going to make or miss a shot.

And this blog post comes to us from Nils Indetrin, who I hope I said that right. And this is a wealth of knowledge in this blog post. I probably won't be able to do adjust this in this recap here, but I'm going to give you kind of my key takeaways of this. And this project is looking at those spurred on apparently from a conversation that Nils had with a friend of his where the question was whether Steph Curry would make a shot or predict him wherever he would make a shot given that he's attempted it. So what kind of data can we use to help explore this in a data driven way?

Well, first, it's a little bit of EDA action, which again is is a very, important technique as you're getting to know what's available. And there is a wonderful R package that makes this very very possible in the realm of NBA and and basketball analytics as a whole, and that package is called the Hooper package. We'll have a link to that in the show notes as well. This is authored by Sayem Golani. Hopefully I said that right. And this package has a wealth of functions for assembling data collected for the NBA and college basketball as well using online sources such as ESPN's NBA statistics as well as many others as well.

So in the first part of this blog post here, Neil shares some of the code he used from the Hooper package, the load, 3 different perspectives or granularity, if you will, of the data going into these predictions. One of which is the team statistics after each game. This is often called the box score data that you often get in a website or in the old days a newspaper. I still remember reading sports sections of those in local newspaper in a in a bygone era if you might say, but the Hooper package has a very simple function to load the team box scores given a range of years.

You can also get the player specific box score data with a another function called load NBA player box. And last, but certainly not least, because we're looking at the grand, the very granular level of whoever Steph Curry is going to make a shot or not, that screens play by play data. And, yes, Hooper has a function to load that as well. Each of these functions provides a nice data frame that once he's able to assemble that, there's a little bit of massaging to begin exploring this data, doing some aggregations, to get a good feel for what the overall performance is for these given players that are in the similar position as Steph Curry, which is called Point Guard. So there's a nice little tidyverse set of code here to do the filtering and the summarization with group processing.

And this, now we get to something I'm really intrigued by here, is that, yes, Nils shows kind of the static output of this tibble, the of the first five rows of these different point guards, but he wants you, the reader, to be able to explore this in a really neat and interactive way. So even though the code SIP is not shown for this, I really wanna see it, but I think I know what's going on here. Nils has created an interactive reactable table, probably powered by the reactable format or package as well, with widgets at the top of the table to let you filter by team, by conference, and by minutes played via a slider. So I have if I have a guess, there's some really innovative crosstalk action happening here to dynamically change the table view in this HTML document.

This is why I just love the HTML format. We get these rich experiences of exploring the data right here in this blog post about you having to boot up r and run this code yourself that is just spot on awesome so love playing with this table here in this format of fantastic I did some sleuthing. I couldn't find this code on on Nils, GitHub profile, but Nils, if you're listening, I'd love to see how you made this table. This looks absolutely excellent. So, nonetheless, once you can explore the scoping, you see that Steph Steph Curry is 4th in some of these metrics in terms of total points, but there are other metrics instead of total points that we can look at here in kind of a multi dimensional view.

The next part of the post is going to create what are called radial charts that are basically a polar coordinate version of these different metrics that we can use that like the vertices at the edges of these circles, and with lines going how how high the value is relative to each other. So there are 6 radial plots here, and what it shows is that in the dimension of point, 3 pointers made Steph Curry is tops, but then there are other dimensions that players such as Luka Doncic who is a superstar in the Dallas Dallas Mavericks who went to the NBA finals last year.

He's got a lot of, you know, optimized metrics as well as well as Damian Lillard, Kyrie Irving. But it just shows that there are multiple facets to being a prolific point guard in these metrics. But then we can also, getting back to Steph Curry himself, there is another package, and in fact I should say another package is powering the visuals in this blog post, and this package is called Basketball Analyzer. That's available on GitHub and that'll be linked in the show notes. And this gives us both those radio profile charts as well as, what I'm about to talk about next, a heat map of the shot chart for any given NBA player where you can color code the shots by whoever they made or missed.

You can also do it instead of just the dots themselves you can also use a gradient kind of more traditional heat map where you see the brighter colors are around the 3 point line as well as near DeBasker when he does those fantastic layups after he makes it, you know, tries to break ankles of a defender after his his deeks and whatnot. So that's kind of giving a profile where the shot volume is coming from for Steph Curry, and that's a great way to help inform the next meat of this post, which is modeling itself.

The first step is trying to figure out what are that this, you know, the type of data we want to feed into this prediction, And that's where there is a set of tidyverse code that join these different granular sets of data data together to get, an overall set that has the right metrics we need. And, in essence, our response variable that we're gonna use for the prediction is called scoring play, which is obviously false if he is Steph didn't score on that play or true if he did. And then the types of predictors going into this model are gonna be the location of where the shot took place.

Those are in coordinates x and y, as well as the minutes during the game, as well as additional metrics such as the opponent that was guarding him at the time, as well as other metrics as well. And once that data is assembled, again, all the this type of code is in the blog post, it's time to use the tidy models ecosystem to split this data into test and training sets. Again, using the very handy functions that tidymodels gives you with the initial split, and then defining the training and testing sets from that, and then setting up the folds for cross validation.

Again, these are all fundamental techniques that have been talked about in previous highlights. And again you're invited to check out the tidy models website if you want a primer on how all this workflow works and in the tidy workflow. It is very very elegant here in this post. Next, it's time to set up the recipe for actually, you know, preparing for the model prediction. And that's where we have to encode all the input variables, especially those categorical ones, and making sure they get the right indicators for that. And that's where there is, you know, very handy functions such as step novel, step dummy to take, and again with tidy selection, the nominal prediction predictions or predictive variables without transforming the outcome variables. So you can get that dataset neat and tidy for the prediction itself. And for the prediction itself he is leveraging one of the very popular methods for a classification that's XGBoosting, and that is defined with the boost tree function. You can see the different parameters that he defines there, setting up the engine, setting up the workflow, and then making sure he's got the grid for the searching of those predictions, and then finally taking advantage of multiple cores to tune that that grid. And then it's time to look at the prediction results, such as model performance.

And one of the fundamental ways to do that is looking at the metric called Area Under the Curve or AUC. And in that you can see in the scatter plots in the post that there is a concentration around of an AUC of 0.75 to 0.8 with these different model metrics that are displayed here as well as then looking at an area of variable importance plot, which I often do in my classification analyses. And this is where things get pretty interesting, where the most important variable by far in this analysis was the opponent guarding Steph at the time of the shot that was indicated by this opponent athlete or no direct opponent indicator.

So, obviously, if there's no opponent at the time he's shooting, Steph's gonna have a higher likelihood of making that shot as compared to somebody with the arms right in his face as he's shooting. The other important variables were the location of the shot, which again would make sense because typically speaking for most people the farther the shot is the harder it is to make. Although Steph seems to make an art in making shots up near half court and look like they were a free throw or something. But then there are other variables as well that all kind of have a similar concentration at the lower end of variable importance.

And then last, a couple additional diagnostic plots that Nils outlines here, or displays here, is one of which looking at the specificity and sensitivity with the area under the curve, which shows kind of a similar story that around the AUC of 0.78 and an accuracy of 0.73. And then and then looking at a confusion matrix as well, looking at how often the false positives, false negatives occurred with the prediction. And this is an interesting insight looking at this confusion matrix is that the model fit here seems to be performing better at predicting shots that aren't going in, whereas it's not as a clear story for predicting the shots that do go in.

Now that was using, I should have mentioned this earlier in the post, was using data from 2022 and 2023, but you might say how does this help with today's data I. E. The 2024 NBA season? So Nils grabs that data again from the Hooper package and then fits the model on that particular set of data. And in there we see that the area then the model predictions there's a nice little visualization via again a shot chart kind of heat map combination here that the Steph Curry is indeed more likely to score when he's near the basket. That seems pretty intuitive.

And then it's interesting that the predict the model is predicting the location of the shots to be kind of from the corners of the court and as opposed to other parts, and he does also a nice little shot chart with the color codings of what were false positives and false negatives. And the false negatives are mostly occurring for the 3 pointers. That's interesting. And then most of the false positives are around the back board or near the the basket itself. So in terms of accuracy, it looks like this model is around that 75% accuracy mark.

Again, not gangbusters great here, but at the same time it's not it's not terrible either. Like it it's it's pretty neat. So it this was a very comprehensive post here. Again, I don't think I could do it justice in this recap here, but lots of techniques at play here. 1st, a very novel use of existing packages to get this data yourself without having to become a wizard at data scraping. And then also the tidymodels ecosystem along with the tidyverse itself for data wrangling to fit this prediction. But as you can imagine there is a lot of future directions this could take that such as using additional data points on the players themselves, maybe, you know, better ways of training the data, and using feature engineering as well, which is a very popular technique, especially if you're going new to this type of exercise or you're new to this type of data. Principal component analysis can help reduce the dimension quite a bit and maybe give you some better prediction performance there.

But this blog post is absolutely fantastic. Again, I'm really intrigued by the visuals here. I love that interactive expiration of the data in the middle of the post here. I would love to see the code that did that. I can guess how it was done, but I'd love to see how how Nils pulled that off. It's a really great read. I definitely invite you to check it out, especially if you're, you know, keen into seeing just what is possible with sports analytics and with the tools that we have in your ecosystem available.

There's a whole lot more we could talk about in this issue, but I'm gonna take time to put the spotlight on an additional find before we wrap things up here. And as I'm getting really knee deep back in the Shiny development recently I just put in an initial release of a very, fresh off the presses app and at the day job and it's been getting great reviews. I've been putting a lot of attention to detail to performance where I can, albeit I've only scratched the surface of it. I've just been trying to optimize my use of reactive objects appropriately, making sure that there are indicators when things do take a little bit of time, and trying to do a lot of upfront work so that the experience is quite seamless for the user.

Well there are other ways of optimizing the performance of your Shiny application and one of them is called caching. So I'm gonna put a link in the show notes. So this additional find is from the Appsilon blog on how you can optimize your Shiny application performance with advanced caching techniques, where there are different types of cache you can use. So the post talks about ways you can do within base Shiny itself with reactives and in memory caching with bind cache, which I have not done as much and I probably should, but then tapping into the ecosystem for more even more powerful ways of caching such as using Redis backed by or with the front end of the Redux package as well as session specific caching and using databases.

So this post gives you a tour de force of how this works with an example application that kind of shows a lot of these principles in action. So another terrific blog by the Absalon, the fine folks at Absalon. Definitely worth a read if you're trying to take your performance to another level. And, of course, there is a lot more to this issue and we invite you to check it out. Where can you find it, of course? It's at our weekly.org. That's where you find all the great content, including this week's current issue, a healthy mix that John has put together, new packages, great tutorials, updated packages, nice little quotes of the week, especially those powered by other services such as Mastodon.

You and I are big fans of Mastodon, I can tell. And also, we have a direct link at the top of the page to all the back catalog of issues. So as the year is closing out, you'll definitely want to reminisce on a lot of these great content that have been assembled by our fully driven community effort of rweekly. It's all about contributors at the top of the show. Right? All of you producing great our content that are coming into our weekly, we thank you for producing all that. And also our curator team, you know, works around the clock. Obviously we have different rotations, but it is very much a community effort. We are not driven by any financial backing here. We are doing this out of our love for the RN community and so we appreciate all of your well wishes as we keep this going because it is not easy to keep a project like our weekly going. There are a lot of moving parts, and I'm just so thankful that we have a wonderful team with this. We could all use your help. And one of the ways you can help is giving us that great content by giving us a poll request for our upcoming issue draft.

You can get to that right at the top of the page with that little Octocad in the upper right corner. It'll take you directly to this this next week's draft issue. It's all marked down all the time, so if you find that great blog post, that great tutorial, that great new package, and you want the rest of the R community to hear about it, give us that little poll request. We got a handy little template for you to follow. It's very straightforward and we're always happy to help if you have any questions, but that is one of the best ways to get back to the project.

And also we are definitely working looking for more curators to join our team. We've had a couple move on or about to move on, so we definitely have spots to fill. And if you're passionate about the art community and sharing the knowledge and making sure those in the community can access this knowledge in an open way driven by the community, That's what our weekly is all about. So please get in touch with us. You can find all those details on the our weekly website and the GitHub repository. I also love hearing from you in the community as well. We have lots of ways that you can contact me and my awesome co host, Mike. One of those is a contact page. Directly link in the show notes of this episode.

You can send us a little note. I'll get that directly in my inbox and I'll be happy to read it, maybe even on the show as well. Also, you can get in touch with us on social media these days. I am mostly on Mastodon with at our podcast at podcast index.social. And now I am on Blue sky as well. I have ventured into that land and so far I'm I'm liking it. So I you can find me on blue sky with at our podcast dot bsky dot social. Hopefully I say that right. It seems like that's the right way to say it. I've been following a lot of my favorite people on there and also new followers. I appreciate you all getting connected with me and I'm definitely looking forward to putting more content on that service as well.

And also I'm very happy to again remind all of you especially those in life sciences that the R pharma 2024 recordings are now on the YouTube channel for R pharma. I also want to thank our curator John Curl. He was an instrumental part of the Asia Pacific track that we have for the first time this year. We have all those recordings on the YouTube channel. There is just a lot of wonderful content there really worth a watch even if you're not in life sciences there's a little something for everybody there.

And with that, I'm going to close-up episode a 188 of our wiki highlights, and we will be back with another edition of our wiki highlights next week."
"31","issue_2024_w_48_highlights",2024-11-27,47M 37S,"What a way to close out the month of November with this batch of highlights! The ultimate teaser for the first-ever native mobile Shiny application (and yes, it is real), how you can expand your network on Bluesky from the friendly confines of R, and the potential of the S7 object-oriented paradigm to streamline and validate function parameters.…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode a 187 of the R Weekly Highlights podcast. And this is the weekly podcast where we talk about the latest highlights that have been shared every single week at ourweekly.org. My name is Eric Nantz. And, yes, November is well, it's our last episode for November. I can't believe the time is flying by quick. For those of you who may be celebrating a an holiday coming up soon, certainly happy safe travels wherever you end up going for that. But, nonetheless, we're gonna celebrate some awesome highlights to talk about today, and I never do this alone because I'm my trusted co host at the hip here, Mike Thomas. Mike, how are you doing this morning? 

[00:00:45] Mike Thomas:

Doing well. Eric started out with some technical issues. If, I sound a little bit different this week, I apologize. But, hopefully, the audio isn't too noticeable in terms of differences, between previous weeks, for the listeners out there.

[00:00:57] Eric Nantz:

We we make it work. But, yeah. We both had our frantic either rebooting or jiggling of settings, but it it happens, folks. It happens. But, nonetheless, Mike is here, and that means we can get this show on the road here. And this week, our issue is curated by Tony Elharbar. And, well, it's a bit of a sad news. This will be Tony's last curation for our weekly. He is moving on to other endeavors after this. And, certainly, as I've told you, every time we close the show, this is a community driven effort, and we certainly greatly appreciate Tony's time. He's been a valued member of our curator team. We're gonna miss him, but, certainly, he'll be, I'm sure, keeping an eye on our weekly from time to time. But, Tony, best wishes to you in your, future efforts in open source and elsewhere. Nonetheless, he's gone if you're gonna go out, you're gonna go out with a bang. Right? And this issue has a lot that we can discuss. So let's not belate the point any longer. Let's get right to it. If you haven't heard by now, both myself and Mike are really excited about the concepts of web assembly with Shiny, and it's opening doors for immense possibilities.

Some of which I have touched on in my positconf talk this year in the realm of life sciences. But there are many other ways that WebAssembly can supercharge the experience of distributing R itself in a self contained way powered, again, by WebAssembly. I admit this one came somewhat out of left field, although I should have seen the hints coming a little bit because what we're talking about now is Colin Faye, who, again, is a fellow curator of our weekend and also the architect of the golem package. And as we covered in the summer of our weekly highlights, has been taking a little bit of an adventure with bringing web assembly into shiny, and even building some custom tools himself called Spider and whatnot to really start pushing the envelope a little bit.

Well, this latest blog post from the ThinkR blog is talking about a native mobile application running R and powered by WebAssembly. Yes, folks. The future is here. So in the in the post Colin introduces us by saying that this has been an effort that he has been looking at for a bit of time now, and he is, like me, convinced that WebR, this is a game changer that's gonna make the ability to leverage R even more accessible in modern web platforms and whatnot. Now notice I said web platforms. Right? Well, here's the kicker, is that with the self contained aspect of web assembly, yes, you can fold this into a native mobile application.

Now we don't have it revealed what this application is actually called, but he talks about a little bit of what's under the hood here, is that this is bundling a set of packages, but it is installed in the native mobile app fashion. This is not what we call a progressive web app, which you might be thinking we talked about packages like Shiny mobile before, which David Grange has been leading for at least a couple years now, and that is a really solid platform to build, again, what's called a progressive web app, an app that looks like a native app, but it's actually more of a web app just, you know, in the UI it looks like it fits right into the rest of your mobile applications, that's still reliant on some kind of web connection to make sure things are working.

And, also, it doesn't it isn't able to tap into some of the native mobile platform APIs such as for your phone. Maybe you need wanna trigger a vibration for a notification. Maybe you wanna tap into some other, you know, hardware related things. A progressive web apps not gonna be able to do that. And, again, no no shame to that. It's just the way the technology works. What Colin is showing here is that with this being installed in a native fashion, this opens the doors to tap into some of those native APIs.

So let's get into what this app actually is or it's a in essence, a game of sorts where, say, from the UX perspective, you're gonna be presented a name of a function and you have to guess which package it comes from. So it's kinda like a quiz app, you might say. How does this actually work? Well, apparently, under the hood, this application is looking at the packages that are currently installed in this installation. It's gonna sample 3 of them at random and then from each of them pick a random function from them.

And then you as the end user are prompted to choose in a multiple choice fashion which package that function comes from. You can, you know, submit your answer, and then it's gonna use the praise package under the hood to send you a message if you get it right. And then even if you get it wrong, it'll let you kind of have a, you know, not so negative message and then try again. Where are the responses being stored? Well, again, because this is a native application, in this case, he is leveraging SQLite, a local database, which I've used for years years in some of my applications where that's being stored. Your responses are being stored in real time from this app. But, again, locally, not going to some server. It's all locally done. And you can imagine that this, you know, this might get a little boring after all. Though, I will admit I did get early access to this, and it is fun. And the one thing I will mention too is that if Colin had not told me that this was a web assembly powered application that had r under the hood, I would never have guessed that this was using r inside. It felt that responsive, folks. That is what I am cannot imagine my excitement when I've got this email. And, first, again, I didn't see this coming out of the at all.

I install it, and I'm literally looking at it right in front of me right now. I wish I could show it, but I've been until this is released, I've been sworn to secrecy. But I'm I'm playing with it literally right now responding to the app, and it there's no lag. It is native. It is truly native. Imagine what this can bring to us in the community. So if you want to have access to this or stay updated when this is released. He does have a contact page at the bottom of the post, so I would suggest if you're listening to this and you're interested, definitely sign up for it. I would be hammering this as soon as I could if I was in the audience listening to this because this is this is revolutionary folks. This is absolutely revolutionary.

I have so many ideas where this could go. And as you can imagine, I mean, it's something you're hearing right now. I have, gently asked Colin for an open invitation to join me on a future shiny developer series episode because I have so many questions for him, and there's no way that I could get it done in just an email. I want him to speak from the source. So look out for that in the near future once this is out there. But, Yeah. Mike, mobile apps on your phone powered by WebAssembly.

[00:08:38] Mike Thomas:

What timeline are we living in here? I don't know, Eric. And, you know, some of us aren't as as privileged to get the, early access. So I'm dealing with pixelated screenshots in this blog, folks, the edge of my seat trying to figure out when this is actually going to come out. I don't think there's a specific timeline here that, Colin mentions, which is which is smart on his end, as to when we will actually be able to get our hands on this, like the general public. This is this is crazy and probably a bad thing for me for all of those times that a client has asked, when we've developed a Shiny app for them. Like, hey. Could this be a mobile app too? And I'm like, nope. You know, this is these are just, you know, desktop web based applications. You know, you could go to your your Internet browser on your phone if you wanted to and use them, and and we can leverage Shiny mobile on that, for you. But it it can't be a true app. Well, now my answer has to be, well, technically, yes.

And that's just gonna open up just gigantic can of worms. Once we sort of figure out what's possible here, what the limitations are, but this is a serious, serious boundary that has been broken through. And to me, yeah, it it it's gonna going to open this world of possibilities for us. And I imagine that, you know, similar to the the WebR project that powers this, it just continues to, you know, branch out in terms of what is now capable that wasn't previously possible, for, you know, R and Shiny development. It's it's pretty incredible.

I'm excited to see where this leads. What a tease. I mean, I don't think I've ever seen an rweekly highlights blog post that teases this hard. So I guess hats off to Colin for for that. He has me on the edge of my seat waiting to hear

[00:10:42] Eric Nantz:

what's next and when I can finally get my hands on this. I I am eagerly awaiting the the mass release of it too. I will speak somewhat unofficially. It sounds like the release is imminent. So my hope is, as an early holiday present, that this will be out there. Let's put it that way. It's sounding all indications of that. You did mention WebR earlier. And one of the things that was very fascinating to me about when WebR was first released, with George Stagg's, you know, immense efforts at posit is that he was able to give us a web page that literally had, like, an r REPL inside. Right? I mean, you can just type in our code, install packages from the WebR repository, and then play with it at your leisure.

You have seen these questions earlier, Mike, throughout the community many years about, hey. I really like the experience of our studio. Isn't there any way to run it on mobile? Just imagine a little r REPL from WebR as a mobile type app in a tablet UI. I don't know. And that's maybe completely out left field, but it's based on the same technology. Right? I mean, that WebR repl, I believe, is literally just a self contained app, if you will, waiting to happen. I don't know. That's not what Colin's trying to tackle here. I'm just saying that it's there are a lot of other possibilities at play here and then getting back to the architecture of this, for example.

He's using SQLite for this, but my goodness, you could swap in DuckDV with this. You could swap in all these different parts that we've been so excited about that are now at your fingertips here. It's been one of my stretch goals to really push my dev skills to the max, so to speak, and create somehow powered by R, a R weekly podcast app where you would just simply listen to episodes of R weekly taking advantage of some of the stuff I've been learning about in the podcasting 2.0 development space with a DuckDb database on the back end? Maybe. Maybe. So as you can tell, I have a lot of questions, a lot of ideas, and I am just as excited as everyone else, I think, when this finally gets released and, you know, the other possibility here, and I'll speak specifically on the Android side of things, is that you're not just confined to installing apps from the Play Store. Of course, that's what Google is always gonna push as, like, the secure, quote unquote, repository. But there is a multitude of open source applications that are distributed on services like F droid and others where imagine and even other services where it's like an app store, but it's based on the GitHub releases of a project. So that, say, I have an app or or whatever you wanna call it, and I push a tag like version 1.2 or whatever, this other app store that I've learned about recently, I'll have to put a link to it in the show notes, will simply query for that GitHub repo directly and then install it directly on your phone. So imagine that as a distribution method. Right?

There are many many possibilities on to get these into the hands of your users. Obviously, iOS is a much different story, and I've heard a lot of candid, candid opinions about Apple's policies on this. But, nonetheless, my point being is that this isn't even just about the architecture now. You can get this into the hands of users really quickly depending on the OS they're they're running. So I don't know where to even go from here other than I'll keep playing with the app and test my art skill knowledge on this, but nonetheless,

[00:14:18] Mike Thomas:

I think I think we're gonna be watching this space, quite closely here. I couldn't agree more, Eric. Yeah. The the possibilities are endless. You have me feeling now, like, I should be on Android instead of iOS to take advantage of, you know, the app store possibilities there. It just sounds more open source friendly. So we'll see. Maybe at my next update, I might have to make a jump.

[00:14:53] Eric Nantz:

Well, there is a lot of excitement in the air in terms of the data science communities and other, scientific communities jumping onto the blue sky platform, which as you heard in recent episodes, both Mike and myself, we now have accounts on that very platform, and I've been eager to connect with the rest of the community, old friends, new friends alike. And you may be wondering as you're getting started with this as I was, you sometimes as you join these new platforms, you're trying to think, okay. Well, what's next after I sign up?

Now credit to a capability that I've seen, you know, pushed heavily in the community is that Blue Sky offers or at least community members in Blue Sky can create what are called these starter packs, which are kind of a neat way for a, say, given topic like data science or, you know, other types of science or technology or whatnot, you can access this pack when you join and then by, you know, somewhat automatically get a list of the people included in that pack, and then you can choose to follow them or not. And that's great for an onboarding, but you may be wondering just how deep is the network out there for those people that you see immediately and maybe who are they following and and so on and so on. You know, typical social network type branches here.

Well, imagine a way that you can expand or at least potentially expand your network directly of our itself. Well, that's where our next post comes in from, Stephen Turner who's been featured on the highlights recently as well. He is very excited about the blue sky initiative, and he has another post where he talks about, you know, for those in the science fields that this is a a terrific option for them. And, yes, he has, you know, in his previous post looked at starter packs, but like I said, he was wondering about, you know, other ways to fill his network, you know, and really start to to grow that a bit.

And he discovered this new, web based app out there called Blue Sky Network Analyzer, which is a, again, very simple web form where you can kind of enter the handles of of your account, and then you can kinda see this kind of relationship of the people you are following and then the people they are following and etcetera. Well, he was wondering if he could do that from r itself, and it turns out you can because there is an r package called atrrattrradder. I don't know how to say it. I'm gonna give it a shot.

This has been authored by Johannes Gruber, and this is wrapping what's called the AT or authentication transfer protocol that is the meat of Blue Sky. And that's kind of analogous to on the Mastodon side of things. It's wrapping a protocol called activity pub, but they're different, but yet I think they're functioning in a bit similar way. But this protocol is open source, so you can build capabilities or applications on top of it and sure enough you can, of course, build an r package in front of it too. So Steven's post talks about how he uses this package to first authenticate to the service. And one bit of warning is that this is gonna leverage a Blue Sky API, and like any API, there could be rate limiting depending on how extensive you you, you leverage this. But getting into the example here, he authenticates to the Blue Sky service and there is a helper function called auth in the package that will guide you for creating what's called an application password for the first time, or if you have an existing one you can just use that as well.

There's a simple function called get underscore follows that will simply grab the immediate of your immediate follows, you know, their handles and whatnot, And then you can kind of take that that list that you get initially and then through a looping function like lapply or whatnot, you can now get their follows. So as you can see, you could go many levels of this and that's where, like I said, you got to watch yourself with the rate limiting factor at play there, but then he shows first in a simple terminal based output after a little bit of filtering all the handles, their actual names, and the URLs to their profiles.

And, yes, you could also make that really fancy by feeding that into GT as well, one of my favorite packages for creating tables in R where you get this nice, he wasn't able to put it verbatim in the post, but you get a nice hyperlink with their handle and their name in that table and how many followers they have, and then you can simply choose at that point as you expand that network in a data driven approach. So, again, you could run this in many different levels, but, again, he cautions that, there could be some rate limiting depending on how often you use this, and he did try this when he kinda got when he tried to run over 5,000 accounts based on kind of this, you know, this triage or this depth of network.

This is where if if I was gonna build something like this, I might tap into the polite package, which we talked about many episodes ago, which would kind of handle that in essence sleeping between requests or pausing between requests in a somewhat randomized fashion, which might help with a service like this. So, again, that's like little, exercise for the audience if you want to try that yourself with with Steven's code here. But, nonetheless, I am excited to see the tooling around Blue Sky start to really take shape, such as what what Stevens, is exercised here in this post and that that aforementioned AT RRR package.

I've also seen Bob Rudis start to look at some of the tooling base and go or whatnot around Blue Sky as well. So there seems to be a lot of excitement around this, and I am all for making any tool like this to make the onboarding to new social platforms like this easier for those in a community no matter what your previous experience is in in these platforms as well. So really innovative approach and it just goes to show you ARC can do just about anything. Right, Mike?

[00:21:20] Mike Thomas:

Yeah. It is pretty incredible. It's a nice little, tool in terms of the script that we have that Steven's put together for us. And and as you said, if you have, you know, just maybe a 100 or a couple 100 followers, you shouldn't have any issues with the API. There is also in in the footnote of this blog post, or at the bottom of this blog post, there is a link to a web application called the Blue Sky Network Analyzer, which I played around with a little bit, which is pretty cool. I don't think it's it's using r, but, it seems like you're not going to get rate limited or throttled as much if you try to use that, web application instead. But a great, you know, blog post here in terms of the code that anybody could run. Just replace your username and your password, or use the auth function from the ATRRR package to be able to interactively, authenticate to your Blue Sky account, which is pretty cool that we have both options there. And a nice little GT table here, showing all of the, I guess, the top 20 accounts, that Steven's followers follow, if I got that correct, which, he does not also follow.

Really, you know, neat blog post here. Really nicely done. I am all in on Blue Sky. Excited about it. A lot of it does feel like old school data science Twitter community. So, yeah, excellent work.

[00:22:52] Eric Nantz:

And, late breaking here as I was prepping the recording here, I saw got wind of, you know, another possible feature that Blue Sky could have, which I've seen also push for mastodon type situations as well, is, let's say, you have a blog post somewhere and instead of relying on proprietary service for your comment system, imagine you, like, have a post on Blue Sky with, say, a link to your blog, but then your blog post is able to embed replies to that post as if they were comments on your on your blog directly.

Late breaking here, but there is not one but 2 extensions in quartile now. One by James Blasto and one by Garrigay and Buoy at the time of this recording that are gonna give you short codes in quartile to embed this into your quartile webpage. So and it works. I tried James, widget or his post where it said, hey. Reply directly on Blue Sky. Refresh the page, and your comment will appear on the page itself. And, yes, it works as advertised. I was comment number 1, so I got that I got that flag standing on that one. But that that my point is is that, like I said, the tooling around this seems to be expanding by the minute here, and I admit I am late to the Blue Sky party. I know there a few others that have been on this train for a bit already.

So I've got some catching up to do, Mike, but I'm excited to see where this takes us.

[00:24:26] Mike Thomas:

Lots of exciting things coming out of quarto this week. I am literally looking at my Blue Sky timeline, and you're exactly right. The top, one of the top posts that I'm seeing is from Garrick explaining, this quarto extension. Looks like it's really easy to just, add this this little one liner. This includes sort of chunk in your quarto blog. It's blueskydashcomments with the post link, and then replies to your post become comments. That's pretty incredible. For those of us doing more antiquated things, there is also in the quarto 1.6, a nice new landscape tag that allows you, for your PDFs to, have a particular page that is in landscape, which I had to do some pretty crazy knitter things, previously to get working. So I'm very thankful to all of the work in the quarto community lately.



[00:25:21] Eric Nantz:

Yes. And, boy, that extension system just does wonders, doesn't it? It is exciting to be able to tap into this, as you need. That that's really exciting times. And last but certainly not least in our episode today, we're gonna revisit, the object oriented framework that we mentioned in a couple episodes ago just had some really nice quality of life updates, and that is the s seven package, which for if you didn't listen to that episode, this is the new, object oriented paradigm that has ambitions to be pushed as a new, you know, might say reimagination of the concepts of s3 and s4 class systems to be eventually, included into base r itself.

And I men remember mentioning at the end of our explanation on that, it will be interesting to see just where s 7 starts to take shape and some of the best use cases for that. Well, we got a pretty fascinating look on how it might help the user experience of even existing functions if they are reimagined with the s seven approach. And this post comes us from Josiah Perry, who's also been featured on the highlights numerous times. He has had a lot of interesting adventures recently with the Rust programming language and that's integration with R, but he takes a little diversion on that and talks about, you know, where s 7 might help in certain situations and to set the stage here in his post, he talks about some of the excellent, functions from the read r package, which is part of the tidyverse that lets you import data files in text format no matter if there's CSV, delimited, or even other types of syntax.

And it has a lot of arguments because it is doing a lot under the hood and has to account for a lot of different issues with these files. And in particular, Josiah starts by talking about the cow_names argument. And as you read that, as a user and may not be familiar with that argument, you may think logically, well, cownames, maybe that's a switch. Maybe that's an indicator of if the file already has a present or not. And, yes, you can set that to true, meaning that the first row indeed has a column names.

Or you could set it to false, meaning that there is potentially no header there, and you're going to have to do that yourself. Well, speaking of doing that yourself, column names not just takes a true or false, it can take a vector of character strings to use as the names of the of the columns that you're importing from that file. Let that sink in a little bit, an argument and take either a logical or a character string. Now that can be elegant in many situations or it can be a little confusing on how to wrap your head around that which I've been on both sides of that fence especially when I was new to like the base r read dot CSV and and, and the read r package years afterwards.

So he talks about a couple different scenarios where he might use each version of this one of which is you it has headers, but you want to give it different headers, So you could do that. He uses, like, an exported version of the iris set just for illustration. You could easily do that. You could also say, well, as if it didn't have the names already, but I want to add the names myself in the function call. But I don't want it to be the names of the file already there, so I want to skip the import by, by one line.

So then you're kind of putting that in yourself. And, again, there could be a situation where there's no headers at all. You just either say you don't want the headers and you want to put them in after the fact or you, supply the column names yourself. The if you get the hang of it, yeah, maybe you can, you know, use that, you know, at the paradigm in the future. But he wonders if there may be a way to make that a little bit less complicated because under the hood, in his opinion, call names kind of has 2 things going at the same time.

1, is a header present or not? 2, do you want to add your own or not? And so that leads us to one of the interesting aspects of s 7 that you could potentially play with here with this reimagination of a radar importing function is that this radar function, these set of functions have over 20 arguments that you can specify. Some of these are common. Some of these are very niche because they're really dependent on your use case. What if there was a way instead of having the you to have these 1 by 1, you know, specified in the in the function arguments that you have, like, an options type argument with more detailed settings. Again, maybe more for the niche ones that have same defaults from the start just like the function does currently, but a little more streamlined way to feed those in.

So he looks at, say, read underscore CSV, and there are quite a few, like 14 to be exact, of these arguments that he thinks should be belonging in this options paradigm instead of in the function call itself. And so, finally, we get to in the post how you can use s 7 to do in essence argument validation, not too dissimilar to what I've done with packages like assert that or checkmate in my functions to determine if a parameter is indeed a string or a logical or, a value that fits, like, the boundaries I'm expecting it to have.

You can do the same thing of s 7 and have these properties, you know, listed where you start a new class, but then you put in kind of the default value for it or a default function call in some cases, and then what you're gonna do to validate that input. So one example is the locale argument, which can have from read r directly the default locale of the system itself, or you can have a validator to look at is this coming in as a proper string and, in particular, sometimes the time involved with that, and you can, you know, validate that with something like our lang checks, like is character or is scaler and whatnot.

And then if it's invalid, you can show a friendly error message afterwards. You can go down the the rabbit hole of this for all the rest of the arguments as well, such as repairing names, missing values, quotes, comments, and you can see many of them will follow a similar format. And then towards the end, you can now have this new class. He's calling it radar ops, and then the properties are each of these kind of property functions that he defined earlier in the post. And then lo and behold, he's now got an example at the end where it's got the 4 or 5 arguments that are most common for read CSV, such as the file name, the column names, the column types, which ones you want to select, and then there's an options argument where then you just feed in this initialization of that class.

But you could put inside that initialization of that class the other options that in there that you want to customize yourself. You don't have to, but you can. So in his opinion, this can help, you know, minimize that cognitive load, so to speak, as well as consolidating those options that are less frequently used in an approach that he says he's gleaned from his experiences with Rust, similar paradigms in the language and how that handles function arguments from both kind of a frequently used standpoint and those that maybe not so frequently used. So fascinating thoughts here, and I like I said, I've been wondering where the best position s 7. So this gave me quite a bit to think about in in my next generation of packages and combining the object oriented approach with these assertions at the same time and try to make the user experience

[00:34:02] Mike Thomas:

a bit more streamlined to boot. Yeah. Eric, this is a really nice applied approach to demonstrating what s 7 can do for us. I I know Josiah has a lot of strong opinions. And this one, I am going to agree with him on because when he talks about how it just feels a little weird that a particular argument could take a Boolean or a character string, it does. You know, I've been there before. It's it's very convenient, I think, to the folks who maybe are earlier on in there. There are journey that that might not necessarily notice.

But I think in terms of, you know, thinking about package developers and making sure that what you are developing for others is is really strong and stable. I think that can lead to a lot of, I don't know, potential things, being a little less stable, a little less robust if you have arguments that are taking, you know, different types of of class objects, if you will. But, anyways, so I thought that this was a great use case for demonstrating s 7 and and taking sort of an opinionated approach to how we could potentially alter the the reader package, in this way. It seems like, you know, the the workhorse functions from s 7, seem to be new property, which is to create validators, if I have that correct.

Exactly. Yep. I'm up to date in my object oriented programming and then a new class here at the end where it seems like we call all of these validators for the different properties that we're going to list in this new class. And, obviously, that new class function from s 7 is going to create a class object, a really nice walk through about how you can, you know, take a look at that class object just by sort of printing it. And then you can also use the at accessor, the the at symbol, to be able to pull out and view a particular property in that class object, which is really neat. I mean, it feels kinda like working with lists. It shouldn't be super scary. Right? The syntax is gonna be a little bit different with, you know, a lot of objects that you're creating via the the new property function for these validators tend to start with a a period, you know, so that might be a little bit new for folks. But it's nice that you can sort of define these these validator tests, if you will, in the in the same place that you're writing a lot of these these logic. So, I'm definitely a fan. I really appreciate this applied walk through. It it helped me wrap my hands around, the benefits of s 7 and and what the syntax looks like and how to use it, compared to, you know, what I've seen in some of the other object oriented programming packages and and, base packages out there as well.



[00:36:57] Eric Nantz:

Yeah. And I'll put a link in in the show notes to from the s seven packages site. 1 of the vignettes is talking about the classes and object structure, and it does have some, you know, very much utilitarian type examples here, but they do talk about validation as well. But I think that's a great thing to put side by side as you read Josiah's post, you know, after this as you wanna think about actually implementing this into, say, a function you're creating in a custom way. And, yes, they also have articles about putting this into a package itself, which I'm sure is going to be the next stage of this, especially when we get to that. Hopefully, not too distant future where s 7 literally becomes part of the base language itself. We're not there yet, but that's our goal and I think they are well on their way to achieving that.



[00:37:44] Mike Thomas:

Yes. Yes. We'll see.

[00:37:46] Eric Nantz:

We shall see. And there is speaking of seem oh, there's a lot more to see in this issue as I said. Tony went out of a bang here. There's a lot of additional great content here. We'll take a minute or 2 for our additional finds here and, hey, I I can't stop being on the web assembly side of train with this one folks, because there is a new package that has just hit CRAN called roxy.shinylive. You can probably tell by the name this has got a couple key concepts here that we want to talk about here. This is authored by Pawel Ruchi who is, for those in the life sciences space, he is one of the chief developers of the teal framework for Shiny applications in a modular fashion that is is very very promising in the realm of using, Shiny with life sciences. But nonetheless, he has been one of the early adopters of WebAssembly like myself. And one thing that he was really hoping to have ever since, you know, all this came about last year. A vision that, Micah, you shared with me on this is that imagine you have documentation for either a shiny app or a function that maybe is part of a shiny app. And if you go to that help page for that package, say on a package down site, you can run an example of that right on the spot.

Roxy Shiny live is making that happen. How does this actually work? I don't have enough to do with justice here, but at a high level you can put in, once you install this package, a way to create a URL for a self contained version of this example that you put in your r oxygen block hosted on shinylive. Io, the free service that posit offers for hosting Shiny applications built with Shiny Live for both the Python side and the R side. Remember when this all first came out, it was just on the Python side for quite a bit. Now the r side gets, cake and eat it too, so to speak.

So I'll have a link to the package downside for the package itself in the notes, but you can see a little simple example where he writes it for a hello world string an embeddable URL that you can use in the package documentation that will embed almost like an iframe that shiny live app on the same package help page on the web version of it in both like the manual page you can also do this for vignettes as well you just have to make sure your description has in suggest roxy.shinylive. There is a specific, r oxygen declarative you have to put in your description file to make sure it's using roxyshinylive when it's compiling the help pages, but he shows an example in the package down site where he's embedded basically one of the functions called cross table, which I believe comes from the TOEcosystem or whatnot, where it's got the embedded a shiny app literally right in the page saying tried and shiny live.

Oh my goodness. This is this is game changing in and of itself with documentation. So this is this is pretty new. He's only been working on this, I think, for maybe part of the year. It's been on GitHub for a little bit, but as of now, it is now on CRAN. So, I definitely encourage all of you to try that out if you're interested in in leveraging or supercharging your documentation with Shiny Live involved. And congratulations, Apollo, on this one dot o release. This is this is awesome.

[00:41:31] Mike Thomas:

This is oh my goodness. We're being blown away this week, Eric. This is absolutely incredible. I'm super excited to check this project out. I think that would be game changing to be able to run examples, literally, you know, in some sort of a, you know, interactive session from your package down site, I think that would be incredible in terms of reproducibility and and ensuring that other folks understand exactly what's going on with your package. You get to play around with it before they install it themselves. Right?

Wow. I'm very excited to check that project out. Another one that I I thought was pretty cool as we get towards the end of the year here, hard to believe, but Cole Barrell did a fantastic blog post visualizing his Spotify Wrapped. So all of the music that he listened to, with some fantastic visuals. And I I really love the theme, that Cole used throughout this blog post. Imagine that we have ggplot under the hood here, and it's it's just a really fun little blog post to take a look at, the his listening history for what looks like, I think, the last 10 years of his listening habits. He was able to get data from 2014 to 2024.

So very, very cool, nice little ggplot visual blog post, if you're interested in that type of thing.

[00:42:56] Eric Nantz:

Oh, this is this is slick. I I really like to see this thing. And I've also been on a you know, throughout the year, Mike's heard about this in our pre shows. I've been really spinning up some what I call self hosting services on my beefy news server that I have in the basement a few feet behind me here. And one of them is like a music streaming service with music I've already downloaded for for creative commons or whatnot. This oc remix, the service that I use for those little bumper musics you hear in this episode. I have almost all of their tracks on myself host a server here, and there's a service called Navidrome I'm using that gives me that Spotify like experience internally and wrap up a tail scale. I can listen to that anywhere I am on my secure devices, but they have an API too. So I might have to do some listening habits on my own on which games just happen to be most frequently used in the music that I'm listening to if I need a little, pick me up, so to speak. So this gives me inspiration based on, what Cole's doing with Spotify. So yeah.

I always have the tunes going when I do my hardcore shiny dev work and package dev work, and boy did I need it with a gnarly issue earlier last week. I somehow got through it, but it also it was a journey, Mike, as you know, that the bugging never stops sometimes.

[00:44:14] Mike Thomas:

Oh, I know.

[00:44:16] Eric Nantz:

We knew that pick me up sometimes. And, sorry, I hope that our weekly itself is your pick me up for the awesome content. Like I said, we, we are blown away by what we see, and this week is definitely an illustration of that. So congrats to everybody in the community, our our curator as well or Tony for curating all this, but also, yeah, Colin, Steven, and Josiah. Excellent posts. We are really excited to see where the future is for these efforts. And the future keeps going with our weekly, but it is a community project. And as we said, we rely on your support.

Obviously, I would hope that over the news I shared earlier that, we are looking for curators to join the team, so we'd love for you to get involved. All the details are on our GitHub repository, but you can find all this at rweekly.org. You can also send us what story we should that should be featured in next week's issue. We're just a pull request away, all marked down all the time. We love hearing from you, and those pull requests are an immense help to our curators for each week. And we love hearing from you in the audience as well. We got a little contact page embedded in the podcast episode show notes here on your podcast player. We also have that fun little boost functionality, another great way to get in touch with us directly. You have a modern podcast app. I personally like fountain and Podverse and cast o matic. There are many others as well at new podcastapps.com if you're interested in that ecosystem.

And, also, we are on the social medias. Yes. I am on blue sky now with at our podcast at bsky.social. Also, I'm Mastodon. I'm so keeping that on. I'm very passionate about that with that our podcast at podcast index dot social, and I'm on LinkedIn. Search my name, and you'll find me there. Mike, where can they find you? Yep. You can find me on blue sky at mikedashthomas@bsky.socialoronmastodon

[00:46:16] Mike Thomas:

atmike_thomas@phastodon.org, as well as LinkedIn. If you search Ketchbrooke Analytics, ketchbrooke,

[00:46:25] Eric Nantz:

you can see what I'm up to. Yep. And I do want a quick little, plug here before I head out here. I was one of the organizers and, backstage y people for the our pharma conference. We just released our recordings for the conference event, and there is a lot of awesome content. No matter if you're in life sciences or not, There's a bit of some for everybody, including a talk from Pablo who we just talked about with that, Roxy Shiny Live package. He gave her great talk about how you how he levers WebAssembly for what's called a TLG catalog. I'll have that linked in the show notes too, but it shows the the thoughts that he's had and, and the amazing techniques he's been using throughout the year on that. So if you like your web assembly, there's there's a lot going on in this space.

But we're gonna close-up shop here. Like I said, for those of you out there that may be celebrating a big holiday this week, we hope you have safe travels and and, good times to be had. I'll be hitting the road tomorrow, hopefully, for a successful road trip back to my stomping grounds in Michigan. But wherever your, travels take you, hope you have a great time. And we should be back for another episode of our weekly highlights next week."
"32","issue_2024_w_47_highlights",2024-11-21,27M 49S,"A summary of key contributions to the R language itself from R Dev Day at the Shiny in Production conference, and visualizing ice thickness in Greenland with the power of the tidyverse and leaflet. Episode Links This week's curator: Jon Calder - @jonmcalder@fosstodon.org (Mastodon) & @jonmcalder (X/Twitter) R Dev Day @ SIP 2024 Greenland ice…","[00:00:03] Eric Nantz:

Hello, friends. We are back of episode 186 of the R Weekly highlights podcast. This is the weekly podcast where we talk about the excellent resources that are shared in the highlights section at every single week at rweekly.org. My name is Eric Nantz, and I'm delighted you're joining us from wherever you are around the world. November's already getting close to being complete, but we're not complete here yet because you know who completes me on this show. It is my awesome cohost, Mike Thomas. Mike, how are you doing this morning? 

[00:00:35] Mike Thomas:

Eric, I'm doing well. I want to extend a warm welcome from on behalf of the Blue Sky community to you this week.

[00:00:39] Eric Nantz:

Thank you, sir. Yes. It it took a little bit, but I found my way there, and it seems like I've I've, connected with a few people already, yourself included. So I'm eager to participate, share my my, little bits of, dev hacking here and there and learnings I've had such as when I the post I shared recently about editing these Our Pharma workshop recordings and learning new chordal tricks from the esteemed Nicole Arini. So, I mean, there's all all sorts of things I'm learning already. Happy to happy to share that and also learn from awesome people like you at the same time.



[00:01:11] Mike Thomas:

Beautiful. Yes. No. Hopefully, going forward, I will be, at least myself, be posting, the Our Weekly Highlights podcast episodes on there. So we have them both on Mastodon and on Blue Sky for wherever you are on social media today.

[00:01:26] Eric Nantz:

Yep. And there's many choices, so that's why we wanna make sure we're we're touching the the audiences where they are, so to speak. So, yeah, Blue Sky is in in our future, so to speak. And but we'll talk about the present here. We got a great issue to talk about today, which has been curated by John Calder, another one of our longtime, curators on the Arrucci project. And as always, he had tremendous help for our fellow Arrucci team members and contributors like you all around the world with your poll request and other suggestions for resources.

And one of the efforts that I've had my eye on for, now a couple years, but it's been really gaining a lot of momentum is, of course, we're here talking about the R language. Right? Our language is built on the principles of open source, which means that in theory, anybody around the world can contribute to the core of our lang your language itself. And there has been a concerted effort to bring in new contributors that wanna be able to impact the R language, get, get involved with open source. And that's what we're here to talk about today, where there was a special session at the recent shiny in production conference run by our friends at jumping rivers, a couple months ago, where a special our developer day.

And this is coming to us from the jumping rivers blog, but it was authored by Heather Turner, who has been spearheading this effort of this our con contribution working group, which has been a joint effort of the our core team, various community groups, including the our ladies initiative, the our forwards initiative, the diversity and inclusion working group, and additional members of the R community that really bring in, you know, an easy way to give back and contribute to the R language and helping with that onboarding process.

But as such of a developer day, they tackle quite a few issues. And so this post talks about some really interesting issues that they are trying to tackle with the R language and the R project as a whole. So I'll give my take on a few things and then pass the baton to Mike. But the first issue on the docket here is talking about translations. Obviously, a very important part of any open source project to be able to display any, you know, diagnostic messages, other helpful messages that are happening in an R session to the language pertaining to the locale that the user is executing R with.

So, Heather mentions, Gabriela de Lima Martin, she was able to figure out how to contribute to R's WebLATE, which is basically a browser that you can see in your R session that gives you, like, the render documentation and other things like that that she was able to build in translational messages, 1st in a conventional way by translating individual strings at a time, but then leveraging a little bit of machine learning to help with that translation layer. But as with anything with automation, you need to have a set of eyes on it after that finished product because you might get some issues from time to time.

Yours truly has seen this. If you've been listening to our weekly highlights and one of the podcast apps that support transcripts. Yeah. The transcripts are about 95% of the way there, but there are some corrections I have to make as after we release it, such as, and I this happens to this day. It thinks the our weekly highlights podcast, the r is spelled o u r every time we do this transcription. So I have to manually update that. So I'd imagine there were similar things with the translations that Gabriela involved. But the good news is is that she was able to translate over 200 messages, produced by R on this developer day alone.

And she this has a link to the issue on the R dev day repository where she put in her notes if you wanna help contribute to this effort as well. But going side to side with that, they wanted to have, you know, as the project as a whole would like a more real time look at the progress of these translations. Where are the gaps? Where are the things that are on track to being completed? So, this separate effort was building a translation dashboard, and this was tackled by a few people, Mario Reaman, midmercilynhausenrabie, and Murad Kamalov.

Hopefully, I said that somewhat correctly, where they wanted to work on this translation dashboard. And within that, 2 specific items to address. One of which is minimizing the dependency footprint by removing the string r dependency, which will help streamline deployments, and then switching from the previous framework of flex dashboard over to quarto, which recently had dashboard support, which I made excellent use of recently in some open source work. So I fully support that that that pivot of the infrastructure, and I think that's gonna set them up for a great success.

And there were some issues that they found with looking at supplemental or markdown rendering, things that they could streamline from the previous, infrastructure from flexdashboard. So there is a bit of cleanup involved and also leveraging GitHub actions and making sure those are working correctly with this new pivot to the portal dashboard. And it does sound like they've got a scheduled deployment for this so that it can be updated in real time. But there are links in the post to looking at this in more detail, but it's really looking promising to get kind of a a fit for purpose look at the progress of the translation of the R language into these different locales that, again, will be a very, very big help for increasing adoption and in in inclusion in general for looking at the R language itself.

And then lastly, from my standpoint, and I'll turn over to Mike after this, but there was a slight bug that was found with the Cairo graphics engine, which is used primarily by 9 maybe it's by default now. I'm not quite sure, but there was a bit as little sub set of bugs that were being tracked on the Bugzilla page of the R project itself and then be able to help reset, looking at this particular issue where in a plot, there's an example of this in the post itself, there should be a solid color. Let's say, like, a 1 bar bar chart or something.

There was a little white line that was, you know, happening in, like, part of the plot. And why? Doesn't make any sense. So they took a look at the GR devices package, which is now part of base r. They kind of see where they might we're seeing that crop in. And there was a lot of just investigation to see where this issue was able to find it, but they learned some best practices from developers on this and to be able to get some advice that was given to them by Paul Merle himself, which has been one of the influential leaders of the graphic devices and R itself.

And they figured out there was, you know, a key statement in this plotting the grid rectangle that this example is based from that there was something being drawn or something not being drawn apparently that had to be updated in the c code. And then they found a workaround that was leveraged from the quartz device engine to fix this white line even happening. So that's really getting into the nitty gritty or graphic devices where for me, I know a little bit about the grid system, but this will be way outside my my expertise at tackle. But it's a great example of team effort of a couple of developers at this dev day. Again, taking advice from Paul Merle and translating that so they could get a reprex of this going and then really isolating the problem.

And then they are working on now the next steps of getting this patch accepted into base r so that the future versions of r won't have this phantom line appearing in these rectangles. So it really had to do with getting involved with not just a graphic device code, but the c code as well, which again, fair play. Great way to level up your knowledge because a lot of r is based in c, some bits of Fortran. So it's a multilingual environment even of itself with the source code. So really fascinating to see where that takes you. But that's not all, Mike. Because apparently, there was some container fund to be had in this dev day too. No. And and as you said, you know, r itself is is built on a multilingual,

[00:10:20] Mike Thomas:

somewhat complex environment. And if you want to build r on your own computer, it can be tricky to do so, you know, for the purposes of trying to contribute, back to base our itself. So another project, that's been undertaken is called the our devcontainer project, which is a Versus Code dev contain well, not necessarily Versus Code, but, that's one way to to leverage it. It's a Docker containerized environment for R, that can be launched. If you're in GitHub, you could use GitHub Codespaces or Gitpod, which is pretty cool to be able to essentially develop in the browser, as I understand, using, you know, this containerized technology that has everything sort of already there pre installed for you, all the dependencies that you would need to, work on contributing back to the R language itself.

One thing I didn't know about GitHub Codespaces is that I think you get like 60 hours, I don't know if it's per month or per year, for free, because GitHub Codespaces sort of, you know, spins up a a server, on the back end for you that allows you to, develop in the browser, which is nice. So I know that a lot of those services have a cost associated with them, but it seems like there is a fair amount of free hours that you would get there. An alternative, if you didn't want to use GitHub Codespaces or Gitpod, would be to, do it locally essentially and you would have to have Docker installed, Versus Code installed, and the Versus Code dev container extension. It's a workflow that, you know, we at Catch Brook love, as I probably talked about on this podcast quite a bit because it allows you to work in a containerized environment, and ensure that everybody has essentially the same dependencies, when they're collaborating on a project together. So it's it's pretty cool.

One of the drawbacks is that, the dev container is currently built for a specific OS, and for a specific architecture that has some buggy things happening on more recent Mac computers. So the the team on our dev day worked on trying to improve, the experience for those who are working on a a Mac machine. And unfortunately, I I think this depends on the the Arm 64, architecture, if you will. And, you know, some Arm 64 machines, I guess, are not available, on GitHub actions. So they they were running into a few limitations there, but they definitely made a lot of progress on solving this issue which is tracked as issue number 112 of the r dev environment GitHub repository, which I did not know existed, until, you know, the the highlights came out this week, which is pretty cool. It sort of warms my heart that, so many of these efforts, you know, not just the fact that it's it's using a technology that I love in in Docker containers, but it seems like there's a lot of efforts going along with our dev day that are trying to make it a lot more approachable and easy to actually contribute back to the R base project, itself because that can be pretty daunting. I mean, when you you think about making a contribution to the language itself and not just a particular package, that's pretty scary to me. And one of the reasons being so is that, as you mentioned, Eric, you know, a lot of this is built on non our code, right? C or Fortran code.

But but a lot of it probably is in our code as well so it depends upon your comfort level and I think there's always a place to get started. If you are, you know, willing to to put in a little bit of time and effort to take a look at the Bugzilla that's out there and all the bugs that are tracked, well as part of the R project and see if you can find one that would be interesting for you to try to take a crack at. And now that we have, the r devcontainer project, you can literally have everything you need to start working on that, essentially at your fingertips in the click of a button, which is pretty cool. And going along with trying to make things easier for those contributing to the project is that, there's an initiative out there in trying to create an R development guide or an R dev guide for short to document some of these processes and document, you know, how, you can contribute to the our project and sort of the workflows that might work best for you to be able to accomplish that.

So it's pretty cool resource, you know, maintained again by that contributor community just like this translations dashboard that's out there. And if you haven't seen the translations dashboard, it's really cool. It was my first time seeing it, which is what we talked about at the the top of this highlight. It's a great visual to take a look at how many functions, and aggregate it up to to packages as well have, translations for the messages, for those particular functions and then how many don't as well. So it's pretty interesting and I have to imagine that as you mentioned Eric, you know, AI could actually be helpful I think in a situation like that. Granted it's probably not gonna get it right a 100% of the time but maybe it'll take some of the legwork out for you. So I I think this is a really cool blog post to walk through, you know, how folks were able to contribute on this r dev day. I think it's it's awesome that they even, you know, hosted and and had this r dev day.

They already have one scheduled for 2025, in October. So if you feel like you want to be part of that, there's a registration form that you can sign up today. So kudos to to jumping rivers.

[00:16:07] Eric Nantz:

Yeah. I'm I'm loving to see, you know, giving the opportunity for developers to to come together and especially bringing in new talent, new developers that wanna contribute. And and I'm really the myriad of issues out there. Yeah. I was looking at the issue tracker itself as you were talking. There is a lot to choose from. There is lots of places that if you're looking at different areas to contribute, there I think there's something for everybody here. Even just, sifting through this, I see there's an issue about would any was there a way to connect the positron ID? Of course, the new ID offered by posit to one of those services I get pod over SSH to have that remote our session as a back end instead of having to have it locally. That's very similar to what I've been doing on my local Versus code setup with my dev container dockerized environment. Albeit, much like you said in the outside of your description there, I've only optimized my dev container stuff on Linux because that's what I'm developing on. I never tried on arm arm, processors for Mac. So I'm sure it wouldn't work there. And it gets really dicey in these situations. But to be able to have these these development environments going wherever the user is, wherever it's x86 base, like our typical Windows or Linux, you know, architecture or the more recent arm 64, like these fancy MacBook m ones, m twos, m threes, m fours, too many m's out there. But, nonetheless, they are becoming way more popular. So to be able to have that experience out there, yeah, it's gonna take a lot of engineering. But just imagine being able to have that whole development stack of our itself without you having to think about uh-oh. How do I compile Fortran code on my Mac? Oh, gosh. You know, having this in a containerized environment just opens the doors. There's so many possibilities here.



[00:17:58] Mike Thomas:

Couldn't agree more.

[00:18:16] Eric Nantz:

Rounding out our highlights today. We got another visit to a visualization corner of sorts, and this is very similar to last week where we had a notebook style presentation of an infographic here. We've got an interesting visualization on the thickness of ice in Greenland. This post is authored by Michael, not the same Mike that's joining me on this show, but Michael. And we don't know his last name because that's how we got on the blog. But, nonetheless, if you're listening, Michael, feel free to send us your, detailed info after this if you like. But, nonetheless, he has authored on his blog this, very notebook style post on looking at how to visualize the ice thickness in a very innovative, mapping heat map structure using data that's either available or or or processed into via typical tidyverse function. So we'll dive into this. And specifically, he is using the Greenland 5 kilometerdemice thickness and bedrock elevation grids. That's a mouthful.

But, nonetheless, he's got a link to download, I believe, the CSV after you register for that service. And once you have that, you can read this into your session. And apparently, it's an ASCII style file, and he's got a link to the user guide to look at this. But he mentions that the wrangling, not quite straightforward. So there is a collection here of data processing with mutating, grouping, modifying, more mutation, more pivoting with the tiny r package. And in the end, he's able to finally get to a format to compose this as a raster image, which again, you're talking to a novice with spatial visualizations.

I always learn on the fly. But the first render here in the post is a war a raw mapping of Greenland, but with the color gradient, if you will, based on the thickness. And the plot itself is only, like, 1 2 lines of code after you feed in that tidy dataset. So the meat of it was in the processing to get this initial visualization. But very nice straightforward, you know, heat map like representation of the ice thickness. And as you might imagine, the closer to the center of Greenland you get, the more thicker the ice is as opposed to the outer layers near the ocean.

But then there's more that you can do here, Mike, and that's where we turn to our friendly leaflet projection time. What do we have in store there? Alright. So using a combination of the leaflet package and the Terra package, I believe,

[00:21:02] Mike Thomas:

Michael's able to modify the original data frame that he called thick, that was used for the first static plot, and use a function called project in RAST, in concert to be able to build this new object called thick WGS 84, which is really the the main object that's supplied to the leaflet function add raster image here. So that's that's gonna be this raster image that he's able to supply to Leaflet that does most of the heavy lifting, and then a really nice, gradiented palette, legend that, is up at the right hand corner of the chart. And obviously, you have the the interactive zoom in, zoom out functionality to be able to explore this, in sort of more of an interactive leaflet context. So really nice, 2 different plots sort of visualizing, you know, fairly the same dataset, on Greenland ice thickness, and it's it's just really well, concisely written code. So if you are in the geospatial space or interested in the thickness of ice in Greenland, this one's gonna be for you.



[00:22:10] Eric Nantz:

Yes. And and some of the inspiration where this came from, at the top of the post, he mentions that this is part of the 30 day map challenge that's currently ongoing. So if you wanna hone up on your visualization skills and kinda be part of a community of of aspiring data scientists and visualization experts doing this, he's got a link to the 300 map day challenge, which again, if I had infinite time, I probably would get my hand at that. But my last adventure at Leafa was ironically for this past our pharma conference where I built this I call haunted places shiny app where I had a leaflet map of these of the US and United States. But then each city where this quote haunted place was located at, I use another data set to get the cornice and throw it on the leaflet. You know, not too dissimilar to what, Michael has done here.

But it's amazing just with a few lines of coding, you get this interactive map up right away. And I'm sure you could add way more to this. But in the end, it does it does the job, and it does it really well and really concisely. So great great example what you can do in the r side of things with with spatial visualization. And, yeah, I'll be keeping my eye out on his blog as well as others on social media to see whatever creative maps that people

[00:23:29] Mike Thomas:

are are creating with art these days. Absolutely. And one thing I I love about the blog post is that he he finished it off by adding, three references that I believe correspond to maybe some of the datasets, that were used or references on the topic itself. So fantastic attribution here.

[00:23:47] Eric Nantz:

Yep. And if I my eyes don't see me. This looks like a Cornell based blog if I had to guess, but nonetheless, great great, great, post on Michael's blog here. So definitely check that out. But also check out, you know, everything else in our weekly we got going here. And then when we do the time, we don't have time for our additional fine segments. But rest assured, there's another great collection of new packages and awesome, you know, shiny app links here that I saw a few there that caught my eye and other upcoming events and whatnot. So as always, every issue has got a mix of everything for everybody.

How does this issue get created, folks? It's not automated. As much as we love them machines take over certain automation things, this is a human created effort, but, honestly, it's gonna stay that way. Well, we rely on you out there too to help with the projects. So one of the best ways you can do that, we wanna make it as easy as possible for you to contribute. And honestly, the whole get and GitHub model is what drives our weekly, where if you find a great resource that you think should be shared, we are a pull request away, folks. It's just a markdown file in the end. Every issue here is just a markdown file. If you can read markdown, you can contribute to our weekly. So it's just a pull request away. And the upper right corner, a little Octocat image, giving you straight to the 10 straight to this week's issued draft. And then you can send the poll request right then and there and help us with that new resource you have found. And, also, we love hearing from you in the audience as well. It's amazing that 2024 is already almost over. But, of course, we love seeing what our weekly has done for you in terms of, like, shared learning or other tidbits.

You can get in contact with us a few different ways. We have a contact page in the episode show notes and your podcast player that you can link link to and click on and send us some feedback there. We are also with a modern podcast app. You can send us one little boost along the way to share your love for the show there directly to us. And, lastly but not least, we are on these social medias as well, Where, apparently, I got another thing to share now, but I am mostly on Mastodon still with atrpodcast and podcast index dot social.

And now I am on Blue Sky as well, as Mike mentioned. I am atrpodcastat bsky.social as well. If you wanna find me there, I'm happy to connect. I've already connected with a few of my good friends already back again. Love to connect with more of you. But, Mike oh, last but not least, I'm also on LinkedIn as well. You search my name and you'll find me there. But, Mike, where can they find a hold of you? Yeah. You can find me on Mastodon, I think at mike_thomas@phostodon.org.

[00:26:29] Mike Thomas:

You can find me probably more often on Blue Sky. Now it seems to be the one I'm checking a little bit more, atmikedashthomas.bsky.social. Or if you search Ketchbrook Analytics on LinkedIn, you can find me there as well, ketchbr0ok.

[00:26:47] Eric Nantz:

Very good stuff. And, also, you know, you do have a blog out there too on your Catchbook Analytics site. I was just checking that out for for some stuff I'm doing, a little little off the cuff here. But in my infrastructure at home, I just spun up a self hosted RSS aggregator. So when you put an RSS feed, Mike, I'll be able to put your blog on mine too. I couldn't quite get it, but we'll we'll follow-up offline on that one. But, nonetheless,

[00:27:11] Mike Thomas:

Mike's got a cool blog out there. Check that out. Yeah. Just don't look at the the last blog post date because it's been a little while.

[00:27:20] Eric Nantz:

Hey. Hey. No no shame in that. Look at my last episode of the r podcast itself, then then then I'll feel ashamed. Nonetheless, I hope you don't feel ashamed about listening to this episode. We do our best to give you the great resources that are shared on our weekly. So that'll wrap up episode 186. That means we're 14 away from 200. One way or another, we'll get there. So until then, we hope you enjoy this episode, and we should be back for another episode of our weekly highlights next week."
"33","issue_2024_w_46_highlights",2024-11-15,49M 41S,"The innovations of the R community never cease to amaze us! How a programmatic approach to generating markdown was vital to a high-profile Quarto site, a novel infograph of Bob's Burgers sentiment analysis, and updates to the next evolution of object-oriented programming in R. Episode Links This week's curator: Ryo Nakagawara -…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode a 185 of the R Weekly Highlights podcast. This is the weekly podcast where we talk about the excellent resources that are shared in the highlights section and elsewhere in this week's current our weekly issue. My name is Eric Nantz, and I'm delighted you join us wherever you are around the world. And he is back. He couldn't stay away forever, but my awesome cohost, Mike Thomas, has graced us with his presence again. Mike, how are you doing? Not that you've had any, like, free time or anything. I'm doing well, Eric. Calling in from a new location,

[00:00:33] Mike Thomas:

full 2 miles away from my old location. Settling in pretty well so far, and I have also migrated locations virtually. And as of last night, I am on Blue Sky.

[00:00:50] Eric Nantz:

Oh, you are? Okay. I'm feeling the the peer pressure now if you're there. It sounds like it's the place to be. Definitely taken an awful lot in the in the data science sector. Now, of course, I do still have a a very, vested interest in the mess, the the fediverse as well, which is just kind of a part of, but kind of it's a little dicey folks. And I'm still trying to sort out just like anyone else. But, yeah, you may see me on there in the future. We, actually did spin up a new account for the R pharma team on Blue Sky that we just put out there before the conference that took place a couple weeks ago, which I'll have a lot more to say about that once we get through the editing of all the recordings. But, nonetheless, that was a major event. But we're here to talk on our weekly, of course. And this week, our issue was curated by Ryo Nakagorora with, as always, tremendous help from our fellow, our weekly team members, and contributors like all of you around the world with your poll request and other suggestions.

And my goodness, Mike, we're gonna lead off with one that really is a both mind blowing, you know, showcase of what's possible with our end portal and just really inspiring as well to see just how far you can take these dynamically generated reports. And this has been authored by a a good friend of ours, from the highlights, Andrew Heist, who speaking of free time, I don't know how he has a free time to knock this stuff out. My goodness. I need whatever he's eating. And Andrew Heist blog, yeah, is always gonna be super detailed,

[00:02:24] Mike Thomas:

probably pretty long, and again, blow me away with how much he was able to accomplish.

[00:02:32] Eric Nantz:

Yep. And we'll dive into those details now because this post is talking about how he approached generating and rendering dynamically computed content written in markdown, but programmatically with quarto, which is something I've had a little bit of diving into with the r markdown days with various snippets, especially of reusable kind of sections in a report. But Andrew just blows my stuff out of the water, what he accomplishes here. So let's let's set the stage here because there's a big picture here. So, yeah, there was a little thing called the election that happened a a weekend or so ago. And apparently, Andrew was helping out in the state of Idaho with assembling the election results and then surfacing them into a really fancy quartile website, which love to see that quartile in the real world, so to speak in a not so, not so boring situation to say the least. Lots of eyes on that on that side of the news.

And apparently, Andrew has put this in a very sophisticated ETL pipeline, which is merging ETL concepts with, wait for it, targets to help process the data from different sources, whether they're online or from another storage and then creating a tidy, I guess, data store, he calls it, that everything else can be built upon. And then another pipeline is gonna take that tidy set of results and then actually generate the Cortl report and website programmatically. He teased that he's gonna share more about that, but I'm saying, Andrew, not that you have infinite time. I'd love to see a deep dive into those target pipelines because I'm always eager to see just the directions you take with that.

But diving into the rest of the poster, he talks about kind of the, the meat and potatoes of what the website was surfacing, which was taking advantage of. It's really neat features in the quarto UI for HTML, where you can have these tabbed, you know, tab, panels much like you do in a shiny app and in quartile as well, which gave both a tabular visualization and an interactive map of the election results across districts in Idaho. So you could kind of choose which which display you like. And then he shows kind of in general, you might have in the portal dashboard set up these different sections with code chunks to actually generate that table or that map.

And sure, for maybe 1 or 2 races, so to speak in that particular state's election, you could, you know, you could just dive into that. But what if you have a 100 or more of these? Yeah. You don't wanna copy paste that stuff left and right. So he looked at how can we generate these, these table and mapping code chunks and the output from that more dynamically, not just the code itself, but where it's actually being placed in the Quartal website, IE, the Quartal syntax that's gonna be built and marked down for that.

So he first knocks out probably an elephant in the room for those that have done this before is that in code chunks, you can use the results parameter and call it as is to basically not escape that content and the basically take it as it is so that if there's an HTML report and you're spinning out markdown or HTML, it's just gonna surface that out right away when you compile the website or the report. But it doesn't always work the way you want. And he shows an example of like the, a bullet list of like the, gap minder country data where it just doesn't quite work when you need to have additional work in these chunks.

And he shows that in an example where he's trying to round certain numbers, just hypothetically the the number pi, it's only spitting out, like, the code that makes the result in the bullet list, not the actual result of that computation. So it didn't render the inline chunks of these items in the bullet list. And, of course, that's a that's a nonstarter if you're gonna do this more dynamically. So the trick that this is all built upon is to actually pre render these in line chunks before they actually appear in the document. So this is a little different. Right? This is not dynamically rendering the content, what I call just in time of the website compilation.

He is saying, let me re precompute that first and then show it into the actual quartile content. In this case, now you get the rounded versions of the pie number in that markdown text. Took me a little bit to grasp this, but I kind of see where he's going with this. And, of course, this is a trivial example. But back to the election website he was creating, he wanted to do a similar idea, but now with those tab sets of the visuals of the table and the map. So the concept does generalize. First, he's using the Gapminder data to kind of show a tidy data set that would be a scatter plot of the GDP per capital and then life expectancy.

Stuff you've seen many times before in presentations about Gapminder. And then he shows, okay, what if I want in my portal dashboard, the panel tab set, and then within each of these continents splitting up 1 by 1, echoing that particular continents, result from that tidy dataset and tidy visualization. Again, you could copy that over and over from icon in 1234 5. But again, going back to the election result, what if you have, you know, 100 of these or, you know, thousands of these potentially if you're doing a lot of, like, biomarker data or something like that?

So, again, he's going to use a hybrid of the glue package and other tricks to precompute these panels dynamically. He's got a handy utility function called build panel where he's feeding in what's gonna be the chunk label, the output that is in string format, the markdown syntax as injecting that set of like the panel title, the index of the plot. So that then that's going to render dynamically in the report itself. He does verify it works first by running the function and he gets the markdown of the heading and the continent name and then the coach on as if he typed it himself.

So mission accomplished on that front. And then he's able to basically loop through this at one point with a combination of the knitter functions, knit function, which again, quarto with the r execution engine is built upon knitters. So knitter, like, we've we've spoken for years on this show, Mike, about the praise for knitter and what the doors had opened. People need to realize if they don't already, that quarto itself is building upon these types of execution engines. And without knitter, there is no quarto. I'll just be hot take 101 with Eric on that. I don't think it's such a hot take. But No. I would agree with that. It is

[00:10:03] Mike Thomas:

almost, impossible to not take for granted everything that Knitter has done for us. As much as you appreciate it, you should appreciate it even more.

[00:10:17] Eric Nantz:

Absolutely. Absolutely. So, again, just this knit function alone, you may take it for granted. That's when you're in the r markdown Jaysus hitting that, you know, compile report button in the IDE. But this is the engine behind all that. So he shows a great great, great example of using that in action. And then sure enough, he's got a nice little tab panel of the different continents and the scatter plots. And that really saves you a boatload of time there. And then he's got another example later on where you could use this in more of a teaching aspect.

Let's say you wanna show the different stages of building up an effective visualization. And he does this with what he calls the evolution of a ggplot visualization, where you can have these plots saved as objects, say, from the first stage of it to the last stage of it. You can make a tidy, you know, dataset with the actual plot object that is in a list column that has, like, the actual object itself, the text around it, and a description. And guess what? You can use the same logic to create a tab panel going from stage 1 all the way to the last stage and just grabbing these different plot objects from this list in that data frame.

My goodness. That's a great way to help teach your students. Like, you may start of initial plot that looks, you know, very utilitarian and all the way to last step. You got a nice theme with it. You got the nice color choices, the background looking much sharper, changing default labels. So quartile is a teaching tool. I mean, it's already getting very popular in education sector, but, man, this is really, really top notch stuff here. So again, you can take this even further with the concept of making now going back to the election results, you got all this, you know, content kind of stitched together.

Quarto does have the concept of kinda having these child reports inside an overall, you know, website or or report or what have you. So he kinda takes this into a bunch of our chunks again that are dynamically generated. And you you can, you know, do this in quartile 2. Again, built on Knitter, you can have child documents filtered into an overall report. But, again, he's showing then that these report texts is all just marked down in the end. Right? The key is just being able to loop through that. Maybe you have a tidy data frame that's like or or overarching all of this.

And then to be able to use the knit function to paste this all in together. And that's where he shows then in this generated output. Instead of, like, separate reports like verbatim or separate files, he has different sections of this continents report where the user can quickly, you know, go through the different continents, figure out what countries are involved, looking at the details in one table, looking at the plot in another. And you put a TOC with that and you got yourself a really intricate, dynamically driven report or dashboard, whatever have you.

But all this is powered by rendering that markdown content ahead of time and then using it to inject that into the overall portal website. I literally literally yesterday did something very similar to this. Albeit, I did take the manual approach, but I regretted it for reasons. But now knowing what Andrew's done here, I can have a function that generates a snippet of this case iframes of another quartile dashboard inside another quartile reveal JS slide deck. I could just have a tidy data frame that has, like, the links to these iframes of the quartile dashboard, loop through that, get the markdown chunks, and then put that into my main reveal JS document, deploy that on pause and connect, and I'm off to the races.

I have eliminated the need for PowerPoint of these one pagers of results that I had to assemble. So, Andrew, you blew my mind yet again. Credit to you for sharing sharing your knowledge here. And, yeah, the possibilities seem endless with dynamic generation of portal content. So mission accomplished, buddy.

[00:14:51] Mike Thomas:

Yeah. This is fantastic. If you were to try to, you know, create some sort of a quartile or R Markdown document that displayed these 100 different is it counties in Idaho, I think that we're trying to, track or districts in Idaho. I think that's that's what it is. I mean, you could do it by hand if you wanted to, but it would it would take you forever and a whole lot of copy and paste, and God forbid you wanna change one little aesthetic, right, that you wanna propagate everywhere else. You're you're stuck doing a find, replace all. And and I think, you know, we've discussed in the past, the the benefits of of functional programming, and they apply very similarly to quarto. And one of my favorite things about quarto, and it's interesting to me because I just came off a project where we did a lot of work exactly like this. So I can't I can't, praise this type of workflow enough. I think, Eric, you probably have this experience with Knitter as well, where if you try to do some our run some our code, execute some our process within Knitter, it'll run a little bit slower than if you did it outside of Knitter.

So what that means is that you will be able to save time, in your rendering if you have pre computed and pre created any of those R objects ahead of time instead of asking them to be created during the knit process. You extend that one rung further and and we start to get into targets. Right? And that's sort of, you know, the exact goal of targets is to be able to pre process things in your pipeline and only re execute things that need to be updated. So it's a fantastic, complement to the workflow here that Andrew has put together to demonstrate this. And, you know, one other thing that I do wanna highlight is the use of child documents. I'm not sure if Andrew Andrew called it out in the blog post. I'm not sure if at the the end of his whole process, which we don't necessarily have full insight into yet. Right?

In terms of this election quarter report that he's put together. If he leveraged child documents or not, he talks about that when you you take a look at some of these code chunks that are in line in the blog post that are some pretty large glue based chunk, code chunks that he he's putting together that we can, you know, execute as a function. At the end of the day, he talks about how some of this this code in his current workflow, might be able to be condensed, if you took advantage of child documents as well. And what that means in a in a quarto, sense is using this special tag, I think, that starts with 2, arrows pointing to the left and and ends with 2 arrows pointing to the right and, includes, no pun intended, the include verb.

I think that allows you to reference another file, another QMD file that you can have, sort of inserted into maybe like a main dotqmd file. And if you're familiar with child documents in our markdown, it's the same type of concept, just a different syntax. Those are things that we leverage heavily because if you're putting together a a large report, or a large document like the one Andrew is is putting together that has a lot of moving pieces, I think it it sort of always makes sense to try to manage those pieces as separately as possible, especially if you're working on a collaborative team. Right? Somebody can just focus on on one piece and another person can be dedicated to focus on another piece. And I think it just allows you to, sort of piece together your final product in a way that's easier to manage and easier to maintain than if you were trying to do it in some sort of a monolith.

So I can't I can't say enough about, child documents within quarto. The syntax that they have makes it really easy to do so if you're on the quarto website and you're not familiar with how to leverage child documents. Just search it on on quarto.org and and the, function syntax to be able to do that, the markdown syntax, excuse me, will be right there for you. I can't can't harp targets enough as well and sort of bringing these technologies together. I would love to see how he leverages that remote storage with targets and that local DuckDb database as well to sort of bring this whole entire solution together.

But but top to bottom, I think it's a fantastic resource for anyone who is building a large scale quarto type of document or looking to just get a better understanding and better feel for best practices around authoring quarto reports. I think the tips in here will be invaluable for you.

[00:19:35] Eric Nantz:

Yeah. And, I do have it on good authority from the, author himself that there's some big enhancements coming to Target with respect to potentially DuckDV integration and even better performance. Like, Target's already performs great. But what Will has in store for us, oh, you all are gonna love it, especially with those that have these, you know, 10,000 plus, you know, branches and pipeline, you know, targets themselves. So, yeah, stay tuned, folks. It's getting better. But, yeah, this this whole thing has so many nuggets to to choose from here that I've gotta I gotta look at this even more.

But we literally at our day job have a couple teams I wanna they're they're not satisfied with the SharePoint world, man. They want to build dynamically data driven websites of these reports that can be shared broadly across the organization that take advantage of the interactivity that the portal offers, whether it's through, you know, things like the portal dashboard, which I've become a big fan of. Obviously, we have some people looking into the observable JS side of it. But you're not gonna get this to SharePoint folks. That's my hot take too for the podcast that the this this in particular, if they can be used in a high profile situation like tracking election results, good grief. It can be used almost anywhere.



[00:21:00] Mike Thomas:

Not fully satisfied with SharePoint? Are you sure? You have that right?

[00:21:05] Eric Nantz:

I may have to double check my references on that.

[00:21:08] Mike Thomas:

A little satire for the audience.

[00:21:10] Eric Nantz:

Yeah. That that hopefully, they got. But, yeah, we've had some internal debates on that one too. And I was, but I I did like I said, I I've used principles of this, albeit not so elegantly, where I was able to get away from having the creative a rather haphazard PowerPoint slide, but I used Cortl Dashboard instead. And because of the integrations we can have with Cortl websites and and iframes going as backgrounds in a presentation deck, I was able to create what that team really wanted, which was basically an HTML based slide of a bunch of quartile dashboards without having to put the quartile dashboards in the slide deck. But as background iframes, folks, like, this is so many mind blowing things we can do with this. I I hope I can give a talk about that later because I I've learned I've learned some tricks, man, but this is this is tricks on another level. What what Andrew's done here? I'm looking forward to that too.

So as I said, yeah, it was about a week and a half ago that there was a little thing called the election, and maybe somebody needed a little pick me up after that thing which side of the fence you're on. Maybe, you know, having a little bite to eat if you had a long night. Well, who knew that this next highlight was gonna high, showcase on a show that I admittedly have not seen before. I've heard about it. But this is coming from Steven Ponce, and he has put together a web a blog post, albeit mostly a notebook, I would say, about looking at the fingerprints used in each of the seasons of the show Bob's Burgers. And admittedly, I have not seen this show before as I'm Kabooie out of my wheelhouse for this. But it's basically an infograph as the meat of this post that is looking at across the different seasons where and I have to zoom in here on my fancy 4 k monitor to look at this more carefully.

The looking at the the transcripts of this, the dialogue and looking at, say, the the length of the sentences, the unique words, variance among the sentiment of the transcripts, a little text mining action here. How many questions there were? How many explanations there were? And it's a pretty neat infograph that for each, you know, faceted by season, you get I believe they call these like the spider plots or the radial charts. I forgot the exact name of them. But it's a it's a good way to put like multiple dimensions in a circular like fashion, but not be confined to the infamous pie chart limitation. So a pretty pretty neat visual there. And it's and again, the big picture is looking at the patterns and dialogue across the 14 seasons of this show, which again, I'm I'm an old timer. I haven't seen this yet. So maybe it has to be in my, in my, queue of shows to watch when I actually get a free time moment. Nonetheless, the notebook and style is that he's got the different steps in the building this visualization much like a tidy verse kind of, you know, flowchart that you would see often in our for data science and whatnot.

Loading the packages, of course. And this is a little interesting here. We don't see as much of this lately, but Steven is using the Pac man package to orchestrate packages, which some have had great success with. I admit my my, my attempts with, Pac man were mixed at best. But, hey, if it works, it works. So he's got the snippet to load the various packages. And yes, Mike and I were remarking before the show. There is a package called Bob's Burgers R that's got the data sets that are being used in this.

There's a package for everything, isn't there?

[00:25:13] Mike Thomas:

Literally, at this point. Yes. It contains the transcripts. It looks like for every episode across, all the seasons that are available.

[00:25:22] Eric Nantz:

Absolutely. So I'll have to look at that in my spare time, but he's also loading additional packages to help do the text analysis, tidy verse, of course, and patchwork, which we've spoken very highly about for being able to compose multiple ggplot objects together in any way you see fit, basically. So and then, also, he's using the camcorder package, which you heard about at positconf as well, to record the different plots as PNGs as you're going through it. So that will come in play later. So, first, the data which again, thanks to the Bob Burgers r package just simply using the transcript data data frame, and he's got it right off the bat. So that that part's done.

It does a little exploration of it. Although, we don't see the result of it, but there is a handy handy function from the skimmer package called skim, which lets you it's not shown here in the output, but get like a terminal base glance of that data set or a data frame. Really handy, especially if you're in a terminal environment. Then comes the tidying stuff. So we got a lot of dplyr grouping by summarizations. And notice that in his syntax of the summary summarize function, he's taken advantage of the dot groups declaration, which again is thanks to the dplyr version 1.8 or later, I believe.

They introduced the dot by and the dot groups parameters in key functions like mutate and summarize. So you don't you don't always have to do the old dplyr group by, summarize, dplyr ungroup afterwards because, Davis Vahn and others from the Tinyverse team were saying that got annoying on a lot of users. So that's a nice little trick

[00:27:14] Mike Thomas:

that that Steven shows here. Couldn't agree more. Yes. I love the dot buy argument.

[00:27:20] Eric Nantz:

Yep. I literally just started using that for a high high priority project and it's like I can never go back to group buy anymore if I can avoid it. So And the dot keep argument within mutate, so you don't have to use transmute anymore. That's right. Yeah. I need to explore that one too. Another great quality quality of life enhancement, I should say. Next comes the visualization. So he does a lot of setup up front to get the the the labels all in order as well as kind of the the CSS, I believe, is gonna be applied to these plots, a little bit of CSS, and then using glue to dynamically put in various things, and then managing the fonts.

Lots of, fonts are added from Google with the, I believe it's the fonts package, if I'm not mistaken. I have to double check that.

[00:28:13] Mike Thomas:

I think it's I thought it was called Google Fonts, but I'm not seeing

[00:28:19] Eric Nantz:

I'm not seeing where he got that from.

[00:28:23] Mike Thomas:

I'll take a look. Keep going.

[00:28:25] Eric Nantz:

Yeah. We'll keep going. Yeah. We're we're learning here, guys. So nonetheless, then he's able to assemble the theme object in ggplot2, which again is a great way if you wanted to find that upfront with the the theme set and then a theme update. That way he can use that theme anywhere he goes from that point on or the rest of the visualizations. And then comes the main plot where it's a the the nugget here is using the geom polygon to get that nice little polygon superimposed inside this, you know, circular type display. Again, people call that a spider plot or a radial plot. Some to that effect. And then adding average lines on that. But again, flipping that to polar coordinates towards the end.

And then defining the labels and the facets by season. And then adding on top of that kind of this pattern type visualization, which, again, you wanna look at the post to to get to get the meat of it. But there's a nice little pattern. I think that's kind of serving as the background, so to speak, on the plot itself. Again, really neat to play with. I haven't done this myself before. So really intricate annotations that he makes here. And then afterwards, he's gonna save all this as, PNGs for, I believe, at least one PNG I should say for the the whole plot itself.

But he assembled that with patchwork before that to combine everything together. And then to be able to draw that, clean things up, and then using the magic package able to create neat little thumbnails of the visualization that he used, I believe, in the post itself. So little nice lot of visualization tricks here that if you're want to up your game with ggplot too. There's some really some real nuggets to share here. And then like any any good, data science citizen, he's got the nice session info at the end here.

And link to the GitHub repository so you can actually see how this is composed in action. So great notebook setup here. Love the way that you can collapse the different code chunks and just get to what you're interested in. But, yeah, with a nice little, tidy dataset of transcripts and Boss Burgers, got yourself a nice little visualization to, you know, satisfy your appetite.

[00:30:48] Mike Thomas:

Puns galore. Mike, what did you think about the visualizations here? Well, the final output that is at the top of the blog, it's a beautiful infographic. It's really nicely done. I like the contrast between the background that's just sort of off white a little bit and the the purple, gradient sort of that that, ticks the spider plot, on values are are represented by. Really, really cool. Turns out it's the show text package that allows you to manage Google Fonts, has the the font add Google, I think is the, particular function within the show text package that allows you to import certain Google fonts and leverage them in your, ggplot graphics.

One thing that I don't do well enough or understand well enough is adjusting, I guess, the graphics device itself. That, you know, arguments like DPI, which I think are dots per inch. Is that what that means? Yes. Correct. The units, I don't do that enough, unfortunately. I usually just in my, quarto documents, I'm just specifying fig height, fig width, things like that, and and messing around with it until it looks halfway decent. That's stuff that I need to learn a little bit more about, but Steven, has a few different places within this notebook where he is is setting those specific configurations. So I've definitely learned a lot there and you will as well if, that's something that you struggle with like me.

One other sort of cool function that I here's a little today I learned that I can't believe I don't even know if I want to admit that I'm just learning this today but there's a ggplot argument called theme underscore update. Eric, I'm sure you knew about this one.

[00:32:35] Eric Nantz:

I knew it sporadically. I never actually used it. So I love theme minimal,

[00:32:40] Mike Thomas:

but obviously, occasionally, there's some additional things that that I wanna do on top of theme minimal, that aren't within aren't doesn't, aren't contained within arguments of the theme minimal function. And typically I'll just add a theme function after that. And I think ggplot2 knows well enough to use sort of the the last, you know, value for a particular, theme argument to set that as what's gonna be shown in the plot. But I think theme update sounds like it is probably doing a better job of that as opposed to sort of overriding, what you had written before it in your theme minimal call. So this was a new one for me. A little embarrassed to say that this is the first time that I'm coming across it but it is one that I am for sure going to be using from now on in many many many places.

Just excellent blog posts top to bottom. Love the code, love the layout here, and the end deliverable is is absolutely beautiful. So take a look for yourself.

[00:33:47] Eric Nantz:

Yeah. I'm I'm doing you know, first, you're you're you're too, kind or too hard on yourself, I should say. There are so many things in ggplot too that I always scratch the surface of. But let's put things in perspective, folks, as I'm I'm gonna take a quick look at the archive of ggplot too. Did you know that jigapotwo, the first grand release was all the way back in 2007? So it's got a lot going on. I mean, that we're coming there 20 years on that thing. I mean, we're over 15 now. So it's not surprising that there are things in there that we we didn't expect to see. But, yeah, that's why we're having this post from Steven. It's a great great reminder of the capabilities of it. So I'll like you, I'm gonna take note of that theme update function.

Lots, yeah, lots of attention to detail here. I absolutely love seeing how the sausage is made. And, yeah, the font add function, I did a little digging while you were talking, is from the show text package. I don't use that a lot in my daily work, but I definitely will take a look at that for my next Gigi Pot visualization. But, yeah, nonetheless, really great design choices. So this is a great showcase of using the principles that I've seen outlined in various workshops, such as some Cedric Sure or others about what are the best ways to build up an effective infograph that, by the way, you don't need to go to Adobe Illustrator for. You don't need to go to some proprietary product, jiji plot 2. We have a little, little getting your hands dirty, so to speak, and get you really the whole way there. It's just a a wonderful plot here. Absolutely wonderful.



[00:35:27] Mike Thomas:

2007, you say, for ggplot. Well, it's incredible to think how much, that package has evolved. And you know what else has evolved in the R ecosystem? Object oriented

[00:35:40] Eric Nantz:

programming. You got it. You got it. And, you know, you may be as an our user, you've probably used this many, many times, sometimes without even realizing it because of the elegance of the language itself. So what are we teasing here is that since about a a year or so ago, there has been a new effort sanctioned by the art consortium no less to build a new object oriented paradigm into art itself eventually. And right now, it is a new package called s 7. The post comes to us from the Tidyverse blog written by Tomas Kalamazowski and Hadley Wickham himself on the new updates to s 7 version 0.2.0.

And for the uninitiated wondering, wait, why is s 7 even exist? Well, in r itself for historically a very long time since practically the very beginning, there have been at least 2 or 3 class systems in the language. 1 of which is s 3, which is leveraged heavily by the tidyverse packages and a lot of base art functions itself to give you that kind of very easy way to say, create a visualization with the plot function. But if you feed it a data frame, it's gonna know the treat that differently than if you feed it like a single vector or, 2 vectors of say, x and y. It's a dispatching system. I'll be it very general, almost to its detriment to some people's eyes.

Then you have s 4, which brings a lot of formality, a lot of guardrails around your object oriented structures. But I can attest that it is not for the faint of heart. It is quite complex to get into the nuts and bolts of. And when I was doing bioconductor stuff back in the early part of my career, I got to know s 4 almost unwillingly well, because of that. But it never felt natural to me. And again, that's just my opinion. There are others that use us for a great success. More power to you. S 7 is trying to be kind of in between that of and sorts, giving some of the simplicity of the syntax of s 3 with some of the guardrails and, you know, safety net and more, you know, formal definitions that s 4 has.

So in this update, what's new here, there's a few, you know, minor, I would say, bug fixes, but also building blocks for new bigger features in place. Some of which include being able to support lazy property defaults, which they're saying makes the actual setup of a class much more flexible. One other idea that caught one other item that caught my eye was that they made an enhanced speed improvements for when you set and get properties using the at sign or the at sign with the assignment operator. Apparently, there were some bottlenecks with that in previous version that they've, fixed this now.

They've also expanded the compatibility with additional s three classes to help bring that transition a little more optimal for those coming from the s three side of things. And then also be able to have be able to convert a class into a subclass with the new, convert function or a modified version of the convert function. Lots more that are in the release notes, but the post also talks about how do you actually use this thing. So there's a great great kind of, example where they use this new class they call it range to help look at kind of the range between numbers, I believe.

And you can kinda see how the class methods, the class properties, the generics are defined with this. And you'll see there is a lot of shared syntax or paradigms of shared syntax of s 3, but yet you're able to define things more formally with the s 4 kind of language inside as well. They realize there are so some limitations here. It's not quite production ready, I would say, for getting into the actual R language itself, which is the end goal here. But they know that they are actively working on it. But like I said, there is a huge goal here for this.

Not just to be a standalone package for the foreseeable future, but to actually get into base r. That's as opposed to things like r 6, the object oriented class used by Shiny and many other packages, that's always gonna kinda stay as is because that's a very at rapidly evolving class system often with its own needs compared to what s 3, s 4, and now s 7 are bringing. So I'll be obviously watching this space quite closely. I have not used s 7 yet, but I know some packages are starting to use it now. So I'll be very curious kind of what the what the developer, you know, shared learning is as authors start to use this more formally. So great to see updates in this space, and I guess we'll stay tuned to see what else is out there. Yeah. It's interesting, Eric. You know, you know, a lot to

[00:41:07] Mike Thomas:

digest here. You know, I think that the idea, as noted in the blog post is that hopefully s seven eventually becomes part of BaseR. It is going to be, you know, an additional learning curve for some folks. Although, hopefully, if you've been doing some object oriented programming in s 3 and or s 4, some of the syntax and the concepts will be fairly familiar and fairly easy to migrate for you. This looks like a project that has now fallen under the R Consortium, which is cool. You can check out the GitHub there to take a look at the project itself.

And, there's 2 limitations that they want to point out. The first is that s seven objects can be serialized, with Save RDS but the way that it's currently authored, saves the entire class specification with each object and that may change in the future. And then, the second is that support for implicit s three classes of array or matrix, is still in development. So some things to watch out for for the the hardcore object oriented program, programming developers out there. But I'm excited to see version 0.2.0 drop, and this looks definitely a little more digestible to me than s 4.

So I'm excited to, learn a little bit more about s 7 and hopefully incorporate it into our projects going forward.

[00:42:36] Eric Nantz:

Yeah. And I know I've seen in the community our our friend John Harman's put s 7 for the paces on some of his exploration efforts, and I see some others. You know, I'm sure they're learning on it. And I'm doing a quick check on the CRAN page. There are, as of now, 4 packages that are importing s 7. So there are there are a few to choose from, and they, admittedly, there is one called Monad. Remember Monad's from our shiny Oh, gosh. Learnings from Joe Chang. So I want to check that one out. But but, yeah, nonetheless, it does seem to be moving along, and I'll be watching the space quite closely and seeing where that fits in my adventures both in Shiny and also in generic package development.

But you could have lots of adventures in the r side of things when data science, and this rest of the r weekly issue would give you, I'm sure, lots of directions to go down different adventures, different rabbit holes, and really ways to supercharge your data science exploits. And we'll take a couple minutes to talk about our additional finds here. And, fellow curator, good friend of ours, Jonathan Carroll, has released on crayon a very cool r package that he's had in development for a bit of time called Nifty.

And what Nifty is is, our wrapper around a completely open source, self hostable notification service called nifty that you could spin up on, say, a cloud VPS or on your internal network and be able to, from r, push out a push notification using this package to go to wherever it needs to go. So let's imagine you're running that big old simulation. You're away from your computer while you let the HPC, Matt, do its magic. What if you want that notification on your mobile device to say it's done? Right? Nifty might be a way to do that. So I may have to take a look at that. I've seen other packages in this space called pushbullet, I believe, from Dirk Eddybuttel. It's doing a similar thing with the Pushbullet service.

But it's great to see our use in in novel ways too. So congrats to Jonathan for getting nifty on the crayon. And I saw on Mastodon, there was already a few very excited users for what they can do with that package. So that'll be in my, things to work look at during my holiday break.

[00:44:59] Mike Thomas:

And, Mike, what did you find? Well, one thing I wanna shout out is a package called Survey Down. I'm not sure if we've talked about this on the highlights before or not. It had a new release out there, and it's a pretty cool open source way for making surveys with R, quarto, Shiny, and a technology called Supabase, which looks like, how the back end data is stored. It's some type of, database. And I think it's I have a lot of use cases potentially where I need to make small forms, things like that, surveys, and I always sort of tend to wanna go overkill and develop a Shiny app instead of using something off the shelf like, I don't know, a Microsoft product or SurveyMonkey or or things like that, just because I'm I like doing those things to myself. Right? Make making my life more difficult.

So I would check out this package if you, like me, have a need to create a form or a survey and wanna do it open source and leverage some leg work that's that's already been done by a great team.

[00:46:03] Eric Nantz:

Yeah. I I've seen this come through, but I haven't dived into it much. But boy oh boy, that would be terrific for, you know, wherever you have, like, surveys you wanna conduct in your organization or some other robust data collection, great to take advantage of the R ecosystem with that space. And, before we get gentle comments, it turns out I pronounced that package completely wrong. I went to the GitHub page. It's actually pronounced notify. It's n t f y. So sorry, John. I I I should have looked at that before I started saying that. Pronouncing things is hard, so correction noted. Yes. It is.

Yep. But, luckily, you don't need to correct anything else with our weekly itself. We we strive on giving you authentic content. You don't have to worry about some, AI generated bots putting that populated feed into you. This is all human generated. We definitely wanna take advantage of automation and certain pieces of it, but, no, that's the value of this project. Completely human element and, you know, written by the community for all of you in the community. And since it is a community effort, we rely on your help. One of the best ways to help is to share those great resources you find you found online. Whether it's a new package, a new blog post, new tutorial, we're all game for all of it. You can send us a poll request, all written in markdown using that top right banner link in the corner of our weekly dotorg. You know, it can be taken directly to the GitHub poll request template.

We show you kind of the things we're expecting. It's very minimal, but we always value your contributions there. We also value hearing from you in the audience as well. We got a little contact page in the episode show notes. We love hearing from you and what you've learned from our weekly. You can also find us on the social medias as well. Apparently, we're gonna we have a new source for Mike that he'll talk about shortly. But for me, it is still the, tried and true, Mastodon account with at our podcast at podcast index dot social as well as LinkedIn.

Search my name, you'll find me there. And I'll be at very much minimal now, the Weapon X thing at the r cast. But maybe I needed to pay attention to another one, Mike. What about you?

[00:48:13] Mike Thomas:

Yeah. I guess the latest for me, which I hope to check a little bit more often than I did on Mastodon, it it feels, like like I'm pretty excited about it. It's gonna be bluesky. You can find me at mikedashthomas.bsky.social. Otherwise, you can, check me out on LinkedIn if you search, Catchbrook Analytics, ketchb r o o k. You can figure out what I'm up to.

[00:48:42] Eric Nantz:

Looks like I need to update my markdown tempo in the show notes, buddy. Sorry to do that to you. No. No. That's easy. That's easy. All marked out all the time for me. So all all good here. Well, with that, we will put a bow on this episode of our weekly highlights, and I admit I was remarking to Mike before the show. We're at a 185 now. That means we're running close to that 200 mark eventually, I should say. And, shout out to our good friends, Ellis Hughes and Patrick Ward. They're on the similar journey. It looks like they're at a 180 some episodes of IDX. So, how about a friendly wager who gets there first? Hashtag just saying.



[00:49:19] Mike Thomas:

I don't know if I wanna make that bet.

[00:49:21] Eric Nantz:

No. I don't either. Holidays coming up. Holidays are coming up, so that'll put a wrench in things. But, nonetheless, we hope you enjoyed this episode of our weekly, and we will be back with another episode. Maybe next week, maybe not. We'll see, soon. Don't know how to close those out. Alright. We're done."
"34","issue_2024_w_45_highlights",2024-11-06,43M 4S,"Eric's flying solo this week, but the show goes on! The eagerly-anticipated recordings of the 2024 Posit conference are now available and Eric shares a few of his favorite gems, plus the Quarto publishing system takes center stage with how GitHub actions brings automation to report generation, and a terrific batch of answers to the recent R/Pharma…","[00:00:04] Eric Nantz:

Hello, friends. We are back of episode 184 of the R Weekly Highlights podcast. This is the usually weekly podcast, where we talk about the great resources that are shared in the highlights section and elsewhere on this week's our weekly issue. My name is Eric Nantz, and, yes, we were off last week because yours truly was knee deep helping with a lot behind the scenes as well as in front of the camera, so to speak, with the virtual Our Pharma conference, which was, in my humble estimation, a smashing success.

I think we had over 25 100 registrants at one point, tuning in during the conference days as well as some really awesome work shops. And you'll be hearing more about those workshops in a little bit in our last highlight, but nonetheless, that's why we were off last week. But I am here this week. Unfortunately, I'm by myself, so to speak, because my, truly awesome cohost, Mike Thomas, is knee deep in his day job projects, trying to get things done, and then he look for he looks forward to being back next week with all of you. Nonetheless, I'll hold the fort down for this week, and I got some fun things to talk about and share with our highlights here, which this week's issue has been curated by Batool Almruzak. And as always, she had tremendous help from our fellow Arruky team members and contributors like all of you around the world with your poll request and other suggestions.

Speaking of conferences, I'm happy to announce in our first highlight here that after a couple months of in the editing can, so to speak, the recordings from the 2024 Posit Conference have finally landed on Posit's YouTube channel. We knew these were coming soon, but we weren't quite sure when. But this is a terrific way to catch up on what you may have missed if you were not able to attend the conference itself back in August. And if you wanna hear more of my take on the event itself, especially from the in person experience, I invite you to check out our back catalog here where I talked at length about this in episode 174 of our weekly highlights. I'll have that linked in the show notes if you want to check that out. But, nonetheless, another great benefit of the recordings, even for someone like me who was able to attend, is that it is a multitrack conference. Right? And you can't possibly see all the talks that you want because inevitably there will be some that overlap, And especially in the case of when I gave my talk on the power of WebAssembly for the future of Shiny and clinical submissions, which, by the way, I will have re directly linked in the show notes. It's great to see the recording back, albeit I haven't watched it yet because even though I had a podcast and I've been doing podcasting for over, what, 10 years or whatever, it's hard to listen to watch myself on video, but I will at some point.

Nonetheless, when I was giving that presentation across the hallway, I believe one of the other rooms, was Pazit's kind of team presentation on the new Positron IDE. So that's a great one I'm gonna be catching up with to see kind of the genesis of that, the comparisons with your studio IDE, and it was kind of the, coming out party, if you will, Even though positron is still not, you know, quote unquote in production yet, it was, posit's first crack at really sharing their story behind the development of positron, and I'll be watching that space quite closely.

And there are a lot of, and I do mean a lot of, other terrific talks. And I dare say there's something for everyone, whether you're knee deep in industry, health care. Certainly, shout out to my friends in life sciences. We had a lot of great talks on the life sciences tracks. Also, utilizing automated reporting and things in quartile, which you'll be hearing about throughout this, segment and the rest of the episode. And, yeah, for the shiny enthusiasts, there's a lot here too. I was watching a little bit, this weekend when I saw the recordings are up.

I rewatched a talk from, the designer, so to speak, of the shiny UX side of things, Greg Swinehart. He gave a terrific talk on the recent advancements in the user interface components of Shiny both for the r side and the Python side and lots of great resources that he shared. And he he always has a unique style to his talks. I I definitely resonate with it. And you would never know the importance of a HEX logo until you really watch his talk and how it kicked off a lot of their, design efforts for the shiny user interface function, especially with bslib.

My goodness. Have you seen the hex sticker for bslib? That thing is ace, and I can't wait to get a hard copy of that. Nonetheless, another great talk going back to the Quarto side of things that I wasn't able to see in person, was by Andrew Bray, where he talked about the new close read quarto extension, how the development journey of that began, and it was a close collaboration with another developer named Jeremy. And, also, it really brings to light a way that you can get that scrolly towing look that you might see on, say, you know, some data readout or data posts from, like, the New York Times or other groups that kinda use that technique.

Well, if you're writing a portal doc, this close reads extension is definitely something worth your time to check out. And speaking of close read, that definitely relates to a little, ad hoc, addition here. I just learned a few days ago that posit is now running a close read contest to see what you can build for an engaging either data storytelling, an engaging tutorial, but really seeing what you can do to push close read to new heights. So I have a link to that, blog post from Pazit in the show notes as well. I dare say I've I've had attempts at doing a scrolly telling kind of presentation before.

Ironically, at a very earlier version of what was then the art studio conference when I had a poster session. And, of course, a poster session in these days means like an electronic display that you're you're standing next to and and kind of walking through with people as they walk by and answer questions. But my my poster session back then was about kind of my take on the shiny community and the awesome advancements and my hope for a future shiny verse. I recall using a package from John Coon, good friend of mine, that kind of gave a somewhat scrolly telling look to a web based presentation.

Albeit there were some quirks and no fault of his own. It was just the the HTML, package he was wrapping under the hood. But I do think, closer to something I really wanna pay attention to, both for some day job tutorials or readouts, but also for some fun narratives too. So I might have to throw my hat in that contest. I don't know. I'm just saying. And there is a lot and a lot more resources in in the recordings that you'll see, on the blog post from posit. Also, speaking of resources, posit also at every positconf, you also have terrific workshops.

And, unfortunately, they're not recorded. However, every workshop has made their materials online, and I have been referring back to the workshop that I was attending in person, databases with R and DuckDB by Kiro Muir. It was a fantastic workshop, and I'm all in on the DuckDB craze right now, which, again, speaking of the recordings, you can hear the keynote from the author of DuckDV, in the recording of the POSITIVE Talks as well. Another great great, development story of DuckDV and where we think the benefits are, and I dare say that we all can benefit from it in my, usage of it thus far.

So, again, lots more to check out. Obviously, I can't do everything justice in this segment here, but I'm really intrigued by catching up on what I wasn't able to attend in person and lots of great ideas that I think you'll generate in your daily work or your fun adventures with R and data science. We're gonna switch gears here to a more automation play story, if you will. I'll be at a mechanism that I think many, many in both the open source community as well as in their particular industry day jobs are leveraging, especially in the piece of automation and making sure that we can, you know, release the most robust code possible, say, for developing packages or if you wanna take away the manual steps of compiling things ourselves when we can let the magical machines in the cloud do it for us, and that is exposed via GitHub actions.

And if you are new to GitHub actions and you just want a quick take on what it's actually doing and getting a really, you know, fit for purpose tutorial that you can use today to kinda get your feet wet a little bit and then give yourself the appetite to dive into further, then this next highlight here is just for you. And is author by friend of the show, Albert Rapp. He is back again with his, 3 minute Wednesday segment where he talks about getting up to speed with quarto with GitHub actions for compiling a quartal document.

And this is not gonna get in so much the theory behind GitHub actions. You're not really meant to you're not expected to understand, not that you even have to, kinda what is happening behind the scenes of actions, but this is about how would you set up a report that you could automatically regenerate whenever you have a a change in the repository where this, report is hosted and to be able to automate this more in the future. So the post starts off with creating a new project, in this case, in the RStudio IDE, and he is careful to enable 2 options that you'll need for this is because this is relying on GitHub acts after all. You're gonna need to get repository locally for it, and he's also checking r env as well.

R env is, of course, a, package in in the r community offered by Kevin Ushay, a a posit, to help you manage the dependencies for your project via its r packages, but in a self contained way. Usually, r m works really well. I will admit though, If I was recording this yesterday, I may not have been the biggest fan of it because I had a almost knockout drag out fight trying to get my app in production with some dependency, h e double l, that I had to deal with. Somewhat self inflicted, but, nonetheless, sometimes RM can be slightly cryptic with its messaging.

Anyway, things happen just like anything in life. Nonetheless, that's gonna be important for the rest of this tutorial when we get the GitHub action actually created. So the report itself that he's demonstrating here is nothing radical. It's simply a HTML based report that's gonna say that this report was rendered at a time, but this time is printed and executed via a code chunk in quarto, which would be very familiar to anybody that's used quarto or markdown before, just a typical code chunk. With that, you may notice that if you may have initialized r m, and then when you hit that render report button and pause it, it's gonna complain that there are some packages missing. So that's where you do need to install in your r mv library the r packages needed for r execution, which is which are, of course, our knitter and r markdown. So once you once you do that with rmd, then you try to render it again, then your local version of the report will compile correctly. And then you can see that, depending on when you ran it, that current day time printed right inside.

Great. You got yourself a report. Now let's imagine instead of just using the typical HTML format for the report output, you would like to render this as an actual document in markdown format going from quartal markdown to markdown, but in a way that GitHub especially can render that in a nice way. That's using what's called GitHub flavored markdown. And Quarto itself is a command line utility as well as integrated with various IDEs. So Albert switches and pivots to a new way of rendering the quartal doc instead of through the click button.

He now shows you how to use the quartal render function to render that, and then there's a parameter tac tac 2 to you to put in GFM, forget a flavor mock markdown. And then he's changing the name of the output file to reme.md. So now you've got a file that can be rendered in a special syntax or special file name. So when you go to the GitHub repository for the project, that readme is gonna be what's displayed automatically under the code, file listing there. Great. Now we got that working. You can push out on GitHub, and you could just simply run that report occasionally, manually compile it, manually push it up.

But, no, that's not why we're here. We're gonna learn about GitHub actions. How does this work? So this is interesting because there is a package, of course, called use this that will let you define a GitHub action right away based on a template of current of more you just of, like, workflows that are pretty typical for an R, you know, developer, whether it's package development or R markdown compilation or whatnot. Albert is gonna show you how to build this from the ground up. And I do think this is important because there are times when you're new to a framework which is an abstraction that, in fact, GitHub actions are really an abstraction on top of building a container with various bells and whistles to do something.

That's really what GitHub, actions are under the hood, and the way you interface of it is constructing a special YAML file that's gonna define your workflow. So Albert leads us through with what do we need in our local repository to make this happen. That is you need a dot GitHub folder and then you need a subdirectory in that called workflows, and this is specific to GitHub here. And then once you have that, then you're gonna create a YAML file inside that workflows file. You can name this anything you want, but in in the end, it's got to be a YAML file, and it's gonna have specific syntax that he's gonna walk you through in the rest of this post. Now it'd be pretty boring for me to read all the bits verbatim here, but I'm gonna highlight at a high level the really important parts to make sure that you're setting yourself up for success the right way.

1st of which is how do you define when the action is going to be run. In this case, it's gonna be run on every push to the main or master branch of the repository, depending on what you call that that that, quote unquote main branch, that's in a certain directive at the beginning under the the narrative where you name it. There's this on declarative where you define, okay, on what operations will this operate? In this case the push operation from there below that then you have the jobs the declaration and this is where you could have 1 or more jobs which you can think of are a collection of steps to accomplish a task So in this case, the task that he's gonna call it is render, and each job needs an environment defined for what you're gonna run this on.

Typically speaking, you want to stick with a Linux based environment, especially if it's not like a, a situation where you have to check all multiple OSes, in this case, combiner report. Ubuntu dash latest will be your friend for this because you're not really caring about the version of it. You just want something that can quickly get quartile up and running and run this report and be on with your way. So that's in the runs on declarative. And then if you want this action to be able to write or commit things on your behalf, you'll wanna make sure to give it the right permissions, and that's in the permissions declarative where you have to explicitly tell it, I want you to write to this repo. And that's another declarative here.

And then the post talks about the different steps. And at a high level, what they are, our first getting quartile installed itself, which is done via another GitHub action. So that's another thing to keep in mind. Just like in functions in r, you can run other functions inside of them. You can run other GitHub actions as steps inside your overall GitHub action. And so the Coral team has generally set up an action for getting Quartle installed, so you really just have to declare it and then define which version of Quartle you wanna install, which in this case is the latest version.

And in this case, Albert is actually being very explicit with this particular step in the pipeline for installing the R dependencies where he is simply calling arbitrary code via a bash line calling r script to install packages that he needs to first get r m up and running and then using r m itself to restore the library. Now to be transparent, there is an action that'll let you do this as well or a couple of actions to do this, but it's good to see kind of how you can do this in your own way when you have to do things more custom. I'll get to that in a little bit.

But assuming you got the dependencies up and running, now the next step is actually rendering the portal document, and that is simply in the run declarative just like how Albert used the r script in that run declarative to install rm, but then run rm restore. You can use this run declarative to run that same chordal render the chordal render, function or I should say command line call in the exact same way he did earlier in the post. So nothing changes. It's as if you're typing that in the command line. You're just doing it in the GitHub action.

And then this part will look a little odd at first if you're not used to it, but then there is a step about, okay, the readme has been updated in the action. I need to push this up to the repository so that it can actually render that finished product. And that's where you can run, again, via the run declarative, the various git commands to tell git who you are. In this case, you're gonna actually define it as a GitHub action bot. You can put anything you want there. And then literally as if you're in git command line mode, adding the readme, committing of a commit message, and then pushing it up. In order to do that, though, the GitHub, action needs to be able to do this on your behalf using your repository secret token. Otherwise, it's gonna complain that it's not authorized. They do it. So every action step lets you have an ENV or an environment declaration where he's able to say the GitHub token, but not put in the token like string verbatim to use, like, this glue like syntax with the curly brackets to inject that variable from the secrets kind of store, if you will.

It there is every repository in GitHub action will have the secrets thing available where you could put almost any environment variable you want that you define ahead of time, but the GitHub token one is there for you free of charge. So that explains that stuff. And then lastly, you push this YAML file to the repository. And if all goes to plan, your report will render automatically via GitHub actions. I say if it goes according to plan because I have never once, in all the times I've used GitHub actions, gotten it right the first time. There is always something that happens whether I mistype a package name in my dependency installation or I'm doing something really crazy with bash scripting and I have no idea how to debug it, and then I get that infamous red x in the actions output in the in the GitHub repo. I've literally went through this yesterday. I was banging my head against the desk almost on this one.

So budget a bit of time. You're gonna need it. I I I wish I could sugarcoat it, but I can't. But once you get up and running with it and you get it working, give yourself a pat on the back because that's a major achievement. It really is. So this is scratching the surface so I can do GitHub actions. I am even if I joke about the the kind of debugging process, when it works, all goodness, does it save a lot of time? To illustrate that, I'll invite you to check the show notes where I link into the show notes of the repository that we've built for this, Shiny app that we're using as a template for a web assembly version of a Shiny app going into a clinical submission.

I use GitHub actions quite a bit, for this this workflow, and the ways I use it are pretty pretty novel to me anyway because I never done anything this in-depth before. I have 3 actions here. One of which is to indeed render a quarto based document in multiple formats, both a PDF format and an HTML format for this reviewer guide. And for the HTML format, I wanna publish that actually to, s 3 bucket so that I can render this as a viewable link in the public domain for our reviewers in case they wanna see the latest and greatest draft of it without having to download it themselves.

So that was a clever thing I was able to to hook in there and be able to render 2 formats. There's a lot of, you know, interesting points on that you can check out on the repo. The other action was to actually publish the WebAssembly application compiled and then publish it to Netlify. Before I knew about GitHub Pages being a a first class citizen for WebAssembly apps. I did Netlify because that's all I knew back then, so there's an action that helps with that. And also to publish a more, standard bundle of this whole project that's gonna be used in what we call a transfer to the regulators directly. That's the 3rd action.

And in each in each of these cases, I'm using bash scripts that are sourced in the action itself via a scripts folder. And I won't pretend I'm the best bash script there out there, but that's another handy thing. If you found yourself adding a whole bunch of commands in that run declarative, you could outsource that to a bash script and then be able to run that on the fly. And so there's some interesting learnings from that as well. So I have done a lot with GitHub actions. I won't pretend that I'm an expert at all of it, but I do admit they have helped my workflows immensely.

And, yes, there are versions of this available on other platforms as well. GitLab in particular has their own take on it. We're gonna what they call GitLab runners. Very similar YAML type syntax. There'll be some differences here and there. I believe in codeberg does this as well. So it's not the automation play, even though GitHub gets the most of the mind share, it's not just strictly related to them. There are many other ways you could implement this as well. So wonderful post by Albert. He goes, gets you up and running quickly.

And, yes, you'll wanna check out the community of resources for the GitHub actions that deposit team maintains that are a many there are many, parts of their workflow that you can get from the use this package. There's even GitHub actions for the shiny test 2 pack. It was to help get shiny, the test of shiny app with shiny test 2 and a GitHub action. There's lots of things you can do here, and I've already blabbered enough about it, but definitely check out the resources and Albert does a terrific job getting you up and running quickly. You have a very relatable issue.

We love ourselves continuity on this very show even, and we've been talking about that GitHub action that would render a quarto document. Well, quarto itself, there are so many things you can do with it, and I do mean many. And one of the, you know, the features that it carried forward from what you might call the previous generation of quartile, which has been our, you know, our markdown, is the idea of using parameters inside your reports so that instead of hard coding everything in the body of the report itself and then having to, like, you know, find, modify, replace when you want to change, like, a parameter value or you want to change, like, a dataset name or a dataset variable, you can use parameters in your Chordal report so that you could define those ahead of time, kind of like function arguments, and render a document dynamically injecting those values into the body of the report. And our last highlight is actually a wrap up kind of follow-up q and a portion from Nicola Rennie, who was terrific and once again being very generous of her time for Doctor Pharma conference where she led a workshop on creating parameterized reports with with quarto and it was a spectacular workshop. We have linked in the show notes the resources from this workshop with the slides as well. So the recordings should be out in a couple months, so you'll be able to watch a recording of it.

Yours truly is gonna be helping with editing on that, and I can't wait to watch it because I'm I'm gonna learn something new, I'm sure. But Nicole is, blog post here is getting into some of the questions that they didn't have time to address in that, 2 to 3 hour workshop. And, well, I'm gonna pick out a few that were nuggets to me, especially in the intersection of what you can do with r itself in the in the compilation of these parameter reports and also with quartile itself. There were some great questions about well, when you have a function, is it safe to add an argument for, say, the data frame itself? Because she's using, I believe, the Gapminder dataset to illustrate the parameterized reports, and she kind of shows the best of both worlds where maybe in a in a first version of the function, you're assuming that the gap miner data is the one loaded, and you're just gonna let them choose put in a parameter for, like, the continent to summarize in the filter statement.

But you could still have that continent as the as the first parameter, but then have a default argument for data that just happens to be the Gapminder. And in that way, if for some reason you wanna change the name of the data frame, you can still do that and be able to leverage all the benefits of the quartile, you know, parameters and everything like that with that data argument. Speaking of which, in order to evaluate that as an object, if you have, like, the name of an R object as one of your parameters, you can use the get function in R to basically translate that string of that object name into the object itself and then do whatever you need to do for further processing.

She also mentions there was questions about could we could we use, the parameters to generate dynamic tables instead of just plots. And absolutely, yes. Right? I mean, you could easily create a table with another handy function to be able to have, say, a a reactable type structure just as much as a as a plot. It all depends on you and what you wanna what you wanna define with it. And then there were also questions about her use of the walk function and the map function from the per package as part of this iteration of creating these multiple reports based on different configurations of parameters.

The walk function is really used for it's not so much you care about the output in R itself. You care about what it's doing as a side effect, like creating files, creating images, creating or doing some kind of uploading of a file or whatever. You don't really care about the object coming back from it. It could be invisible for you all you care. But if you have iteration where you wanna do something with that result, map is the way to go. So there's there's a nuance there, but once you get the hang of purr, it'll it'll hopefully be easy to grasp once you have that.

There are some fundamental questions in that. What is the biggest difference doing quarto and r markdown in this case? Well, again, r markdown is great, you know, expect I mean, look, I've built so many things of r markdown. Right? You don't have to switch a quartile if you don't need to. I mean, quartile is gonna, you know, get probably some more utilities added to it. There is a lot of developer resources behind it now. And with its cross language capability, we're seeing a lot of data science teams really embracing that. But, hey, r markdown is stable.

R markdown is very dependable in the r ecosystem. You're not compelled to switch if you don't have a need to, so don't feel like just because you're seeing all this material that you have to go away from r markdown. I mean, it's still very much a fundamental pillar of the r community in my humble opinion. Another nugget that I didn't know about in respect to let's say you have a lot of R code in this report and you want to just source the or execute that R script itself in your quota report. I didn't realize about the, parameter called file where you give it the path to that particular script, and it will basically, source that into your your execution.

And that is pretty handy. That that means you could, you know, do a good job of modularizing your code structure instead of having everything in one big, you know, setup chunk if you will. You could, you know, export that into different scripts and then use them use them as you need to throughout your report. And there are also great great questions about the concept of styling and formatting, such as do those fancy call out blocks that you get in quartile that look great in HTML, do they also work in PDF?

Well, yes, they can. You just may not be able to do the collapsing stuff because it's a static representation. But quartile is very careful to make sure that you can get most of the features in each output format. And in the case of interactivity, at least getting static versions of those. And I was able to, you know, learn this earlier this year and last year with that reviewer document that I was making as part of that submissions pilot project, I could use the call out blocks and it looks really darn good in the PDF. Like, I I'm pretty happy with it.

So much so that now we have another work stream about the spin up about using quarto and more of these, submission documents. So I'm really excited for that. But getting those nice enhancements in the style, and, of course, we're keeping an eye on typest as well, it's a great time for those that still have to live in the world of static documents. I think, you know, Chordle is still gonna be very helpful for you. And, also, there are some real nuggets here about how you can generate these multiple reports from the command line. One thing that took me some getting used to, and, again, I haven't watched the workshop yet, is that you can have a YAML file with these params defined, like, say, a default value of them. And that can be fed into the command line version of quartile render so that you can just feed in that YAML file and then you'll be able to render that document on the fly. But if you want to do this within R, you've got to do a little trick here that I didn't really realize is that if you want if you have to get these parameters as kind of these key value name pairs, and if that's where in the quartal render function that the quartal package itself exposes the r package, I should say.

You have to give it the name, the list rather than just the YAML file name. And she has this great tip of using the yaml.loadfile function from the YAML package. And that way, you can just feed that that evaluation of that function in the execute params parameter itself instead of you having to manually do, like, the list, if you will, yourself of those parameters. That is awesome. Man. I I'm so glad I learned that tip because I was dealing with that just recently with a project at work and I kinda kinda gave up on doing it from the our side of things, but now we can definitely do that. So that just is my quick take on it. There are lots of terrific resources I mentioned that Nicola Nicola has has shared. At the end of the post, some more workshop materials from others, some more blog posts, and like I said, the recording should be up in a month or 2 depending on how fast I can get the editing chops on. And I can't wait to to watch this one again because I literally use quartal parameterized reports for a pretty fun daytime project where I was able to escape the confines of a PowerPoint and have a dynamically rendered quartal dashboard, but tailored for each project using parameters. It was awesome once I got it working, and I was like, I I've gotta be able to push this more into the mainstream of how we communicate these results. So I'm I'm definitely excited to see where this takes us and another wonderful workshop in the space of parameterized reporting.

This is a great companion to another resource we shared in previous episodes from JD Ryan on her workshop on parameterized reports or quartile. So you got 2 top notch instructors from the art community giving you level up knowledge on quartile parameterized reports. What what a time to be alive, folks. And there is a lot more I could talk about here, but, you know, this episode is getting long already, so So I'm gonna close out the episode here of an additional find that caught my attention when I was perusing this issue here.

And in my additional find this week, I'm gonna, you know, put a great spotlight on another great post from Steven Sanderson. He has been, you know, a machine almost or some of his great, tutorials on the fundamentals of R and data science. And he has this great post, this great guide, if you will, on how you can create lists in R. List is one of the most important object types I have had in my, you know, last probably 5, 6 years of my day to day work because there are so many creative things you can do with it. A list for those uninitiated is simply a collection of other objects and r, but they don't all have to be the same.

Unlike a vector where all the say if you have a numeric vector expecting that they're all numeric, But if you have a string in there, it's gonna automatically convert it to string the moment it sees one string in there because they have to be the same type. You can't mix the number and string. So the list is a way around that sort of thing. And, plus, the list, you can create as much of a hierarchical structure as you like. And that can be really important, especially some of these more complicated data structures with, like, digital readouts, digital machines, and we call it digital biomarkers in our line of work. A lot of web data is coming as list type structure from JSON.

Being able to know the ways you can create lists, name them, and do various operations with them using either the built in apply family of functions or the per package. Lots of awesome things you can do with lists and they are again a huge huge part of my my daily workflow. Once you once you get a hang of them, there are so many things you can do with it, such as a package I've learned about from our pharma. I've heard in the grapevine, but I I saw a talk about it, about the cards package by Daniel Sjoberg and Becca Kraus, where you're creating basically a results type data frame of these different, like, statistics for a given variable or a set of variables.

But these statistics may be different. And some may be a numeric result, and some may actually be more of a character result, especially when you're dealing with model attributes or things like that. Their way around it to mix all these different types of results together is to use, wait for it, a list column inside your data frame. This is awesome because then you get to have a lot more control and flexibility in what you're doing with these, you might say, wrapper type data frames than either putting, like, entire data frames as, like, a cell value in a list column or another model fit object, which the broom package does that cards is kinda taking inspiration from.

So with the list object, you can do so much stuff. I highly recommend this post to get get up and running quickly, so it was great to great to see this featured in the issue. My goodness. Even flying solo here, I realized I've taken a lot of time already. So I'm gonna get you on your way here, but we have a few items to close out, of course, is the R weekly project is a community driven project. We do not have sponsors. We we do this all for you in the R community and the data science community. All we ask is for your help to make sure this is up and running quick, you know, sustainably, and that's via your contribution. So if you see wherever you authored it or someone else authored it, a great resource that should be featured in next week's issue, you just go to rweka.org.

That should be in your bookmarks. If it's not, I dare say, hot take. You should have rweekly in your bookmarks. Hit that little, ribbon at the top where it'll take you to our draft of the upcoming issue and a way to do a poll request right there in GitHub's web UI. You just need to do a little markdown, folks. I'll markdown all the time just like with quarto, and you can give us that great resource. And the the next issue's curator will be glad to review that and merge it in. So we're really, really eager to have your contributions on this very, very important project.

We also like to hear from you in terms of this show itself. We have a little contact page linked in the show notes. You can find this hand you a web form to fill out. You can also with a modern podcast app, I really like Podverse and Fountain these days, but many others in the ecosystem, you can send us a fun little boost along the way. They give us feedback directly without anybody in the middle, without any corporate overload trying to say, oh, nope. You can't say that. You can be as as unfiltered to us as you like.

Luckily, all the feedback we get is usually positive. But if I make any fumbles, I always like to hear about that too. And, also, you can hear you can get in touch with me on social media these days. I'm mostly on Mastodon with my atrpodcast@podcastindex.socialaccount. Also on LinkedIn, you can search my name and you'll find me there saying something usually or responding to other people. I am contemplating getting a blue sky account because I am seeing a lot of traction in the art community going to this. But honestly, Mastodon's been pretty nice to me. And I know there's some people wondering, oh, is this replacing Mastodon? No. I don't think so.

I think they both can exist, but I think Mastodon has been extremely helpful for me for both my R and data science, you know, friends getting keeping in touch with them and meeting new friends along the way, as well as my podcasting adventures. So Mastodon is not broken at all. In fact, I'm gonna keep going with that as long as I can. And, big shout out to Dan Wilson speaking of Mastodon. He's the one that maintains the r stats dot me server for Mastodon. That's been a great one to follow and he he's been, you know, doing a lot of work to keep that up and running. So, Dan, your your your efforts are not going unnoticed for sure. I I greatly appreciate what you do for us.

Nonetheless, that's gonna close-up shop here for this episode of our weekly highlights. We hope to have Mike back next week, so you don't have to hear me babble all the time. Nonetheless, I hope you have a wonderful week for wherever you are. Again, have fun with your journeys of R and data science, and we will be back with another episode of R Weekly Highlights next week."
"35","issue_2024_w_43_highlights",2024-10-23,49M 59S,"Bringing tidy principles to a fundamental visualization for gene expressions, being on your best ""behavior"" for organizing your tests, and how data.table stacks up to DuckDB and polars for reshaping your data layouts. Episode Links This week's curator: Jon Carroll - @jonocarroll@fosstodon.org (Mastodon) & @carroll_jono (X/Twitter) Exploring the…","[00:00:03] Eric Nantz:

Hello, friends. We're back with a 183 of the R Weekly Highlights podcast. This is your weekly podcast. We're gonna talk about the great resources that are shared every single week on this week's Our Weekly Issue. My name is Eric Nantz, and I'm delighted you're joining us from wherever you are around the world. Already near the end of October, it's hard to believe the time is flying by. The air is crisp in the mornings as I ride my bike to my kid at school. You can feel the the chill in the air, but, nonetheless, we're heating things up here in more ways than one with this episode. I can't do that alone, of course. My, next, generator of our heat, if you will, is right here next to me virtually, Mike Thomas. Mike, how are you doing today?



[00:00:52] Mike Thomas:

I'm doing pretty well, Eric. This is the first I guess I'm spoiling it a little bit, but this is the first our weekly I've seen in a couple weeks, we where we are not discussing AI in any of the three highlights. I don't know if that means that the AI buzz has has cooled off maybe.

[00:01:03] Eric Nantz:

Or they know that we need a break from it either way. Probably both. It could be both. It could be both. But, yes, it's good to have variety in life as we say in this very show, and we got a good variety of content to talk about with you today. And our weekly, if you're new to the project, this is a terrific resource. We aggregate all the awesome use cases of R across data science and industry, academia, and new packages and updated packages and great tutorials. And we got a mix of all of the above on this one.

And it has been curated this week by our good friend, Jonathan Carroll, who is also gonna be involved with helping chair for the first time ever the R pharma conference. It's not just having a, you know, US or western hemisphere based track. He is helping chair the APAC track for the Asia Pacific region. So we're very happy to, John, to have you involved with our team. He's already been hard at work preparing for that conference. But as always, with our weekly, he had tremendous help from our fellow, our weekly team members, and contributors like you all around the world of your poll request and other great suggestions.

And like I said, we're gonna heat things up initially on this very podcast, and we're gonna talk about a fundamental pillar of visualization across multiple disciplines, and that is the venerable heat map. And if you you know, as usual on a podcast, it's hard to describe everything in audio fashion. But a heat map, if you haven't seen that before, is a 2 dimensional, you might say, grid where each cell is kind of the, you know, the expression, if you will, of how large or small a quantity is. And we see this a lot in, for example, correlation analyses where you might look at all the pairwise correlations across a group of variables, and each of those combinations is a cell in the heat map with a higher correlation, which is, of course, between 01, might get either a brighter color or darker color depending on how you invert the palette and whatnot.

And there is another domain where heat maps are very much in play and very much a fundamental, you might say, pillar of data exploration, and that is the world of genomics and biomarker analyses. From my days many years ago dealing with the Bioconductor project, I create a lot of these heat maps often using the core set of Bioconductor packages that would have nice wrappers around the typical object types that you get in Bioconductor, which is typically the matrix. A matrix is a fundamental data input in many of the classical heat map functions.

Maybe you're getting data that already isn't quite in the Bioconductor like layout, but you want to take advantage of some of these great visualization techniques like heat maps. And that's where our first highlight comes into play. It is a blog post author by Thomas Sandman, who is a scientist at Denali Therapeutics, and I can tell who has vast experience in the world of genetic and biomarker and PKPD analyses because he is actually attempting in this blog post to recreate some very nice heat map visualizations from a recent manuscript looking at the that the effect of a specific mutation of a gene for understanding more of the pathology behind what turns out to be neurodegeneration, which many people are familiar with the Alzheimer's disease as one of the manifests of that.

But there has been research in many, and I do mean many decades, on trying to find the best genes and other biomarkers to target to try and hopefully minimize the impact of this, you know, debilitating disease and hopefully even cure it altogether. It's been a lot of research. There's been a lot of misses along the way, but there are some promising avenues in this manuscript of a few lipids and also other additional biomarkers that they did an experiment with mice to see what the gene expression would be in certain parts of their brains on this regulation of these of these genetic markers.

And the heat map is, like I said, a classic way to visualize the impact, in this case, of some linear models looking at the different, you know, characteristics of these lipids after this experiment. And so the post starts off with giving us, of course, a nice link to this manuscript, which we'll link to in the show notes as well. But this manuscript, if you do dig through it, has a little bit of a description on the methods used and some of the r packages that were used mostly from Bioconductor and, thankfully, example Excel files with the results of the stat analysis.

However, there was no article shared with this, which, again, unfortunately, is kind of common in these areas. Right? You may get snippets of this for reproducibility, but you don't quite get the full picture. So with that said, the the journey begins with trying to figure out, okay, how do we get this data ready to go for a statistical analysis or a visualization, I should say, with heat maps. The first step is to import this data from Excel, and there's some nice, I can tell this blog is written record, also got some nice call out blocks that you can expand to look at the different functions that Thomas has outlined here.

Really nice use of ReadExcel and TidyR to get the data into the right shape for the eventual, heat map visualizations. Also, there was a additional spreadsheet that the researchers shared with the statistical analysis, so he's got, CSV import of that particular file as well if you just want to get the finished product of that. And to make generating heat maps easier, Thomas is spotlighting the package tidy heatmap, which is a nice front end to another package called complex heatmap, which again would expect the data going into it to be in matrix form.

But tidy heat map lets you have a more, you know, tidy ish type data where you've got your rows as observations, columns as the variables. And he's got a nice way, once you do a little filtering and a little pre processing to match kind of the groupings that went into this original manuscript's heat map visualization. It literally is a function called heat map feeding in the data frame, which variable corresponds to your rows, which variable corresponds to your column, which in this case is the sample IDs of these different mice because there are different samples taken for each for each specimen.

And then what is the value going into it? In this case, it's called an abundance measure, which is the fold change log transform of the gene expression of these markers. And right away, you got a pretty nice looking heat map along the way that looks like straight out of ggplot2. I believe it's using that under the hood, but you can do a lot more to this. And that's what the rest of the post talks about. How do we go from that very quick starting point to a heat map that more matches what the manuscript outline? So there's a few steps here involved, one of which is to change the color palette a little bit.

And there is, a set of packages that are being used here. 1 is called circlewise, which I haven't seen before, which has its color ramp 2 function to basically say, okay. For this amount of breaks in your numeric axis, use these set of colors going from navy to firebrick with the different ranges, and then you can now feed that into that original heat map call. So now you've got a heat map that definitely looks a little more like you would see in many of these manuscripts already colorized differently. And then also there is additional ways that you can put grouping annotations on top of those columns, which in this case were depicting the sample IDs.

They have an inherent grouping between those, so this is going to let you do, like, a visual split, kind of like a little bar with an annotation in the middle over each of these groups. So you can quickly see then the different types of samples based on the different type of genetic marker it was targeting. So that's already very familiar in the ggplot2 side of things with fascinating. But that's not all. You can also add custom annotations as well where there is a handy function called annotation or annotation_tile, which now under those headings that you saw above each column, you can then do like a in essence a color like legend that depicts different groups within each of these overall groups. In this case, the batch, which is these samples usually goes in a different batch for the experiment, and then also the gender of these mice. So those can be neatly colored so your eyes can quickly see then on top of this one genetic marker what were the batch the batch ID in terms of color and then also the gender associated with that. So it's a really, really handy way to visualize that grouping in a categorical fashion but with colors instead.

Lastly, but certainly not least, we got some additional processing to do, and that's where we start to look at how do you annotate additional quantitative information underneath these these additional group grouping of colors based on the cell number that these markers are coming from. So that's another handy function waiting to happen with an additional use of annotation tile and annotation point. So, again, audio won't do this much justice, but underneath those color regions, he's got little dots that that depict the cell number with a little y axis right at the left. So already done about 3 or 4 different types of variables above the columns.

And then the last part is about how the rows are organized, and this is gonna take a little more dplyr data munging magic to make sure that the groups match kind of the grouping order based on a) the expression level and then also doing a more manual grouping fashion to match kind of different overall groups that we saw in the heat map earlier in the manuscript. So all the code is reproducible. Again, this heat map function has got a lot of parameters inside. But depending on how in-depth you want to make your customizations, there's typically a parameter for it. Like I mentioned, this column grouping, the way you can organize the different rows, the way you can put these annotations together.

I never knew how much power was exposed to us with these heatmap functionality. So next time I whether I do a biomarker analysis in the future or even go to my tried and true correlation analysis, I'm gonna have to give tidy heat map a try. This looks really fun.

[00:12:53] Mike Thomas:

Yeah. This is pretty incredible. If it is ggplot under the hood, it's insane, you know, how far we can push it to customize all sorts of stuff here. I I really like sort of the approach that has been taken to to tidy up, if you will, by Thomas, the complex heat map package. If you're somebody who's more familiar with the tidyverse, I think you're gonna find the API here to be a lot more friendly. I took a look at the complex heat map, our package, and it actually has some fantastic documentation. There's, like, a whole book on using the complex heat map package. But as you mentioned earlier, Eric, sort of the the root unit that you're gonna pass to this this heat map function that starts with a capital h, we can talk about that later, as opposed to a lowercase h, is a matrix as opposed to a data frame.

And I think, you know, this complex heatmap package tries to maybe reduce the number of parameters passed to the heat map function and abstract away, some of the heavy lifting for you. But I it seems to me when it goes about doing that, that it it makes probably a lot of assumptions off the bat about how it's going to, display that data, sort of what column should be named as, things like that, how the data should be structured. And I think you just have a little bit more control with the API, at least in terms of getting started for those who are coming from a more, you know, tidyverse, background with this, new package that that Thomas has put together. So we have some clients that do some, you know, bioinformatics, biomarker type of work, and the heat map is, like, the most important tool, data visualization tool, for them to be able to use. And oftentimes, they wanna push those heat maps, as far as they can go with multiple different legends, dendograms on the side. Right? We have, also on, you know, sort of the other vertical axis of the dendogram's on the left side of the heat map, on the right side, in this example here that Thomas has. We have, for every single sort of row on the heat map, a particular label.

And those can be very, very small, difficult to see if you're not providing arguments in your API to be able to adjust all these things to try to make these heat maps, which are very complex by nature, as digestible as possible to those who are going to be making the decisions based upon the data that they're seeing in in a way that they're interpreting interpreting the heat maps that you're putting together. So I I think it's incredibly powerful that we have sort of this much control over how to develop these heat maps. I really, you know, like the the syntax that, this API has in Thomas's new package. And I think folks who are looking to take their heat maps to the next step and certainly all the folks that we work with in the the biomarker and bioinformatics space, I'm going to pass this blog post along to them because I think this is going to to save them time and be able to help them accomplish all of the little nuances that they wanna introduce into their visualizations.



[00:16:06] Eric Nantz:

And I do stand corrected, my friend. Real time correction here. I did check the packages that it depends on. It actually does not depend on ggplot2. It's it's depending on the grid system, of course, which is a fundamental foundation of ggplot2. But interestingly enough, we'll have a link to the tidy heatmap, package down site in the show notes. They also integrate with Patchwork, which is what we've talked about previously to stitch multiple grid based plots together, which now it all comes together in real time, doesn't it? Those nice, you know, annotations that we talked about, you know, going between the column labels and the actual content of the heat map itself, that seems like patchwork under the hood if I had to guess with those custom visualization types above the above the main the main area. So really, really interesting use case. I had no idea the kind of directions that that could take, but I and the yeah. Definitely check out check out the heat the tiny heat map site. There's lots of great examples on top of what Thomas has done in this blog, which is, of course, focused on the more genetic, you know, biomarker suite of things, which I'm sure many of you will be very appreciative of. But, yeah, there's lots of great examples in the vignettes that I'm looking at already, in the overview. Lots lots of things to to to digest, if you will, in your heat map exploration.

So this is going to my massive set of visualization bookmarks for future reference. Mike, we always love our continuity on our weekly hallways podcast. And last week, we were talking about a novel way to help organize maybe a many, many sets of tests that have some relationships amongst each other with a nesting approach. And if you're not familiar with that, go listen back to our previous episode 182 where we had talked about Roman Paul's announced or blog post on organizing tests with a nesting approach.

There are always more than one way to do things in the r ecosystem, and our next highlight here comes us from Jacob Sobolowski, who is a r and shining developer at Appsilon, who if you haven't been following his blog, he has been all over the different ways of how he thinks about testing complex shiny applications, many thought provoking posts that he has on his blog. But in particular, this one here is talking about another approach to organize and really notes for future use, so to speak, on the different tests that you can organize that have some relationship with each other.

So to motivate this, he has a simple example, again very trivial but hopefully you can generalize it, of a simple set of expectations to test that the median function in base r is performing as expected. So in the test that call, which, again, test that is the package that's helping, you know, drive the engine behind many of the automated testing, paradigms in r that you see in package development, Shiny apps, and whatnot, It has a simple call to test underscore that, and the description reads, does median works correctly.

And so within that, there's a set of 6 expect equals in this case of the different, you know, numbers that he feeds in and does it get the actual number in the answer. Again, very very straightforward, but you are seeing that the median should, when you look at these functions, handle a few different use cases. Whereas, if it's only one element, should give you the same value back. Or if it's 2 elements, should be the average. If it's an odd number of elements, should be the middle number in sequential order. Blah blah. You you understand that.

Now imagine that had been a more complex function. Imagine that the expectations may not be as obvious at first glance when it's really trying to infer here. So his next, snippet of code is now going to write test that, but each of the tests is corresponding to those individual expectations. So like I just mentioned earlier, he's got, in this case, 4 different tests that are, you know, verifying the different behaviors based on either how many numbers are going in or, you know, in fact, that is basically it. And Revver, does the same thing for order and unordered elements.

Now that's, again, a perfectly valid approach. It is illustrating that test that when you put these descriptions inside, that test that function is named that for a reason. You kind of read it as test that and then something. However, in this case, it's like the median should return the same value if the vector only has one element. You you get the idea there. But here comes the kicker. In TestDaT, there are additional functions to organize your tests even further. And this is getting into a paradigm that admittedly is not quite comfortable to me just yet called behavior driven development or BDD syntax and test that for I'm not sure how many releases up to this point has spurred the use of a function called a set of functions called describe and it.

So in this last snippet of code, Jacob has an overall describe call that just simply says median. And then within it are a series of it calls and then the description in these it function calls is kind of the behavior itself that he's trying to test, where we have about the same value if it's only one element, the average of 2 values if it has an even number, the middle value in sequential order if it's an odd number of elements, and that it's the same value for ordered and unordered elements. I've seen this once before when I in Pasa Comps in recent years, I've had some interesting conversation with Martin Frigard, who has written a book called Shiny app as packages or something to that effect. But he has a specific chapter on behavior driven development in in light of specifications and requirements and how your test that test can kinda mirror that approach.

Admittedly, I have not had enough practice of describing it, no pun intended, in this workflow, but maybe it is quite helpful in your use case to have, again, another approach to how you organize related expectations into an overall testing block. So it is kind of a paradigm shift. Right? We could have either the nesting approach that we saw last week with the test that and multiple test that calls inside or the describe, and then these it functions are gonna have the expectations directly. Which is right for you?

I don't know. Mike, what's your take on this?

[00:23:34] Mike Thomas:

This is the 2nd week in a row, I think, maybe 2 out of 3 weeks where I have found things in the test that package that I had no idea. Literally no idea existed. And, Eric, as I am, I know you are, we're very organized people and probably fairly opinionated about how we organize our code as well. I am open to this. I think if you were looking for a hot take on why we shouldn't consider this and and why the old way is the best, I'm not sure you're gonna get it from me right now. And it might be still because I'm I'm reeling from, learning about this new describe it paradigm.

But just like I was sort of blown away by the organization in the in the blog post around, nesting your tests last week or the week before. I'm similarly kinda blown away by this additional functionality here where we can leverage this describe it paradigm. I'm actually even more sort of not necessarily blown away, but but more keen on the the final sentence of this blog post. It says, if you wanna push test readability even further, check out how we can use the arrange, act, assert pattern to achieve that. And if you take a look at that link it's another nice short blog post as well by by Jacob. And, I I really like sort of the concept around how he he specifies arranging, the inputs sort of acting and and evaluating those inputs, and then passing those evaluated values to a set of test assertions after the fact. And, again, in that blog post, he's usually I know this this one isn't part of the the rweekly, but it's it's a cousin. It's close enough.

Again, he's leveraging that describe it paradigm. I think it's sort of I don't know. The more I look at it, it looks like it's it's sort of, you know, 6 in 1, half dozen in the other. I'm not sure how much of a difference it it makes. I I think it's just a matter of of preference, you know, if somebody on my team decided that they wanted to use describe it versus, you know, the traditional test that, verb from the test that package. I don't think I would I would care too much either way because I think it's it's legible both ways. I think it's fairly similar, in approach. It's it's sort of just different verbiage, if you will.

But it's very interesting for me, you know, to to know that this exists, and see that potentially maybe there there might be a use case for me in the future to try to adopt them. I'm just gonna stick one toe in the water, I think, for now. You know, I was taking a look at the some of the most recent packages that Hadley Wickham has put together, Elmer being one of them. And I wanted to take a look at the unit tests in there to see if Hadley was leveraging, you know, the older type of framework that traditional test that framework or or leveraging this describe and it paradigm.

And it looks like Hadley is still utilizing the old ways, if you will. So not necessarily adopted the describe it functionality, but I believe he has or had at one point in time a large hand in, authoring and making these decisions around the the test that package. Don't quote me on that. So I imagine perhaps, he was involved in maybe a pull request or 2 that that included this describe it functionality. So I was interested to see, you know, if the if he was going to be one of the folks who had also adopted sort of this new paradigm for code organization purposes purposes because I know that Hadley preaches, you know, good software development practices and and trying to articulate your code as as best as possible.

So I guess at the end of the day here, interested to see that we have these two new functions that I did not know about in the test that package. Am I all in on it? No. But I'm not all out on it either.

[00:27:38] Eric Nantz:

And I've I'm giving this the old college try as they say with this, app at work that I've just put in the production now. I took this behavior driven approach, describe its syntax for the the test. I will admit it it felt more cumbersome than right initially, but I'm thinking the payoff is not so much for me personally in the short term. The payoff hopefully is if I get additional developers to help me with this app as a, quote, unquote, scales, buzzword for all you industry folks out there, where I might need some help to orient somebody relatively quickly over. It's a dedicated member of my team or if it's contracting resource and whatnot.

And so being able to read that more verbose, but yet under you know, might say legible, might say more digestible way of organizing the test, then, hopefully, it makes it easier for them to write tests in that similar framework. And then we all can kinda have a common, you might say, style that we can gravitate towards for that particular project. Not too dissimilar to the whole paradigm of a given project having the same code style for all of its, you know, coding files, which, again, I can speak from experience when teams don't adhere to that.

There'd be dragons when you do code reviews, and I won't say that for another time. That's my hot tech. But I I think this is I think for future proofing things, I could see this being quite valuable, especially in the other context that I was dealing with. Right? I tried to I tried to have an attempt back to Martin's book that I'm gonna put in the show notes here of really articulating those user stories if you wanna use the agile methodology into what I'm actually trying to accomplish in that test. So if there's a, quote, unquote, project manager that wants to see how I how I assess all those different user stories even though the basically, that project manager is me for this particular project. But let's say, hypothetically, it was a more larger scale project. It was a good practice to see how it goes. So, again, it didn't feel comfortable yet, but maybe the proof is a year from now. So ask me a year from now, Mike. We'll see if I take changes.



[00:29:52] Mike Thomas:

Put it on my calendar. Oh, I don't think so.

[00:30:09] Eric Nantz:

And over the course of the last, you know, might say year or or so, we have seen an influx of really top notch resources and discussions in the in the r community as a whole with respect to the data dot table package. It's been around for many years. But thanks to recent NSF grant, they have had some real robust efforts to get the word out and also putting in the different situations and different context so that users that are new to the package can really understand what are the benefits of data dot table in their daily workflows.

And so our last highlight is coming from the our data table community blog, which we've talked about in previous highlights. In this case, the post is coming from Toby Dylan Hocking, who is a statistical researcher and the director of the LASSO lab at the University of Sherbrooke, which I believe is in Canada. And he has a very comprehensive post here about the comparison of data dot table functions for reshaping your data as compared to 2 other additional packages in your ecosystem. We're gaining a lot of momentum lately, which are DuckDV and Polars.

So first, let's set the stage a little bit here because especially if you're very familiar with with database operations or if you're not so much like I was before, say, a few years ago, there is somewhat different terminology between what we use and, like, the tidyverse type of explanations of reshaping or in general data manipulation and the database equivalence of this. So when we talk about a long format, we are talking about not many columns but many rows in your dataset. And often, there are groups of these rows based on, say, a variable type or a feature or whatnot.

In SQL lingo, that's called unpivoted. That was new to me a while ago. Versus the wide format when you have one record, but then the columns are representing maybe different variables, and they are literally specific to that variable. You have, again, many columns, potentially fewer rows. That is called pivoted in the SQL lingo. Again, new to me, but that's where we're we're operating on where a lot of times, you would see in the community blog posts about benchmarks talking about the classical SQL like operations, like filtering, adding new variables, group summaries, or whatnot. But now we're literally turning it on its head, so to speak, by looking at the different ways we can change our data layout.

So the first part of the post is talking about just how data dot table accomplishes this, and this terminology is actually familiar from, I would say, previous language and dplyr for some of these other operations. But first, we will talk about going from a wide format to a long format and that unpivoting operation, if you will. And in data dot table, there's a function called melt for that, which if you're familiar with tidr back in the day, there was functions called melt and cast, I believe, which has now been changed to pivot longer and pivot wider, but that may be familiar for the tidr veterans out there.

What was interesting about data dot table's take on this on this melt function is, in this example based on the iris dataset, when you deter when you want to say what are your variable that are variable or variables determining kind of the grouping of these observations, you can actually have multiple columns generated at once. So in this example here, going from the the iris dataset, which has in wide format columns like sepal. Width, length sepal. Width, petal. Length, petal. Width, this melt function is taking a measure. Vars argument where you can feed in this measure function the names of 1 or more variables that are defined in these groupings and either a separator in the original name of the column or, down later in the example, regex to get to those column names.

That, I must say, is quite handy and eliminates a post processing step, which, as Toby talks about later on, you need additional post processing to accomplish that that same step in both polars and DuckDV. So there's already kind of a nice concise syntax, you might say, advantage at least at first glance with data dot table going from wide to long. And it's got some example visuals of what you might do with that data. But next, how do we accomplish this in Polars? And if you haven't heard of Polars before, this is a new binding to Rust for very efficient database data like operation and tidying operations.

And yes, they do support reshaping or re pivoting these data sets as well. And that is a function called unpivot going back to that SQL language. But in the example snippet, you will see that at that the first attempt you cannot do more than one variable for that grouping of the observation. So you would have to do that in post processing afterwards, which is not difficult. But, again, it's just an extra step. But, of course, it can be done. Lastly, for DuckDV via SQL queries, you can use the unpivot SQL keyword and then feeding into what are the variables that are going in, what are what's the name of the variable that's going to define that grouping of the long observations, what's the value the column name that you wanna use as, like, the numeric result.

Once again, in this case, DuckDb cannot do that that nice convenience of 2 variables at once. You have to do that in post processing as well. So there's example snippets and and Toby's code of the blog post here to talk about that additional post processing. That that is, again, all achievable. There is another example where you can reshape into multiple columns, and that might be helpful for different types of visualizations you want to conduct. And in this case, doing an additional column for or additional columns both for sepal and petal, but then there's a grouping variable called dim, which determines if it's the length or width. So data dot table has another way of doing that as well.

Again, the measure function comes into play there. You can see the example in the in the blog post. And then comes the comparisons for this operation. This is where my eyes open a little bit. So Toby does, a call to a an interesting func a package called a time. I'm not as familiar with this as I am with, like, the benchmark or a bench package, but he is taking a set of n values representing the number of rows in this, like, fictitious high dimensional Iris data set doing just some resampling and runs 3 different functions: 1 with the DuckDb on pivot, 2nd with the polars on unpivot, and lastly with data dot table and melt.

And across these cases, the ggplot that's put in the blog post clearly illustrates that the data dot table approach is definitely faster in this initial benchmark here than either of the other ones, especially as the end values get large. In particular, this was surprising to me, DuckDb had the worst performance by a pretty healthy margin when the end values got to, like, a 1000 rows and above. Now, again, maybe this isn't so surprising if DuckDb, of course, is based on the column type representation of data. Maybe it just isn't as optimized for these transposing operations. That very well could be a play. I'm still learning the ropes on the internals of it.

But that was interesting on top of the convenience function that data dot table has with MELT to get those multiple variables at once. It is showing, at least in this benchmark, a speedier process, especially as the number of rows increase. Now, again, maybe practically speaking, that won't be a huge impact to your workflows, but it was thought provoking nonetheless. But he does acknowledge that there's a lot of confounding in these comparisons, so it may not be quite apples to apples because of the different post processing that comes into play.

And then when that is taking place, there are some additional visualizations when he kind of teases that out a bit that show the differences in even more, more fashion than what you saw in the previous plot. So interesting thought provoking exercise, but that's not the only operation, Mike, because we can also go the other way around. We can go from long to wide. So why don't you take us through that journey here? Exactly. If you are,

[00:39:50] Mike Thomas:

for some strange nonanalytics reason, looking to take your nice long data and reshape it into to wider data. And I shouldn't say that. There are some R packages that maybe it makes more sense to have each, you know, sort of sort of, a wider dataset, you know, representing your categorical data in multiple columns instead of a single column. But for me, this is a use case, that I face less often than going from from wide to long. But for those particular use cases where you do need to go long to wide, the function that you're gonna be using from data dot table is called dcast, and they provide some great examples here, Toby does, of how exactly to do that. Again, you can leverage a separator, if you would like to do that.

In this case, they're using that that period separator again, and the code is is fairly similar, to what you saw in the Melt code and allows you to pivot that that long column into a wider data frame, with multiple columns in it. If you are using polars, the method that you're going to be using there is literally called pivot. And, again, you know, the syntax is is pretty similar. You're sort of doing the opposite of what you did, when you were going from wide to long format. And then lastly, if you are using DuckDb, you're going to again use, the same named function as in Polaris. It's it's called pivot, the pivot command, which can be used to, as they say, recover the original iris data, the way that it is in wider format as opposed to long format.

And the SQL there is is pretty straightforward to look at especially compared to what we just walked through on the unpivot side of the equation. So drum roll, what everybody cares about here, right, is the benchmark comparison analysis. And this one goes a little bit inverse of the, wide to long approach such that, the engine, if you will, that has the best performance appears to be DuckDb, then closely followed by polars and then followed thereafter by data dot table. These benchmark lines that they have here are all pretty tight. I I wanna mention that that there's probably, you know, a much tighter a much tighter gaps between the 3 benchmarks here, across the 3 different engines, compared to what we saw in the wide to long approach.

So your experience is probably gonna be minimal, up until you start really scaling up to to datasets larger than, it looks like, you know, 24,000,000 rows roughly is is what DuckDV was able to handle in 0.1, seconds or or 10 milliseconds, if you will. So that was, you know, interesting to see sort of the benchmark there flip. One of the things that, Eric, I wanted to talk about here that I was curious about is I know and this is from listening to, especially, a recent podcast, not in the super data science podcast, with the author of the Polars Library, and I know DuckDV has some of this too, is both Polars and I think DuckDV have an option to do sort of a what's called a query plan and maybe lazy execution of, you know, chained functions that you might have together. Right? So we were talking about before where you can't do, you know, both in an unpivot and, you know, use a a separator in a value to split out additional columns all in a single function. You would have to chain that together in a couple different functions, in polars.

And I believe, you know, the way that polars works is depending on the default, and don't quote me on this, you know, it may default to what's called, like, eager evaluation, which is, you know, actually running those functions sort of in the order that they are written. But it provides you the option through some sort of binary flag to evaluate, your your query, if you will, or the code that you've written lazily, which means that behind the scenes, polars will put together a an optimal query plan that it believes will execute your code in the the most efficient and and fastest, if you will, way possible.

And I imagine that DuckDb does some of this too. And I also imagine that that query plan may take some time, right, under the hood to execute. So if we're only looking at these benchmarks up to 0.1 seconds, you know, I'd be interested in taking a look at datasets that are are maybe even larger, right, in the order of, like, a 100000000 rows, something like that or or larger. And seeing, you know, how these benchmarks compare after that at 0.1 second threshold to see if things change after the query planning, you know, algorithm is essentially run and the the time that it takes to do that has finished. And then we're really, you know, sort of just running, the actual code to or the actual query to do some of this data reshaping. So I'd be interested to see these benchmarks, I guess, you know, a little bit further out, extrapolate it a little bit further out in both time and magnitude of the dataset.

But, just an interesting thought that I had upon reading this blog post and it's it's super interesting to me, you know, where these, different engines sort of beat one another, if you will. You know, at the end of the day, if if you can't wait the extra 0.1 seconds or or or whatever it is, you know, half a second, just because you wanna use your your favorite engine. I guess it depends on your use case if you're standing up an API or something like that and you need response times as quick as possible and the data needs to be pivoted or or unpivoted, this is a great blog post to check out. But in sort of a general analytics, you know, approach here to to your day to day analysis.

The really cool thing I think about the ecosystem right now is is benchmarks are are getting to be fairly negligible, across most normal sized datasets.

[00:46:04] Eric Nantz:

That's a very, very fair assessment. And also with the advent of our computing technology with the processes processor cores going up and everything like that, we are seeing that for a data science workflow where let let's be real here. As much as the advancements in cloud happen, we know a lot of data science happens on people's laptops as well that with these compiled, you know, you know, I say back ends, wherever going with data dot table really based in c, and then you got polars based in Rust, the number, you know, getting a lot of attention in terms of performance. And, of course, DuckDb with its bindings. You are seeing this experience of these type of datasets. You you've got great choices here. And on that topic, like, query planning, if you want a way in especially in the DuckDV landscape, kind of get to what that query language actually looks like, I'll put a plug in the show notes for the Duckplier package, which had a CRAN release a few months ago because they have a handy function called explain that literally shows in kind of a plain text printout all the different operations that are about to be conducted. But it's doing it in an optimized order, I believe, to get the best performance as DuckDBC's fit for that particular operation, which then you can use with your familiar dplyr verbs of, like, mutate and summarize and whatnot. Again, those kind of classical operations that we've seen, you know, for for those type of work. So that was an interesting learning for me.

I do think the concept of the different data layouts is a frontier that I haven't seen as much attention on. So I'm certainly appreciative of Toby's post here to highlight, you know, the different ways that this can be accomplished. And I think, again, your use case may determine different things. I want, you know, for my back ends to have very minimal dependencies. And with that, to be able to fold into potentially a Shiny app for very quick operations about hogging the user's memory, for that particular app session. Those are my biggest, might say, criteria, if you will, as I'm looking at these.

And so I may have a use case where DuckDV does exactly what I need. There may be another use case for data dot table is king based on existing workflows. So, again, choice is gray here. Like you, Mike, I would love to see this expand in some of the different benchmarks and different scenarios to see where that plays out. But you can experiment with all these and kinda see what best fits for you. But, again, really comprehensive post by Toby here. And, again, it really shows that this funding that they're getting from this NSF grant is being put to great use to spread the word out about these different the different ways that we can accomplish these very common tasks in data science.

Well, we're we're running a bit well on time here on this episode. So we're gonna wrap things up after that great discussion of the highlights. But if you wanna get in touch with us further, we have multiple ways of doing that. You can send us a quick note in a contact page in our episode show notes. You can also send us a fun little boost along the way if you have a modern podcast app, or you can get in touch with us on social media. I am mostly on Mastodon these days, Yvette R podcast at podcastindex.social.

Also find me on LinkedIn. Search my name and find me there. And, Mike, working on wisdom has got a hold of you.

[00:49:35] Mike Thomas:

Yep. You can find me on Mastodon at mike_thomas@fostodon.org, or you can find me on LinkedIn by searching Catchbrook Analytics, k e t c h b r o o k, to see what I'm up to lately.

[00:49:48] Eric Nantz:

Awesome stuff, my friend. Thanks again for a great episode, and thanks again for all of you tuning in from wherever you are. Have a great day, everybody, and we'll hopefully see you back here next week."
"36","issue_2024_w_42_highlights",2024-10-16,52M 30S,"A helpful way to organizing your growing collection of unit tests, how interfacing with LLMs just got easier in the R ecosystem, and a clever use of AI to summarize a large collection of blog posts. Episode Links This week's curator: Eric Nantz: @rpodcast@podcastindex.social (Mastodon) and @theRcast (X/Twitter) Nested unit tests with…","[00:00:03] Eric Nantz:

Hello, friends. We're back up so 182 of the R Weekly Highlights podcast. This is the usually weekly show where we talk about the latest happenings that are highlighted in every single week's, our weekly issue. Now we were off last week because yours truly did have a a bit of a vacation he forgot about until after recording last week with my kids being off for fall break. But nonetheless, I am back here. My name is Eric Nanson, and as always, I am delighted that you join us wherever you are around the world.

And, yeah, fall is in the air as, my co host can see I'm wearing my one of my our hoodies here because it is a little chilly here in Midwest. But, of course, I gotta bring him in now. I also my host, Mike Thomas. Mike, are you, experiencing the chill there too? It is a little bit chilly here, Eric. Not gonna lie. My office, for whatever reason, seems to be the coldest

[00:00:52] Mike Thomas:

room in the house. I don't have great zoneage set up here. So been trying to, put the sweatshirts on and off in between teams calls and get the space heater out, but it's 'tis the season.

[00:01:05] Eric Nantz:

Yeah. You know what's a great space heater? A really beefy server like the one right next to me. No. That's a good idea. Yeah. This one's not too bad, but if you go behind it, yeah, you can more up your hands hands a little bit. But, I I will mention in our week off, probably one of the funniest things I did was we had gone to a place here in Indiana about a bit south of where I met called Blue Spring Caverns, where basically you can go about 200 feet below the surface into literally this set of caves with a waterfall, and we had this fun little boat tour.

And as you get deeper in this, with the tour guy turning his his fancy light off, pitch black, like, you can't see anything. And then if you are completely silent, it is completely silent. Like, it is the closest I've ever gone to a place where I'm literally away from every single thing imaginable. But it it was a good time. You don't wanna be stuck there because you could you know, hypothermia could happen. Luckily, it was only about 40 minute tour, but, yeah, that was, that was kind of like a meditative type experience. So that that was a highlight for me. Very cool. Yep. So I made it back in one piece, and luckily, my kids didn't cause too much trouble or try to tip over boats or anything. So that was a win too.

They they were not alone. There were other kids on that, but, of course and when the tour guide said, okay. Let's try this whole silence thing. In about 10 seconds, one of my kids just blurts out laughing. I'm like, gosh. Naturally. Yeah.

[00:02:38] Mike Thomas:

Surprised it lasted 10 seconds.

[00:02:41] Eric Nantz:

Yeah. Yeah. Me too. Yeah. Because I I could've been a lot worse. But, nonetheless, yeah, we had a lot of fun. Good time on the on the fall break, but I'm happy to be back with you. Tell him about the latest r w two issue. And, yeah, guess what? Just serendipitously, the timing worked out that I was a curator for this past issue. So that was a late night, Friday Saturday session to get this together. But, luckily, I think we got a awesome issue to talk about here, but I can never do this without the tremendous help from our fellow Rwicky team members and contributors like all of you around the role of your awesome poll requests and suggestions, which I was always very happy to merge into this week's issue.

So first up in our highlights roundup today, we are talking about a pattern that as a package developer or an application developer, you definitely wanna get into the pattern of building unit tests in your package and and application because future you will thank you immensely for building that test to figure out any regressions in your code base, figuring out that new feature, making sure that you've got a good set of rules to follow for assessing its fluidity and quality and whatnot. Well, as you can imagine, Mike, you can speak from experience on this too.

As your package or application gets larger, start building more tests, more tests. Yes. Even more tests. This could be spread into, like, massive set of r scripts in your test that folder or whatnot. And you may be wondering, yeah, what's, what are some best practices for just keeping track of everything or keeping things organized? Well, I was delighted to see that, in our first highlight here, we have this terrific post by Roman Paul, who is a statistical software developer in the GSK vaccines division.

On his blog, he talks about a neat concept called nested unit test with test that. And this was something that I probably thought was possible. I never really tried it out. So let's break this down a little bit here. So as usual, he starts off with a nice example here with, in this case, a simple function for adding two numbers. Of course, that's one of the most basic ones, but that's not the point here. But as you develop, you know, thinking about what are the best tests for a function like this, you might come up with more than 1, like in the first part of the example here where he has a testing of whether the addition of 21+2 is indeed equal to 3.

But you may also want to build in tests for, like, error checking, such as detecting if an error occurs if one of the inputs like a and b are not numerics. And now you've built 2 tests back to back in your script. Now they they, in essence, even though they're separate script they're separate test that calls, they're still using the same function under the hood that they're actually testing. So naturally, what he goes do next is that you can actually bundle both of these tests inside an overall test, might call a wrapper test. He simply calls it ad. But within this, there is, a little trick that we'll get to in a little bit, but he's basically nested in those 2 separate test that calls for the baseline functionality and then testing those input values inside this overall test. Now there is a trick to do if you've tried this before and maybe gotten some, you know, warnings from test stat about, you know, a a, testing or an empty test.

He has, at the beginning of this overall block, a call to expect true whether a function exists, called add. And that that apparently will suppress any warnings about an empty test. That was new to me as a little trick there. So this this paradigm works great for, again, this 2 test case. But now in the rest of the posties talks about why would you wanna invest in this instead of just having all these as separate on top of just in the more cleaner organization. But there may be cases whereas you're building a larger code base of tests, you might want an easy way to skip your tests occasionally as you're iterating through development.

So in the next example, always added as a line after the expect true of skipping all the remaining tests below it with a a simple call to skip. So in that way, if you know you're breaking stuff and you don't wanna quite test again, and you just throw that skip in there, keep iterating, and take that skip out, and then it'll just go ahead and run those tests again just like it would before. And then there are some additional benefits you can have there but takes advantage of a couple different concepts that are familiar to an R developer.

One of those is having a little shortcut, if you will, for that function that you're testing. Maybe it has a long function name and you just wanna do a simple letter for it for the purposes of your test. So he gives this add function an assignment to the letter f, and then simply he's able to use that in all the expect equal or the expect error calls going forward. Again, that might be helpful if you have a pretty verbose function name. And then another one is taking advantage of much like in functions themselves, they have their own scoping rules where you might have then we might wanna reuse different parts of this test scope that may be more self contained for this group of tests, maybe not for remaining tests that are outside of this block, you can take advantage of having function having objects inside this overall wrapper, you know, test that that's going that's the outer layer. And then those can feed directly into those sub test stack calls much like you would with nested functions. That's another interesting approach if you wanna cleanly organize things about duplicating these objects in each test stack call of of objects that are related to each other.

Again, it will depend on your complexity to see just how far you you take this. But, yeah, the possibilities you you can go many different additional directions with this. And in the last example, he talks about how you can group the unit tests by maybe some additional criteria. Perhaps it's based on the type of input data. Maybe it's based on another factor. In this case, he's got ways of grouping it based on whatever he's testing for positive or negative numbers. Interesting functionality there that you can build and play.

But in the end, the key part I take away from this is that the test that function is pretty open ended after all. Once you know kind of the the quirks to get around that empty warning, you could do some pretty interesting organization in terms of nesting related tests together, sharing objects between them, and overall having a slightly cleaner code base for your your what could be a massive amount of tests, which is I'm just releasing an app in production this week. I have a lot of business logic tests, so this may be something I have to look into for the future as well. So very night fit nice, fit for purpose post here, and I'll definitely take some learnings from this in my next, business logic testing adventures.



[00:10:27] Mike Thomas:

Yeah. Me as well, Eric. This is really interesting, and like you, it's not something that I had tried before. You know, what we're talking about here is a test that call inside another test that call. You know, not just having multiple expect type calls in the same test that block, which I'm I'm sure folks who have written unit tests before are familiar with, hopefully. Otherwise, your your, your code would be quite lengthy. And and one of the ways that I learned to to write unit tests was to look at the source code of some of my favorite r package developers, primarily Max Kuhn and Julia Silgy.

Because unit tests are are kind of interesting, you know, they're they're very different than a lot of the other R code that you'll write. And, they sort of execute in their own environment as well, which makes some of this stuff a little tricky. I learned this the hard way, you know, trying to download a file inside my testing script and handling, you know, the creation of a variable within a test that block that maybe doesn't exist in the next test that block. And as an r package developer, and I I know, Eric, you share a lot of these same sentiments, you can quickly become sort of obsessed with organization of your code, and I think testing is another place that that applies to.

And these concepts, I think, are really useful from an organization standpoint. I feel like I need to go go right back to some of the r packages that we have right now and take a look because I know that there are opportunities to improve, you know, leveraging what Roman's written about here, you know, particularly in some of the the lower code code blocks in this blog post, underneath local scoping where you're developing some variables within this outer test that call, that you're going to use in some of these inner test that calls that I believe will really just make, you know, some of the the code, run quicker and more lightweight instead of defining, like, global variables at the top of your script that are going to stay there for the remainder of all of the tests being executed.

These can kind of be local to just this particular outer test that chunk. So really interesting. I feel like it's kind of a tough blog post to do justice without actually taking a look at the code and reading through it, but it should click really quickly if you take the time. And it's a pretty short and sweet concise blog post here, to read through it. But, another takeaway for me was that I did not know about the skip function from test that, which skips all of the tests below the skip call in that same test that block. So it'll execute the ones above it, but, you know, the all of the ones below it within that same test that block, as far as I understand, will be skipped, which is is really interesting if you are in the the middle of development, you know, if something's failing and you you need to just sort of, make sure that you're able to lock down, what you understand and then, you know, re execute the the tests that are firing so that you can, you know, work on the bug fixes in a safe and efficient environment. So really, really helpful blog post here, some concepts that I had never honestly seen talked about before or used, that I think are really, really applicable to anybody who's developing in our package or maintaining in our package out there.



[00:13:51] Eric Nantz:

Yeah. That skip function was, underrated gem that I dived into a little bit before this only because of necessity. You may find yourself in a situation, we're gonna get in the weeds on this one, where for my app I'm releasing to production, I had a different set of tests that were more akin to operating in our internal environment and our Linux environment versus when I would run it in GitHub actions, that was technically a different environment. And so I had to skip certain things if it was running on GitHub actions that were more, I'm gonna be blunt here, very crude hacks to get around issues with our current infrastructure for the tests that were appropriate for internal use. So, yeah, the skip there are variants of the skip function as well. There's like a skip if CRAN type function, which is a wrapper that detects if this is running on the CRAN server or not. So it won't run those tests where maybe you wanna run them in development, but you don't care as much for the actual CRAN release of a package. Again, there would be different use cases for these. So, yeah, you're invited to check out the the test that vignettes. There's a lot of great documentation there. But this concept that's been covered here, has not been mentioned in the vignettes of test that before. So I'm really thankful to Roman for putting this together for us because I'm gonna bookmark it and test that related resources to use going forward.

Doesn't it seem like yesterday, Mike, when we were sitting near each other at deposit conf, listening to Joe Cheng's, very enlightening talk about

[00:15:40] Mike Thomas:

using AI chatbots and Shiny. Don't you remember, my friend? Oh, I remember it like it was yesterday, Eric. I was gripping onto the edge of my seat.

[00:15:49] Eric Nantz:

Yes. And I I teased you a little bit before that because I had an inside preview. But, nonetheless, I was still blown away when you actually see it in action. And while I hope the recordings will be out fairly soon, once they are out there, you'll wanna check it out, for you and the audience because Joe Chang demonstrated a very interesting Shiny application. At that time, he was using Shiny for Python that had a, in essence, a chat like interface on the left hand margin, and then it was visualizing through a few different, like, summary tables and visualizations, a restaurant, tipping dataset.

But when Joe would write an intelligent kind of question in the prompt saying, show me the the tip summaries for those that are males, Instead of being a shiny app developer trying to prospectively build all these filters in yourself, the the chatbot was smart enough to do the querying itself, and it rerendered the app on the fly. It was amazing. Many in the audience were amazed. Mike and I, of course, are looking at each other. It's like, yep. We wanna use this.

[00:16:59] Mike Thomas:

Yep. We've been skeptical about AI. Have some harsh feelings for it, at some points in time, but, yeah, we wanna use this. I agree.

[00:17:09] Eric Nantz:

So as I mentioned, that was using Shiny for Python, and he had mentioned that the R support was coming soon. Well, that soon is now, and you might call this an inside scoop, so to speak, because while we're talking about is public, it hasn't really been publicly announced yet by time deposit team, so you get to hear it here first. But there is a new package out there, by Joe Chang himself called shiny chat. This is basically giving you as a Shiny developer a way to build in in a module like fashion the chat user interface component inside your Shiny apps. And it indeed on the tin works as advertised where you can call a simple function to render this chat interface in very similar to, like, a chat UI call.

We have a little server size stuff we'll talk about shortly, and then your app will have a very familiar looking chat interface that you can type queries in and get results back. There are some interesting things under the hood here that respect the Shiny itself and giving you that that chat like experience that you see in the external services with some clever uses of synchronous and asynchronous processing under the hood. There is example in the in the GitHub for the package that you can take a look at as well.

But there is there is another component to this, Mike, that ShinyChat on its own doesn't do all the heavy lifting much like, you know, robust Shiny development. We always say that for heavy lifting of, like, server processing or business logic, you wanna put that into its own dedicated set of functions or packages. So hand in hand with shiny chat, we will put a link to this in the show notes, is a new package that's being authored by Hadley Wickham called Elmer. Elmer is in our package to interface directly with many of the external AI services that we've been using, for a while now, such as, of course, OpenAI.

There's been now support for Google Gemini. There's support for Claude and support for all WAVA models if you've had those deployed on an API like service. So Elmer is actually the way that Shiny chat from the server side is going to communicate directly with this AI service, and this is where you feed in any customizations to the prompt before the interface is loaded, which is something I've been learning about, recently, prompt engineering. So Elmer will give you a way to feed that in directly, And then you can also take advantage of, of course, the responses being returned to you, and you can do whatever you want with those.

But the real nugget here, and, again, this is actually how that restaurant tipping app works under the hood, is in addition to the prompt being sent to this chat interface, you are able to select what are called tools for this. Now what is a tool in this case? We're not talking about, like, a, you know, tool in the traditional sense. You are letting the AI take advantage of, say, a function or maybe functions from another package to help assist you with taking the result from that initial query or or that message you send to the chatbot and maybe calling something on your behalf to help finish off the process.

So an example I can illustrate here is if you ask a chatbot, hey, how long ago was, say, the declaration of independence written? Guess what? Due to the stateless nature of these chatbots, it doesn't really know what time it is for you right now when you call that. It may try to guess that you're calling it in October or whatnot, but it's not gonna really know that answer. So how to get around this is you can help feed in via this tool argument in Elmer when you set up the chat interface, a function that it can call to help answer these kind of questions.

This opens up the possibilities to help the AI bot get answers to things that through its, you know, training that it's done externally or internally, depending on how you're using it, would not be able to answer on its own. This took this taken me a little bit to get used to, but that time example was one thing that kinda clicked. But that, in theory, is what's happening with this restaurant tipping app that we saw at Pazitconf where Joe built in a tool, I. E. A function to basically execute that what became a SQL query as a result from that question that we asked it. And then it would execute it on the source data that was fed into the app. It was set as a reactive doom. Every output is refreshed in real time after that query is executed.

And that is something that is amazing to me that now we can give the chatbot a little more flexibility. But just like if I was training, like, a a new colleague to get used to one of my tools or one of my packages, You've gotta be pretty smart with it or or I should say be helpful with it. We have a function. What do we talk about? Have good documentation. Have good documentation of the parameters, the expected outputs. That is going to be necessary for Elmer to take that that source of that function or that function call and then help assist you with the syntax you need to feed this in as a tool to the AI to the AI service.

So there is a lot to get used to here, but we'll put a link in the show notes to, again, the the restaurant tipping examples. You can see this in action. But now the combination of Shiny chat and Elmer, we can start building these functionalities into our Shiny apps that are written in r, and we are always scratching the surface here. This stuff I wanna stress is an active development. These are not on CRAN yet, so there could be features that are breaking changes or whatnot. So be wary of that. But they are very actively working on this as we speak, and I'm excited to see what the future holds. And my my creative wheels have already been turning about ways that I can leverage this at my in my day job as this gets

[00:23:47] Mike Thomas:

gets more mature. Yeah. Eric, I mean, you you summarized it very well. I think that presentation by Joe really blew a lot of us away. I think it, created a lot of opportunity, probably a lot of of work for us, right, to to be able to try to integrate these things, especially if any of our bosses saw that presentation, to try to integrate this type of functionality and user interface and, you know, capabilities into our Shiny apps to, you know, I I think not only reduce the number of filters maybe that we need to create, but also sort of allow this sandbox environment for users to play around with the data that we're displaying in our apps such that we don't necessarily have to worry about boiling the ocean for them anymore and, you know, being able to to handle every possible little edge case that anybody could ever want to see, before we just yell at them and tell them to learn how to write SQL against the database. Right? Not that I've ever been there before. Oh, never.

But it it it's very interesting how a lot of this tooling by posit seems to be, you know, sort of quietly taking place behind the scenes. But, obviously, you know, I think this Elmer package is doing a lot of the heavy lifting, and it has a great package down site if you haven't taken a look at it yet. And this tool calling Vignette is is a fantastic introduction to to sort of how that works and how we'll be able to I don't know if customize is is the right word, but really be able to nicely integrate these large language model capabilities into our our workflows, right, and not have as much of a disconnect as I would have imagined there to be at the beginning. So it it's pretty incredible that we we do have this tooling. You know, I think as as Joe said, there are a lot of considerations that you need to take around, you know, potential safety to ensure that these large language models aren't actually getting direct access to the data in your app and ideally just executing a a or the models are returning a a query maybe based on the schema of your data without ever actually seeing, you know, your data itself in the case of, you know, that tipping example where the the model is really mostly returning a SQL query, essentially, that you can leverage to execute against, you know, your underlying data source, which, in my opinion, you know, those are the situations unlike this, you know, this this time example is interesting.

But I think, you know, the the former that I was talking about are are probably the most useful, as in terms of low hanging fruit that I can see applications of these large language models into Shiny apps. So it's really interesting to see all of this start to come together. Obviously, we saw some of this on the Python side first and and really excited that, the ecosystem on the the R side is expanding, just as well. So lots to see here already. I can imagine only more to come. And you, I I believe, Eric, had the opportunity to participate in a little hackathon around this stuff. I haven't gotten my hands quite as dirty yet, but that is really, really, on the forefront of what we're we're trying to do,

[00:27:16] Eric Nantz:

at the day job here. So, hopefully, I'll be able to report back on on some of our findings pretty soon. Yeah. That was, very, very fortunate that I I got the invitation to see some of the preview versions of these interfaces, and, you know, I wouldn't turn that down for anything. And, Mike, though, as we talked in the outset, I have been resistant is kind of a strong word, but it has been very not not very tempting for me to adopt a lot of this yet in my daily work. But I think with the right right frame of mind in this and the right use cases, this can be a massive help. I think as long as we have a way, especially in my industry, to only let it access what it needs to access, but yet this tooling functionality gives us a way to then act as if maybe the AI did get us 80% of the way there, and then we would have to manually do the rest of the 20%.

We're basically giving them a way to get to that 20%, but still run it in our local environment to do it. It's not going back to them to run it. That that is or to get the results back itself, so to speak. It's acting on my behalf. That's a nuance I'm still kinda wrestling with. It's still something that seems a little magical to me, but I think what we'll have a link to is the r version of this side restaurant app example in the show notes as well. The key another key part of this along with the tools supplying the tool function is the prompt itself. So in the example we'll link to, Joe has put together a markdown document that is the prompt that's being sent to it. Basically, it's reading the entire line this markdown file where it's very verbose with the AI, you know, the AI bot about what it has access to, what is expected to do, what it should not do, and then making sure that it doesn't go outside those confines.

It's interesting to see just what the nuances would be when you try this on different back ends, so to speak. Because I have heard that, yeah, things like OpenAI, this works really great almost a 100% of the time. There may be other cases where a self hosted model isn't quite there yet, but that's just the nature of this business. Right? Some of these models are gonna be more advanced than others because of the how many billings of parameters that are used to train the things. So there's a lot of experimentation, I think, is necessary to figure out what is best for your particular use case, but you get the choice here. That was one of the things I was harping on when I first heard about this is I don't wanna be confined to just one model type. I wanna or one service. I wanna be able to supply that as the open source versions do get more advanced. So there's the responsibility aspect is still very much at play for what I'm looking at.

But the fact that now this is our disposal about me having to tap into and again, no no shade to our friends in Python, but I've heard that the langchain framework can be a bit hairy to get into, and Elmer is really trying to be that very friendly interface on top of all this about me having to engineer it all myself. So I'm super impressed that with about 4 hours of work I did in this hackathon to build an extension of this restaurant type example with podcast data, I was able to get to even about, like, 85% of the way there in that amount of time.

I'm telling you, when I picked up Shiny, it took me well more than 4 hours to get to where I wanted to get to initially. So I I'm very intrigued by what we can accomplish here. Well, we're going to stay on the AI train for a little bit here, Mike, in our last highlight here, because one of the things that has been one of the more, I might say, low hanging fruit for many use cases across different industries and different sectors is being able to ingest maybe more verbose documentation or more verbose, sources online and generate a quick summary of that.

We're seeing this in action even with some, you know, external vendors. Those of you that live in the Microsoft Teams ecosystem probably know that there are things like a chat copilot for meeting summaries, which in Middle East can sometimes be a bit odd when their results come through, but that's that's that's all happening. That's all happening here. Well, imagine you have a lot of resources at your disposal, and maybe you could take the time to go through each of these. Maybe they're a blog, and you just summarize them quickly for maybe sharing on social media or sharing with your colleagues.

But is there a better way? Well, better or not, no. It's up to you to decide. But our last highlight here, we've got a great interesting use case from Athanasia Mowinkle, who has been on the highlights quite a bit in the past, talking about her adventures of creating summaries of posts from her blog with artificial intelligence surfaced via Hugging Face. So as I mentioned, her her, motivation here was she's got a lot of posts on her blog, and it's a terrific blog. You should definitely subscribe to that if you haven't already.

And she wanted to see what she could do to help get these summaries for additional metadata to be supplied in, and he she saw a colleague of hers that was using OpenAI's API to create summaries of his blog, and she wanted to, you know, again get away from the confines of Python on this. She wanted to give Hugging Face a try by calling it with directly an R. The rest of the post talks about, okay, first getting set up with a hugging face API, which again, you know, pretty straightforward. Having getting an API key, which is pretty familiar to any of you that have called web services from r in the past.

And then she is going to leverage a very powerful ACTR 2 package to build in a request to Hugging Face to speed a set of inputs and then doing the request, you know, the request, summarization or the request parameters. It's a lot of a lot of, like, low level stuff, but once you get the hang of it, it works, which by the way, that Elmer package we were just talking about is wrapping HTTR 2 under the hood to help with these requests of these different API services. So maybe in the future, that will get Hugging Face support. But nonetheless, she has a nice tidy example here of using you know, making a request to, feed in, I believe, some summary text and then preparing that for a summarization afterwards where she's got a a custom function to grab the front matter of her post, determining if it needs a summary or not, and if it does need it to basically now call to this custom function for the API to grab that content back and then to summarize that into a file so that it can go back into the markdown front matter of her post.

We've actually covered her explorations of adapting front matter to her post in a previous highlight, I wanna say, about a month ago. So this is very much using those similar techniques. So go check that episode out if you want more context to what's happening in these functions, but she's reusing a function that add the con the post summary to the front matter and then being able to write that out as a pretty nice nice and tidy process. So in the end, she links to the entire the entire collection of functions. It's about 7 or 8 functions here. Not not too shabby. It's a great way to learn under the hood of what it's like to get results from these API services like Hugging Face, and you could use the same technique with OpenAI as well from the r side if you wanted to.

But this is this is really going under the hood of what now I believe the Elmer package is gonna try and wrap for you in a nice concise way. But for me, sometimes the best way to learn is to sometimes brute force this myself and then be able to take advantage of the convenience, but knowing what that abstraction that, in this case, Elmer would bring you is really doing under the hood. So it's a really great learning journey that she's she's authored here and then she seems pretty satisfied with the summarizations that she's able to get here. But Hugging Face does give you an interesting approach to feed in a model of your choosing to that to your account and then be able to call that via an API.

So I believe you can feed in things like llama models and other ones there as well. I haven't played with it myself yet, but it's another intriguing way to take a set of, in this case, publicly available content via her blog post, grab some interesting summaries, feed that back into her post, front matter so that her blog can show these summaries very quickly without her having to manually craft this every step of the way. So again, great examples of h t t r two in action and Waze and her examples talking about the custom functions that she's done to grab these posts, grab the content, put in the summary from the API service from hugging face back into the front matter, and then rerender the site. So there's a lot of a lot of interesting nuggets at play here. And, again, great use of engineering with the new tools available to us to make your, crafting of these summaries a lot easier, which I might have to do for our very podcast here, may Mike, maybe I don't have to write my summaries anymore. Hey. That's not the worst idea. And you know what?



[00:37:22] Mike Thomas:

I don't, you know, I don't know if this is sad or a good thing, but I feel like large language models because they're trained on the Internet in terms of, like, SEO or or and and summarizing some text, you know, so that it's going to look the best online. I gotta hand it to them. They're probably better at that than I am in terms of trying to boost the SEO for for my own website. So I should probably take a page out of Athanasios blog post here and, try that myself. You know, one of the the things that just always, you know, makes me happy, I guess, is looking at h t t r two code for building these, API requests. You know, we don't have to write 4 loops anymore for retrying an API if it if it fails the first time. We have our request underscore retry function. There's this request cache function, to be able to build a cache within, your your API request, which is really fantastic, and it's just this really nice pipe syntax to build this up. It looks so clean, compared to some of the crazy stuff that I used to do to try to call, APIs in a a neat manner.

And it looks like, you you know, again, the Hugging Face ecosystem is one that I should know more about. I don't, unfortunately, but upon some research here, it looks like there's there's probably a lot of different text summarization models that, Athanasia could have chosen from. The one, it looks like that, she leveraged here is called Distilbart CNN, assuming that's for convolutional neural network 126. I think there's a lot of different trade offs in terms of, you know, the the size of these models that you can use. I don't know what the pricing looks like for Hugging Face, in terms of, you know, how many requests you can make to a particular, you know, model endpoint, before you start getting charged or how much it costs, you know. And I imagine that the the larger size models, which are the better performing ones, probably cost more than the smaller size models, but that's just something to watch out for if you are, you know, taking the time to try to recreate Athanasi's blog post or leverage, her work here to be able to do this for your own purposes.

But, you know, as you mentioned, Eric, the ability to integrate your model of your own choosing, into this function, I think, is a really nice feature in the way that she's laid out this ad summary, function here at the end of the blog post that sort of, encompasses everything, you know, that we're doing in this workflow and being able to choose between some of these more closed source models versus some of the more open source models, like, llama, I think is is really a neat thing that we have at our fingertips. So fantastic, blog posts here that she's put together. You know, love the code. Love the way that she formats her code. I think it's really legible.

And I guarantee that I will have a use case where I'll need to probably ask you, hey. What was that blog post that, was on the highlights, you know, a few months ago, when it comes time for me to try out this exact same type of thing for my own purposes?

[00:40:40] Eric Nantz:

Yeah. And there's there's other nuances here, again, tying back to what we talked about recently here. What I noticed as she mentions as she was preparing the the the post content to go to the the AI service on Hugging Face is as she mentioned that she had a little gotcha when she kept it in the YAML structure that the results weren't quite making sense coming back. And this is one thing that it's taking my old timer brain a bit much getting used to. When you feed in this information for it to digest, it can be as if you just wrote it in plain text, basically. Like, you can treat it. Like, even with a prompt example from the restaurant app, that's as if, like, you and me just took, like, a half hour to write maybe to a colleague what we wanted them to do. It's kinda like instructions for a task.

Like, it it's I'm just so I don't know. I have this, like, muscle memory of, like, I have to do things so structured or it's gotta be in this syntax of YAML or JSON and whatnot. These prompts are just like plain text stuff. It's like it doesn't make sense how it can parse it all out. But, again, I'm just not giving it enough maybe credit under the hood to ingest, like you said, these online resources and to be able to do what it needs to do even just with somebody banging at a keyboard to write this out for a half hour or so. So she's got, again, a convenience function to strip all that YAML stuff out of the post content so that it can just go directly in. It's it's just amazing just what they're capable of. And, yeah, I I realize that sometimes I can sound too optimistic on this show sometimes, so I'm gonna have to play with this myself with some of the sources I make. But, spoiler alert, the podcast service that we do use does give me a plug in to execute to generate summaries for me. So maybe I just have to switch that bit on, but I imagine it's doing something like this under the hood. Very interesting. No. I'd like to see how it does that.

In this, for for the rest of this issue. So we'll take a couple of minutes for our additional finds here. And certainly, in the world of exploratory data analysis, especially if you've been in industry as long as Mike and I have or even if you're new to it, you know that in terms of business intelligence and, you know, looking at data, there are some established players in this space, IE Tableau and Power BI, which give you that yeah. Yeah. And we're we're doing the, the thumbs down from us on the video here. But, it can be hard to crack that nugget if you're an R user and you still want a way to quickly build these, you know, great shiny dashboards, but, a way to get started.

Well, I am I'm intrigued by this this one I'm gonna mention here by Bruce Yu in his post called EDA reimagine in r with the gwalker package plus duckdb for lightning fast visualizations. And so, basically, what gwalker does, which I'm gonna have to dig into some more, is it takes a data frame or a data or a database, and it's able to translate into a drag and drop type UI interface that will look very familiar to those in the Tableau and Power BI ecosystem. But then the user that's consuming this can simply use their mouse to explore the data with drill downs, with nice little histograms above like column names.

It looks really intriguing even to let you look at kind of quality metrics of your data and, again, be able to have this interface built right away to do this. It's pretty fascinating. So apparently, there's also a Python package that accompany this called pygwalker that if you're on the Python side, you might want to look at as well, but that could, lower the barrier of entry quite a bit for those maybe at your respective organization. If you get some pushback from certain people, say, yeah. Shiny's cool, but it's just so hard to get something up and running quickly.

Maybe g Walker could get you halfway there, especially for an EDA context. So I'll be paying attention to this more closely.

[00:45:14] Mike Thomas:

That's pretty interesting, Eric. Another one that I would check out that I think is kind of in that same vein is the Apache Superset project. Have you seen that? I have not. So it's open source, modern data exploration and visualization platform. It's like a dashboarding tool, kinda looks like Tableauy, allows you to write custom SQL if you want to as well. Pretty pretty incredible under, I think, that whole sort of Apache umbrella. But this GRarker package reminds me quite a bit of that. So this is this is very interesting as well.

One thing that I found that I was really interested in is, this blog post titled post processing is coming to tidy models. It's a blog post from the tidy models team, Simon Couch, Hannah Frick, and Max Kuhn, essentially about sort of this new I think it's our package or function. Yes. It is in our package, called Taylor. That is sort of like recipes except for at the end of your model in terms of developing the workflow at the beginning of your model. And what it does is it takes the results of your model and does some tuning, tailoring, if you will, post processing of those results to, you know, one example here that they use is, a situation where your model is predicting, you know, your your class probabilities of of 0.5 are going to 1 class, and and the ones that are less than 0.5 are going to the other binary class in your dependent variable. And if you look at maybe your model results, you know, most of the data is coming in on one side or the other. It's not sort of evenly distributed, on either side of that 0.5 threshold.

You know, a lot of times, we see this in situations where there's class imbalance in the dependent variable. And let's be honest. I mean, if you are out there working on machine learning problems all the time where you have great class balance, in your dependent variable, I think that you're in the minority because the rest of us in the real world, face this issue constantly, unfortunately, where it's it's hard to find, you know, that that one case where where things are going, bad, if you will. And I think probably that stems from hopefully, if you're running a business, you know, things are going well more often than they're going bad. So you probably have less observations in your data of things going poorly.

But I'll save that tangent for another day. But one place I think that we've struggled a lot in machine learning in the past is trying to find the right approach to dealing with class imbalance. You know, resampling can kind of conflate results, different ways. And I I think this post processing technique that might allow us to be able to better handle those types of situations. So there's great examples in here for both, you know, binary, dependent variables as well as, continuous dependent variables to be able to adjust, you know, sort of your your output and improve your root mean mean squared error and things like that.

It's a fantastic blog post. I'm really excited about it. There's more to come here, but all of this this code looks like a really nice complement to what already exists in the tidy models ecosystem. It should feel pretty natural to folks who are comfortable with the recipes, our package, leveraging, you know, similarly named functions, if you will, just within this tailor package on the back end of your model. So something I'm I'm really, really excited about, and and really, thrilled to be able to to try to kick the tires on this as soon as possible.



[00:49:13] Eric Nantz:

Yeah. I I know post processing in general has been especially, like you said, class imbalance. I've never met a dataset with perfect balance in my wife yet, especially in my day job. So having a way to to interrogate this more carefully and getting these these summary metrics and visualizations. Yeah. This is this is very, very important, and I can I can tell that it looks like the team has taken their their their due diligence, so to speak, to build this in with a nice unified interface? So like the rest of the tiny models ecosystem, there should be something that's very, you know, quick to adopt in your workflows, and I'm really excited to see to put this in action. And so if our messy, predictions

[00:49:52] Mike Thomas:

that we've done in the past, for sure. We got survival modeling. Now we got post processing. I don't know what's next.

[00:49:59] Eric Nantz:

I don't know, man. That that Max over there and Julia, they they know how to crank them out every every year. There's always something new. So credit to them for, you know, blowing our minds almost every on a yearly basis. But, yeah. And we hopefully the rest of the our weekly content will give you some mind blowing moments as well. There's a lot more that we couldn't get into today, but definitely check out our sections on the new packages out there. I saw some great stuff related to life sciences that has landed as well as some other great tutorials out there, from familiar faces and new faces in the r community.

And how do we keep this project going? Well, a lot of it relies on all of you out there to send us your suggestions for great new content. The best way to do that, head to rweekly.org. Hit that little top right ribbon in the upper right corner to send us a poll request to the upcoming issue draft where our curator will be able to get that great new blog post, new package, tutorial, package update into our next issue. And, again, huge thanks to those in community that sent your poll request. They are much appreciated. It makes our our job a lot easier on the curator side to have that given to us.

And, also, we love to have you involved as well in terms of listening to this show. You can send us a fun little boost if you're listening to one of those modern podcast apps like Podverse Fountain or Cast O Matic. They're all great options to get you up and running with that functionality. And we have a nice little contact page directly in the show notes. I mean, it should be in the show notes if the AI bots don't remove it. We'll find out. But nonetheless, we also like to hear from you on social media as well. I am on Macedon these days with at our podcast at podcast index.social.

I'm also on LinkedIn. Just search your search my name, so to speak, and you'll find me there. And occasionally on the Weapon X thing, I may not post very much, but I'll definitely reply to you if you shout at me. So, Mike, where can listeners get a hold of you? You can find me on mastodon@mike_thomas@phostodon.org,

[00:51:57] Mike Thomas:

or you can find me on LinkedIn by searching Ketchbrook Analytics, ketchbrook, to see what I'm up to lately.

[00:52:05] Eric Nantz:

Very good stuff. And, Mike's posts are authentic so this week. Sometimes on LinkedIn, you can see some of those, you might call, should've rated posts, but Mike's always Mike always keeps it real. Try to. Absolutely. So we're gonna close-up shop here for this edition of our weekly highlights, and we'll be back with another episode, at least I hope we will, of our weekly highlights next week."
"37","issue_2024_w_40_highlights",2024-10-02,52M 19S,"A monumental achievement for bringing the Nix package manager to reproducible data science, travelling deep through the in-place modification rabbit hole across multiple languages, and a sampling of sage advice from the Data Science Hangout. Episode Links This week's curator: Tony Elhabr - @tonyelhabr@skrimmage.com (Mastodon) & @TonyElHabr…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode a 181 of the Our Weekly Highlights podcast. If you're new to the show, this is the weekly podcast where we talk about the latest happenings that we see in our highlights and additional resources every single week at rweekly.org, your weekly curated content for all things news and other awesome events in the art community. My name is Eric Nantz, and I'm delighted you join us wherever you are around the world. And joining me as always, it seems like October is already here. Where did the time go, my friend? But join my my awesome co host, Mike Thomas. Mike, yeah, where did the time go? I can't believe it. Can't believe it either. The 2024 is almost wrapping up,

[00:00:44] Mike Thomas:

coming to a close here, but we're gonna try to enjoy what we have left of it. I'm looking forward to 2025 coming soon.

[00:00:52] Eric Nantz:

Absolutely. So many of us are probably in the midst of these year end activity wrap ups, getting releases out the door, getting other things documented. I'm definitely in that in that mode right now, but this is always the the highlight of my day, no pun intended, for what we do here today. So I'm happy to, record this awesome episode with you as always, Mike. And our issue this week has been curated by Tony Elharbar, who didn't have a a big break between his last curation, but he's helping with our he's kinda scheduling switches here and there. But if you're not familiar with the our weekly effort, we have a rotating list of curators that step in every single week so we kinda spread the spread the work around, but it's all community effort driven. But as always, yeah, tremendous help from our fellow ROCE team members and contributors like you all around the world with your poll requests and other suggestions.

As I saw the the highlights come through in our little voting at the end of the weekend, it was very obvious to me what I was gonna lead off with on this episode because I am delighted, super excited to share the news that an effort that we have been covering on this show, whether in the highlights section or in our additional finds for over a year, has officially hit CRAN, and we are talking about the ricks package authored by Bruno Rodriguez to help facilitate the use of the NICS package manager for reproducible data science workflows entirely in r.

I am, again, super excited and huge congratulations to Bruno on getting this release out. It is absolutely monumental to see this happen. And as he alludes to in the blog post, and we'll have Wing 2 as always, there has been over 15 months of rigs being in development, over 1300 commits, a 143 closed issues, 175 closed poll requests, and not only that not only is ricks on CRAN itself as of earlier last week, It is also under the rOpenSci umbrella. Another major achievement. And frankly, when you think about it, it makes total sense.

We are talking about a huge, huge boost in making reproducible data science workflows. And rOpenSci is one of its biggest core principles is the idea of reproducible data science. So it does seem like a match made in heaven to me, so I'm very excited to see where this goes. And as a result of being under the Arkansas umbrella, one of the ways that a package or an effort gets onboarded is a very robust and a very transparent peer review process to get the package into our OpenSci, which we can actually look at in real time, but also I have the link to in the show notes the very, poll request or the issue with all of the review feedback and all the review steps that Bruno has done to to address feedback. It's all there, all transparent. It was a very looks like a relatively smooth process.

Lots of great documentation there. It's another showcase just how robust the opens side, effort is for getting high quality software under their umbrella. So with those, notes out of the way, you may be new to this effort and wondering just what the heck Nix actually is. We're not talking about Linux. We're not talking about Unix. We're talking about Nix, and Nix is a package manager under the hood, not too dissimilar to what some Linux distributions do for distributing software in their respective systems.

Distributions such as Ubuntu and Debian use the apt package manager. Red Hat, Fedora use an RPM based distribution system. Arch is definitely the host not for the faint of heart, has their own custom repository. Nix, however, you don't have to use Nix on Linux. You can actually use it on macOS as well as Windows itself, thanks to WSL or the Windows subsystem for Linux. So no matter which OS you're on, chances are you could get Nix up and running. And one of the ways that we recommend, I know Bruno recommends this as well, to get started with Nix in a very smooth way is using what's called the determinant systems installer of Nix. That's a vendor that's trying to bring Nix, to a first class way to enterprises and in open source and the like.

And I have I have a good friend on on that team named Martin Wimpress, who is helping a lot of their documentation and and other advocacy. So getting getting NICS on your system, I highly recommend checking out the determinant NICS installer. We're also gonna have linked in the show notes a guide called 02 NICS. If you wanna get more familiar with what's happening with Nix and trying out a few different ways in your set of development environments. But as an R user, Rix is going to handle a lot of this integration with Nix for you in the sense that you can bootstrap the necessary manifest files to tell the Nix package manager not only in the version of r to install in this isolated development environment, but pull down the packages based on a snapshot in time.

In total, Nix has over 80,000 packages across the entire spectrum of software, and R is a first class citizen in Nix along with other languages like Python and Julia and many others. So what Rakes is gonna do is help you, you know, bootstrap these environments and then to be able to say on my host machine, which may or may not even have R installed, you can have this, again, self contained development environment ready to go, reproducible. You could commit these files that are generated, which are called default dot nix, as well as a dot R profile file that's going to help the R session specifics.

That could be on your GitHub repo. And if you have a collaborator that's also running next, they can pull down that exact same environment. That is huge. And if that doesn't sound familiar, you might say that does sound kind of similar to what Docker would give you and sounds similar to what RM would do from a package management perspective. You're not wrong. They're they are somewhat similar in that sense of the overall goal, but the way they achieve it is a bit different. Docker is a core space and container technology, and that can be a heavy footprint depending on your perspective.

But it also, admittedly, has a lot more mindshare in most of the dev ops sector and even in in sectors of data science with things like the rocker project that we speak highly about. So Nix may not always be right for your use case, but I am very intrigued by the promise it brings you. And full disclosure, I've been using Nix on a brand new server I have here in this basement a few feet behind me, which has been a great learning experience in and of itself to have more reproducible software stack for my infrastructure.

But I have been putting Nix through a couple tests in my open source work. And what's even more intriguing is that on top of the R environment that I can bootstrap for Rix, I can also easily add in other utilities such as a custom instance of what's called vscodium to have its own IDE in that same environment and bring it up as a GUI and have it ready to go. It's completely segregated from the rest of my host system, but I can tailor it to my liking. I could put extensions in there. I can have a pretty neat experience with r on Nix in a custom IDE to boot. Like, the possibilities are almost endless with this, and that's why I'm still I'm still learning the ropes on this, quite a bit.

I will say I haven't had a perfect experience of it just yet. There was an instance, which if you're using R on a Linux system, you're probably all too accustomed to this, where I was installing a package, I believe it was called rag or something like that, where it needed an image library development libraries to help compile. And for whatever reason, when Rix was bootstrapping this, this development environment, it didn't pick up that I needed that extra library. So I had to manually add that in. So there's gonna be, I think, some cases that are still still getting worked out, but the the future is certainly bright here.

My other picky wish that I know has been mentioned in the Rick's issue tracker, for those of us that are coming from an RM mindset where we have projects that have an RM lock file, Oh my goodness. I sure would be nice to import that directly into Rix and then magically get the package installation. There's a lot of engineering required to do that. So I'm not surprised that it hasn't landed in just yet. But I am I am getting a bit more, you know, to say I I want this feature more and more because I have I I I'm I'm not I'm not gonna exaggerate here, Mike.

I probably have in my day job over 90 projects that use RM in some way. And if I wanna convert all those to Nicks, that's a lot of manual Mhmm. Manual effort to do. So having some kind of automated way to take a lock file, magically, you know, transform that into the necessary default dot nix and getting the right package hashes, that would be massive. I have seen Python having these custom utilities that tie that poetry framework for their package management with nix. I think they call it poetry to nix, where they can somehow do this magical inference of a poetry lock file, which looks a lot similar to an rmlock file, if you're familiar with that. And they can somehow parse out then from the next package manager which Python packages to install and what hashes that correspond to that particular version.

So I have been spoiled by seeing that on, like, the Python ecosystem. I believe they have a similar thing for Rust, maybe for Julia as well. So maybe it's a matter of time. I don't know. But once that comes in, yeah, that's gonna really ease the onboarding even further for those that are coming from a different framework such as RF to Rix. But nonetheless, I don't want that to discount my huge excitement for this. I think it's a huge step to seeing Nix really taking a a solid footing in in the realm of reproducible data science.

And also in the post that we're gonna link to, Bruno has a short little 5 minute tutorial video. They get you up and running with Rick's quite quickly. But we invite you to check out the rest of his blog because there are literally about 14 or 13 of these other posts that chronicle this entire journey, which you can tell is interspersed with his learning journey of Nick's along the way. It's great to kinda see that evolution happen, and it's all been open, so to speak, for us to watch. So, yes, am I excited? Oh, heck yes. I am because the the future does look pretty bright here. I'm interested to see whatever's in the art community do with next.

I've had some brief conversation with George Stagg about it. He's intrigued by the possibilities as well.

[00:12:46] Mike Thomas:

But, yeah, the future's looking pretty bright. So I'll be honest with you, Eric. Before the Rix package came out, Nix was sort of intimidating to me. I had built up a lot of, you know, personal education around Docker, and containerized environments. And when Nix came along, I don't know, it just felt like too big for me to, you know, wrap my brain around at the time. I've switched since then because this Rix package makes me feel, and the 5 minute tutorial video that Bruno has put together, showcasing, you know, how to get up and running with this package, makes me feel like it's so much more accessible.

And I think that sort of the our functions, which I think there's only 6 or 7 in this package right now that help you get up and running, with a Nix environment for your art project. Take care of a lot of the things that may have intimidated me in the first place about Nix, and and sort of make it much easier to to get started with. And if you watch this 5 minute video, you you start to realize that maybe it's not maybe it's not all that intimidating, after all. And I think, you know, one of the potential benefits, as you alluded to, Eric, of having Nix as compared to maybe the Docker and and our end environments is that your Docker images that you create can get fairly heavy fairly quickly.

And I think that there may be use cases out there where Nix would do the same thing, but the end product would be a lot more lightweight. That end environment might be a lot more lightweight. And that might be really important, depending on the the type of project that you're working on, depending on where you're trying to deploy to, you know, on the edge or something like that, a a system that doesn't have a lot of space to store, you know, large Docker images. So I I I'm really intrigued by the next project. I think this Rix package makes it a lot more accessible. I think the, fact that this package is now part of the rOpenSci project says a lot about the amount of work that Bruno and the others who've contributed to this package have put into it, and its potential importance in data science as a whole. So, very excited to see it hit crayon, and very excited to to start playing around with it because right now, at this point, there's there's no excuse. I think Bruno has done all of the work to make this as easy as possible to get started with for our developers like us.



[00:15:23] Eric Nantz:

Yeah. And and and speaking of the ways of making it easier, when you look at what the Rick's package offers you, there are 2 other additional nuggets that even go above and beyond, in my opinion. One of which is being able to leverage an another service out there for next package binary is called cachex to be able to help build an an environment and have it cached there on their infrastructure. That way when somebody pulls it down, is not having to recompile a package of some source. It'll get binary versions of the packages and other dependencies of that particular project, which, again, if you're familiar of running R on Linux, you love having at your disposal binary installations because that cuts the installation time from sometimes hours to minutes, if not seconds. So you you'll definitely wanna you'll appreciate that when you find yourself in those situations.

But he's also put this additional nugget here. Bruno has also been one of the bigger proponents of the targets package by my good friend, Will Landau. And there is a function in here called TAR nix GA, which will help you run a Targus pipeline, bootstrap by nix on GitHub actions. Oh my goodness. Like, that's almost sensory overload of all the integrations there. But what a great fit for having an automated data science pipeline, but then next having the full control on your installation of your dependencies.

That that's massive. I I I can't wait to play with that myself. Yeah. No. It's incredible, and I think that

[00:16:59] Mike Thomas:

Bruno has focused on those types of applications where I think Nix can be the most powerful, you know, in terms of GitHub's actions. We have functions here that are wrappers, as you said, around, you know, building an environment on GitHub actions and making sure that, you know, after it's run the first time, everything is cached on that cache server, so it's gonna run a lot faster. The second time sort of makes me, think of, you know, Docker Hub. Maybe you could sort of, equate cachex to that, but I think there's some differences there. But I think the idea is is fairly the same. Right?

So, yeah, I I I think just the applicability and the the usefulness and new utility that Bruno has built in to, sort of the concepts behind this package are are pretty incredible.

[00:17:47] Eric Nantz:

Yeah. So I invite you to not only go to his post, but also the Rick's package site. Again, very well documented. There are a lot of vignettes, and they're ordered in a logical way to kinda start when you're brand new to Rix up until some of the very advanced use cases. So the the website looks absolutely fantastic. Again, a great fit for our OpenSci in general. So, again, it's always a team effort when these packages come up. So I wanna make sure I give kudos not just to Bruno, but also Philip Bowman, who is one of the coauthors of the package. I've seen their their work on the GitHub repo. There's been a lot of commits, as I mentioned earlier. And, this is only the beginning. Like, I can see lots of improvements coming along to Rick's end, especially as adoption grows.

Heck, I I if I get more time, I wanna help contribute to this r and piece of it. Because, again, that is somewhat scratching my own niche, but I can't be the only one that wants to transfer some legacy projects that maybe to to have a more robust workflow with Nix, down the road. So we'll be watching this quite closely. And one other mind blowing thing that, again, when you have the next package that kind of blew me away when I first did this is that with Rix itself, if you want to get one of these files, you know, boostramp or the set of files boostramp for a particular project, you don't even need R on your host system to even get Rix running at first. Because with the next package manager, you can have a special command that basically shells out to compo to grabbing the the the R environment that Bruno has baked into the the package on GitHub and then pull that down interactively in what I call an interactive shell.

You can then run RICs on that directory and get the the necessary files in that temporary environment and then exit out. I guess it's similar to what Docker gives you, but it's still it just can't I can't believe that's even possible. So on this new laptop that I just got, I don't have r on it. It's all RICs. RICs or Docker. There's no r trace of it because each of the project has its own environment, and I didn't even need r to get RICs up and running. That how what magic time are we in, Mike? I just can't believe it. Gotta love it.

Well, we're going to exercise a little bit different part of our brain on this next highlight, Mike. We got a fun little programming adventure for one of our fellow rweekly curators this week. And this blog post, this highlight, has been authored by Jonathan Caroll, and he is continually pushing the envelope and trying to solve what seems like almost all the programming problems that he can come across over the next year or so that he's been doing this series. And he comes to us this week to talk about in place modifications in R and other languages.

And this all stemmed from a recent post or a video that he saw based on a problem, which in a nutshell involved having an array of of integers, and then you define some integer k and then another integer he's calling multiplier, where the task is you've got k operations to perform on this vector of numbers. And in each of these operations or iterations, you've got to find the minimum value of this vector. And, of course, if there are multiple, you know, ties in this, you just take whatever appears first. Then you replace that number you found with that number times the multiplier, and then rinse and repeat until you've exhaustively gone through all these k iterations.

So when the video he saw that there was a pretty elegant way in Python to do this, And in particular, this Python solution is taking advantage of what it has built in a compound assignment type operator, the star equal to that to that result. That is, again, calling it an in place modification. Well, r itself doesn't quite have that built in operator. He does have a solution, but that solution is technically speaking making a copy of that updated number every time. You can inspect this when you look at the memory kind of hash of an object in r, and you would see that after that operation, which looks like it's in place overriding.

No. It's not. It's actually making a copy of that object updated copy of it. And this is another case wherein in John's r solution, he has to use a for loop. He's not able to use the map function because the iterations actually depend on what happened before it. So Markov chain. Markov chain. Oh, give me flashbacks to grad school, my friend. That's, that's some frightening stuff. But, yes, Markov chain's for the win here. So as he's going through this, he realizes that the closest thing that he's seen in the R ecosystem to this in place modifier is is from the Magritter package.

If you've used this before, it's the percent sign, less than sign, greater than sign, and another percent sign. I just short aside here, this operator has given me fits for debugging, so I stopped using it many years ago. Did you have those same issues as well? I've heard the same. Yeah. In stories. Never used it.

[00:23:31] Mike Thomas:

It scared me just just visually to look at, but I had heard similar things.

[00:23:38] Eric Nantz:

Yeah. I I lost, I lost a few hours one time. I was trying to be too cute with a deep fire pipeline, and I I wrecked havoc on myself. And I was like, nope. Not gonna do that again. None nonetheless, as, John's exploring, what possible ways could he kind of replicate this in place modifier in r itself? Because his motivation is, yeah, when you have a small variable name, it's kinda easy to take a reference that variable that's probably as a vector or some numbers. And then in the square brackets, call a function on that vector to grab the right element, but then reassign that a new value.

He says them says in this, well, what if that variable name is long? We don't have to keep typing that over and over again. Is there a more convenient way to do this? Turns out not really easily unless you take some, rather risky shortcuts, and that's where I'm kind of gutting to this meat of of the solution here. Again, this is giving me flashbacks to some bad practices I did in my earlier days where you're looking at the parent environment of the function, you know, that's being called in and then using a combination of the assign operator and then the deparse substitute for that variable and injecting into a basically a copy of that, a hidden copy of it.

Oh, you tell about the bugging nightmares. A lot of times in my early days, I would either get code from my old projects or a collaborator that did this, and it was so difficult to make heads or tails on what was happening there. But that is kind of the closest John was able to achieve with this solution, but he acknowledges he would not really wanna do that in production, that kind of solution. So he started to look at other languages and what they offer. And sure enough, in the Julia language, there are ways to have this in place operator with a slightly different syntax that or they call mutating, where they give the name of a function but then an explanation point afterwards and feeding in the name of the object that you want to modify in place.

And sure enough, he shows her this example of, like, reversing the order of numbers that you can, in Julia, get that in place modification of a simple vector without having to call the name more than once. So that that's pretty slick that Julia has that. And he experimented with that a little bit more, started to look at, you know, how could he do more elegant solutions to this, kind of do a little hacking around it. And so he's got at the end of the post a little modified version of this as based on, like, vector operations. He's calls that set underscore if.

And sure enough, it kind of does the job. But he's wondering if he could avoid if he can make that even easier and if maybe there's an r solution after all. So another update at the end of this post. Turns out John did end up writing a solution to this in r after all. He's got a package called Vec. It's available on GitHub. I'll link to it in the show notes with some of these features kind of baked in. And he said that there are some suggestions already from members of the community that in the base of what Vec is going to do is give you a new vector class that is gonna help with some of these in place modifying another optimized operations of vectors. So, again, great to see this. I haven't actually seen this yet.

And then yeah. So it was a great a great exploration into what's possible with these in place operators and gotten the learning journey of seeing r not quite having enough. And then in the end, like anything, you don't know if it you don't see any something there? Write a package for it. So fun journey nonetheless and a good little brain teaser once again that enters the highlights here.

[00:27:52] Mike Thomas:

Yeah. Absolutely. It was fun to, I guess, just see how a a leak code problem, could be, you know, as Jonathan, I feel like often does, push to its limits in a lot of different ways and then turned into an r package that potentially may have some practical applications for folks looking to to do this type of, you know, in place replacement, if you will, without consuming, more memory than necessary, you know, as we have typically had to do in r. And this new Vec package that he's created is is pretty interesting right now. It looks like it's fairly early on in its life. There are a couple of issues out there, looking for folks to help implement, some generics for, these these Vec objects, which are this new class of of vectors that hopefully will be able to to do some of the things that John mentions in the in the blog post, but have a lot more extensibility than what we get out of the box in in base r with, vectors. So that's pretty interesting as well.

It looks like in the GitHub issues, there's some discussions around this concept of of broadcasting, which wasn't a term, I believe, that I I saw in the the blog post, but, I think it intersects with maybe some work that Davis Phan has done as well on a an R package called R Ray, which I'm taking a look at at now. So it'd be I guess, this is all, you know, pretty interesting stuff as we're we're fairly in the weeds here as Jonathan often does in terms of taking a look at at really the internals of r, and I I really love the comparisons to Python and and Julia, and and taking a look at, you know, what's available over there and bringing it potentially into the r ecosystem. I feel like that's a concept that we see more and more of nowadays.

I've seen a lot of conversation lately about the the polars package in Python and and its potential, inspiration from the tidyverse. Right? So we have, I feel like, a lot of cross pollination going on to try to build the best products possible and and take a look at, you know, what are the best concepts across all programming languages, such that when we do create something new, we're trying to to create the best version of it. So very interesting blog post, by John. Cool thing about open source is that we can, you know, develop something that that somebody else can use pretty quickly here when we come up with an idea. So I highly recommend checking out the vecvecpackage if, this is a topic that is of interest to you.



[00:30:29] Eric Nantz:

Absolutely. We got that linked in the show notes as well. And, certainly, if not only is John's, you know, post insightful about his current take on these solutions, There's always, like, I wanna call history lesson per se, but he was venturing into some of the ideas of other programming languages. Some of them definitely have a a long history, you might say. Even this language that he references called APL, which at first I thought was a license abbreviation, but, no, that literally stands for a programming language, which has a book itself dedicated to it. So he's he's I love the way he intersperses kind of the lessons he's learned from these programming, you know, brain teasers, these exercises, but seeing the mindset across whatever languages are doing. So, certainly, if you wanna sharpen your dev toolbox, John's blog is a is a great place to go.

And rounding out, we're gonna get a bit, you know, back to the practical side of data science in our last highlight here because we have a very great showcase of some of the valuable insights that we've seen from recent sessions of the data science hangout, which is, hosted by Rachel Dempsey. And as of recently, her new co host is also the author of our last highlight today, Libby Heron, who has just joined POSIT as a contractor for helping with the data science hangout and also POSIT Academy. She authors this terrific post on the POSIT blog called Data Career Insights: Lessons from 4 Senior Leaders in the Data Space.

So we're gonna walk through 4 of these examples here in our in our roundup here. But first, I just want to mention that hangout is a great place, a great venue for those that are looking at learning from others in the community on their data science journey. If you're not familiar with the effort, there is what we call a featured leader or featured, you know, maybe co leaders that Rachel talks with and Libby talks of as well in the session, but it's a welcoming environment. Anyone at all are welcome to join no matter which industry, no matter which spectrum of data science you fall under.

I had the pleasure of being on that about a year or so ago. It was it was a a a great time. We've we've learned a lot even just through the questions that we would get from the audience, and the chat is always on fire, as I say, whenever whenever those sessions are on. There's lots of awesome insights on there. So she has links to all that in the blog post that we're mentioning here, but we're gonna give our take on some of the lessons that are highlighted in this post here. So the first, insight here is focusing on learning how things work and not being afraid of this feeling that you should already know how things work. I boy, does this resonate a lot? So this came from a conversation with Jamie Warner who is a managing director at Plymouth Rock Assurance, where she was talking about, yeah, you may have heard some advice before, but in the data science world, but there may be a legacy process, a legacy model that some it may be explained to you, but then there may be other situations where you may not know exactly how something works.

But when you are insight you're inquisitive, I should say, and you talk to a subject matter expert about, you know, what what is the mechanics behind this, getting around their thought process, Yeah. That is amazingly helpful. People end up being a wonderful resource. You can only do so much on Internet searches in your company's Internet or whatnot. Getting to the source of an effort or an author of a tool and getting to know their mindset on how things work. This is gonna help resonate or help illuminate a lot of ideas for you.

There are times where I, again, will have impostor syndrome about some of the Bayesian methods that we use in our team on various tools. And in in in past, I would say, oh my goodness, Eric, you got a PhD. You should know how Bayesian works. No. I don't know how long the Bayesian algorithms work, and that's okay because I can talk to the some of the my brilliant teammates who have authored these solutions. And lo and behold, we had a great project that we're about to release in production that incorporates some, you know, fairly standard with a little sugar on top Bayesian models for clinical designs, but I was not shy about learning about why they chose certain approaches for, like, a binomial approximation versus a normal approximation and things like that. So I still have to catch myself sometimes. I'm thinking, oh, Eric, you've been in that company for 15 years. You should know about that process. No. It's okay. It's okay. It's better to ask than to not ask at all. That's my biggest takeaway from that.

So moving on to another great advice and this one coming from JJ Allaire who, is the founder of Posit. He this this part is titled getting clear about what you like doing and then finding teammates that are excited by the stuff that you don't really like doing. So in JJ's example, he mentions that he enjoyed, you know, bootstrapping these companies or founding these companies such as Rstudio, which became POSIT. But he's not exactly one that likes a lot of the managing kind of stuff that goes with, you know, running your own company. And I'd imagine, Mike, you you deal with this quite a bit in your your daily work as well. So one of the in JJ's opinion, one of the best decisions he made was helping expand his team to help those managerial administrative type tasks of running a company so that he could focus on what he's interested in is product development and really getting into the technical weeds of these solutions, which if you follow JJ's path of Rstudio and now posit, you have seen how active JJ has been in ecosystems like Rmarkdown, now Quartle, and other efforts as well. It's not just that he's the founder of Pozid. He is one of their top engineers in getting these efforts off the ground. So all you have to do is check the GitHub repo, and you can see him on top of many issues, on top of many poll requests. He is really in that. But he surrounded himself with people like Tariff and others at Pasa that are helping running, you know, the the enterprise aspects of a company while he gets to really do what he is passionate about. Again, that product development and getting into the technical bits of these products that he's working on.

So really great insights there. And and each of these, you know, kind of summaries here, there are direct links to the recordings of these actual Hangouts on YouTube, the archived Zoom, you know, Hangout sessions. So if you wanna hear it again, you know, from the source, you you're able to do that. But there's some more nuggets. Mike, why don't you take us through those? No. That's some great advice from from JJ, especially as a as somebody running a business that I can

[00:38:06] Mike Thomas:

definitely appreciate and probably need to to listen to a little bit more about delegating and making sure that you're you're focusing specifically on the the the most important parts of your job function. 2 more that we have in this blog post are from Emily Riederer and Ben Aransabia. Emily's is all about essentially making sure or or understanding that, you know, sometimes the the less flashy parts of data science, maybe not building the the fanciest machine learning or now it's AI, right, model, maybe the more gratifying parts of your data science job and your data science journey as well. And that it's important to even as we see, you know, in all the marketing hype that the the machine learning and the AI and the the ML stuff, is is, you know, maybe the stuff that gets the the most press out there, taking the the time to to learn the maybe less sexy parts of data science, is is really important to not only building your foundation, but you might find that, you know, sometimes those are the the most interesting parts of of your day to day work and being open to all of the different, you know, facets of data science and understanding that that maybe there are some of these facets that are underappreciated, but, might essentially provide you with with the most gratitude in your your job on a day to day basis. One that I can think of is is our package development and our package management, something that I absolutely love to do. I love to be able to build in our package that somebody else can install, and and and run on their their own machine and all of the different less glamorous things, right, that go into maintaining that package, ensuring that all of the different, components of that fact that package and the different requirements that that go into in terms of of files and things like that that go into ensuring that your R package is is robust and, you know, less as as less prone to to troubleshooting or or errors for somebody else as possible, is something that I actually get a lot of gratification from. And it's it's why we build our a lot of our packages, both open source and and proprietary at Catchbook for others to use. So I really appreciated, that advice there. Emily always comes with really, really practical advice. Her, talk at positconf this past year on Python ergonomics was super practical.

And, yeah, anything that that she puts out there into the world is something that I enjoy consuming. And as I said, Ben, who's the director of data science at GSK, I think had some some great advice that's probably more general than just even if you're in data science, and it's the importance of saying no. Once in a while, Eric, I have to say I have to say no to you or or no to this podcast. And and as much as we absolutely hate to do it, sometimes we have to ensure that we're keeping the correct balance in our life. And by by saying no, you know, it can be a powerful word, but it can bring peace, I think, in your day to day life, not only, you know, professionally, but but also personally because we we can't do it all as much as we often try to and as much as I know you often try to, especially. So this is was a great reminder as well.



[00:41:42] Eric Nantz:

Yeah. I I find myself living that a lot lot these days. Yeah. I I'm okay with the nose from you. Sometimes my kids on your overhand, sometimes they say no a little bit too much, but that's a different story altogether. Oh, gosh. Oh, mighty. Yeah. In any event, yeah, I another example that I thought of where it's a combination of saying no to something, but then also finding a better way to do it was I had, joked with some colleagues at the day job where I made a shiny app a few years ago that was in essence a glorified PowerPoint slide generator based on dynamic data coming in with some ggplot2 based vector graphics going into, a PowerPoint via the officer package. Again, awesome tooling. Trust me. I I really love the officer package for when I need to do this stuff. But then the time came where I was kinda asked to do this again in a more, I guess, a more simplified way, albeit trying to use the same machinery.

And I did try. Like, I really tried, but this template just was not gonna cut it. It just was not gonna cut it. And I could have brute forced my way. I could have tried to find some clever algorithm to modify the text size based on number of characters going into these cells or these areas of a PowerPoint site, but I thought, no. You know what? There's a chance to do it a better way. Quartle came to the rescue. But quartle dashboards, a responsive layout for this template where it didn't matter how much text was in it, they could expand that little, you know, expander button in each of the boxes and then get a full screen view of that particular area and then wrap it back down and just zoom in, zoom out as needed. That team loved it. But I I I I probably did too much time trying to retrofit the old approach, but trying to know when to say no, even if it's just to yourself when you're trying to come up with a solution, is is a is a is a skill that I've had to learn from experience. I'm not great at it, but that was an interesting example this year. I could have said I could have said no sooner and done the portal thing right away, but it did. Sometimes you do have to struggle a little bit in these experiences to make it happen. But, yeah, I I really resonate with everyone in those posts, and there are all sorts of those nuggets on each of those hangout sessions. So if you've never been there before, again, Libby's got all the links in in this post to those previous recordings, but it's super easy to get that on your calendar and join every Thursday at noon PM EST for those wonderful hangout sessions.

And so, yeah, I came away, you know, really excited after I had my little session last year. So, yeah, lots and lots of fun stuff there. Alright. And there's a lot more fun to be had when you look at this, some entire issue of our weekly that we have curated by Tony this week. And as much as we love to geek out and talk about the rest of the afternoon, we're gonna leave you a couple additional finds before we close-up shop here. And I told you earlier in this episode, not too long ago, that sometimes I feel a little impostor syndrome of Bayesian modeling because I did not have a lot of training of that in grad school, and I have some really brilliant teammates. I can do wizardry with it.

Well, sometimes I just like to see, like, relatable examples to help connect some dots, if you will. And this this post here from Rasmuch Bath is titled a Bayesian Placket Loose Model in Stan that's applied to, of all things, pinball championship data. So if I I love me some pinball, man. I have lots of lots of quarters ways on that in the arcade when I was younger. But what a great way to illustrate these great concept of taking, what in essence, our pairwise comparisons between different items of something such as when you have, you know, 2 teams or 2 players head to head and be able to predict them what is the likelihood of winning those particular matches based on this custom Bayesian Placket Luce model.

So, they got they got the great, you know, you know, set of formulas here with the r codes of reproduce the visuals and then how to turn that into a stand model, which is, you know, a very robust way to fit Bayesian modeling, looking at all the trace spots for kind of the best fit. And then in the end, a nice little visualization, kind of hybrid line chart with the dots and the and the skill the skill of each player to see just who is kind of ranking above the rest in a somewhat forest plot like diagram. So lots lots of interesting nuggets here for a Bayesian novice like me with a domain that makes you wanna turn back time 20 years and go back to the arcade in my hometown.



[00:46:45] Mike Thomas:

Mike, what did you find? I found a great blog post from, Ma'el on it's called cover and modify some tips for our package development. A lot of it is about, maybe the workflow and and types of tests unit tests that you may want to incorporate when you are developing a test for a function that already exists. So some great, you know, little pieces of advice here, include, you know, take a look and see in the roxigen documentation above that function if there is any example code that you can leverage, to within the unit tests that you need to write for that.

These are 2 types of tests that I I commonly write. It's, tests that expect type, and and, you know, you're setting maybe double or character or or or, maybe a table or something like that, a data frame, to make sure that the function is returning the type of object that you are expecting, as well as, you know, testing expect equal and actually maybe running some data through that function and ensuring that it is spinning out the the number or the value, or the character string that you would expect it to return.

And as Ma'el points out, it turns out that those two types of tests have a name, and they are called characterization tests, that lend themselves to this concept of cover and modify. So some really interesting little nuggets in here. Again, you know, anything that Mel authors is is kind of required reading for me. And I as I just mentioned in the in the last highlight that one of my, passions, if you will, is our package development and management. This is really applicable to that.

[00:48:27] Eric Nantz:

Yeah. I feel like you and I need to be accountability buddies to put these principles in practice because Yep. My, test driven development skills are not quite up to snuff yet.

[00:48:38] Mike Thomas:

You and me both.

[00:48:40] Eric Nantz:

Yep. Yep. Well, we'll keep each other honest on this. But, yeah, it's amazing to see as my mentions in the early part of this post, how she's been working with the iGraph r package quite a bit, all these different lessons that she's learning and these different techniques. Isn't that the way, though, we find ourselves knee deep in a project that may not always be comfortable at first, but then we're adapting to, you know, finding new solutions. And then you can take those solutions and use them in the rest of your, you know, development workflows.

So for me, that whole portal thing I just mentioned, I'm now a huge fan of chordal dashboards because of what I learned because of, you know, putting that for the paces. So, yeah, we there's always something new to learn. Only only one one brain in my head, but I feel like I wish I had 2 or 3 of them so I could keep this track for. If I could convince all my clients to accept

[00:49:29] Mike Thomas:

HTML output, I'd be right there with you, Eric.

[00:49:33] Eric Nantz:

Yeah. I don't say I have it all figured out yet, but at least in this one case, I was like, which one would you rather have? And everybody said, I like that web version. So we're we're on the way. It's much nicer. Yep. It is very much so. You know, the Rwicky project itself is nice for your r learning. And then again, it is a community based effort. We we thrive and we frankly depend on the contributions from the community as we go through your awesome resources that you're authoring on your blogs or authoring these great packages, perhaps even compile with Nix. Who knows? But if you wanna contribute to the r weekly project, there are a few ways to do so. First of which is if you find that terrific resource, put get it in touch with us because we can do that via a poll request on the main repo at rweekly.org.

There's a link in the upper right corner, that little octocat ribbon there. You can click that and get taken directly to the poll request template in markdown format, all markdown all the time. You get a very quick primer with that template of what the contribution should be, and you are on your way. And, also, we love hearing from you for this very show. We love, to hear have you give us a fun little boost if you're listening to one of those modern podcast apps. I just listened to a a fun webinar about the podcasting 2 point o landscape, really nice insights to some of the top developers.

So I may put a link to that in the show notes if you wanna listen to the the post recording on that. But, also, you can get in contact with us through our contact page, which is linked in the show notes of this episode. And we are on social media sporadically sometimes, but I you can find me mostly on Mastodon. I am at our podcast at podcast index on social. I am also on LinkedIn as well. In fact, I will be sending a note probably in the next week or so when we put the recordings out from our recent Gen AI day for our pharma that took place a couple weeks ago. There'll be fun fun talks to to watch there. I'll put an announcement on that shortly.

And then also you can find me sporackling on that weapon next thing with at the r cast. And, Mike, where can the listeners find a hold of you? Yep. You can find me on mastodon@mike_thomas@phostodon.org,

[00:51:47] Mike Thomas:

or you can check out what I'm up to on LinkedIn by searching Catch Brook Analytics,

[00:51:52] Eric Nantz:

k e t c h b r o o k. Awesome stuff. Yep. Definitely a fun read as always whenever you put your post out there. But we can't have my fun much longer. Our day jobs are calling us back, so we're gonna close-up shop here, and wish you best of luck in your data science journeys for the rest of the week, and we will be back with another episode of our weekly highlights next week."
"38","issue_2024_w_39_highlights",2024-09-25,42M 37S,"How the latest release of patchwork is saving a cozy space for gt tables, a new package in the ggplot2 ecosystem to lend a guide for your guides, and a prime way of using R to brute-force the answer to a mathematical brain-teaser. Episode Links This week's curator: Tony Elhabr - @tonyelhabr@skrimmage.com (Mastodon) & @TonyElHabr…","[00:00:03] Eric Nantz:

Hello, friends. We're back of episode a 180 of the Our Weekly Highlights podcast. You know what? That's a cool number. Maybe we should 180 and call this the SaaS Weekly Highlights podcast. Oh, oh, I get it. No. I'm just kidding. No. No. No. No. Buckle up, folks. We're we're still the rweeklyhighways. I couldn't resist. Anyway, this is the weekly podcast where we're talking about the awesome highlights and other resources I've shared every single week at rweekly.org. My name is Eric Nantz, and I'm delighted that you join us from wherever you are around the world. We're back at our usual recording time. But, of course, just like usual, I never do this alone, at least not unless I'm forced to because I still have my trust to cohost Mike Thomas with me. Mike, how are you doing this morning? Doing well, Eric.



[00:00:47] Mike Thomas:

Had a look a tiny, tiny little time change. I appreciate you being accommodating here, but we're we're getting it in.

[00:00:54] Eric Nantz:

We're getting it in. One way, look like crook. No matter if we're moving or doing chaotic virtual events, we always find a way. Right? Exactly. Exactly. And just like finding a way, our weekly finds a way to release an issue practically every single week, and that is not possible without this being a community driven effort by our curator team. And our curator this week is Tony Elharbar. Of course, he was, made famous a few years ago with his participation in the slice competition. I saw a fond memories of watching that. But, of course, he had tremendous help from our fellow Arrowrocki team members and contributors like all of you around the world with your poll requests and other suggestions.

And we're gonna actually, for our first two highlights, we're gonna definitely deep dive into some novel visualization upgrades for your toolbox. The first of which is a package that has gotten a lot of positive attention since it was released a few years ago. If you've ever been in the situation where you had multiple plots that you wanted to put together in a seamless, like, multi figure, multi panel environment and may and have them be completely separate types of plots, Patchwork is your friend here. Patchwork is the r package that's been authored by Thomas Lynn Peterson, who is a software engineer on the tidyverse team over at Posit.

And in particular, Patchwork just had a major new release, version 1 dot 3 dot o. And we're gonna cover a few of the major features landing this release. And to start off with, I have some good friends in another, industry, we'll call it, named Bubba Ray and D Von. And, boy, they sure like their tables. They were telling me, Eric, get the tables for Patchwork. Well, guess what? We're gonna get some tables for Patchwork because in this release, you are now able to not just put a ggplot2typeplot in patchwork.

You now have first class support to put in a table created by GT. GT has made a lot of waves in the table generation ecosystem and r, which, of course, has been authored by Rich Yone, who did a wonderful presentation on the past and the future of GT at Positconf. But a lot of this feature is possible because of yet more terrific contributions to the open source GT package. This one, authored by Toon Wendenbrand, who will be featured on another highlight later on, but he laid the foundation so that GT objects can now be composed or rendered as GROBs.

Where if you're not familiar what a GROB is, that is the underpinning type of object that the system used by ggplot2 is using to compose the plots together in the underpinnings. So with both now a gt object as a GROB and, of course, ggplot2 objects as GROBs, they now can fit seamlessly into patchwork. And we'll just, I'll cover a couple of the major things about this new feature. And, again, this is brand new, but already out of the box, you're gonna have terrific support to just literally just add this GT object in your patchwork call. Just literally adding the 2 objects together, and you're gonna get the patchwork rendered composition right away.

And you get some niceties under the hood or as part of the functions with this because you will now have first class support for adding titles, subtitles, captions, just like you would in a normal GT, and they will show up just fine in a patchwork composition as well. But if you wanna customize things further for how tables are wrapped into this, there is appropriate enough a function called wrap underscore tables that's gonna let you customize how the table utilizes, say, the space around it. So in Patchwork, each object gets put in what's called a panel.

And with this wrap table, you can actually define which parts of the table are fitting into that panel. By default, it's every part of the table, which includes, like, row labels, titles, column headers, and whatnot. But you can actually customize which parts are actually going outside the panel area, such as maybe the top, the left margin, the right margin, and what have you. That's nice if you wanna line things up in a more custom way. And then also with that, there are some other enhancements as well to how you define the layouts. And what's cool is that it's even gonna be intelligent enough if you're doing, like, a top down composition and your table and your plot are sharing, say, the same variables in, say, rows or columns.

They will automatically kinda line up next to each other. So your your eye can visually track maybe that plot at the top with that particular variable and then the table at the bottom and line those values up. It's really slick how they do that. I have no idea how they did it, but it's a lot of awesome engineering around it nonetheless. And the one the one caveat I wanna mention has to do with accessibility. Because under the hood, when these tables are rendered as GROBs and then put into a patchwork composition, the table is converted to an image PNG representation.

Whereas by default in GT, if you're just creating the table, the default output format for that is HTML, which, of course, HTML. You can actually get the text that's representing that table and use that in accessibility type features. So if you are going to put a GT table in a patchwork setup, make sure to especially if it's going in, like, a report, publication, or whatnot, make sure to complete the alt text for that entire patchwork to include not just the text around the plot itself, but also the text around the table. It might take a little getting used to, but if you're already well versed in how accessibility features in GBOD 2 works, it should be a pretty seamless workflow for you. And it will also have a link to the the show notes for additional content with regards to the GT release notes where this, GROB feature landed if you want to learn more about how that worked.

But wait. There's more, as I say, in this Patchwork release. And if you've been looking for a little freedom in your Patchwork compositions, well, that has you covered with a lot of bells and whistles around that, doesn't it, Mike? Yes. It does. And I believe in

[00:07:46] Mike Thomas:

the previous version of Patchwork that was released, there was this new function called free, that was added to the API. And I know that Thomas Lynn Peterson, who I think is is one of the leads on the Patchwork project Yep. Not only talked about, you know, this function at at PasaComp last year, but was also really wanted to be really thoughtful about, you know, how this particular function called free was was going to work and and really careful because, you know, as you noted, Eric, Patchwork does some pretty incredible stuff for us, under the hood that, you know, you and I don't even understand necessarily how it works in terms of aligning axes and and things like that.

So so Thomas wanted to be really careful, and I think that the Patchwork team wanted to be really careful that, they made the correct decisions when it came to how this new free function was going to work. So there's there's some additional arguments, that are now introduced into this free function that allow you to better sort of, align, you know, multiple charts with each other. So one of those arguments is is called side, which allows you to supply either a a character t, r, b, or l. I think that's top, right, bottom, and left characters to indicate sort of which side you want to free up and not necessarily, you know, fix, axes and things like that.

So there's, you know again, we're doing podcasts on data visualization. You're gonna be best served if you take a look at the blog yourself, but he is a great example of, using the side equals l argument to to free up the left side of the plot, while the the right side of the plot sort of stays fixed, if you will, with the the plot below it. There is also, a type possible argument within the free function as well, which prior to, that type argument being introduced when you were putting 2 stacking, you know, for instance, vertically, 2 plots on top of each other, sometimes a plot that had a an axis label, that axis label would be pushed way out, if the other plot above it, if you will, in in this particular example, did not have an access title.

So that that access title in the the one plot that did have an access title was kinda floating way out in space in in situations where you had, long, sort of, access labels, on one plot. And, again, the the blog post does a great job of demonstrating an an example here. So now we have this type equals label, argument that we did not have previously that pushes that axis label on the the one plot that does have the axis title, excuse me, much closer to the axis labels themselves. So it's not that axis title is no longer, sort of, floating out in space, which is is really helpful. There is also a type equals space argument, that sort of just better condenses, from what I can tell, you know, your plots and the the white space in the margins between your plots, which is always something that I've had a tricky time doing, you know, especially when we are trying to in a lot of instances, you know, we might be saving this as some sort of an image file that we're then putting into a a quarto presentation or or something like that.

And, you know, white space becomes very important when you were trying to fit a lot of content on a slide, for instance, or in a PDF. Right? So we have this new type equals space argument that allows us to to better manage that. And I think that there there might be a couple additional bug fixes in this highlight as well, but the new functionality, you know, is is really helpful. I've been doing a lot of work lately, unfortunately, in matplotlib. Some of this stuff is equivalent and possible, but some of it wrestling with with spacing and white space and what are called subplots in matplotlib, is is very, very tricky. And I wish that I had the patchwork,

[00:11:52] Eric Nantz:

package available over there. Yeah. So once you start using it, you don't know how you live without it. I mean, this has been an issue that I wrestle with so much back in my earlier days of helping do a lot of biomarker analysis and creating custom PDF reports so we wouldn't just have one type of plot. We would have multiple types and then trying to get into the grid system to do all the alignment manually. It was a major undertaking. Patchwork is just giving you such a elegant API on that fits so natively with the ggplot2, you know, layered philosophy of the grammar graphics. It it really is hard to replicate this across different different languages. So anytime I need these type of gg plots to be present, especially in a report, yeah, Patchwork is gonna be my go to for it. My greedy ass has been and I've been not the only one asking this, but if you recall, there was some work to create animated ggplot. So ggAnimate that, Thomas took over from David Robinson.

Oh, I'm so greedy. I would love that patchwork work with those, but that's okay. I I can I can wait for that because this is already spectacular in terms of the compositions we are have at our disposal? And now with GT, the possibilities are even more more endless in my opinion. And like many things in the r community, there's a lot of synergy in respect to this issue because our next highlight is coming from that same contributor I mentioned that helped build the foundation for getting GTE pack tables into patchwork.

And this next highlight is a brand new art package that has hit the ecosystem authored by Ton Vandenbrand who is a PhD student at the Netherlands Cancer Institute. And, apparently, he's been working on a lot of ggplot2 stuff in his previous open source efforts. But in particular, this new effort is a new r package called gguidance. I have to catch myself from saying ggguidance because of ggplot2, but, you know, hopefully, I got it right. But, nonetheless, if you've ever wanted or found yourself in a situation where, yeah, you get a lot of nice customizations already out of the box with respect to your guides and other aspects of how you, you know, customize your axes, labels, and your legend attributes in ggplot2, the vanilla version of it.

G guidance is gonna give you kind of a more elegant way to bring some additional customization with some of its own versions of these guides. And the and this, vignette that we're gonna be highlighting here and that we'll put in the show notes as well as a nice summary of 2 kind of major categories of these features. I'm gonna lead off with these really great functions that are prefixed with guide underscore axis. So these are tailored towards bringing some additional formatting possibilities to the way your axis labels and tick marks are specified.

The first of which is guide axis custom. And this is great for when you have, you know, your typical two dimensional plot with an x and y axis, and those axes are some kind of positional type metric, maybe a discrete scale or or a bin to continuous scale. And this is great when you want to add a little customization to it. Say one of the things that comes out of the box is on the tick marks. You can add what are called bidirectional tick marks where, again, as an audio podcast, it's gonna be hard to hard to say this, you know, very clearly, but it gives you kind of both the the line at the bottom of that or the axis line and above it as well. So in case you want that additional that visual cue of where the axis label tick actually starts, that's a it's a nice little visual cue around that. But the real selling point in tones of vignette here about this function comes when you decide in your plot to switch maybe the coordinate system going from, say, the typical scatterplot like x y, you know, 2 dimensional layout to a more radial, you know, type, you know, circle layout.

In the past, you would have to also modify the guide yet again in your usual route to call when you switch those coordinate systems. But with Guide Access Custom, it is intelligent enough to know when you switch that coordinate type of system, and you don't have to customize it any further. So there's a great example in the in the vignette here where he goes from, like, the scatterplot type visualization to a, to a radio plot, but he could keep the guide, access call exactly the same. That's pretty handy when you have to switch these, you know, on the fly with a lot of friction in between.

And then another really neat feature, and I don't remember when this landed in Jigetwapattu proper, but now on your axes you can have nested axes where maybe you have in the case of the in the vignette here you've got a bar a stat a bar chart where you've got different flavors of a product of food maybe, you know, coffee, tea, apple, pear, or whatever. And then you've got a more overarching category for these. So you can put that overarching category underneath the appropriate labels, and you can do this with ggplot2 proper.

But what, is offered in this package is a way to customize how you're exactly doing that categorization, or maybe you have a leftover category in your nested top, you know, sublevel, and you don't care about putting that under any top level. So you just wanna leave that blank. You can do that by manually defining a key of these labels as a simple data frame like setup. But then you can also customize instead of just having a horizontal line that separates these heading or these nested labels. You could have, like, a, like, a curly bracket, or you can have, in fact, up to 7 types of brackets that come out of the box for that separation between the sub labels and the top level labels.

And, yes, if you're adventurous enough with creative enough, you can create your own bracket style by feeding in a simple kind of matrix of x and y coordinates. And in the example, he's got, like, a a jagged line instead, whatever you wanna do. That's beyond me. But the cool point is if you really want full control over the visual cues of these nested labels, you've got a lot of power exposed to you with this with this awesome function of guide access nested. So I'm definitely gonna be checking that one out, but there is a lot more to this with respect to color customization.

So, Mike, why don't you take us through what he offers you in this package? Yes. There is. There are these two functions,

[00:19:18] Mike Thomas:

guide call bar and guide call steps, which we'll talk about first, which are are pretty incredible that allow us to be able to either have, you know, sort of this continuous gradient gradient, in our legend, color gradient, or we can have, something that looks more like a discrete color gradient, where we have, you know, specific sort of bins, for the particular colors that we're going to show, in our legend that correspond to the colors that are shown on the plot as well. There's this, notion of caps as well, where you can sort of fix your color scale in your legend and have it bounded with a a lower limit and an upper limit. And then you can have a shape, that also has a corresponding, you know, perhaps different color. In this case, it's it's gray because, he's trying to show that, you know, these values would be out of bounds anything above 30 or below 10, and and he has a triangle on either side of the, the color guide here in the legend to represent, what are called these caps on the the legend or the guide, if you'll which are pretty cool.

You can also, you know, implement those caps as as shapes, and he uses triangles in the example here. But instead of, you know, having them be these out of bounds values, you could have them be, you know, these colors that, sort of, still correspond to your gradient that you're using in your chart. You can cap just one side, either the top or the bottom of the legend, which is pretty interesting as well. Just a lot of different flexibility to do things here. And then sort of similar, Eric, to what you were talking about in terms of brackets, you know, on the axes of the chart, you can also add brackets to the legend. So there's one example here where, he's he's sort of creating brackets on the legend that classify values, in a group a or, you know, if you're higher up in the gradient, then you would belong to to group b. And you can stick those brackets right against, the side of your legend, which is a really, really cool feature, something I had had never really thought about myself as well.

He he has an example here where the legend itself is is radial. It's it's a circle. It kinda looks like a donut chart or or a clock, if you will, where you have this this sort of polar coordinate system for your legend, which is really interesting. I can't think of a use case where I would necessarily employ that over a a legend that's, you know, just just rectangular. But it's it's wild that we have the ability to do that. So a lot of customization here, a lot of flexibility. I think, you know, one of my main takeaways in terms of the the most what I takeaways as some of the most useful functionality of this package is it's almost like chart annotation.

Right? Here's one example in here where, you know, the the x axis is time. It's it's years, and it's some sort of a political, I think it's unemployment. Right? Right? And it's it's, you know, intended to be a political type of chart. And he has these horizontal brackets underneath the x axis values that show the administration, you know, the name of the president at the time, you know, and those those values are gonna change based upon whether they serve 4 years or or 8 years or or different, you know, the the horizontal width of those particular bars. And, you know, I think that annotation is is a really powerful addition to the chart, you know, in regards to the story that you may be trying to tell.

Obviously, the the subgroups as well in the bracketing and the boxes, that we talked about, I think, can be really, really helpful for, again, you know, doing the additional things and custom things that we need in our visualizations to ensure that we're getting our point across as effectively as possible.

[00:23:14] Eric Nantz:

Yeah. I wanna, you know, dive in on that as I've been digesting this more since I read about it. I feel like this is really still trying its best with these additional aesthetic kind of customizations to keep the the the viewer's attention on the main event, so to speak, of the plot itself, but then putting some nice annotations around that main area without you having to really switch context and maybe looking at a a paragraph above below the plot that talks about, like, categories of these color ranges or, like, these different, like, inherent groupings that are comprising these data points, like the time axis or whatnot.

I think this is wonderful from, like, a best practices when you've gotta you've gotta nail that that initial look at these data. And there have been, you know, countless workshops about effective visualization with jigapod too, and those, you know, those great materials by Cedric Shearer and others are still must reads if you find yourself doing this routinely. I feel like this this package, g guidance, is giving us, our users, that don't really wanna dive into the realms of utilizing another program such as Adobe Illustrator or other software to do those, like, fancy customizations you often see in blog posts or whatnot.

Now we can do a lot of that here and especially with making it the striking the balance between overwhelming the user but then not having enough information. I think this is hitting the sweet spot even in this early stage, which I am, you know, thoroughly impressed for. This isn't even on CRAN yet. This is its initial release, I believe, of this package. So I'm I'm I am, like I said, very impressed by this. I'm definitely gonna recommend this both to to future collaborators. I wanna level up their ggplot2 visualizations with these nice annotations, and, of course, I may be using this in the future as well. So job well done, Tone, and, we can't wait to see what's next in this package.

Well, let's, take a little break from our visualization round up there, and we're gonna go back to school a little bit, Mike. And I hope you're ready because you're gonna have to dust off some concepts, ironically, my, oldest son is starting to learn about, in this last highlight here. And this was spurred on by a LinkedIn post that was discovered by Peter Ellis, who actually was featured on the highlights of last episode with his blog post on the Australian Census. Well, he's back in a decidedly different topic this time around, whereas I mentioned, he saw on LinkedIn this post about how a particular number, 397, and this was conjecture to be the largest prime number that can be represented uniquely as the sum of 3 positive squares.

So let's step back a little bit because, you know, I'm gonna dust off my convoys here a little bit. If you're not familiar of what prime number is, this is the concept where there are no smaller numbers other than that number times one that you can use to derive the product of to get to that same number. So a classic example, the number 5. Right? You can't do, like, 2 times 2 or whatever. It's gotta be 1 times 5 or 5 times 1. That's the concept of a prime number. And, Mike, you're gonna keep me honest here. We're talking integers.

Integers. Yes. Thank you. Integers. Yeah. Good caveats. Yeah. I knew I'd be rusty on this. But, as as Peter read this, he wanted in his mind, he's going to change the wording a little bit to this, as being 3 97 is a conjectured largest prime number integer that can be represented as the sum of 3 positive squares of integers in exactly one way. So had I been very much smarter, maybe there would be a way you could figure out a clever mix of linear algebra or other optimization to solve for this in a, you know, rigorous mathematical proof.

I'm not one of those people. So, like like, what my approach might be, Peter decides to take matters in his own hands. He calls it a brute force approach, which, actually isn't too brute force, but you can kind of get the logic of this as we talk through it. He decided to do a couple things for a given what we'll call max integer. He used 30 in this example. He first computes all the squares of those numbers, saves that as a vector. And then for a visualization later on, he's actually gonna leverage a package called primes, which I never heard about until this post, and I did some research on this.

This is a package that's authored by, that's authored by Oz Keys, who I have a link to in the show notes, for all sorts of derivations and testing with respect to prime numbers and r. So he's going to have that kind of on the on the back burner, but he's going to leverage the tried and true expand dot grid to take basically all those squares, that vector of squared numbers, and do 3 columns of them that represent the three numbers that have to be added to get to this, you know, magical prime number. He's gonna expand grid all the combinations of those and then filter for when those are uniquely different from each other and that the the sum of that is 397.

And sure enough, when he does that filtering in the blog post, you can see that it's numbers 964324 that get you to that final result. Now that's great for, you know, the smaller cases. Certainly, if you want to beef this up further, yeah, you're going to start needing some processing time. If you go above 30 for the possible other answers, you could derive here. But it's just he does some additional customization to count how frequently some of these other results might happen with these different combinations.

And, say, they're like for the number 54, he found that there were 3 instances where you can satisfy this requirement. But as you think about all the ways to explore this, this could balloon up pretty quick. So to wrap it all up, he's got this nice visual set of visual charts of ggplot2 to look at all the possible prime numbers, with a given range on the x axis and the number of times that these are these are occurring on the y axis or the number of ways that you can derive to that to that particular prime number.

And as as you can see, he does a nice annotation on the first one to show where 3 97 wise. And then as you bump up that number from 30 to a much higher, such as 10,000 and then ballooning it up even more, yeah, it's it's gonna it's gonna eat up some horsepower on your machine because, of course, default r is memory based. Right? You're gonna be making this massive dataset of these different combinations. So he conjectures whether we could have, like, a way to fit this nicely into a, like, a database structure, which sounds like a a nice challenge for you readers out there if you wanna try that out. But a nice little exercise that once again with the power of r and its, you know, capacity under the hood, You don't always have to have that fancy mathematician mindset to figure out data driven approaches to these answers. So maybe I give this to my son as a homework assignment one day. I have no idea. There you go. No. It's a nice little brain teaser. I enjoyed it. I thought it was a really unique and interesting post and I I thought the data visualization aspect of it was interesting. I I'm not sure if that's

[00:31:33] Mike Thomas:

an approach that I would have thought of when, you know, trying to solve this problem and and come to this answer and I think it does a great job. We have a few different plots here essentially representing the same exact thing, that really demonstrate that, you know, this number 397 is is the largest number, that is uniquely able to to be, the sum of 3 positive squares of integers and that's that's the the highest prime number, if you will. Like the like the ggplot annotation, the little red circle, around the dot across the three 3 plots, which is really nice. I believe it's just all base ggplot, that we have here and some dplyr code. And it's incredible, you know, sort of simulation based analysis, you know, what we can do with really just 2 packages, dplyr and ggplot.



[00:32:23] Eric Nantz:

Yeah. And this gave me flashbacks to a a project at the day job where we had a situation for smaller sample sizes. We couldn't rely on the normal approximation to help derive some of our probabilities of success and and p value inferences. So we had to make kinda make our own exact type of solution, but we couldn't, like, be exact from a mathematical sense. We had the brute force what you might have had in your classical statistics or data science courses when you're learning about p values for the first time. And maybe this doesn't happen now because yours truly is a dinosaur in terms of the the world now. But we have lookup tables of the p values in the back of our textbooks where, like, the t distribution, the the normal distribution, the f distribution, or whatnot.

And we had to do this kind of thing with our, situation where we made up this distribution, but we did, like, a preconfigured set of configurations of these p values, not too dissimilar to having, like, a configuration of ends in these p value lookup tables of yesteryear. And, yeah, we found out that if we did a bunch of these combinations, our resources, this data frame holding all that would blow up, and it will slow our app down immensely. So we've we fixed that quite a bit in this, upcoming release of this reenvisioning of this app. But I got flashbacks to, like, if you don't have the right answer, sometimes you gotta brute force it somehow, and sure enough, this is another interesting use case of that. And, of course, the real geeky side of me thinks, what if you could do all this in DuckDV and have, like, a huge massive table out of all this that you could explore with, like, a shiny app. What am I thinking? I have no idea. Mike, you gotta watch me on this. I have no idea.

That'd be pretty cool, Eric. Sounds like a challenge. I may have just opted myself into by accident. Nonetheless, what's not an accident? The rest of this issue is no accident and how awesome it is. Right? This is all curated very handily and and very carefully by our curator every week. In this case, it is the aforementioned Tony Elhar Bar. And there's a lot more to this issue, but we're gonna take a couple of minutes for our additional finds that we talk that we, that caught our attention. And for me, it's, as I'm starting to learn more about leveraging AI modeling into my workflows, I'm still very much in my baby steps of this journey.

One thing I have been pretty, you know, passionate about is where we can leverage the open source model tooling and creative ways of utilizing that. So Stephen Turner's recent post on his blog we we featured Stephen in a few highlights ago. He has a great blog post about how you how he was able to create the route the newly released olama3.one.four zero five b, which basically is the way they denote how many parameters, in this case, 405,000,000 parameters. And now he was able to set that up on a GitHub repository to incorporate into Hugging Face to expose as a custom model deployment.

Not something I've ever tried before, and he does warn in the post that with this amount of parameters, the responsiveness is not always great and that there might be alternatives if you really need speed such as using the 70,000,000,000 parameter model instead. But it's another kind of showcase to me, some of the flexibility that we get with these open source models, which in my industry is gonna be something we have to pay a lot of attention to. So I thought that was a pretty clever use of at least one existing platform, but opening the possibilities of what we can do, with these model choices.



[00:36:07] Mike Thomas:

That's super interesting, Eric. It's something that we've been toying around with. So I'm gonna have to take a look at that that blog post as well. I found a great blog, from Shannon Pileggi on her blog called Piping Hot Data. Shout out Shannon. I had the opportunity to meet her in real life at PositeConference this year, which was pretty cool. And she has a very applicable blog post, something that we've wrestled with quite a bit, which is our end and, the sort of repository that you're going to use, CRAN versus POSIT package manager.

And if you are not already, using POSIT package manager or, I guess some other service that, you know, maybe our universe that provides binaries, you definitely should with your r n projects. And a lot of times, by default, r n in the lock file, specified is crayon, which isn't always going to install, install binaries of the packages that you need when you run rnd restore. So Shannon points out this handy file in the rnd this handy function in the rnd package called lock file modify, which allows you to, change your repository, for example, from CRAN to posit package manager.

And then there is a second file a second function in r end called lock file write that would allow you to overwrite your lock file with that change, which is pretty cool. One of the questions that I have here, something that I'm gonna have to dive into myself, is to ensure that this would also update, the specified repository in your r n block file that is there for every package as well. So your first entry in your lock file is is typically the repository or repositories that you're going to use. And then and every individual package in your lock file as well also, you know, links to the particular repository, that was specified in that header that you want to use to install that particular package.

As a sort of a cheat, sometimes when we're switching from CRAN deposit package manager, I'll leave the the name or the label of the repository as CRAN, but we'll change the URL from from CRAN to, package manager.posit.co instead, so that we don't have to change all of the package entries. But if there is a much better way than control f and replace all to do that, which it seems like, you know, these functions that Shannon's pointed out for us might be able to accomplish, I am absolutely all ears. So really appreciate, this blog post because I think it's an applicable use case for all users of the r env package.



[00:38:47] Eric Nantz:

Yeah. I've I've done that exact same hack or trick that you caught literally this week as, those of you that well, nobody listened to the pre show of this when Mike and I were exchanging. But the war stories of this with me and my recent shiny test 2 go on headless browser adventures with Docker, that's been a nightmare to deal with. But there was one point where I had to replicate my dev environment to a Docker container. And in my work setup, we have an internal repository that's, like, cram but in our firewall.

Of course, that's not gonna play nicely with Docker, especially on GitHub Action. So, yeah, I switched that to positive package manager, but I'm gonna have to take the advice that Shannon has here and see if this is a a more elegant way to dynamically do this instead of being like a, you know, a a silly person and manually do that every time I pivot to a different architecture. So I the the creative juices are going after reading this post for sure. But, yeah, if you wanna get creative, we invite you to check out the rest of the issue of of our of our weekly. There's a whole bunch of additional content with new packages, new resources, and upcoming events. And in fact, you mentioned Shannon earlier, Mike.

She and I will be part of a team that's gonna we don't have the full details ironed out just yet in terms of the timing of it. But later in October, as part of the r pharma conference, we're having a diversity workshop with respect to getting started with version control. So Shannon will be one of the the leads on that. I'll be teaming up with my, fellow RUKI member, Sam Palmer, on a project. Get those that are new to Git that want a gentle introduction to that up and running. It's gonna be a lot of fun. We're putting all the finishing touches on that, so we'll be able to talk more about that later on. But, yeah, that that's just one of the things that you might find in that upcoming events section along with many other great resources.

So how's the best keep the project going? Well, we can do this about you, the community. So any great blog post, resource, tutorial, anything that you think is useful to the r community, whether it's by yourself or by members of the community. We're just a pull request away on our GitHub page, but you can find all that atarwika.org. A little link to the pull request is in the upper right little ribbon there. It'll take you to the template. Very easy to fill out, all marked down all the time. Very easy to parse that through. And then our curator for the week will be able to get that merged in and be on your way.

And, also, we love hearing from you in the audience. We've got a contact page in your podcast, player show notes for this episode. Directly link there. You can also send us a fun little boost with one of those modern podcast apps like Fountain Podverse, Cast O Matic. Lots of great happenings, especially in the fountain ecosystem these days. I'm paying attention to lots of great choices on that. And, also, you can get in touch with us on these social medias. I am on mastodon at our podcast at podcast index dot social.

Also on LinkedIn, just search my name, and you'll find me there. And sporadically on the Weapon X thing, although really not so much anymore. But if you must, I am at the r cast on there. Mike, where can listeners find you? You can find me on mastodon@mike_thomas@phostodon.org.

[00:42:07] Mike Thomas:

You can find me on LinkedIn if you search Ketchbrook Analytics, ketchb r o o k. You can see what I'm up to lately.

[00:42:15] Eric Nantz:

Very good. Always a fun time when you see you see your post on there. But it's always been fun recording with you as always, but we're gonna close-up the docket here before more things get mangled and I respect the setup. So with that, we're gonna say goodbye to this edition of ROK Highlights, and we hope to see you back for another edition of ROK Highlights next week."
"39","issue_2024_w_38_highlights",2024-09-18,40M 28S,"Hide a picture of Homer Simpson in a residual plot of all places? Oh it's real, you could say ""surreal!"" Plus a data-driven approach to investigate recent changes to the Australian census, and a cautionary reminder to check just where those numbers are coming from the next time you build a prediction model. Plus the quest to make R the official…","[00:00:03] Eric Nantz:

Hello, friends. We're back with us on a 179 of the R Weekly Holidays podcast. We're a day late because yours truly was, busy with other, production stuff with a virtual conference that just happened yesterday, which you might touch on a little bit later. But, nonetheless, we are excited to have you here for this weekly show where we talk about the latest highlights and other resources that are shared on this week's Our Weekly Issue. My name is Eric Nantz, and I'm delighted you join us from wherever you are around the world.

And, you know, never a dull moment in your in his life, but my awesome co host, Mike Thomas, is here with me. Mike, how are you doing? I'm doing well, Eric. Busy busy busy as as usual.

[00:00:41] Mike Thomas:

In the process, I think I said this last week of of selling a house, so I'm currently in a room that doesn't have much furniture. It's quite echoey. So hopefully that doesn't come through too bad for the listeners this week.

[00:00:52] Eric Nantz:

We, completely understand, and, yeah. We we hope that that gets more stable for you soon enough, but luckily, we can give you at least a few minutes here to live vicariously through these great resources we have shared for you, this this episode. And before I go on further, I wanna share a little up follow-up to what I teased last week with some of these other podcasts I've listened to and our quest to get the r language to dominate all the things in software development. Well, I'm happy to share that, our quest to get r as the default language for Decoder Radio Show is working very well, so much so that we earned the perk of having a custom song generated for the r language on the Dakota radio program, which I am going to play for you right now.



[00:01:47] Unknown:

Down by the meadow under the banyan tree where old folks gather jabber and take their tea. Talking about numbers, trends to foresee in the world of digits, none purer than thee. And visions divine. Your cold unwrapped stories line by line. Oh, Oh, sweetheart, silent has the dew in the world of 1s and zeros. You're a lighthouse true. When umbers sing and scatter plots bloom through dire slapper and few lead us past gloom.

[00:03:00] Eric Nantz:

And a huge shout out to Brian who goes by b h h for creating that song. And I'll have a link to his profile on Nastur in the show notes as well as the Coda Radio episode, where this came from, if you wanna learn more. Us and the r, you know, community and data science, we tend to we tend to be pretty chill about a few things. There aren't a lot of things that rattle us, and I think I think that suits us quite well. So job well done when we couldn't thank the program enough. And and, yeah. So that was awesome. Nonetheless, we're gonna move on here with the rest of the show here, and our issue this week was curated by Nariel Nakagura, another one of our longtime curators on the ROV team. And as always, he had tremendous help from our ROV team members and contributors like all of you around the world with your poll request and other suggestions.

So we're gonna start off with a visualization a visit to our visualization corner, you might say, in ways that you definitely would not expect. So this harkens back to my days when I was a TA in graduate school where most of the time I was helping in our computation lab, doing some cool at that time, building some innovative r stuff with, I still remember these days using, like, the LAMP stack and doing custom databases and randomly generated exams and whatnot. But there was a semester I had to teach the intro stats course. And, of course, it can be difficult to get the students engaged, especially when you get to, you know, some of the more harder concepts like regression model fitting, trying to make sure you're instilling in in the students the fact that, well, you can come up with the terms in the model, but you do want to check how those models are performing with some various mix of normality assumption tests and also tests about the quality of the model.

Oftentimes, we would use a classic type of visualization, plotting the residuals of your model against the fitted values and looking for any distinct patterns. Or, hopefully, you don't see a pattern, which means that your assumptions are are working well. Well, what if to motivate students to do that kind of checking, you give them a little, hidden Easter egg, if you will, throughout that process? And that's exactly what this first highlight is doing in the form of, you guessed it, an R package that has a lot more behind the scenes here. So we are talking about the latest package called Surreal, awesome name, by the way, authored by James Balamuta, who goes by the cultless professor on social media and LinkedIn. Always enjoy his work. He's also been hugely instrumental in incorporating Quirter and WebAssembly and WebR alongside George Stagg. So I always follow what James does, but this came kind of out of left field on my media feeds and, hence, in our our weekly issue.

But this is a package that gives you data sets that, on the surface, may not seem like very much. It's data sets in x and y coordinates. But when you fit a visualization about those or fit a model around those and you look at the observed, you know, or let's just say the predicted and the residual plot, you can embed basically a hidden image or even text in that actual plot. This is this is mind blowing, but let's let's give some more context here. This stems from research from back in 2007, approximately around that time, by a professor Leonard Stefanski.

He was at NCSU or North Carolina State University back at that time, and he wanted a way to motivate his students to learn about these different model assessment techniques. And one of those, like I said, is that visualization of the residuals versus predictive values. And he came up with a clever algorithm that I'll try to describe, but in essence, you want you know what kind of image or text you want the student to see in this plot. So you start with that, and then you use a a mix of image processing and and some other extraction processing.

So you get, like, a black and white version of that image. You feed that into programs such as ImageMagick or whatever. So you can get on a two dimensional plane kind of like the x and y coordinates of what maybe the black dots are that comprise that image or text as compared to the white, like, the the background, if you will. So then if you also, depending on the image itself or the text, you are he's and his algorithm goes by assuming that the product of the residual vector, if you transpose that and multiply that with the fitted or predicted value, vector in matrix multiplication, that that product is 0, meaning that the residual and the predictive value vectors are orthogonal.

That's another assumption we see in things like principal component analysis and whatnot. But when you go by that, you can come up with an iteration algorithm to introduce new data that meets that requirement that may have 1, 2, 3, or more of these explanatory variables or predicted variables against an outcome variable. So if you do, like, a plot that looks at all these variables together and, like, those pairs plots that we like to do when we have multiple variables, you're not gonna really see much of a pattern to that new data. But then when you do that, that aforementioned visual look at the residuals and predictive value, you're gonna get that original image back. So this, package got a few datasets that correspond to some of these different images that you could that you could use. But I'm also gonna throw in the show notes well, actually, in the in the package site itself, he's got the r logo, the original r logo in this kind of black and white, you know, 2 d dimensional plane. But then when you look at after the transformation that I mentioned, it just looks like a scatterplot of 5 variables against the y, but looks like kind of random blobs really. Nothing nothing discernible here, but it's a hidden image of looking at the quality of the model where you start to see that actual, you know, Easter egg type image.

So there are other datasets here too. He even shows how to do a custom message saying r is awesome. We always agree with that. But then the one I'm gonna throw in the show notes is an archived version of Leonard's, I guess, original site that had more datasets that were fed into this, including some fun, Homer Simpson images and many others that you would not expect in a scatterplot that ends up being there. So if you want more background on just how this all came to be, check out that archive site. And I believe through that, you'll also be able to dig into finding the manuscript and a presentation that this professor did way back in the day.

So if you're looking for ways to engage your students on teaching regression principles, you wanna give them a little fun along the way, like, I wish I'd done back in 2,005 ish when I was doing a TA work on this. I would have loved to have had this to be a way to motivate the students, but real fun package. I can't wait to put some hidden things maybe to do, some pranks on my colleagues at work. I don't know. Could be fun nonetheless. But, Mike, you're gonna do some new residual plots out of this.



[00:10:49] Mike Thomas:

Oh, for sure. We'll stick this in all of our model validation reports at the end. Just some nice Easter eggs for our clients. And just when I think just when I think the art community, you know, can't come up with anything more creative than the last thing that we came up with, a package like surreal drops. And the project I I had never heard about, you know, Stefanski's work back in 2007. And it sounds like there are some earlier art implementations as well, by a few folks, John Stoudenmeyer, Peter Wolf, and Yul Ricky Gromping, who maybe attempted to to try to do the same thing. And the fact that you can stick, you know, images or messages here in your residual plots is pretty cool. And I I agree, Eric. I think, you know, a a really fun utilization of this package and a real good application for it would be in the classroom for, you know, statistics professors trying to teach, how to analyze, you know, model fits and to look at residuals.

I think that would be a pretty fun thing for your students to be able to to see. So this might be, you know, applicable for them and and interesting to them, and it's just, you know, another sort of example in the art ecosystem of all of the different crazy things that we can do with the art language.

[00:12:04] Eric Nantz:

Yeah. Like I said, this this came completely out out of the blue, but, oh, it's just amazing with a little math knowledge what you can accomplish in these in these, assessments. So I I got I told Mike in the pre show, I went down a little rabbit hole last night looking at all these different images that have been generated. There's some real comical ones there, even some nice messages about, you know, George, Box is famous, all models are wrong quote, and many other little things that, again, may get the students thinking not just about how did how is this actually accomplished, which, again, if you look at the manuscript in your spare time, you'll see there is a lot of clever matrix multiplication and iteration here. Like, the this this would have been a fun way in my linear algebra class to learn some of these concepts. But, again, I I I'm I'm in dinosaur age, so to speak. So this even predates that research when I was learning that stuff. But, yeah, this I think this could be great for especially in this day and age where I'm sure there's gonna be more models that are thrown to us may or may not even be generated by humans these days that looking at the integrity of these models, no matter machine learning or in the classical regression sense, is hugely important. So whatever we can do to to spark that light bulb in in students' minds and even those in in the field that are working daily, I'm all for it. So credit to James for putting this through. And, again, I'm I I can't wait to play this more. Yeah. This is like Anscombe's

[00:13:31] Mike Thomas:

quartet on steroids, sort of. And for the folks who are feeling festive or may celebrate Halloween, there is a function in the surreal package called jack o'-lantern surreal data. That's a built in dataset that you can use such that your residuals, when plotted, show a nice jack o'-lantern right in the middle of it.

[00:13:50] Eric Nantz:

That is awesome. Yeah. I think that'll be a good thing for my kids to say, oh, hey. Guess what I can do with r? And they'll be like, no. We want the real pumpkin. But you know what? But

[00:14:13] Unknown:

whatever.

[00:14:15] Eric Nantz:

And, in this next highlight, there's definitely been a lot of news that's been happening across the the pond, as I say, with our, friends at Australia with a recent update to the census that our next highlight is looking at a data driven way to look at the impact of additions in questions, really get a feel for how you can segment populations and looking at different differences amongst them. But, again, taking a data driven approach that I think we all can learn from. And this next highlight comes from Peter Ellis, who is the director of the statistics for development division at the Pacific Community.

So he's also based in Australia, but he in his latest blog post is looking at a recent addition, pushback and then readdition to certain questions in the 2021 ABS standard, which is, I believe, the Australian Bureau of Statistics and their recent update to the census and some of the discourse that follow through with that with respect to sex and gender variations and characteristics that they were trying to measure in an updated census. There was some pushback, and then apparently they've been reintroducing some of these questions dealing with, you know, what is a person's gender, how does that person describe their sexual orientation, and has that person been told that they were born with a variation of sex characteristics.

So certainly a a major topic around the world. But he wanted to not just, you know, react to the discourse that's online. He wanted to really dive deeper into with what we currently have, what is a way to convey the impact of trying to accurately distinguish some of these populations and seeing what that relates to other questions in the census itself. So, Peter does a great job in background, which, again, we'll invite you to check the preview or check the link yourself if you wanna read more background. We're gonna dive into more of the data analysis side of it where this is a very much a tidyverse inspired workflow with a real world use case of importing spreadsheet data from the from the ABS in a spreadsheet. So again, that's very common in the in this part of the world of of data analysis. So he's using the read excel package, one I've used great with great success in importing this type of data with a bit of, tidyverse cleaning up with a little bit of filtering for making sure they're getting non missing values and as well as different derivations of variables, a little bit of grouping. So again, pretty, pretty logical. We get to a dataset, eventually leads to a visualization that when you read the blog post, you're gonna have to zoom in on this a little bit. But Peter is trying to illustrate some of the key differences as measured by the data between the LGB plus and the heterosexual Australian responses on this survey from back in 2020 and then seeing what what impact that would be in various questions. And you do expect there will be some differences not just in the in the age categories but also responses such as, like, family composition in their household, income questions, and the like. But he's using a pretty nice kind of, I think, pseudo lines, you know, dot chart but showing the the difference in in arrows pointing at the the shift from the heterosexual side to the LGB plus side of it. So you can kind of see where the wider differences are.

But he says he struggled with how to put this plot more accurately, and he went through quite a few iterations on that. So at the end of the post, he's got kind of what led to that final visualization, a set of faceted bar charts that are basically looking at each of those questions individually in different categories as well as the facets. So you can get a good read on that as well. But he was trying to distill some of the findings from this side by side bar chart analysis into this line plot as well. So as as I said, you can kind of see different types of domains are showing a wider differential in terms of percentages, which again may or may not be expected. But I'm always up for letting the data tell the story and and certainly, hopefully, that in the future, the the census from anywhere in the world is able to accurately portray these different populations that, again, deserve their voices to be heard. So really, really great summary here by Peter. Definitely a lot to digest if you're looking into the space of census analytics.

I think a lot of good ideas to follow here. And again, great visualization, great reproducible code that you can run yourself if you want to import this data. And I'm sure similar techniques could apply to other, census data that's from available around the world. So great post by Peter, and I look forward to learning more about this space.

[00:19:34] Mike Thomas:

Yeah. I thought it was an excellent post as well. Eric and I appreciated the the two different approaches, in terms of data visualization to to representing this data. In the past, you know, when we've worked with survey data, it's for whatever reason, seems to be a little bit more tricky to work with than some of your traditional datasets, you know, that are exported out of a, you know, a database system that is the result of users entering data into your application or something like that. Right? There's a lot of, pivoting, reshaping, you know, some some filling, NA values as well, you know, some work, deciding between whether you want to use a continuous or discrete scale when you have Likert scales and and things like that going on. So I I certainly appreciate, you know, the work that went into curating this data into the visualizations that it became. This is also another reminder for me that I need start leveraging, you know, whether you wanna call them lollipop charts or donut or or, barbell charts or things like that. But, it's this might be the first time that I've ever seen a chart like this, which I think you might call, like, a barbell chart that has two dots, for each line on the y axis representing, you know, the differences between 2 categories in the legend.

And then there is actually an arrow, pointing the direction that it it, you know, increased or decreased. And, typically, I I I feel like in a lot of these charts, I've seen a line, just a straight line with no arrow on it, connecting those 2 each of those two dots. But the fact that, he was able to to employ an arrow here in this visualization was was really, really cool to me. You know, again, sort of demonstrated the the power of ggplot and the fact that that arrow is is colored the same as the the dot that it, you know, sort of is trying to represent the most, but there is also a little alpha component too that makes it a little more subtle than the dot itself. I think it's just a a really nice sorta chef's kiss, on top of that particular data visualization. And, you know, as always in a lot of these blog posts, the the ggplot code is all there. The dplyr data wrangling code is all there.

So it it's it's fantastic in the way that it it ends with this nice faceted bar chart as well. Man, I was working in Python, this week and was using the plot trying to use plot 9. Unfortunately, for a a donut chart, wasn't able to use plot 9 and had the to switch over to matplotlib instead. And I wanted to facet, you know, have these these faceted plots as well. And it's just not quite as easy as, you know, that that one beautiful facet wrap or facet grid, function that we have in ggplot. So it gave me that additional appreciation that, I I always have for for ggplot in the r language, particularly. But fantastic blog post talk top to bottom here, and and love the data visualization.



[00:22:30] Eric Nantz:

Yeah. We've always been spoiled in this community with the the power of visualizations both on the static variety and the interactive variety. So there's, yeah, there's a there's a lot going on in in the code here, but, yeah, I definitely invite you all if you're interested in replicating this to take a look at Peter's code here and and give it a spin for for your next, opportunity for these cool little barbell plots. So, yep, really, really powerful visualization and looking forward to seeing more in this space.

And our last highlight today is going to be a healthy mix of some of the additional questions or items that I mentioned in our first slide about modeling techniques. But being careful about the data you're using in your model is definitely a key motivating factor for this this last highlight we have today, which is coming to us from John Mount, who is one of the principal consultants at WindVector LLC, the consulting resource. And they've historically have had the WindVector blog, which, I've been reminiscing about this in previous episodes about what were some of my original resources learning are back in the early 2000 mid 2000. But, John Mount's blog has been right up there as one of the resources I've had bookmarked for many, many years. So it's always great to see posts from him come to the highlights here. But he is motivating a situation that's been inspired by real projects where there can be a bit of a disconnect between the data scientists and analysts performing modeling for prediction and perhaps data engineers that are supplying this data that maybe aren't giving you the full information, so to speak. So in his example that's inspired by real things but kind of more fictitious is that you have, an issue where maybe you wanna run a prediction on forecast, I should say, attendance at a movie theater, and you wanna incorporate attendances from past, you know, events or past movies as part of that along with other factors that might drive that, you know, that influence in the prediction.

So you may be getting from a data engineer or data warehouse, whatever have you, some metrics for each of the dates and each of the movies that were showing in that date, the time of it, and the number of people that attended that. So he's got starting the post, a little example of reading a hypothetical CSV file where we've gotten the month in the month of August, you know, a few movies. I'm not sure these are real names but whatnot, but with the attendees. And you can see there is quite a bit of variation in some of these attendance metrics, which, of course, could be the quality of the movie, but could be other factors.

But to motivate the goal of building a model that relates the attendance to one of the key you know, maybe one of the key perks in a movie theater these days or always has been is how much popcorn you sell and whatever snacks you sell. Maybe that is a driver that someone might wanna look at. There could be other factors too along with that. But but one thing to note is that when the the engineers look at what's going to happen in the future, there could be a disconnect between the type of data. Historical data is looking at the actual numbers of attendees, but the stuff that's being estimated is, of course, not known yet. So continuing on with the example, imagine that they look at the different popcorn sales. They they merge that with the attendance dataset.

You're doing what looks to be a pretty decent, you know, model fit with the data you have. But now when you start to do predictions, when you look at the month of September and knowing that the month of September is giving you slightly different types of values and you see that the estimates are in the line plot that he shows here really inflated, like massively inflated from what we see the model saying in the previous month of August. So and the takeaway is that the model is predicting or predicting popcorn sales in the future are going to be double what they see in the month of data that they use to train the data.

That doesn't quite check. So doing a little diagnosis, doing some nice little distribution plots, you see that the pattern of attendance doesn't look anything alike. They look heavily skewed in September compared to the more balanced month of August. And why is this? Well, the estimated attendance figures in September were based on the capacity of the movie theater, the number of seats inside, which is different, of course, than what the actual measures were back in the month of August where they used actually the number of people sitting in the theater at that time.

So this is illustrating a case where maybe there's a bit of a disconnect in terms of the type of data that's going to go into your forecast versus the type of data you're using to train the data. So to motivate what John would do in this, you know, solving the issue, instead of using the actual number of movie attendees in August, he recommends moving using the capacity of the theater as additional or replacing the metrics with the capacity numbers instead. And when you do that, then you get data that looks more realistic in his opinion compared to what we saw before.

So my takeaway from this is being consistent with the type of measures you're incorporating in both your training and then eventually your your val your validation or your future prediction set as compared to mixing the different types up. Even though they are technically the same variable, they are in much different context. So this is something I haven't really encountered myself because I'm not as much into the forecasting realm of it, But I could definitely see how this might be a bit of a monkey wrench to a certain, you know, say, to scientists or statistician that see such wildly different metrics when they look at their predictions versus what they actually observe. So interesting thing. I'll probably have to do it on that a bit more, but it was an interesting illustration of what can be a disconnect but hopefully isn't when you have harmony between the engineering of data side of it and the actual data scientist or statistician side of it. So a little bit of food for thought for your prediction,

[00:29:37] Mike Thomas:

adventures in the future. Yeah. Eric, like you, we don't do a ton of forecasting as well, you know, or more along the lines of machine learning and and some Bayesian regression analysis and things like that. But I really appreciate sort of just the the ideology here that as data scientists, we need to do a great job of articulating the work that we're doing. And, you know, in the conclusion here, John has this this quote that really stuck with me, and it it says, you know, most date time questions, unfortunately, you know, can't be simplified down to, you know, what's the prediction for date x? You you really need to ask a question that's more like, you know, what's the best prediction for date x using a model that was trained up through what was known at at date y and taking inputs known up through date z.

So you have sort of these these three different moving time components here, and that's where it gets into the discussion about bitemporal databases that allow you to see what data look like at at different times. And, it resonates a lot with me. It makes me feel like, you know, this AI can't automate our jobs as data scientists because there's so much nuance and and so much that we have to caveat and make sure that we are, articulating correctly. You know, I I had a presentation on on AI recently, and, unfortunately, you know, as much as I like to not look at my notes when I'm giving presentations, the way that we communicate these things is really, really important, and the words that we choose is is extremely important. So I I find myself, you know, making sure that I am spending a lot of time curating my notes for presentations like that when I'm communicating concepts around, you know, machine learning or or AI or statistical modeling, whatever we're calling it these days.

But but, really, that attention to detail and making sure you are explaining things, concretely for for end users is is really, really important because there's a lot of nuance going on here. And I think that, you know, unfortunately, out there in the wild with, you know, a lot of the, maybe, automated machine learning, I saw, Shannon Maklis post something recently where I think that the latest version of some sort of GPT, allows you to just upload a data set, and it'll make predictions for you. I don't know if you saw that come across, Mastodon.

Yeah. But, you know, that made me sort of hang my hang my head in sadness. And I would imagine that if John had seen that post, he he might be doing the same thing as well. So, you know, I appreciate John taking the time to to level set us on the importance of how nuanced, predictive modeling can be.

[00:32:19] Eric Nantz:

Yeah. And a lot of times, you may not really see it again until you look behind the scenes, so to speak. You look at the quality of your model fit. It's not just that p value. You've got to look at what's really happening in the actual trend of your prediction versus observed and looking at that time course. Yeah. It it's very powerful to have that critical eye looking at this carefully. And, yeah, we we cannot expect realistically that we're gonna have AI be able to figure out all those nuances. Certainly, there's a lot of responsibility at play here. That was a a similar theme to what I heard at the recent, Gen AI and pharma conference I was helping out with yesterday when we were as I'm recording this, that nobody had the illusion in our our speakers about this taking away anybody's job. And in fact, we just it's evolving the types of skills that we're we're gonna need to navigate this, but to do it responsibly and making sure that we're looking at these with the right lens.

And there is even some good talk about having the terminology be consistent or or adopting new terminology sometimes when you look at the evaluation of these models compared to when we look at in codevelopment or or maybe application development concepts of CICD. I'm looking at, you know, regression testing and things like that. There are starting to get equivalents of that on the model assessment side of it, but it's quite different in the AI space. So I'm even, as I speak, trying to learn the best way to translate some of the things I know from traditional statistic model building, traditional, you know, software development to this new age of augmenting with these AI tools. And even though this wasn't an AI post, you could definitely see something like this happen if you're gonna use some of these resources without a lot of care in front of it. So yeah. Great. Great. Again, thought provoking post that I can't wait to dive into more.

And with that, there's a lot more to dive into in the rest of the r Wiki issues. So before we wrap up the show, we'll take a couple of minutes for our additional finds here. And for me, I don't know about you, Mike, but I I like to, keep my GitHub, you know, repos organized with, you know, very, clear issues and with that and project boards. But with that in GitHub, one of the features I use a lot is the concept of labeling. And I got a pretty good handle on how to do that for my actual development projects, but this this, additional fine I wanna call out here is coming from Greg Wilson from the rOpenSci blog about some of the types of labels he uses, not just codevelopment, but also actual manuscript and, you know, course development.

So he calls up labels for technical writing projects, and the post itself is a pretty easy read, only a few paragraphs. But you look at interesting labels such as, like, what's the difference between adding a new feature called add versus changing something? And there's a change label that things are already there but could be better. What is required from more of a governance standpoint of the project about kind of meta issues versus general discussion? Or what's more around the infrastructure side for tooling versus, like, what's a one off thing he calls tasks that you have to accomplish? And maybe once you get it knocked out, it's going to be done. But and then also what is actual written contact with prose versus, like, the actual code itself which is in software. So it definitely gave me got me thinking for my projects where I might have a Shiny app, but then within that app, I'll have documentation, maybe an accompanying package down site to help users learn the app. I can have labels that are more tailored to that side of development and not just the actual app development, so to speak. So interesting post, and I'm I'm I I got some interesting things to ponder when I put up my next project in GitHub and what labels I use there. So, Micah, what what did you find? No. It's a great find, Eric, and I'm always interested in in what Greg has to write. He was the instructor for my,

[00:36:34] Mike Thomas:

tidyverse certified trainer course a whole long time ago. So that was that was pretty pretty cool opportunity to be able to to meet him, there. I found a blog post on Medium, published by Numbers Around Us, is the the name of the blog post. I'm I'm not sure exactly the name of the author. Maybe we can stick it in the show notes if I'm able to to pull it out here at some point. But it's it's called Express to Impress, Leveraging IBCS Standards for Powerful Data Presentations. And if you're not familiar with the IBCS standard, it is, the IBCS stands for the International Business Communication Standards Framework.

And its intent is, I guess, sort of like, you know, our our grammar of graphics, which it it sort of aligns pretty well with, is to accomplish, I think, really, these these three different goals for any data reporting work and data visualization work that you do to ensure, that your visualizations are clear and focused, that they're consistent and standardized, which I think, especially, you know, in an organizational context, at your company, you're probably going to want to be providing, you know, consistent types of consistent looking reports, consistent visuals over time, that your users can become familiar with. And then the last one is is one that really sticks with me. It's it's actionable, and ensuring that the visualizations that you're providing are somewhat of a call to action and a very clear call to action in terms of, you know, what you expect the audience to be able to to take away from that visual and, you know, what you expect them to to actually do because of it. So there's some some great examples of, you know, migrating from pie charts to to bar charts instead, and a whole lot of great ggplot calls in here, but an awesome other database post for this week.



[00:38:30] Eric Nantz:

Yeah. Really. Well, I'm looking through it now. There's a lot to a lot to digest here, but all really great concepts here that I'll definitely take with me as I keep building more visualizations in my packages and apps. So great great find there. And there's a lot more to this issue. We always can't talk about all of it, and we don't have enough time, but you're invited to check out the issue itself, which is linked in the show notes at rho.org as well. And, also, this project is a community effort, so we invite your poll requests for new resources, new, blog posts, new packages.

It's all right there, all in markdown. So head to rho.org for all the details now to contribute there. We also like hearing from you in the audience. We have a contact page linked in the show notes that you can find in your podcast, player as well as a way for you to boost the show if you'd like to hear from us and get in touch with us directly. If you get value of this, then we appreciate value back, so to speak, the value for value mindset. Details are in the show notes as well, and we are on social medias. If you wanna get in touch with us personally, I am mostly on Mastodon these days with at our podcast at podcast index.social.

I'm also on LinkedIn. Just search my name and you'll find me there and occasionally on the weapon x thing with that the r cast. Mike, where can the listeners get a hold of you? You can find me on mastodon@mike_thomas@phostodon.org.

[00:39:54] Mike Thomas:

Or you can find me on LinkedIn if you search Ketchbrook Analytics, k e t c h b r o o k. Now you can see what I'm up to lately. Very good. Very good. And, again, special shout out to our friends at Jupiter Broadcasting and Encoder Radio,

[00:40:08] Eric Nantz:

Mike and Chris for keeping the our language, train going for the official language. I'm not sure how long it'll last because those, Go and and, other folks are are nipping at the bud here, but we'll see how far it takes us. Nonetheless, we hope you enjoyed this week's episode of R weekly highlights, and we'll be back with another episode of R weekly highlights next week."
"40","issue_2024_w_37_highlights",2024-09-11,49M 7S,"How being fair to your research has a new and important meaning than what you may expect, the power you can unlock with custom roxygen tags, and a collection of tips you can apply today for your next visualization. Episode Links This week's curator: Batool Almarzouq - @batool664 (X/Twitter) Making your blog FAIR Create and use a custom roxygen2…","[00:00:03] Eric Nantz:

Hello, friends. We're back. That was a 178 of the R Weekly highlights podcast. Almost midway through September already. Time is flying by, but what never slows down is also the fast pace of the r community, where in this podcast we cover the awesome highlights and other resources that are shared every single week at rweekly.org. My name is Eric Nantz, and I'm delighted you join us today from wherever you are around the world. And joining me at the hip with never a dull moment in his life going on right now is my co host, Mike Thomas. Mike, how are you doing today? Doing well, Eric. Hanging in there. I am potentially in the process of of moving, which is both

[00:00:41] Mike Thomas:

exciting and and crazy and stressful all at the same time. So all the emotions.

[00:00:48] Eric Nantz:

Yep. It's it's hard to grasp just one at a time. It all just happens at once. So our our, best of luck to you, and hopefully, you don't lose anything valuable on the move

[00:00:59] Mike Thomas:

over. I'll make sure that the microphone stays intact, and we'll be sure to to make sure that the new podcast space is is set up ready to go.

[00:01:08] Eric Nantz:

Well, you know, that that's that's the real important stuff. You know, those those pets or whatever. Ah, no. You need the mic. No. I'm I'm I'm I'm good. I we know where your priorities are. We we we we get it. We get it. That's right. Yeah. I'm, not moving anytime soon because that would be a herculean effort in this, house here to try and pick up everything there. But luckily, I don't have to think about that. I get to think about the great art content we get to talk about today. And in this week's issue has been curated by Batool Almarsak. And as always, she had tremendous help from our fellow Arruki team members and contributors like all of you around the world with your poll requests and other suggestions.

And so one of the biggest reasons we do our weekly is the great content that all of you out there, whether you're in the data science world or other parts of industry, have been showcasing your learnings perhaps through either personal websites, personal blogs, and whatnot. And, you know, it does beg the question sometimes when we put these resources online, how do we kinda guarantee really tight linkages or ways to get to that content even as the landscape of, like, web hosting providers or other ways that we're deploying these sites change over time?

Well, in our first high, we're gonna look at a pretty innovative way to kind of future proof potentially ways to make your your insights on, say, a blog more easily shareable that could be durable for the test of however evolving the tech landscape is. And in particular, this first highlight is coming to us from Athanasia Mowinkle, who is a neuroscientist over in Europe where she talks about making her blog fair. Now I'm not talking about the fair that I often hear from my kids gripe to me when they say, it's not fair. I don't get to do this and that. No. I'm not talking about that kind of fair. The fair we're talking about here is an acronym for findable, accessible, interoperable, and reusable.

Boy, that sort of speaks to me with a lot of the reproducibility aspects we talk about all the time with framers, like r markdown and quartile and whatnot. But, of course, this applies to any insights we author or generate. Ethe Nagia has, has worked on her blog for quite some time, and she was inspired actually about a newsletter that she subscribes to from Heidi Seibold about her approach to making her blog this FAIR compliant, if you will. And in Heidi's approach, we get an outline that it's basically, hosted as a Jekyll website, which for those aren't familiar, Jekyll was a framework and I believe based in Ruby to do, like, a static site kind of generation, was used very many years in the GitHub world.

But Heidi also has, you know, a newsletter that she turns into RSS or Adam feed, and then that will post automatically based on the content in her site. And then she also adds her blog to a service called Rogue Scholar, which I believe is an archive for more of the academic or research type blogs. But what comes with that is robust DOIs or these kind of unique document identifiers and additional metadata that could then be searchable and easily accessible. Athanasia's workflow is pretty similar. It's a Hugo site. Now, again, Hugo, I've been a big fan of for many years. That's what powers the blog down package that EYC has created with, his work at Posit earlier and that. And then she also hosted on GitHub Pages. Again, a framework I like to use quite a bit for hosting.

Again, all marked down all the time. And with Hugo, you get an automatic RSS feed. And then she has an idea for a GitHub action that will register each new post to a service called Zenodo, another one of these research type archival services. And then via their API, she's going to be able to get these robust document identifiers as well as adding additional metadata so that that can be searchable as well. There are a little nuance differences from what Heidi approached, but we're going to walk through just how Athanasia has accomplished this in her workflow here.

So with that, we're going to need some packages, right, because she wants to do this all in R because we want to do R all the things here. So we need to communicate with a web service, the Zenodo API. So with that, the h t t r two package authored by Hadley Wickham is a great candidate to interface with those connections. And then with Hugo and as well as Portal itself, another of these static site generators, many times your content will have a YAML front matter at the top that denotes certain metadata, certain options, or whatnot.

So how do you interpret that? Well, appropriately enough, there is an R package called YAML, y a m l, to let you interrogate both from a consuming perspective and actually writing YAML if you need to to documents or whatnot. And then the last step is she's going to take a bit of a detour after what Heidi did. She wants to make sure her blog posts, which often have a great mix of visual content as well as tabular summaries and other narrative to be easily self contained in one artifact. So she wants to convert these posts to PDF, and for this she is going to turn to quarto for this part of the workflow.

So we're going to walk through this step by step, but the first step of this is actually trying to prepare the data of these posts so that it can be archived. And she's got some some example code here. The first grab all these markdown files that make up the source of the posts themselves, And in Hugo, you can do what are called page bundles, which basically means every new page that you make in your site is its own folder with within it an index dot MD file and many other supporting files such as images or whatnot.

So she wanted an easy way to grab the source of all those index dot MDs. So she's got a little, regex action going here and list that files. Not too difficult. She knows the file name, so it's literally index //md is like the regex on that front. Once she got that, now she's got this this vector of files on her file system to have all of the blog content. Okay. Now, examining, we walked through one of these posts that she has, in this case, about random effects. She then reads the contents of that file with the relines function, and you see if you print that out like in the example here, you've get each line or each element is a line from the file. So you got the YAML at the top and then a few blanks and then you've got the actual content itself.

So we've got the content read read in r now. Now how do we extract that metadata first? Like, let's parse the YAML first, and then we'll deal with the content afterwards. There is a handy function within rmarkdown itself that she was able to get some recommendations, from a good friend of hers that we'll hear about later in the highlights of a function called YAML front matter exposed by R Markdown itself, where you feed in the contents of that post with both the YAML and whatever the narrative you have. And it is intelligent enough to scrape that YAML section and then make variables for each field with its actual values. So you see it's got, like, the title, author, you know, date, and even the nested elements are like nested list elements. It's very handy, very, very straightforward.

And so she's able to get that YAML metadata, but she wants to add more to it than just what's in the YAML. She wants a high level description of the post, which in the Hugo world doesn't quite come automatically. You have to kind of inspect the rest of the post to kind of maybe take a portion of it. So she wants to make some code that will do this. So she wrote a custom function that's doing a bit of a grip of, like, all that content that's after the YAML delimiters and then be able to grab that and then see what are the positions where that content actually starts.

And then from there, she's able to then look for what's like the first paragraph of that post or maybe the first few sentences. She's able to grab that with more custom functions of a summary of it and then inject that into the existing metadata variable. So now she's got a simple 2, 3 sentence description vector for that metadata summary. So with that, she's got the metadata ready to go. Now here comes the fun part. As I mentioned, she wants to use the Zenodo service to start archiving the content and have it searchable, but just like anything, an API has got its own set of documentation, what it expects to have.

And, apparently, she's had to do a bit of trial and error, which I can certainly relate to because sometimes I get the requests horribly wrong, and sometimes you get those cryptic errors back of, like, what happened and what didn't happen. So through this, exploration, she was able to compose kind of another list structure of those metadata fields, but in the way that the Zenodo API expects. So it looks like kind of a nested list of elements, which isn't too bad, but it's a lot of just brute force trial and error to get to that point.

So then that's not all. As I mentioned, she wants a static archive of the content itself, not just from the metadata, but of the entire post. That's where our good friend Quartal comes in where once again she's able to leverage a function from Quartal, in this case Quartal render, to compile the web based version or the markdown version into a PDF. But she had a little rabbit hole to navigate here. You know what she tried to do, Mike? She tried that types method, and that didn't quite work. But there's always a plan b. Right? And plan b is good old LaTeX.

As much as I rag on it sometimes sometimes it's still the best tool to get the job done.

[00:11:59] Mike Thomas:

Yeah. Unfortunately, Athanasia tried types first and some of the images were not being resized correctly is what she said so that's why she had to switch back to Latex. And, you know, that's a little disheartening for me to hear. I think it's, you know, they're still working on ironing out some of the kinks of of types and how it interacts with quarto. I I do still believe it's the future, but, if you are trying to do, you know, very specific things and, you know, size images a very particular way on the page, you might have a little bit more customizability and full control when you're leveraging LaTeX, as compared to to types, especially if you you actually know how to write a little bit of LaTeX as well.

We are are in the process at Catchbook. I was hoping for our next big model validation project, which are these PDF deliverables, that we could switch over to, types instead of our our current sort of LaTex, framework. But we do need a lot of that customizability to make things sort of look perfect on the page for our clients. So, I'm I'm hesitant to make that switch yet, especially after reading this this blog post. But one of the nice things is you can try the 2 and the the quarto code is still the same. Right? You're still using quarto render with your output format PDF, for the most part.

So it really cool that she was able to do that. I guess it's sort of unfortunate that you have to to render to PDF for the Zenodo service.

[00:13:32] Eric Nantz:

Right? Yeah. Yeah. My hope is that it would have some kind of dynamic view of that content, but it is what it is, I guess. Exactly.

[00:13:41] Mike Thomas:

So, as you said, sort of the the 3 packages that come together here are h t t r 2, YAML, and then what's the last one that I can't have? Quarto itself. And then Quarto itself. Right? So, the sort of the way that she's able to actually send this final request, to the HTTP RT or or to the Zenodo API via HTTR 2 is, obviously, whenever you're dealing with one of these APIs, you're gonna need an API key token. So she stores that as an environment variable called Zenodo API token, and then she has just a variable an object called Zenodo API endpoint, where she specifies the, you know, the URL endpoint of, this Zenodo API where she's going to be sending this request. And you gotta love h t t r two it just makes everything look so absolutely clean.

The the pipe syntax that we're using here in h t t r two starts with a request, then a a bearer token, essentially, where she passes that API token, and then your JSON body as well. And don't forget to use that auto on box equals true if you're switching back and forth between our lists and, JSON data that you are sending up to the API. The nice thing about HDR 2 as well that Athanasio demonstrates here is you can use the request dry run function to be able to see the query that's going out before you actually send it to the API. Take a look at that request and everything Once everything looks good, you can use, request perform to actually send your API request to Zenodo, get the status that comes back.

In in this case, it was a a 201 created, which, is is not the response that she got for many attempts as you said, while trying to figure out the all of the different metadata components for this API. And if you take a look at the blog post, there are a lot of pieces to this list, this metadata list that she puts together that need to be sent to this API, you know, essentially, different parameters that the API expects, you know, within the metadata. It's not just a title description, you know, the creators, her name, her or orchid, you have upload type, publication type, publication date, and about 5 other different parameters that have to be nested correctly in your r list in order for them to be passed correctly via JSON to the, to the, API service. It's you're able to take a look at what came back, from that JSON response and everything looks looks really, really good, and at that point in time, I think she has everything that she needs for the DOI that's actually going to be, get sent, via the GitHub action. It's gonna get published alongside the GitHub action for the most part.

So sort of the last section of of this blog post is actually adding that DOI to the YAML header or front matter, if you will, for the post, that is going to to end up being posted and published. So Afnazia is actually able to put together a single script. It looks like it's about maybe 200 lines here, that sort of put everything together, in one script and mostly it's it's functions here, which is really nice functional programming, that she's put together that walk through all of the different steps that we talked about in this blog post, in terms of, you know, sort of finding the the end of the section, you know, taking a look at whether or not a DOI is needed, you know, actually publishing to Zenodo, which is probably the biggest function, the biggest custom function that she's written and that's going to return, I believe, the path to the PDF file that has been generated by quarto.

And then once you actually have that PDF file and, you're ready to go here it's it's just a few lines of code that she has here, with a nice little s apply to to process files that that do not have a DOI, but are published and supply, those posts essentially and and run all those posts, that need a DOI to her service. And it's it's pretty incredible. Her next step is to to work on the GitHub actions component, which I I think should be fairly straightforward given that she's put, you know, all of the r code here together as best as possible. Obviously, when you're dealing with GitHub actions that's gonna be running on a machine that's not yours.

So she will have to ensure that, you know, all of the dependencies including latex, which is probably not fun, when you're doing that in GitHub actions is are installed correctly, and execute correctly on that that GitHub actions runner. But I'm very excited to hopefully sort of see a follow-up here for the next step in this process, which is the GitHub action that's gonna do this all programmatically. So every time that she authors a new post on her website, it will automatically reach out to Zenodo, get that DOI, and publish things, the way that they should be.



[00:18:47] Eric Nantz:

Yeah. As you were talking, I went to that, GitHub repo, and sure enough, there are a few actions here. And I believe she's do indeed has that script that you just outlined in the GitHub, you know, workflows area itself. So I think she's got it. Credit to her. She's actually got 4 actions on there. My goodness. She's a GitHub action ninja, looks like. So I'm gonna have to take some learnings from this. But my goodness. What a what a great, you know, showcase of automation, but also just what you can do when you stitch these services and and workflows together. So I I think this shows a lot of inspiration as we're thinking about, hey, anybody generating or authoring their content, trying to future proof this to some extent? And, you know, we can't predict everything in the future. But, yeah, I'm intrigued by what this Sonoto service is offering her, but I may look at other ways too of making sure that what I produce, maybe it's, you know, the stuff I used to do with the shiny dev series or ever or this very podcast. Right? I wanna make sure this stands the test of time that no matter which fancy technologies out there, we can find a way to search it and be able to to grab it back. So, yeah, lots of lots of interesting nuggets here. And then I really appreciate towards the end of that of that script that she was outlining, she's got a check to make sure that she doesn't do this DOI process for posts that already have it, which is great to minimize, again, calls to an external service because the last thing you wanna do is accidentally DDoS something or worse yet lose your access to that API venture. So being intelligent enough to only run what you need, that's a that's a lifesaver when you do have APIs or even HPC computing. So lots of nuggets in that script that I think are quite inspirational here.



[00:20:41] Mike Thomas:

Absolutely. And, Eric, you and I, you know, don't do a lot of, I guess, academic publishing, but there is a gigantic community of people out there who do that, and I think that this blog post is going to be incredibly powerful and helpful for them.

[00:20:55] Eric Nantz:

Yeah. So definitely check out her, catalog on the re of the previous blog post. So there's a little bit of something for everybody in that world. Really, really good stuff. And one of the nice things about Athanasios' post is that she mentioned that she reached out to a couple of her trusted friends to give some recommendations on how to proceed. And one of those friends just happens to be the author of our next highlight here because Ma'al Salman returns to the highlights once again to talk about her recent discovery about creating and leveraging not just r oxygen 2 but custom r oxygen 2 tags.

Now this has been something I've never dived into myself, but I'm starting to see other packages in the our ecosystem try to do things like this where they'll have a custom tag. And based on that, their package will do additional processing or additional things on the side with it. So she, you know, introduction here, such as making sure that there are internal functions that are documented maybe for only developers and not for like the end user of a package. Also, being able to leverage or record certain standards that are expected for statistical functions.

And lastly, having tests from within our scripts themselves, and that in particular has been contributed by a package called roxy test as well. So I've seen these come through, but I never really knew what was the nuts and bolts on it. So I was supposed to gonna walk us through her journey on this and that she was looking for a way to help with the iGraph package. And for those that aren't aware, iGraph is one of the, you know, more standard and frankly, quite powerful packages that deals with network, network generation, network analysis for simple or complex networks.

Now are the r packages are binding to the c library of the same name, iGraph. And I I remember compiling this back in the day when I was getting into network analysis for the first time. And, admittedly, iGraph is, a bit of a bit of an ordeal to get into, but it's not insurmountable. But what she's learned as she's been working with iGraph is that there may be places where the r function definitely has documentation as it would have any r package, but it's depending on some source in the c library to make all that happen.

And she wanted a way to link for a particular r function to the relevant c code binding and whatever it needs in terms of additional considerations within each of the functions manual pages. So that looks like something that I wouldn't even know heads or tails of how to accomplish it, but we're gonna walk through how she, did this herself. So she starts with, outlining the workflow that ends up doing this. 1st, having, some kind of mapping of the existing documentation for the c library. Maybe you have to hand generate that yourself or hopefully you can scrape that somehow. And then in the rOxygen documentation header for a given function, add a new tag called atcdocs that's going to point to the source c function that that particular r function is using.

And then she says that's going to be kind of manual for now, but there may be ways to do that automatically later. And then building upon the tooling that our oxygen 2 gives you, giving you a way to parse that custom tag and do the necessary bits to write the manual page to link to it. So thankfully, not just showing telling us the workflow, she's going to walk through it for a hypothetical example here where she's got a minimal R package called HGB, where it's got a simple hello world type function. But she's going to put in a custom tag with the custom, literally custom tag, in the rOxygen header. And, of course, it by itself, it looks fine. Nothing's bad is going to happen, but rOxygen by default is not going to know what to do with that yet.

So now she's leveraging what's documented in the rxn2vignette to say, okay, there is a method called Roxy tag parse Roxy tag custom. Yes. That is all one function call separated by underscores there, which is telling our oxygen 2 what to do when it sees a set a tag of a certain type. So she's got some example snippets here where you can see straight from the vignette, you can write these custom functions that are going to either look at the tag and then also x and then be able to write it out as a new section in the docs based on seeing that tag named custom, taking in what was the material from it, and then putting it into this new section.

And then well, we were just talking about LaTex earlier, Mike, when you look at an r help file, you see, like, a web view of it. But behind the scenes, that is LaTex style code when these are compiled. And so she has a last section a last function to format that custom rd section that she's making and puts in literally the the latex style code for the section and then an itemized list of the value of, I believe, that source file. And then she's got the building blocks here. Now it's not quite done yet because then she's got to create what's called a rocklet that's going to help for this custom tag to be shared potentially with other packages that wanna leverage this. So this is an optional step.

But if you think you've got a workflow that could benefit the community in your particular package, this is a great way to share that via what she's done here. She's running a custom Rocklet function that's going to declare this Rocklet. It's a fun name to say out loud. Hopefully, I'm saying it right. But then be able to process that and then to be able to export that out as a way for other users to to leverage that. So again, that might not always be the case, but that is one way they set the building block of actually sharing it.

And then there comes the question of, well, do we actually need a separate package to hold this new custom tag processing? And you may or may not need that all the time. There are cases where maybe it's so specialized it would be only your particular package is going to use this custom tag. It may not need to go to that external that extra effort of making an extra package out of it, but others might say, you know what? This is good enough or this is good for the community. Maybe we do want to share it. So there are ways that you could do that because you've got some narrative about where it might be helpful and where it might not. So in the case of the eye graph here, this was a custom tag that was already meant for eye graph itself. So she thinks that could live in the package itself. But again, there could be cases where you want to share that with a community.

So I've been fascinated by this because, again, I've seen packages come through our weekly recently where they say, add this custom tag in your oxygen header, and we're gonna do something with it. Whether it's like this testing generation or in particular that newer package that came out recently that gives you kind of a a workflow for ingesting, like, data in an ETL process, they're also built on our oxygen tags as well. So lots of interesting use cases for this. It'll take me a little bit to wrap my head around this, but it's nice to know that the machinery is there if we wanna do some custom processing with these documentation tags.



[00:29:37] Mike Thomas:

Yeah, Eric. This is interesting. Every once in a while, I come across a package that, allows you to essentially do things in your own package with, you know, these these are oxygen decorators. Right? I think Plumber is an example potentially of a package where we do that. Right? There's sort of That's right. These custom Roxygen, tags that that will stick on top of our code and and quote unquote decorate our code with that serve a specific purpose or, you know, execute the code that is underneath it or the functions underneath it, a particular way actually based upon the roxigen that's been written. You know, one of the the real standout things for me in this blog post is how many functions there are from the roxigen 2 package that I had no idea about, this tag markdown function, rd section function, rocklet function, all of these things that allow you to develop, the custom roxigen as Mel, you know, so eloquently sort of outlines here. I haven't particularly had a use case to do this, yet myself, but I I do leverage some packages, including Plumber and a couple others that that do this. And they they must have, you know, gone about this type of a process in order to develop, you know, this custom roxigen that serves a a particular purpose. So being able to sort of peel back the curtains, for those types of packages that do that, you know, leveraging Mel's blog post here was was really neat to see how it all comes together, how it all works together, you know, all of the different things that you have to watch out for including updates to the the description, file within your R package.

You know, leveraging, this this these particular packages, and the the the config, the needs, and the build, and all sorts of things like that to be able to specify exactly what you're doing here, the the custom rocklet. You know, I took a look at the vignette that Ma'el said she used for for quite a bit of this blog post, called extending roxigen 2, and was able to to take a look, at, you know, how they explain and describe going about this particular process, you know, adding a new dotrd tag and and a lot of the things that Mel did here. Obviously, you know, we look peel back the curtains on that package itself for Oxygen 2 and no surprise Hadley Wickham's the lead developer on that with a couple other folks, including Peter Dannenberg, Gavarsardi, and Emmanuel, Uster.

So a really powerful team working on this set of very powerful functions, and it's it's sort of another way. I don't know. I think of it somewhat akin to, like, object oriented programming where, you know, we're able to just push the boundaries and extend R to do, you know, some pretty incredible things just beyond sort of the functional side that we all think of.

[00:32:27] Eric Nantz:

Yeah. And the package I was thinking of at the name escaping until now is called Maestro. The one that we're there we covered a few weeks ago on on the show. It's gotten this new take on these data specific pipeline, but their their whole API, if you will, for getting opt into it is to decorate your function with these custom Roxtrogen tags called, like, Maestro frequency, Maestro start time. So I have to look at the source, and and if I look at the source of it, it'll probably be very similar to what I was outlining here. So like I said, it's been fascinating to see behind the scenes what's happening with the power of our oxygen to take what you can customize yourself, but then to build the machinery to do things with it. So, yeah, fascinating area.

In my daily work, I haven't had to, but now that I'm consuming more of these, types of packages, it's great to know what's happening here in case that does inspire future work. So, you know, I feel like I've only scratched the surface of what rOxygen 2 can offer. This is just another kind of eye opener for me of just the immense power that that particular package has for not just generating your documentation, but frankly, to give you the gateway to do so much more.

[00:33:42] Mike Thomas:

Yeah. I think it's time to spin up a side project where we kick the tires on this.

[00:33:47] Eric Nantz:

Yeah. Don't please stop nerdcyping me, Mike. I can't take it anymore.

[00:33:53] Mike Thomas:

Yeah. No. I always thought it was just pure magic, those packages like Maestro and Plumber, but, turns out there's actually something to it.

[00:34:12] Eric Nantz:

Well, honestly, Mike, it wouldn't be in our weekly show without at least some visit to what I like to call the visualization corner. And we even know we're an audio podcast, we love talking about the best practice of visualization. And one of the key thought leaders in this space over the years has been the author of our last highlight today, Nicole Rene, who is, of course, a lecturer for health sciences over at the Lancaster Medical Institute. So in this great blog post, she talks about 5 particular tips that she's been using, especially in her latest visualizations where she shares her takes on the TidyTuesday datasets out there that I think we all can benefit from.

One of which, this is one that I've encountered quite a bit, is that there may be cases where by default, you'll get a certain range of your y axis that is a little more truncated, but there may be cases where maybe you don't wanna do that. Maybe you wanna show what in theory could be the lowest number or the lowest limit, and in particular for in the case of bar charts, having a start at 0 for, say, your frequencies or percentages or whatnot. Most of the time, that's going to help you potentially navigate what could be misleading representations on that. And in this example, she's got looking at the, the very fun penguins dataset looking at the number of penguins per island.

And on the left side of this, she's got a bar chart with those frequencies where the number on the y axis starts at 0 and then another where it starts all the way at 40. Now that might not be exactly a fair representation depending on what what you wanna do with these data. Now there may be there may be cases where it ends up being appropriate. But maybe in the case where they all have a similar height, maybe you don't get a lot of going all the way to the theoretical lowest limit. But there are other cases where other parts of the data that you might superimpose on these bar charts might be more accurate if you wanna impose, say, standard deviations or other types of metrics on top. So, again, that's kind of a case by case basis. But in case of frequencies, a lot of times, especially in my line of work, we still have to start from what literally is the ground 0, so to speak, what kind of frequencies or percentages those numbers could be based on.

And so like I said, yeah, there there is a trade off. There may be some situation such as in line charts where maybe starting at 0 kind of loses the key focus of that visualization. And the next example, she has a line chart where it shows the time course of how a variable, in this case the population of Germany, is changing over time. And in one plot, she does indeed start it at 0 on the right, but then on the left she starts it at a much higher number. So you can see the the increase, the kind of more, you know, what looks like a pretty rapid increase in the early debt part of the century versus kind of leveling off and then increasing again.

You kind of lose that nuance if you started at 0 and already the first measurement was already starting at a value already above, say, 60 or 70, you know, in 1,000,000 on that or, yeah, in 1,000,000 for that. So again, it's not like a one size fits all. You got to look at the context of the visualization you're telling here, the story that you're trying to tell or the insight to really say to yourself, do I really need to go to that lower limit or not? But that that's a nice segue into when you have, say, data of, like, 2 continuous or numeric variables and you're doing the tried and true scatterplot, sometimes you got to look at ways of doing appropriate range for that because sometimes you might not quite get the full gist of it if you are truncating that range too much or widening it too much.

And in her example here, she's got a scatter plot that we often use in model assessments and regression models, a scatter plot of the residuals, which is basically on the y axis. You've got the residual from observed versus the fitted values on the x axis. If you are looking for patterns like, say, symmetry or other, like, you know, indications that maybe you're violating a assumption such as the normal distribution, you wanna be able to compare pretty clearly to those points approaching, say, the zero line on the y axis where the residual and the fitted and observed are basically the same.

So when you truncate the axes going more in one direction versus another, it's hard to really tease out that pattern of potential symmetry if you have, like, the points above 0, the axis limits more than the axis limits below it. So there's a good example where it's a little nuanced, but you can kind of see it'd be harder to tell that pattern if you are not consistent with the level of the range you have above and beyond or above and lower that zero value. And she says similar advice could be done with another tried and true metplot assessment called the qqplot, also used to assess normality.

So again, making sure that you're akin to what type of visual or insight you want to tell to help dictate just how you're gonna, you know, change those limits on those particular axes. Up next, Mike, I didn't wanna touch this one. I'm gonna leave it to you because this can be a controversial topic in some circles in the visualization world, is dealing with the potential of multiple y axis. So why don't you walk us through her advice here?

[00:40:25] Mike Thomas:

Yeah. I remember a time where, you know, having 2 y axes on the same chart, was really, really frowned upon. And I think it got so frowned upon and and so much flack that that maybe, some folks out there started to say, hey. Well, it's maybe it's not as bad as we're completely making it out to be. But I think probably the general consensus is still that it's not the best idea and there's probably a better way to go about ensuring that the conclusions that are drawn from your visualization are the the correct and appropriate conclusions, and your audience is not being misled, by, you know, the differences in scale across these 2 y axes on on the same chart, which can have a big impact in, you know, how the data is interpreted, especially when there's intersection, between those those two particular lines, in my experience.

So I I love, the idea that Nicola had here, where instead of, you know, plotting this chart with these 2 y axes because the scale of the the y axes are very different, instead, bring them both to a down to a, like, percentage change basis, such that you can have a single y axis and you can compare, you know, these on a percent basis a little bit more apples to apples comparison, as opposed to, you know, having to wrangle those 2 very different, y axis scales. And, you know, I I think the plot that she shows here after converting to a percentage change, that the values for both, you know, sort of categories here in the legend, really tells the story that, the author would be trying to tell to the audience much better, than taking a look at what's going on in those dual y axis plots. So that's that's a great idea.

The last one, that she talks about is is how alphabetical categories often don't make sense, and you wanna ensure that you are ordering, you know, in the case of a bar chart in her example, ordering those bars in a way that is going to make the most sense for a user. So if your x axis is representing days of the week, probably not a good idea to order your x axis alphabetically. You probably wanna order it chronologically, right, from from Sunday to Saturday or from Monday to Sunday, in her example here. So you really have to think about and and we'll wrap up here at the end, but I I think a lot of what we're talking about here is just coming down to context and thinking about, you know, the practical ways that your audience are going to consume the data that you're you're showing them, and and making sure that you're thinking critically about what you are showing them. Another one here is, you know, instead of, on a horizontal bar chart, you know, if you're using chord flip and ggplot2, with a with a bar chart. Instead of, you know, ranking that alphabetically again, you might want to start with the bar, at the top that has the the highest magnitude or extends the furthest to the right, if you will. So, you wanna you wanna order your plot by the values on the y axis, if you will, as opposed to the the labels on the x axis prior to to flipping and inverting that scale. Another thing, you know, this is a very Albert Rapp style blog post. It reminds me of a lot of the the work that that he's done here that I think Nicole is doing a great job articulating as well. A little free tip out there that I think I got from either Nicole or Albert in the past is stick your conclusion at in the title, And that will really help, you know, drive home the point that you were trying to make. If there is a point that you're trying to make, you know, sometimes we're really just purely displaying data. But a lot of times, you know, when you're doing data visualization to communicate the data to an end audience, there's there's really, sort of, a standout point to that chart that you want to get across to them. And instead of saying, hey, this chart, shows, you know, car type by weight, we can say, you know, linking Continental has the the highest weight across, you know, the 20 cars in this this particular data set or something like that. You can stick that in the title as opposed to just really making that title an explanation of what's on the the x axis and what's on the y axis. So, excellent job by Nicola. You know, she's fantastic in these blog posts. Obviously, interspersed within these blog posts are the visuals as well as the collapsed code that she used to generate these visuals, I think a great way to round out the highlights this week.



[00:44:59] Eric Nantz:

Yeah. And I that particular point about the bar charts and, you know, the reorder that based on frequency, you often see that quite a bit as you look at the impact of different machine learning models when we look at the impact of variables inside those models, like variable importance or reductions in root mean square error or whatnot, they'll often order that by the one that's most influential according to that metric and then going down further. So, yeah, I've seen her put a lot of these things in practice even in her machine learning workshops that she's been giving a couple of times now.

So, yeah, I think that the idea of having a bit of common sense and maybe, you know, some things that I often do with my app development too is getting feedback early on in the process of, like, that particular output of that visualization, making sure that the key, you know, stakeholders or customers that are gonna be viewing that report or reviewing that app are understanding the intent of what that visualization is trying to show. So getting that feedback sometimes from other people, even if you yourself feel like you've nailed it, so to speak, but getting that extra set of eyes is never a bad idea. And we know the world of visualization can be a bit subjective sometimes. So being able to get that get that opinion early on is is a massive help for sure. That's a great point as well, Eric. Yep. And, we're we're running a little long here. So we're actually gonna wrap up the shop here a little sooner than earlier, but we're gonna leave you with our usual, calls to contribute to our weekly itself, which to get keep the project going. It's, you know, it's, of course, driven by the community.

So one of the best ways you can do that is to share that great new blog post, that new package, that new insight that has got your workflows all all, Jim, you know, improved or whatever. So you can do that via a poll request at the main website at arewehere.org. There's a link to a poll request in the top right corner. It's all marked down. You just fill out a little template, and then you'll be glad for the curator of that particular week to get that resource in the upcoming issue. And, also, we love hearing from you as well, so you can get in touch with us through a few different ways. We have a contact page linked in the episode show notes that you can grab to directly from your podcast player.

You can also reach out to us on the, the the podcast apps that are more modern, like Podverse or Phon or Cast O Mac, and send us fun little boost along the way, which a little, side tangent here. But, my good friends at Jupiter Broadcasting that run the CODA radio show authored by, Chris Fischer and Michael Dominick, they've been running a little contest to see what should be the official language of the show. You would not believe which language is up to run. Based on some recent boost, the r language is almost number 1 now.

So, who knows? We may get this in vected into other podcasts as well. We'll see about that. So that that's been fun. If you listen to that show, you may hear from yours truly on that from time to time. Nonetheless, several ways to get in touch with us are on social media as well. You can find me mostly on Mastodon these days with at our podcast at podcast index dot social. You can also find me on LinkedIn. Just search my name, and you'll you'll find me there. And it's frankly on the weapon x thing at the r cast. And, Mike, working with us, get a hold of you. You can find me on mastodon@mike_thomas@phostodon.org,

[00:48:29] Mike Thomas:

or you can find me on LinkedIn if you search Ketchbrook Analytics, k e t c h b r o o k. You can see what I'm up to.

[00:48:37] Eric Nantz:

Very good. Very good. Well, that's, never a dull moment in our lives. So we're gonna close-up shop here, but, of course, we thank you so much for listening from wherever you are. And, little programming note, next week, we might be on at a different time as yours truly is gonna be involved in a virtual event at our usual recording day. So we will we'll communicate one way or another what happens. But, nonetheless, we hope you enjoyed this episode of ROCE Highlights, and we will be back with another episode of Rwicky Highlights either next week or soon.

"
"41","issue_2024_w_36_highlights",2024-09-04,44M 58S,"A peek behind the curtain of how R handles that batch of code you send to the console, an adventure in automating the translation of Quarto documents to multiple languages, and there's no time like the present to give your code a little linting love. Episode Links This week's curator: Sam Parmar - @parmsam@fosstodon.org (Mastodon) & @parmsam_…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 177 of the R Weekly Highlights podcast. This is the weekly podcast where we talk about the awesome highlights and other resources that are shared every single week at rweekly.org. My name is Eric Nantz, and as always, I am delighted you join us from wherever you are around the world.

[00:00:21] Mike Thomas:

And you know what, folks? All is right with the world because I am not flying the plane solo this week. I am my awesome cohost, Mike Thomas, making his triumphant return to the podcast. Mike, how have you been? Been doing well, Eric. Yes. Speaking about from around the world. I feel like I've been around the world a little bit. I was fortunate enough to be able to attend posit comp and and see you there and I know we weren't able to record that week, and then I sort of went straight from there to more conferences in Boston and elsewhere and and finally just getting my feet back in the on the ground at home, back in the office. And, I apologize to to you for having to make you do it for a couple weeks, on your own, but you did a great job. And, looking forward to being back at it today.



[00:01:07] Eric Nantz:

Yeah. I survived, but, I think the listeners will agree that things are more correct now with you here. But, yeah. Back in it was a couple episodes ago, I gave my kind of, conference experience. I wanna make sure you got some time to talk about what you thought about Positconf and anything you wanna share with the audience about what you took away from that. Sure. Well, it's always just so

[00:01:30] Mike Thomas:

sort of invigorating to get to be around other, you know, data scientists and and like minded folks and listen to talks and just connect and and collaborate and be in that community face to face together. So I I thought they did a great job putting on the conference.

[00:01:46] Eric Nantz:

I was able to to see some of my team members at Catch Brook that I actually hadn't seen in person before Yeah. Which is really special and really I was able to be at the table when you saw him for the first time. That was awesome. Yeah.

[00:01:58] Mike Thomas:

That's right. Shout out Ivan. And it it was it was a fantastic experience you know some of the highlights for me I guess on the shiny side were you know really Joe Chang's, talk about leveraging you know large language models within your your Shiny app that are able to essentially put together a DuckDV query show you that DuckDV query just from natural language and return the results on screen to you which was pretty incredible I think we have probably no less than a 1000000000 opportunities, to incorporate that into some Shiny apps that we have have currently, but you know there were a bunch of fantastic talks. I think it's easy to not or take for granted all of the work that goes into putting together these talks and and how hard folks work to be able to put these presentations together for us. So just a big shout out to to everyone that that participated and, gave a talk this year and was able to provide us with some fantastic content. So I had a had a great time. I I got to see a little bit more of Seattle as well, than I had seen in the past. Sometimes at these conferences, it's it's tricky because you're, you know, you're just sort of glued to the conference and sometimes it's hard to to make it out of the hotel but I was able to go out with a few clients as well finally was able to see the the big Pike Place Market I think downtown there which was cool and did a little walking around and enjoyed enjoyed getting out to the West Coast and seeing Seattle. So we'll see what, I guess, Atlanta brings next year.



[00:03:30] Eric Nantz:

That's right. You're going to Hotlanta next year, so it'll be my my first time there. But, yeah, I'm seeing it's always funny trying to get a read of the room whenever they make those announcements. It's always, you know, mostly cheers, sometimes a couple groans, but in either way, it should be a good time nonetheless. I don't think there were any boos, so that's good. Yeah. That's true. Yeah. Yeah. No no heat, so to speak, from that announcement. But, yeah, that was that was, yeah, so that will be here before we know it because I know the the months go by fast. But, yeah, it was, again, terrific to see you once again and, just to hang out with you a bit. And, yeah, like you said, I was in I was pulled in in many different directions. So I didn't get to see as much as Seattle as I would like, but who knows? Maybe I'll be back there next year for other events or whatnot. But, nonetheless, it was it was awesome. And and, yeah, I hope that the hype I I put around Joe's shiny announcement was worth it because I for the listeners, I I had a sneak preview of that before Joe made that presentation.

And poor Mike here was just asking me, any hints, any hints, any hints, and when's it coming? When's it coming? It's like, stay tuned for Wednesday. And, yep, it seems like it didn't disappoint.

[00:04:37] Mike Thomas:

Yep. For anybody that needs this information, Eric, it's a great great secret keeper.

[00:04:44] Eric Nantz:

When when I'm told to zip the lips, I know how to zip the lips, usually. Usually. But this time, I honor good old Joe on that one. But, nonetheless, yeah, amazing experience. And, you know, we gotta get get to business here. We got our fun our weekly issue to talk about here. This has been curated by Sam Parmer who was also at the at the POSIT Conf. I got a chance to hang out with him multiple times, and I actually had some nice dinner with him one of the nights too just walking around the Seattle area, but he and I always have lots to talk about. And for this, he had tremendous help as always from our fellow iRwerky team members and contributors like all of you around the world with your poll requests and other resource suggestions.

We're gonna go technical here, Mike, on this first one in areas that, admittedly, I kinda take for granted, but I guess could bite you if you're not careful. Now with r, r is one of those you might call interpreted languages, which means that when you launch r, whether it's through the console directly, through its own GUI interface on Windows, or through an IDE like pod you know, RStudio or the new positron or whatever have you, you're gonna get a REPL, which is kind of what the r console looks like. But if you don't know what repl is, and believe me, even when I first got in, because I had no idea what that is, that is the read, eval, print loop. And both r and Python and other languages come with this so that you can type a command, hit enter, and then you get the result.

Sometimes when you have a long command, you can actually space this out to multiple lines if you want and just hit enter, and you'll get with a little plus sign, not plus an addition, but plus to the new line that you're adding to that that console input. And you can do this for as much as you want, albeit you might get tired if you do this a lot in the console directly. So for most of the time, we end up writing our scripts or Python scripts depending on which language you are. And that way, you can run the whole file or you could send certain bits of that file into the into the console itself.

Apparently, recently, there has been a user that had sent a lot of input to this console or to the the interpreter itself ended up being hitting a limit that r has had for historically for a while of 4,096 bytes. And I did a little crack research before this. Apparently, a byte is a character, so it's kind of a one to one translation, which means that this user was sending a string of command that was over 4,096 bytes in that in that session.

[00:07:37] Mike Thomas:

It might not actually be that long, though. Right? If it's a character, it's not like it's 4,000 lines. How many characters in a if you you get your 80, your 80 width. Right? It's what we try to adhere to. Well, yeah. For styling, sure. Yeah. For styling purposes. So in a regular 80 80, width set of lines, that's gonna be let me do some quick math here. Oh, boy.

[00:08:01] Eric Nantz:

Dangerous.

[00:08:02] Mike Thomas:

51 lines?

[00:08:05] Eric Nantz:

Well, there may be something else going on here, and this is where, again, we're gonna get into the weeds a little bit by the author of this post that comes directly from the r blog. Thomas Calabrera, who is one of the members of the r core team, apparently has been diving into this issue as a result of this report that went on the r develop mailing list. And, apparently, now there have been advancements in our the development version to help get to, quote unquote, unlimited lines so you can throw through the parser.

So I'm gonna give you my take on how this works. This is a lot of machinery here that, admittedly, I've not gotten into as much. But one has to wonder just what process was generating that massive set of input that was causing all these bytes to be limited. And, again, maybe I have it completely wrong on what a character translates to bytes, but, nonetheless, let's talk about how when you send a command to r, how it actually gets in there. So, first, Thomas is quick to point out that the parser is not the issue here because the parser needs the functionality to look at code with input of technically unlimited length that could span multiple lines.

And one concrete example that he highlights here is an if statement. Because in an if statement, you need to look at that entire if statement, everything going into that to know what to do in terms of the rest of the console input. So that is always going to have to be parsed in the theory of an unlimited length depending on how extensive that is. So that's where that needs that needs to happen. But then there's not just the parser at play here. There's also the REPL itself, which, again, as I mentioned, is getting input from either the user or maybe copying pasting that code into the console.

And there is under a hood an API function in r itself. They call it r underscore read console, which is modeled after a c function called fgets, again, outside of my wheelhouse here. But, apparently, that has that has a capability to know whether it's getting one massive line or if it's getting multiple lines that was separated by, say, a carriage return. Hence, like I mentioned earlier, when you type a line, hit enter, but you're not done yet, you'll get that plus sign in the console. It's gonna know how to interpret that.

But that buffer that basically that empty space, if you will, that the repo uses to grab all this all this input has been in a constant definition for the limit length which has been 4,096 bytes apparently since 2,008. And before that, it was actually 1024. So it was much smaller back then. So, apparently, there are ways up until recently where you could overflow this repo buffer. I don't think that's to be confused. I'm I'm abusing terminology here, but I often hear about buffer overruns when code crashes. I don't think this is quite the same, but, nonetheless, the consequence here up until recently is that if you had an input that was expanded past this 4,096 limit, it would just truncate it. So it's as if you just didn't type in the rest of your code, so to speak.

So that that again, that's been a gotcha, apparently, that some people have seen, but now seems like that is a thing of the past in the development version. So there are improvements that have been made, but there are some nuances to keep in mind when term in terms of where you're actually interacting with r. So there is on Windows historically been, since my early days of r, the r GUI that comes when you install r on a Windows machine that has again, it looks like its own mini IDE, if you wanna call it that. It's got its own console in there, but there were issues that have been identified with that, that make it so that that buffer size was getting hit more often than it should.

And now they fix that by giving kind of an intermediate interface between the user typing in, hitting enter, and then that going to the REPL and for interpretation to help account for lines that could exceed this this limit. So, apparently, that's landing soon in production r. We don't know when yet, but it looks like there have been some improvements on that side of it. And then the r term itself, another thing within Windows, that's like the console itself, which is getting enhancements as well to help circumvent some of these limitations.

Again, I'm not a Windows user anymore, so I don't know exactly if that will affect me day to day. My guess is not for those out there. And then Linux, you know, Unix in general also have a a function here because they use a read line library whenever you start typing in the console and then sending that to the r interpreter that has had, limits before. You may run into that give or take depending on what OS you're on. I mean, Mac OS also encounters this. But in in conclusion, Thomas says that there are ways you can defend yourself against this. I think the biggest way is to put everything in a file usually if you have a huge massive chunk of code.

Otherwise, it looks like if you are grabbing this code from, say, another process that's generating it, I wonder if it was kind of akin to when you have JavaScript and you can make what's called a minified JavaScript file where it takes away all those carriage returns, but then you get this, like, massive wide text that the JavaScript parsers know how to deal with. I wonder if something like that was going on with this user, and they were just getting this massive, massive string of our code that was causing these limits. Again, speculation on my end, but it looks like these limitations are hopefully gonna be a thing of the past if you've encountered them before.

So, again, pretty we invite you to check out the blog post because I've only gone high level on what Thomas has outlined here. But if you find yourself in this situation, then, yeah, this this post may help you encourage you that the new versions of r will have a more unlimited limit, if you will, as opposed to this more truncated 4,096

[00:15:10] Mike Thomas:

limit. Well, I tried to do it justice anyway. Like, what are your thoughts on this? No, Eric. This is a tricky one and I think you, boiled it down for us as best as possible. I think it, again, reminds me of, like, how much actually goes on under the hood when you execute in our function. Alright. There's a lot of wizardry to compile that code and to get it to execute, right, that the actual c code, right, that is is getting run under the hood. And a lot of work goes into making this all work perfectly across different operating systems as well. So it's it's it's pretty incredible how easy it is for us to to take for granted about how well that works and it's a this is a limitation I haven't run into myself necessarily before. I've, I think, seen some strange things when I I try to copy and paste, you know, large amounts of code into the console and I certainly try to avoid that especially when I'm jumping between IDE's and RStudio versus Versus Code and dev containers and things like that. I'll I'll run into some strange issues. So I think as you said, you know, a best practice to keep in mind is is to just, you know, contain everything in scripts. If you have something really long that you want to execute, you can you can source that script if you want or just highlight the the code that you want to to run and execute that code, itself as opposed to, you know, copying and pasting large amounts of of, you know, code into directly into, your your console.

It this also brings me back to my early days of r and R GUI, which is something that I had not opened up in in quite a long time and it brought back some memories pretty quickly because that is admittedly where I started in R. I'm not even sure if RStudio existed

[00:16:58] Eric Nantz:

when I was getting started and are not to date myself too much but, at least Speak for yourself. My goodness. You know what ID is I'd use in the past? And

[00:17:09] Mike Thomas:

and you know when you do install our I was looking looking at my local R installation and and seeing, you know, our GUI dot exe, our term dot exe, and you know these are I guess things hanging around that we we sort of take for granted. Right? That that are packaged in with our local installation of R. But if you're using R Studio or if you're using Positron nowadays or or Versus Code or whatever it is, you know, these sort of legacy items, I I would imagine that our term is still applicable but probably our GUI, isn't applicable for folks using, you know, one of those those IDEs. These legacy, executables and applications, if you will, are still there, you know, if you wanted to get some nostalgia, pop into them and and try a little bit. I remember when I after I found our studio, I started a new job and and was working with somebody who was writing a little bit of our code, but we were sort of sort of siloed so we weren't working, together very well and I went over to his desk one day and he was using our GUI and didn't know that you could write in our script and, like, save in our script that you could rerun later so he thought that you just had to, you know, save commands in like a notepad text file or something like that and that was the way I guess that his professor taught him in in university. It never showed him that you could save in our script which is is a little scary but it makes me think about how far we've come but that really really interesting blog post, you know, it sounds like this is a potentially solved issue in the coming releases of our and maybe something that we don't necessarily have to to dive too deep in the weeds on in the future or or worry about because they're handling so much of this behind the scenes stuff for us.



[00:18:53] Eric Nantz:

Yeah. Again, you know, all the machinery behind the the hood here, or under the hood of of our yeah. It there's we definitely need to appreciate it more in the fact that we can have modern tooling on top of it and still leverage, you know, the language in many different ways. Yeah. That's a it's a testament to how far things are going and, yeah, development is not slowing down anytime soon. So, yep, a lot of a lot of things that you can go down rabbit holes of, and and, certainly, this is this is one of those.

And, yeah, speaking of having, like, modern interfaces on top of R and whatnot, of course, one of the the great interfaces or new add ons that many are using in terms of reproducible research and and then the like has been the quartile ecosystem, the quartile compilation engine for documents that can be coded up in markdown, but then have code embedded inside, whether it's r or Python and whatnot. And the but one of the best things about Quarto has always been the ability, of course, to write in markdown.

I mean, we all I always joke kind of at the end of each episode, if you don't know how to write mark markdown in 5 minutes, e y c would give you $5. So remember that presentation in NBSW years ago. But, you know, markdown, of course, can be written in any language. Right? And we have a very diverse community in data science as we all well know. So what if you're in a situation where you've written this great mortal document and now you want to send that to collaborators that have a different you know, spoken language or primary language.

Well, this next post is gonna give you some insights on how you can do just that with the advent of technology. This blog post is coming to us from Frank Aragona, and he talks about how he was able to translate Quiridon and ends any markdown file into any other language. So there are some services in play here that can account for this, but he did take a look at, you know, the existing services out there. Unfortunately, a lot of them, in his opinion, unfortunately, required API access that ends up even needing your credit card even if you're not planning on paying for it, services from Google and others like DeepL or whatnot.

But he did find into found another avenue called Hugging Face transformers, which do provide APIs to get pretrained models that are tailored for translation. And so now the key is, well, how do you actually use this thing? There is an existing R library for Hugging Face, but it required Conda to install some Python libraries. And like the author here, me and Conda don't quite get along, especially in my work environment. So, of course, I look for ways to simplify that. So he ended up coding up a more friendly wrapper to install these packages via Reticulate and the pip, you know, install interface for Python.

And, of course, this does necessitate Reticulate. And then through his package, it'll download the hugging face transformer Python bindings, which, again, I'm not as familiar with. But once that's in there, he's got a Python package now or an R package called translatemd. He's got the coding in the blog post on how to get that onto your system once you have reticulate ready to go, but it's gonna spin up a virtual environment for you, which again in the Python world is like what we have for RM for containing your dependencies, calls it our transformers, but you can rename it whatever you like.

And then once you have that ready to go, you can feed in a quartile document, and it will basically have a multi process approach. It'll parse the quartile document, apply the translation, and then in that kind of tidy form of the parsed translation, it'll then now rewrite to a new quartile file a new document of the translated language. So he's got a snippet of how this looks when you start parsing it, and much I already talked about in previous highlights when you talk about parsing code files, you do see different areas that have been parsed, whether it's the YAML, inline text, headings, more inline text, blocks of of of code or whatnot.

And then once you unwrap that and then translate it, then you start to see you can see side by side in the post what he's gone from the English version to, I believe, the Spanish version. So that looking pretty good, and then he's got a little picture at the bottom of the post that shows the 2 documents side by side. But like everything, Mike, there are a little bit of gotchas here that you might want to be aware of before you translate this. Why don't you walk us through some of the little off bugs he's found here? Yeah. There's a couple a couple of bugs it seems like you might have to manually

[00:24:21] Mike Thomas:

adjust for and it's actually pretty some of them are somewhat evident, just from the 2 screenshots which are are really cool. I think an awesome feature this blog post showing on the left, the English translation or the original, I guess, quarto document if you will and then on the right side the equivalent Spanish translated version of the same quarto document. So it's really cool to see these 2 things side by side. One of the things that unfortunately happened during the translation is that a particular section header that had a single, you know, pound sign to be able to to have that as sort of an h one header, the pound sign got removed or for the the hashtag, for for the younger audience out there. So you can see sort of on the right side, that that bold heading that would correspond on the left side doesn't exist in the Spanish translation. Translation. So it's just something that you would have to go into, I think in in add that pound sign yourself. Not a huge deal, in a small document probably in a big document situation it would be a little painstaking to have to go through and do that in a lot of different places.

We just rolled out 9 reports, 9 quartile reports, that were due on the 31st that we rolled out to our clients and each one of them was about 77 pages long. So in that case would have been difficult for us to do but if you have a small little, you know quarto report it shouldn't be too big of a deal to go in and add those headers and there's there's always control f too, right, that we can try to find the particular header text and then go in there and just just stick that pound sign in front of it.

The Light Parser package which is leveraged here has a a known bug as well with, quarto chunk YAML parameters and in particular if you have a chunk that is specified as eval eval false excuse me such that the code shows up but it actually doesn't get evaluated it translates that into eval no the n o instead of eval false So it looks like the Light Parser packages is working on rectifying that issue. I haven't looked into it yet. Frank, the author of this blog post, it says that hopefully this is fixed but maybe we can do a little follow-up next week to take a look at whether or not that bug still exists but, Frank also wraps up by letting you know that it's probably a good idea because we're potentially automating a lot here to go through the translated document and really with a fine tooth comb make sure that there are no other bugs because I think in situations like this right it can handle a lot for you you know maybe upwards of 90% but some of those edge cases depending on what you're trying to do in your quarto document, we do a lot of crazy stuff with include chunks, you know, child documents and things like that. So you never know, you know, if there are particular edge cases that this translator has not been able to solve yet at this point. So I think it's a good idea to to take your time and take a look at the output. Know that a lot of, you know, the the manual work that you would have had to have done has been taken care of for you, but there might still be some some little spots you might have to tune up.



[00:27:36] Eric Nantz:

Yeah. It it's not a direct one to one analogy here, but I've been looking at things like this even for the production of this very show. The podcasting service that we use has a functionality where it'll produce a transcript for our episodes. Right? I mean, it's kind of like a translation from audio to to text, so it's not quite the same. But just like you said, Mike, about maybe doing a double check before you sign off on it, I even noticed that will mess up a a few keywords here and there that I tend to spot and then put a correction in, but there may be others that I don't catch. So you won't get everything perfect in these, but, of course, we we cannot, you know, understate how much time this can save, especially if you're gonna do this routinely to multiple languages. I think that's a huge win for accessibility and certainly depending on your needs. I could see a use case where maybe you have an application, whatever, Shiny or something else, and you're you know, the user's doing a bunch of stuff, and then you want them to download or reproduce a report of those findings. Hence, a cordial document or a markdown document, whatever have you, you could plug something like this in and then make have that report have multiple languages. There's lots of automation ideas you could have at play here.



[00:28:49] Mike Thomas:

Very, very cool. Yeah. A little radio button allowing the user to pick which language they want, they wanna download the report in.

[00:28:56] Eric Nantz:

Yeah. I think, yeah, the possibilities once you get your hands on these tailored models, there's just so much fun stuff we we can do with it. So credit to Frank here for another awesome use case of both automation and portal to to to check a lot of, nice solution here. Gotta love it. Last but certainly not least in our highlights, episode here, we've got another great use case of, speaking of cleaning things up sometimes. Yeah. Yours truly code sometimes needs to get cleaned up a little bit. I know, Mike, you write perfect code every time. Right? Right?

I wish. You wish. Yep. You and me both. Well, that's where you could manually look at what you've messed up, which again, I often do and make the corrections, but there are ways that you can have something else scan your code and make your life a bit easier. And that, again, is our last, highlight for today. Another great blog post coming from the one and only Ma'al Salman who, again, has been a frequent contributor to the highlights for a very long time now where she talks about a really great way to get started with using winter, the winter package to get your code base up in tip top shape.

And I admit when I first heard of linting, I didn't know what heads or tails this was all about. Now I'm starting to come around to it, but this is a post I wish I had seen years ago when I started to see this mention or I see this magical stuff being used in other people's dev sessions or selling their code with a keystroke just so it's perfect. And I'm like, how does that even work? Well, now we're gonna figure out how that works. So, again, the lint r package is what drives this in the r ecosystem, but many languages have equivalents in their in their, package ecosystems.

The first step is you need to tell winter what it's going to do, and that is through a configuration file. And so that is with the syntax name of dot winter. So it's by default a quote unquote hidden file. But if you put that at the root of the project that you're gonna use the linter on, you can then put different options here. And you can start by letting linter do everything it wants to do, and that's where in the snippet she has here in the post, there is a function called linters with tags, and tags is an optional parameter. If you make a null, it's gonna use everything, every check that it wants to do. And then also the encoding as well, which most of the time you can do UTF 8. Sorry for any Windows users that have to do something different in that, but that's usually what I stick with.

And then once you have that, if, say, you're writing a package, like in the use case of this post, you wanna lint it. It's just lintrcoloncolonlint_package, and then it's gonna depend on the environment you're running in, say in RStudio ID, you're gonna get another tab appear next to your code or console where it'll highlight all the different issues that the linter has identified. And, yeah, yours truly sometimes has a long list of this, and I have to parse through this a little bit. So now you know what the issue is.

What are you gonna do about it? So Miles outlines a few points here. Mike, why don't you take us through what you might wanna do once you found the problems?

[00:32:45] Mike Thomas:

Yeah. Once you've identified, you know, some of the problems, some of the things that you can do, I guess, for for one example that Mael provides here is is maybe you have a function that is is reading a super long path and sometimes, you know, hopefully, we're not doing too much of hard coding a specific path on our machine. We're using things like the here, the here package to be able to sort of build out that path, you know, relative to anyone else's system who might be running our code. But you can break that up instead of supplying that path directly within the function argument. You could define that path as a variable first and then pass that variable into the following function that is going to, you know, access that particular path or or apply the logic against that path. So it's it's interesting things like that, you know, some simple stuff that you can integrate.

One of the nice things, about the this linting functionality is that and I know that I always have, you know, particular edge cases in my code where maybe it's just not possible to get this line down to 80 characters, for example. Like, it's not feasible. I would have to use, you know, glue or or paste statements and it would start to get ugly and it's, you know, it's only 83 characters and it just really sort of makes sense to leave it the way it is. So if I wanna skip the linting for a particular line, you can actually add, this this string called no lint, and then the name of the linter exclusion to a specific line in your dotlinter file, I believe.

One of the cool things as well about the linter package is if I remember correctly that you can sort of set up, custom linting, you know, based upon the the styling guidelines that you might have internally. So, you know, depending on if you like your arguments written in wide format versus long format, there's only actually one correct answer there. It's long format but we could have a whole podcast about that sometime soon. So you you can do interesting things like that and then you know one of the I think final really nice things as well is the ability to, you know, put this all in a GitHub action for CICD purposes where you can just specifically lint changed files which is nice so that you're not running this CICD and spending your GitHub actions minutes on your entire, package repository or your entire just our repository if it's not necessarily a package but you can just run your linter against the new stuff, as part of your your CICD process and then get those warnings or corrections you know during that that pull request, before things end up in the main branch of your repository and headed to prod. I I think that's that's really interesting. It's something that we haven't leveraged at Catchbrook, before. You know, most of our, CICD is is just around either rebuilding the package down site or running unit tests, but but linting I think is a really cool additional additional use case for, you know, having these automated checks as part of your, you know, continuous integration workflows and I really appreciate my Elle calling out the ability, to be able to introduce, linting as part of your your CICD process and particularly only linting, the change stuff because that's something that we struggle with, as well on occasion is using up a lot of GitHub actions minutes, on code that's already been checked before.



[00:36:13] Eric Nantz:

Yeah. One interesting thing about, you know, the relatively newer IDEs out there or new uses of IDEs out there is that even just writing our code itself depending on your settings, I'm thinking, of course, of the Versus code r extension, it will, by default, run linter already, and it will already highlight in your code where you're like, yeah, pass 80 character width or other things where things you're referencing a variable that hasn't been defined yet. And I admit there are times where, like, I don't wanna see that right away. I just wanted you to see that, like, maybe when I'm close to, like, my cleanup stage. So there there are ways that as as Mel's hotline here, you can turn certain checks off. I'm still getting to know, like, the best use cases of that for me. I admit I haven't put in GitHub actions yet because I always I don't wanna say I have trust issues, but it's like something that's gonna edit your code. You're kinda little nervous. Like, is it gonna get it right? When we just saw about the translation thing, it might not get all these translations correct. So but maybe I need to trust more. Trust the process as a old sports cliche goes.



[00:37:21] Mike Thomas:

Me too, Eric. Yeah. It it's hard during development when you see, like, you know, in Versus Code, the the problems, panel. Right? Just just loaded down with all sorts of problems with the linting of your code and it can, you know cause you to to I guess spend some unnecessary time at the beginning of the process as opposed to doing that that final cleanup once things are in a better state. But maybe maybe that's just my workflow and maybe it's actually helpful to to rectify those things up front. So teach their own.



[00:37:52] Eric Nantz:

Yep. Exactly. I I still remember doing those, Twitch livestreams back in the day, and suddenly I'd boot up Versus Go and this file has, like, 10 or 20 these squigglies on their line. I'm like, oh, the poor the poor viewers are seeing my sloppy code. Oh, no. So then I quickly turn that winner off, and it looks like everything's perfect. Wink wink. Exactly. Not really. The they each your own. Exactly. But, yeah, as as always, the the tooling is there. It's your experience, right, how you wanna tweak it, but it's, again, great that we have all this at our disposal for sure. And we have a lot at our fingertips, if you will, when you look at the rest of the article we issued that Sam has curated for us. And we'll take a couple of minutes here for our additional fines.

And a huge congratulations must go out to my fellow, members of the R Consortium submissions working group because the R Consortium submission pilot 3 has officially been approved by the health authorities at the FDA. This is monumental in this particular pilot. This was looking at the ways of using R to create what in the pharma industry we call ADaM datasets. That's a specific format that is kind of longitudinal in nature, but it's often a key intermediate layout that is used to populate, tables, figures, and listings that often go into our clinical study reports.

So, again, a great way to show that, yes, we can use r in many aspects of the clinical submission process. This was certainly a key focus of my positconf talk on the efforts of Shiny and web assembly. But again, we're looking at all the different parts of a pharma submission process. And like always, they are all the materials are reproducible. All the materials are on GitHub. We wanna make sure it benefits all the entire industry. And again, we invite you to check out the blog post where it's got all the key contacts. I've been involved with this. So my thanks to everybody that led the project, go from the working group side and the regulator side. We could not do this without them. So, again, major congratulations.

And that means I'm up next, so to speak. I'm on deck for pilot 4, so I'm super excited.

[00:40:24] Mike Thomas:

Congratulations to everybody involved, Eric. That is monumental, really exciting to see, especially the industry that that you're working in really adopt are so heavily and, really sort of pushing the language to be able to to change the world in a lot of ways. So so that's fantastic. Really excited for you. One additional find that I had is a look what looks like a, you know, shiny live potentially app, that's hosted, by Sharon Machlis that she created, and it leverages the Artut API, which interacts with, Mastodon.

And, what she has is a nice little interactive table here. It looks like d t, that posts that a list of all of the accounts on Mastodon that have made a post with the r stats hashtag at least twice in the last 30 days. So there's some familiar names on here. I see, I see Dirk Edelbuttel, Steven Sanderson, Luke Pembleton. I see a lot of folks that I follow in the r Kelly Baldwin data science community as well. So it's really cool little app to be able to explore and, I believe congratulations goes out to Sharon as well on her recent retirement, but it seems like she is not retiring from doing, continued r and data science exploration.



[00:41:48] Eric Nantz:

Yeah. I got credit to her. She's always been very, enlightening her her work to the fall, and she seems to be enjoying her next stage of life. And she was recently on the, data science hangouts with with Rachel and Libby this past week. So, hopefully, the recording of that will be out soon for those that weren't able to tune in live on that. But, yeah, I always like to see these great resources shared. Again, taking advantage of technology, things like, again, compiling Shiny into a web browser. You cannot have it better than that.

And Exactly. There's a whole bunch of more to choose from in this issue. We wish we could talk about it all, but we got our our day jobs to get back to. But we're gonna leave you with our usual, where do you find all this? It's at rok.org. The for the current issue is always at the home page. And, of course, the archive is available too if you wanna check all the back catalog out as well. It was searchable bar if you wanna search for specific topics as well. And this project is driven by the community, so we invite you to share that great resource you found online wherever you wrote it or you found someone else's great resource.

Please give us a poll request, which is linked at the top right corner ataruk.org, all marked down all the time. You won't need a fancy API to translate that. Just put in your link. You are all set to go. We have the template right there for you. And as well, we love to hear from you in the audience that is we got a few ways of doing that. So you can get in touch with us, via the contact page, which is linked in the episode show notes in your favorite podcast player that you're listening on. You can also send us a fun little boost if you have a modern podcast app as well.

You can also get in touch with us on these social medias, and the aforementioned mastodon is where you'll find me the most. I am at our podcast at podcast index on social. You'll also find me on LinkedIn. Just search my name, and you'll find me there. And on the x thingy, occasionally, we've got the r cast. Mike, where can the listeners find you? Sure. You can find me on mastodon@mike_thomas@fostodon

[00:43:54] Mike Thomas:

dot org, or you can find me on LinkedIn. If you search Catchbrook Analytics, k e t c h b r o o k, you can see what I'm up to.

[00:44:03] Eric Nantz:

Very good. And, yeah, you got a lot of things cooking, man. It must have taken a lot of time to write all those quarter reports. Great job nonetheless.

[00:44:10] Mike Thomas:

Thank goodness, for parameterization.

[00:44:13] Eric Nantz:

Let's just put it that way. Oh, yeah. That was a hot topic on the last episode if you wanna listen to that as well. Yep. I've been using that a lot in my day job too. I cannot live without it. So we can't prioritize everything in our life, but we can prioritize reports. So with that, we're gonna close-up shop here again. Great to have you back in the saddle once again, Mike. It's great to not have to do this alone for a 3rd week in a row.

[00:44:40] Mike Thomas:

No. I'm back for a while now. That's awesome.

[00:44:43] Eric Nantz:

Awesome. Yeah. Exactly. So we're gonna close-up shop here, and I'll do it for episode 177 of our weekly highlights. And we'll be back with episode 178 of our weekly highlights next week."
"42","issue_2024_w_35_highlights",2024-08-28,46M 31S,"A few tools you can use to find those elusive bottlenecks in Shiny app performance, adding a dash of interactivity to a reactable table, and save yourself many hours of manual effort with Quarto parameterized reporting. Episode Links This week's curator: Colin Fay - @colinfay@fosstodon.org [@ColinFay]](https://twitter.com/ColinFay)…","[00:00:03] Eric Nantz:

Hello, friends. We're back at episode 176 of the Our Weekly Highlights podcast. This is the weekly podcast where we talk about the latest great resources that have been shared every single week at rweekly.org. It is basically your one stop shop for curated art content made by the community and for the community. My name is Eric Nantz, and I'm delighted you join us from wherever you are around the world in your favorite listening device or wherever you're tuning in. And, well, once again, I am flying the plane solo here again this week. Mike is still heads down on his, urgent project that he's tidying up now. Hopefully, he'll be back next week, and we can hear all about his adventures.

But, nonetheless, we got some great highlights to talk about today, just you and me. We have a little cozy session here. But, of course, the Our Weekly Project itself is driven by the community in the form of our curators. And our curator this week was Colin Fay, the author and architect of all things Golem, which, by the way, congratulations. Go to Colin and his team, I think, are for the release of Golem version 0 dot 5 that just got released. We'll put a link to their announcement blog post in the episode show notes. But, nonetheless, he had tremendous help as always from our fellow, our weekly team members, and contributors like you around the world with your poll requests and other suggestions.

Speaking of Shiny, have you been here before? If you create an application that's used by, say, more than just you, You get it done. You get it released. Everything seems fine, and then you get that user feedback. This can be both a good thing and maybe not so good thing depending on your perspective. A lot of times in my experience, my feedback would be, you know what, Eric? The app looks good, but it's just kinda slow compared to some of the other ones I use. And I'm thinking to myself, boy, when did where did I go wrong? There wasn't anything completely obvious. Right?

Well, sometimes that puts you in the situation of trying to get to the bottom of this because in the end, the user experience of your Shiny app is a critically important piece to, obviously, get your users on board and to make sure that the application is having a sustained life cycle, if you will. So you wanna get to the bottom of this right, but you're not quite sure where to start because the term profiling may give you a little bit of fear and when you hear that, but no fear here. We got a great blog post that showcase a couple of different tools that you can utilize along with a new tool that I wasn't aware of to look at different aspects of your shiny ass performance.

And in particular, we're gonna be talking about a blog post from the Appsilon blog that was authored by Harsh Verma, one of their data scientists and developers at Appsilon, where he is talking to us about the different ways that you can go about profiling that Shiny application that has given you maybe a little bit of performance fits, if you will. There are a couple different perspectives that he highlights here. The first of which is trying to figure out just you made all those inputs, you know, the elements that you can use in your Shiny app to, like, you know, enter text, change the slider value, drop down menus, and whatnot that the user is interacting with.

And, of course, those are probably going to feed into some type of outputs in your app, maybe a visualization, a table of data, which we'll hear more about later, and other, you know, output, maybe HTML widgets, for example. And in between, you probably have a set of what are called reactive objects that are helping kind of facilitate processing into objects that are gonna be continuously updated depending on, say, inputs that are changing or other factors that are going on on the client side. And as you build up more of these, it can be pretty easy to lose track of just what is actually connected to what and just how much you might say chatter is happening between the what the user sees and what the back end is producing to get to the outputs that the user sees.

And that's where the first tool that Harsh, highlights here is the react log. The react log is something I hold close and dear to my shiny heart as a wonderful way to visualize this dependency graph of your inputs, your downstream reactives that are feeding into the outputs of your application. And trust me when I say, maybe it starts simple, but as you build the complexity, especially when you add modules in the mix, this can be a lot of things to keep track of. So the react log is a widget that will display, interactive flowchart of sorts where you can basically run the application in a typical flow, and then you can stop that and then put the react log in focus where it's going to let you traverse through that previous session where you were clicking inputs and then generating those outputs. So you've got a great way to kind of stop the steps, so to speak, so you can hone in on them further, and it's got all the connections between these inputs, reactives, and outputs in a very clear diagram. Again, highly interactive as well, so you can zoom in on certain points.

You can also filter based on different names of inputs. You get the state of the inputs such as whether they're ready for something new, whether they're actually calculating, or the process of invalidation where now they know something's wrong in that particular upstream dependency or should say outdated, so it's got to do some recalculation and whatnot. So this is a great way for you to see maybe you have a lot of reactive processing in between an input and a downstream output. Maybe there are opportunities to refine that a bit.

And really seeing that that graph in front of you is a wonderful way to highlight just how complex that application's dependency graph has actually become. And, again, I hold the prof again, I hold the React log very close to my dev toolbox. And if you wanna hear more about react log itself over in this little, you know, few minute overview I gave you, you wanna tune in to a previous episode of the shiny developer series that we have linked to in the show notes where we sat down with the architect of the react log from the Shiny team, Barish Slurkey, for a really fun conversation about the enhancements he made to reactlog many years ago.

Now that's great to kinda get the overall picture of your dependency layout in your application, But there's also the fact of, well, when you're actually doing the calculations, what's really going on in terms of resource usage and time spent? That's where a professor is, another wonderful, package in the R community authored by Winston Chang, also a member of the shiny team. That's where it comes to play to really help you in these situations. It can be used for things things besides shiny apps, but it runs very nicely to shiny apps as well.

The flame graph is a great way to see, you know, a snapshot of just the number of steps or functions that are actually being called in these different operations that you're running in your application and then seeing just how long they're taking because the the x axis, the horizontal axis of this flame graph is the time spent in this application to create or, you know, produce the result of that function call. And this is where you can maybe see, perhaps there's a, say, a data processing step that's just taking a lot of time and you can hone in on these different bars of these different pieces of your app functionality and start to see just what's really happening here. It's a great way to, again, pinpoint the bottlenecks and figuring out just how complex some of these operations actually happen to be.

So a lot of times, the time spent is probably where you're gonna most focus on, but, again, it will have other outputs in terms of memory usage and other, number metrics that might be helpful to you. Now this isn't like an either or. I think they both complement each other quite well, and that's what Harsh, talks about in the middle of the post. There isn't really a thing about which one is better. It's more about these are giving you different perspectives on the complexity of your Shiny application from that dependency graph all to just what is actually happening when the code's being executed.

And then his post concludes with a new tool, new to me for sure, called shiny dot TikTok. And this is not an R package. This is actually a JavaScript utility where you can simply include this into your application with simply a call to a JavaScript script that's linked to in the blog post as part of your way in Shiny where you can arbitrarily execute a JavaScript, you know, script file. You just feed in via the tags $script function call and feed in the actual, CDN link to this utility. And then once you have that in your app, you can just run your app as usual, say, in your local setup, do much like what I mentioned in the React log, just go through a typical usage.

And then to view the output, you're gonna go into your browser's developer tools or browser tools, depending on which browser you're using. This is often what I'm using to look at the CSS behind certain elements, especially, you know, trying to theme things like my quartile presentations or for Shiny apps or bslib or whatnot. But there's also a JavaScript console in this browser tools where you can type in JavaScript commands on the fly. And so he's got a, you know, narrative here for which commands you can run here, such as showing all the measurements in this job that were produced by the JavaScript utility.

You can download all these into a CSV if you wanna post process them into R or other languages, and you can get a nice summary report, also in HTML format, where you could look at that, you know, as a as a separate web page. And it's kinda like a hybrid of the flame graph from profit is, albeit it's, you know, a little more, you know, organized, a little more streamlined, you might say. But it's also got color coding based on what are the outputs, also for calculations that are happening on the server such as, like, you know, server side loading or whatnot, and then also custom message handlers, which you might be utilizing in your app. But that's an interesting way for you to kinda get a quick take on how long some of these processes are taking, kinda like emerging between the maybe not so much emerging between the React log and the prophys visualizer, but it's an interesting take on it none nonetheless.

And what's nice is you could use this in your production apps. I don't believe it's causing a performance hit, although I haven't tried it yet. So you may wanna, you know, try that out yourself before you put it into production right away, but you can most certainly use this in a development version of your application locally as a quick way to get that, you know, more streamlined profiling output. So as always on our weekly highlights, I learn something new every time I talk about these great stories, so I will be adding shiny TikTok to my, developer toolbox alongside you know, again, much love to the react log package, and profit is has always been very illuminating in my, shiny profiling adventures.

So excellent blog post by Harsh, and, yeah, really looking forward to playing with shiny TikTok after this. You know, I can recall, you know, I would say many years ago as I was first getting into the art community, you know, fresh into my day job and fresh off my dissertation. In the way back years of, say, 2007, 2008. Again, to date myself, I've done that too much on this podcast episode already. Nonetheless, one of the things that was a bit of a challenge at the time was finding a really neat way to create polished looking tables outside of the typical Latex type setup. Nothing against Latex, but, it gave me, quite a bit of fits in my grad school days.

You fast forward to the today, and we have what I call an embarrassment of riches with how you can create really polished, aesthetically pleasing, powerful tables, both in the static format but also in the interactive format. And I'm really a big fan of the interactive format, hence I was, you know, psyched to give a submission to the recent POSITABLE contest of an interactive table. And the package that drove much of my contest submission was reactable. And it was a thrill to see or I should say, to meet the author of reactable, Greg Wynne.

And it was a thrill to actually meet the author of reactable, Greg Lynn, from posit at the recent positconf. I just, you know, thanked him many times for reactable. I just had a tremendous time using it. So reactable gives you, again, yet another great way to create a really nice looking table in R, and our tutorial to talk about this of how to change it from a static representation to a very interactive table is coming from Albert Rapp, who returns to the highlights once again. It's been a little bit, Albert, but we're great to have you back on here.

And he is a business analyst working on some really interesting AI and ML stuff, apparently. He's also producing great content on his YouTube channel, so you should check that out if you haven't already, but he wrote a recent blog post with an associated video tutorial on how you can create an interactive table of reactable with a few lines of code and some great examples throughout. You know, I I really love making tables in r. Some might say I am the table sometimes. That's a little inside joke for some of you. But, nonetheless, let's get right into it. Shall we? So as usual, let's find some fun data to put in this table.

And even though it's not the focus of this tutorial, the GT pack is by Rich Yone. It has a great set of built in datasets, and, yes, it happens to be with maybe some of my favorite food, pizza, of course. So he's got, Albert sets up with the, a filter data set of this pizza place data set for the pizza sales from Hawaii. Man, that's on my bucket list to go to someday, maybe someday. Nonetheless, he does a little processing. We got a nice little tidy table here with the the sales by month and quarter, and the revenue that's being generated at that particular time.

So what's nice is we just wanna get started right away with this tidy dataset. You just feed it into the reactable function. Feed in that dataset, and you've got yourself a nice interactive already kind of interactive table because in the blog post, you see this in action. It shows by default the first 10 rows, and there are actually 12 rows, one for each month. And you'll see at the bottom a toggle already go to the next set of rows, but Albert didn't have to write any code for that. It was already in the table function by reactable. So you already got a little interactivity along the way, but this is just using the default column name. So, of course, you might wanna spice things up a bit. So the first step is to add better, you know, human, you might say, human label type column names.

Now this is where there is a contrast to the GT package where in GT, it very much follows follows a ggplot or piping kind of flow where you can iteratively modify or add to the elements of your table. With reactable, everything is basically done in one particular function call, but with parameters that have a lot of associated sub parameters or general structures inside. One of those is for the case of column labels, you wanna leverage the columns argument of reactable and feed in a list with each list element named to the particular column you wanna add an attribute to.

So in this example he's adding the new names of these columns using what's called the caldef function, also part of reactable, where you can feed in different aspects that you want to tweak in this column, and in this case it's just tweaking the name of the column. So once you get the hang of that, very easy to now have nice nice looking labels in your table of column names. Now that's just the table on its own. Maybe you wanna start thinking about how you wanna publish this or share this with the community, so you wanna add a title and subtitle.

There are ways in Reactable. If you know CSS and JavaScript, you can add almost anything you want to these tables, but there has been another great package in the r community that I've often used. In fact, I use this for my table contest submission as well. It is a, you might say, a an extension to reactable called reactable formatter or FMTR. I'm just spelling it out formatter there. And this package is authored by Kyle Kuiva. Hopefully, I'm saying that right. But he, or he or she but reactable formatter has a handy set of functions.

2 of them are adding a title and adding a subtitle. And when you change the style, though, should you wish, whereas he's a or Robert is able to change, say, the font weight of the subtitle to not be bold, but to be a normal type weight of font. So we've got a nice title above the table. But if you look at the table in the post, you notice that the revenue column is not quite nice yet because it's in that typical decimal notation. But we're talking about money here in US dollars. That's what the data set's using.

So in that cal def function, you you can change much more than just the name of a column. You can also change the format of the cells, and that's using the cal format function where this could be anything from, like, a date or, in this case, currency, or even customizing a custom format of your own. So here, Albert just simply specifies as the USD for the currency, you know, type, and then to add separators, I e the comma the commas between each each three digit for larger revenue. And now you've got a table that has, you know, nicely formatted lined up values of revenue in that last column.

Now as you look at this table, you notice that we know we don't just have month, we also have quarter as well. But wouldn't it be nice to be able to let the user kinda toggle easy navigation between these groups? Maybe they're more interested in, say, the Q2 or the Q4 or whatnot. Well, reactable gives you a nice way to add groups, and, again, this could not be more simple in the reactable construct because there's a parameter called group by. You feed in the name of the column that you wanna use as your grouping, and right away, you'll see in the output from the blog post, you've got a nice collapsible element next to each quarter, a little caret arrow, if you will. You click on that, and now you've got the month sales and revenue showing up. And you can expand all of them, you can expand none of them. You get to choose, and now we have and now we have an even more interactive table for you to play with.

Looking nice here. Right? Well, maybe let's take it up a notch if we wanna share this with, say, a stakeholder or the community. You might want some nice, you know, summaries for each of these quarters as well. And what can we do for that? Well, what we can do there is we can say, okay. In the quarter view, if everything collapsed, what if we wanna add all the values under sales so that that displays when it's, like, nested, but when you unnest it, then you get the individual values right below it. So that's where you can now add an aggregate call to that call def of the sales column, and you can choose which aggregation method you wanna do such as the average or mean or in this case a summarization or sum.

So that's great. Now you can have for each group the total sales showing up in that default view with everything collapsed. Again, we're talking about interactive tables. Right? Well, what if you wanna instead of point and clicking, you wanna filter for a particular month? Well, reactable, again, has a handy little parameter in calldef called filter easy for me to say. Has a handy little parameter in the calldef called filterable where you enable that toggle to true, and then that particular column now has a nice little text box right under the column heading where you can type in the name of a particular, say, month you want, and it will auto complete, or I should say dynamically update based on the first few letters.

So I just start typing j a, I'll immediately get January showing up under the quarter quarter 1, you know, grouping. Again, very easy way to add filtering elements right away. You could have done that with sales as well, but in the end, this is a great great little showcase of even more interactivity without any custom CSS or JavaScript required. Now hold that thought for a second because we're gonna dive into that a little bit here, because what if at the very bottom of this table you want, you know, a common question to be answered of just what is the exact total of all the years revenue and all the years sales.

So there is a way to do this a couple of different ways in reactable, one of which is adding a table footer using our code to basically cuss define a custom function that takes as inputs the values for that particular column and the name of it, if you wish. If you want to do some of it, you can. And that's where you can use any ordinary r function. In this case, he's using the sum function again to summarize all to add up all those revenue values. And then from the scales package, he's making sure that's represented in the near currency format using the scales dollar function.

Again, that's based in r. That's not based in reactables formatting. And once you do that, yeah, it it does work, but there's a little nuance here. Let's say you wanna do that filtering after all. Maybe you wanna go to January instead using that annual text box filter. Well, you notice when you do that, that revenue summarization and that sales summarization does not change. It's not respecting that filter because that filter is being done in what we call client side interactions. The the summarization itself was computed ahead of time before the table was actually rendered.

And unlike in Shiny, there's no, like, direct communication between what the user is doing on the filtering side and the underlying r function that created that summarization. Well, have no fear, folks. If you're willing to invest in a little JavaScript knowledge here, you can have a dynamically updating summarization with JavaScript code. Now this is where, you know, if you're new to JavaScript, it'll it'll look maybe a little different at first. But once you get the hang of it, I think you can do little basic things like this without too much trouble. But the example blog post, of course, has the code where Albert is defining a custom JavaScript function that initializes an empty total of 0. And then basically for each row that's currently present using this state object, again, the state is like what JavaScript sees is what the user is seeing in that table.

So for example, if they're creating a filter, they're just gonna see what that state object is just gonna represent, what is actually showing on that table. Great. So he takes that and then for each of the rows that are in that state of the filter, he's gonna add up the values based on the column ID that's being supplied here and then return that total. So he does that for both the sales and the revenue along with some formatting of the revenue using some JavaScript, you know, functions as well, which, again, you can see in the show note I mean, in the blog post.

And there you go. Now when you do that filtering, I'll even type it while I'm recording here. Sure enough, when I filter for January, that total is now updating accordingly. So this is terrific. A terrific way to make sure that you get that shiny like experience, but again, this is all done in client side, and reactable has JavaScript bindings that you can customize as much or as little as you like. Lastly, speaking of making things look good, Albert wants to change the style of those, what I call, nested group rows versus the, you know, the rows of the actual data itself.

His first attempt at this actually ends up coloring every row in the same color, which kind of defeats the purpose, doesn't it? But that's a artifact of using this row group this theme function with the reactable theme, function, and that, by proxy, is selecting every row for that theme. So, uh-oh, that's not quite good. What can we do differently? Well, again, we can go in the JavaScript world where he defines a JavaScript function to say, you know what? I'm gonna take the row itself, which is called row info. It's like metadata for the rows each in each of the in each of the table.

And for each of these groups, that first row, which is just showing the quarter name and whatnot, that's level 0. Remember, JavaScript index start at 0, not 1 like an r, so that might trip you up a bit. But he's saying if that level is 0 that's being shown, then do the custom background on the cell, you know, color. Otherwise, leave the rest the heck alone. And sure enough, at the end product, you've got those quarter rows with that nice little, you know, shading so you can quickly visually distinguish those from the actual data cells as well.

And last but not least, he does a little change up to the style of the footer, and this is again very simple to do. Again, a little bit of, CSS demonstrated here where you can add a footer style with custom CSS function from the HTML tools package, or you can give it, like, the font weight, and in this case a little line at the top called border top. So you can quickly see just like you might be writing this on paper, putting a line under the numbers you're calculating, and then making sure that visually pops out.

So again, a tour de force of how you can take a native reactable table that just comes with that data frame being fed into it, all the way to a very nice, polished, and again highly interactive, multiple levels, table for you to look at. Again, reactable and GT, wonderful packages for creating tables. Albert's been doing a lot of content on each of these, so you wanna check out the archive of his post for of his blog, I should say, for many of our posts that talk about using these these great packages in action. So I am thrilled thrilled to see this.

And rounding out our highlights today is another great showcase of another awesome piece of technology using a framework that's quite familiar to those that have been in the R Markdown ecosystem to save you a bunch of time and a lot of future effort. And, yeah, I think future you will thank you for investing in such techniques. We are specifically speaking about the Quarto publication system, which has been a hot topic ever since it was announced at Posiconf back earlier in 2022. I can't believe it's been that long, but, yeah, Quartle has been around a little bit already.

And in particular, it has a lot of functionalities under the hood, but one of those which is definitely been inspired by the R Markdown predecessor is the idea of creating parameterized reports, in that you can compile a report and feed into it a set of values, I e parameters, to maybe update or dynamically change some content depending on those parameters. And one of the real innovative thought leaders in this space of taking advantage of parameterized reporting from the Rmarkdown ecosystem and now Quartal is the author of this latest blog post on deposit blog, JD Ryan, who has had quite a career transition, I must say, as she has gone from being literally in the field or you might say in the streams working in the Washington State Department of Agriculture as an environmental technician, and now she is leveraging her data science skills more in the, I guess, an office type environment.

But I had the great pleasure of seeing JD again at the aforementioned positconf earlier this month, and I must say for the many conversations we had, whether it was at the data science hangout or, you know, here pepping me up for my talk or even afterwards as the conference is winding down. Yeah. Her, her energy is infectious and, sometimes I wonder where did I get that energy? Where can I get that? That fountain of youth that she seems to be drinking from? But nonetheless, it was great to touch base with her and and yes, she was kind enough to give me a little sticker too. And by the way, if you want some awesome swag, if you're a cats lover like she and I are and you want to flaunt R at the same time, you want to check out her awesome little shop on Etsy, which I'll link to in the show notes. Nonetheless, JD has been in this, world of creating privatized reports for quite some time, and she mentions how much it's been extremely helpful as she was transitioning to her data science role in terms of creating in upwards of hundreds of these reports for farmers across the area based on their particular data needs.

And instead of hard coding all this 1 by 1 for, like I said, over 300 of these, she was able to leverage the parameter equals the parameter functionality to create both interactive and static or PDF versions of these reports and have them dynamically render in a batch setting. So how do you actually get there? That's where the meat of the post dives in here, where there are different kind of high level goals that you want to look at depending on how far you want to take it. Obviously, if you only have a few reports, maybe maybe manual is the better ever for you, and that might be might be what you're used to. But then, as like I said, the scale starts to escalate.

If you find yourself regenerating a lot of these things manually, that's when you're on and start investing into the idea of parameterized reports. So how do they actually work? Well, no matter if you're doing a report based in r or based in Python, you will have a capacity to be able to specify these parameters either in the front matter YAML of that quartile report if you're leveraging the knitter engine, I. E. Using r, or if you're in the Jupyter setting, if you're leveraging Python, you can do a specific code chunk with a optional, tag attribute called parameters, and you can literally just type in the values 1 by 1.

Obviously, the parameter values are more optimized for, you know, textual type parameters, maybe numbers. You're probably not going to be able to use complex classes because it's got to be something that can be specified in text, by the renderer. But once you have that going and once you have those parameters defined, then you have many different ways to actually compile this. First of which is within a IDE like Rstudio or leveraging the quartile extension in Versus Code or positron, you can compile the report and it will use the default values of those parameters that you specified so you can get a feel for if the parameters are working as expected.

And so another option is maybe you want to keep those default values as is, but you wanna experiment with changing maybe 1 or 2 of them. Well, quartile is not like a r package, like rmarkdown was. It is actually a utility, a software utility on its own with its own command line interface. So you can run the quartal render function and feed in the quartal markdown document, you might call it template, and then you can specify 1 or more parameter values with the dash capital p flag, and then give the name of the parameter, a colon, and then that parameter value.

So that's a great way if you don't want to change your your default parameters and you want to get a quick take on maybe changing those parameters on the fly, you can quickly do that. Or if you don't want to type those command line flags, you could have a separate YAML file called params. Yaml, where then you can simply put them much like you would in that frontliner. Yaml for the default parameter settings, and just in the Kortal renderer call use the dash dash execute dash params flag, feed in that YAML, and now you've got again that that report compiled using those new values.

And there is more, of course. If you are in the R ecosystem, there is the quartal R package, which again is not the quartal engine itself, it's just you can think of it as like an API type package to the quartal static rendering engine, where then that has an execute underscore params argument, and you can feed the parameters as a list object, and then feeding it as input the name of the file that you're going to render. So you could do that from R itself, and now the wheels are probably turning here. You got all these different ways of executing and compiling the report with parameters.

What are ways you can actually automate that? Well, there are a few techniques here and I've used a couple of these already, but they're they're really, really solid. One of which is to have a data frame which has all the combinations of the parameters that you're interested in rendering as each row. So in her example, she's got a year, a producer ID, and literally does a expand grid of these 2 year values and 4 ID values, and you'll get then this, 8 row data frame with each of those parameter combinations.

That's pretty straightforward, and what you do with that is you can augment then the different output formats you want. Maybe it's HTML, and then the actual file name using a little paste magic, and then you can have then a nested column that has these parameter values as lists, I. E. The columns that are already in this data frame, and make it a named list. And she's got a nice little snippet of code that does this with the purrr package, another one I simply can't live without, and again, if you love your cats, then how can you not love the purr package? So it seems like fit for purpose here.

Then once you have that, then you can have a customized tidy kind of data frame organized by 1 row per iteration, and then back to purr you can use the handy function called p walk which you can feed in the data frame itself and then that basically means that each column of the data frame is like a function parameter That can get fed into the quartal render function, and those parameters are going to be fed very clearly, as named arguments. So again this may take a little getting used to, but again you can leverage her snippets of code that she has in the post and try it out for yourself, and this can be part of a larger scale pipeline.

Just imagine that you want to do these on a routine basis, you know, you can find ways to automate this. Heck, you you know, quarto is a first class citizen output format of the targets package that I speak so highly about. You could lever something like this in your targets reports as well. There are lots of interesting ways you can use this, technique, again, to save you a boatload of time and effort in the future. So JD definitely has a lot more materials than what this blog post has here, and she's done a workshop about this in the past. She's done a presentation at Pazitconf, last year about this workflow and how it applied to her daily work. So, yeah, you'll be invited to check those out as well And again, love love the cat pics in this. I got it. Yeah. Makes me miss my cats from the yesteryear. But nonetheless, really great blog post by JD here. You can tell it's got all the bells and whistles of quarto itself in the blog post because it got nice little tabbed interface to go from the r snippet for that automated execution to a bash scripting, which links to another blog post by Solomon Moon who talks about how you can do this in a bash script. So again, whatever your flavor is, you've she's got you covered with different examples here.

And if that wasn't enough, boy, this issue has got a whole bunch in here that we wish we could cover today, and on in its entirety, but, yeah, time's not unlimited, unfortunately. But you can find all the additional resources at roku.org, and we'll take a minute here to talk about one of my additional finds that I think, might be worth looking at as you listen to the show and afterwards. And we heard announced at Posicomp in one of the talks a wonderful new extension for Quartle called Close Read. And what this is giving you is a way to have that kind of scrolly telling functionality in your quarter report that you often see in these websites that are kind of this hybrid of infographics and other, you know, neat neat utilities.

And the way you scroll through it, it kinda updates the content dynamically based on where you're at. It maybe freezes certain elements. Well, this this extension, Close Read, is really really great in this space. We haven't really had any we've had attempts at this before in the art community, but I think this one really nails it. So this, link is from Georgios Cameronis. I believe he also gave a talk at Posikov and this is a visual journey through world exhibitions and you kinda have to see it to believe it. I can't do enough, you know, enough justice on the audio form, but it's got this great interactive map that depending on where you're going in the story, it's gonna zoom in on different regions wherein it talks about the narrative behind these different exhibits.

It'll splice in some authentic pictures from these exhibits, but it's a great showcase of what is possible with close read and I am definitely going to take a look at this as I think about content that can be engaging to different audiences. I mean, gosh, my wish is I could do something like this in life sciences. I'm not sure if that type of material would be fit for it. Hey, you never know. Maybe so. But maybe I could find some other uses for it. But, Close Read is still early days, but it looks like a few people are already putting it through the paces.

In fact, I saw another contributor to our weekly, highlights in the past, Nicole Raney has also done an example of her tidy Tuesday visualization in a close read document as well. We'll have a link to that in the show notes. So starting to see it in the wild, and I'm definitely interested in seeing where the uses of that extension end up going. And yeah, like I said, there's a lot more to this issue, but we're gonna have to wrap things up here as as we're winding down. But of course, I like to always close with telling you how you can get involved to help the project, as well as this very podcast.

First, where can you get involved with our weekly? Well, again, visit our weekly dotorg, and we welcome all your contributions to that great new blog post showcasing R and Data Science, maybe a great new package that you discovered or something that's had a new release, a great workshop tutorial, or maybe another podcast or videos out there. Either way, there are different categories for each of these type of content, and the best way to do that is via a pull request already linked in the top right corner in that handy little ribbon on the Our Weekly site. You click that, you'll be taken to a predefined template. You'll fill out a little bit of metadata, and all we need is a link, all marked down all the time. Just like when you're writing that fancy quartile parameterized report, you use a markdown for that too.

So we invite you to share your resources on there and the curator of the week will be glad to merge that in. And also, we are definitely open for a curator spots as well. We would love to hear from you on that front. We have details on the process at rweekly.org, and that will take you to the GitHub repo where you have a nice reading that's been put together about how to get involved on the curation front. And also we'd love to hear from you on this very podcast, so we have a little contact page directly in the show notes that you can you can use to quickly send myself and Mike a message.

You can also, if you're on a modern podcast app, some of my favorites are Podverse and Fountain. Also for iOS users, Casa Mac, I hear great things about. With one of those apps, you can send us a fun little boost along the way that goes directly to us on the on the podcast team and no middle party involved. So that's available as well, but also you can find me on these social medias out there. Mostly on Mastodon these days at our podcast at podcast index.social. And I did, have a good friend of mine on one of the other communities I'm part of trying to figure out the best way to get started with Mastodon.

If that is something you as well are trying to get into, I'll put a link to a couple, great write ups that I found helpful in the community. If you're new to your Mastodon journey, because I'm starting to see a lot of my friends from data science and the art community are starting to get on Mastodon, but the key is finding the best way to get in touch with them so you can find their content. So I have some resources about that in the show notes of this episode. You can also find me on LinkedIn as well. Just search for my name and you'll find me on there and somewhat sporadically on the weapon x thingy with at the r cast.

Well, that'll do it for this episode of Rookery Highways. Can't believe we're at episode 176 already. Gosh. We're 200 is not that far away, so hopefully we'll get there. One way or another, I hope. But again, next week I hope to have my trusty companion here, Mike, back on the mic with me, so that will be, you know, a welcome change instead of just hearing me blab all by myself. In any event, we're gonna close-up shop here that will wrap up episode 176 of our weekly highlights and we'll be back with a new episode next week.

"
"43","issue_2024_w_34_highlights",2024-08-21,37M 2S,"Eric flies solo for this episode with a recap of his positconf 2024 adventures! Also how not to panic when you see a merge conflict in Git, the genesis of the new R ARUG community in India, and a great primer on creating your own Quarto templates. Episode Links This week's curator: Eric Nantz: @rpodcast@podcastindex.social (Mastodon) and @theRcast…","[00:00:03] Eric Nantz:

Hello, friends. We're back of episode 174 of the Our Weekly Highlights podcast. My name is Eric Nance, and I'm so delighted you joined us from wherever you are around around the world on your favorite podcast player or other media device. This is the weekly show where we talk about the latest happenings and resources that are shared every single week on the rweekly.org site. And, again, my name is Eric, and this is usually the part where I tell you I really don't do this alone anymore. But, unfortunately, I am flying solo today because my awesome co host, Mike Thomas, is cranking on on a tight deadline. He's he's creating some, sure, very innovative solutions with VAR and probably Shiny involved. But he's heads down with that, so he's not able to join us this week. Then he'll be back soon enough. And with the week off that we had last week, I wanted to make sure we got an episode out to you this week. So, yeah, I am fine solo, but this was a rather unique situation where, yeah, guess who the curator is. It was yours truly. Yay.

And it just so happened to be the week that I was at Positconf, which is actually gonna be when I first talked to you about here. And, by the way, if you're listening to this show on one of the more modern podcast players, you may notice that we have chapter markers inside. So if you're not as interested in learning about my experience at POSITCONF, feel free to skip over to the next section. But since it's fresh off, my week at POSITCONF, I wanted to take a few minutes to share some of my takeaways and just my overall experience.

First of all, yep, Seattle was a nice area. I've never been there before, but, yeah, the Pacific Northwest was was quite nice on the ice, so to speak, and had relatively smooth travel there. And right away, I was, you know, pretty busy, I must say. My first day, I guess, conference related, if you will, was attending what we call the r pharma summit, which happened last year at PasaComp, but I could not join because I was actually teaching a workshop about Shiny with my co host, Mike, last year, which again was very well received. But this time around, they had the summit the day before workshop. So then a lot of my colleagues are in the pharma industry that I've been collaborating with quite a bit with the r pharma conference and my efforts of the submissions working group and whatnot, and it was, very nice to collaborate on some potential new project ideas, going over some new things that we can do with Shiny in the clinical space, and things we can do with data formats and package validations and qualifications as a whole. So great shout out to all my our pharma peeps that were there, lots of great ideas and had a lot of engagement to pursue some new adventures that you may be hearing about very soon from yours truly.

So then the next day was the workshop day, and I have been enamored with the DuckDV database system, which you've heard us mention in many highlights of this year and last year. So I was thrilled to be given the opportunity to learn at a workshop this time around led by Kyro Muir who is the lead of Syncra Research, a consulting firm in Europe, and he is such a wizard with databases. Some of you know that he has spearheaded many of the R database packages. He's actually worked with the R Consortium to get funding for that effort, and it was thrilled to see him lay out some of the basics, but also some of the advanced features that DuckDV offers you as an R user, data scientists, trying to analyze large datasets, and I am more than ever convinced that this is a tooling that I need to pay attention to, and I want to start implementing in my future projects. So the workshop was awesome. A great mix of lectures and hands on exercises.

You know sometimes cliche but Wi Fi issues hampered us for the first hour but I was a good little student if you will and downloaded all the materials beforehand and was able to get it working on positron no less I decided to live dangerously and use positron beta version as, like, the IDE for working on the workshop exercises, and I've done a little bit of quirks here and there and actually performed quite well. So that was a good way for me to test the waters of Positron in a, real applied setting. So I'll definitely have more thoughts on that as I get more use of it, but, overall, again, the workshop was spectacular and DuckDV is definitely going into my toolbox very, very soon.

And on to the main conference itself. So yours truly had the pleasure of giving a a talk about my adventures of WebAssembly and Shiny in the context of my efforts of the r consortium submissions working group. It was early in the conference. It was right after the first keynote and one of the first sessions, which had the awesome name of drugs not bugs, you know, efficient uses of orange and Python in in clinical work. And, again, really really fun presentation. I hope it was well received. It seemed like it was.

Definitely had more of a story to tell in the initial goings, but my main point I wanted to articulate was that this is a fundamental paradigm, you know, shift in technology that I think can really supercharge many efforts were tedious static reports of some nature could be made interactive and shared efficiently across, let's say, different reviewers or different audiences. So I was thrilled to be given the opportunity and, certainly, my thanks to Pazit for accepting my abstract and letting me be a part of this session.

Had a really good turnout in the room. Lots of great questions both, in the q and a panel as well as afterwards. So, again, huge thanks to everybody that listened there live, and the recordings will be out fairly soon. And I can't wait to share that with all of you. Nonetheless, I will put a link to my slides in the show notes in case you wanna look at that before the recording. It's got some nice links to the GitHub repository and whatnot that I've created for the projects, and like I alluded to in the presentation, there's much more to come in this space as we finalize our submission to the FDA, as well as our adventures with the container piece of the submissions working group pilot as well.

So, yeah. 1 then I could breathe again so to speak with the presentation done and I attended many talks and, you know, as usual it's hard to choose and you got a lot of good talks happening at the same time. I've learned some great new things about Shiny development that I'm gonna take back in my my work, especially with some production apps I'm working on currently. Some good best practices of user interfaces and other great tooling that I think I'm gonna adopt quite quite soon in terms of structure and ways I organize some of my files and whatnot. Lots of good content as well.

And, honestly, one of the best features of these, conferences is the hallway conversation. So lots of great conversations both with my fellow compatriots in life sciences, but also listeners out there that are fans of this very podcast that came to say hi and I was very appreciative of that. It's always humbling to hear that this little humble effort that myself and Mike lead here is, helping people in their data science journeys That never gets old no matter how long I've been doing something like this, and having that feedback is a great way to keep motivated and keep going with this effort.

So I met a lot of new faces as well. I met, Colin Gipsy from jumping rivers. He was there at the conference. I had lots of great conversations with him. And, also, I was thrilled to meet Charlie Gao who had a joint presentation with my teammate, Will Landau, on the use of Mirai and crew for high performance computing and asynchronous processing. Wow. It was an awesome presentation. I can't wait for that recording to go out because Charlie and Will absolutely knocked that one out of the park and it was just the enthusiasm was infectious when you can see them talking about this, great tooling that they've been working on and, yeah, I was just kind of beaming with pride in the audience there where I barely found the seat it was almost standing room only so lots of lots of excitement in this space. So credit to Charlie and Will for a fantastic talk there. But, yeah, it was an absolute thrill to meet Charlie in person as well and met many other many other people I've seen online, you know, lots of great conversations and in fact we had a mini reunion of sorts of our streamer group. The the group of including Tan Ho, Daniel, Kyle, and others, and Jeremy as well from posit. We we met him for for the first time in person.

It was great, great to catch up on memories even though we've all, for better or worse, have not been able to stream as much as we would like since those early days of the streaming during the COVID pandemic. Nonetheless, it was great to catch up on memories and see what they're all up to and, yeah, lots of lots of fun time to be had in those conversations as well. And certainly, yeah, the conference kind of flew by at that point. And before I know it, I'm flying out early Thursday morning back here to the Midwest.

That was a early early flight so that day was kind of foggy, but I was already looking at my notes of things I want to research when I get back, and I'm already doing that research as of now. So lots of overall a really great experience, and next year it will be in Atlanta, I think, for the first time. So who knows what my adventures will lead to in terms of that conference. However, I'll be given another talk, maybe another workshop. You never know. But, nonetheless, it was a great experience. Again, I think my biggest takeaways are catching up with some of my my good friends from online, as well as meeting some new faces as well, and, yeah, some really spectacular talks and offer conversations throughout. So again, the recording should be out hopefully in a couple months, and you can bet that our week we will be featuring those when they are released.

Well, enough of my positconf wrap up, and certainly when Mike rejoins us on the podcast we'll be sure to get his takeaways from his experience, and it was always fun to meet with him as well. We don't get to meet face to face often enough. We haven't met since last year at the conference, so it was great to catch up with him as well. Well, without further ado, let's dive into our actual r weekly issue. And as I mentioned earlier, it was curated by yours truly, but thank goodness we have some great tooling and automation to put the post together.

So I had tremendous help from our fellow Arruku team members and contributors like you all around the world with your poll requests and suggestions. Our first highlight to discuss today is actually stemming from the aforementioned PosiConf and one of the talks that I could not see live, but I was able to catch the special, you might say, sneak peek at the recording. And it is a topic that if you've been developing our code for a little bit, whether solo or in an organization with team members, you're likely leveraging version control.

How you leverage it, you know, that can vary across the spectrum, whether you're, you know, a git wizard or you're just doing enough to get that commit up to GitHub so to speak but our first, highlight today is a presentation from Megan Harris who is a data scientist at the prostate cancer clinical trials consortium and in fact she was also one of these students at the database workshop I remember she was sitting in the front row where I was close to but in any event her talk was titled please let me merge before I start crying and other things I've said at the git terminal.

Yeah. I feel seen with my early days will get when I read that title. So, obviously, when you see the recording of this, you'll get a lot more of the I'll call the first half of the presentation. We got the links of the slides in the show notes. And, by the way, Megan, you are easily one of the top, like, slide crafters I've ever seen with quartile. These slides need to be seen to be believed. It's a great mix of visuals and just really gets to the point of what she's trying to address. But, yeah, really in great engaging talk.

And some of the takeaways I'll call out as I as I saw the talk and thought about how it relates to my journey we get. The biggest thing is that it is easy, especially if you're new to this, that when you encounter an issue we get to kind of almost freeze a bit and might have, like, a panic moment of figuring out what did I just break and more importantly, how the heck do I get out of this emergency situation. But one of Megan's big talking points here is that there really aren't a lot of emergency situations we'll get.

You can handle this. You just gotta kind of step back, calm down, and, you know, leverage some practical tips to get through situations such as merge conflicts. So she does a great job in this in this presentation about, you know, just what exactly emerged conflict actually is and really giving you as the as a git user a few different ways that you can interact with trying to solve these merge conflicts. And, certainly, some of these are gonna be very basic, whether you're booting up a text editor, looking at the ways that a merge conflict is denoted, web what's the syntax for the branch that's trying to come in that particular side of development versus the branch is trying to merge into. There's different notation around how that looks and it can look very kind of cryptic at first, but once you get the hang of it, it's really not that bad.

And she has a great snippet in the slide deck of her literally solving the example merge conflict in her slides with the dates being different for what was called a data cut, and she literally just shows how in a text editor within our studio itself she just took one of those two dates cut out the cruft committed it and pushed it up the main or master branch so it does once you kind of get that sense of, you know what, all merge conflicts will eventually boil down to something like that, and perhaps there's more than a handful of lines, but the process is the same.

Step back, don't panic, and find what you're comfortable with to resolve that conflict. And, really, it's up to you how you resolve that. She has a fun little, shout out to retro gamers like me about how you could put some fun ASCII art in your in your, in your, final file that you're gonna push up. Maybe that's not a good idea for production code, but the retro gamer may sure loved it. But they really the other key takeaway I'll highlight here is that merge conflicts you often will find yourself, especially you're new to this, blaming git for all your troubles. Like, if I just didn't have to use git, I wouldn't have to worry about this.

You know what? Maybe, again, take a step back because these conflicts are not really related to Git itself. They are more practical in nature. Maybe you and a team member didn't communicate clearly enough on who was doing what. And maybe there's a more optimal way to set up the structure of your project so that merge conflicts are minimized. And, honestly, to be proficient in these situations, you do have to do a little bit of, you know, upskilling, if you will, on, you know, the basics of git. Maybe you can get away with using a third party tool like GitHub desktop or the, the pane in r studio or the the version control pane where it gives you some point and click ways to get your commits done or revert or things like that, but it can't do everything.

Sometimes getting a little dirty, so to speak, with the terminal and exploring ways to kind of get out of danger so to speak if you're really nervous about dealing with that merge conflict. There's a there's a command called abort for your merge that you can get back to right where you left off before you started that merge, that merge request so there are little nuggets like this throughout her talk that I think once you get the better handle on and you have some real practice with it's not like something you can just watch this talk and immediately you know be changed forever You have to really put these things in practice.

Speaking of practice, she doesn't have this in her resources, but I'll call it out because it's been featured in our highlights podcast before. But my Al Salmon has created that fondly named r package called seperpultate. I probably got that wrong again. I sorry, Ma'al. But this is a great way for you to literally practice different situations of Git right in the confines of r itself. So we'll put that link in the show notes as well so you could put a lot of the principles that Megan is talking about here into practice in a non production type project, but it's gonna mimic these real world scenarios. So I think booting up that package in RStudio or your preferred IDE of choice is a great way to explore, you know, dealing with merge conflicts, dealing with the tooling around it, and not frankly ever tooling around git itself.

I think the best way to learn with git is to actually do something with it, and if you want like a you might say a safe space to do that with, my else package is a great way to do that. Again, fantastic talk by Megan. Can't wait for you all to see the recording, but she related to a lot of her journey we get to, her adventures of parenting and travel, which again is a parent who recently traveled there in the summer on a long road trip yes I completely relate to all that and it was a great to make those connections so again slides are in the show notes the recordings will be out hopefully in a couple months but highly recommend to check this out especially if you do get that little twinge of panic or despair whenever you think of git Moving on to our next highlight today. It's always wonderful to see the diverse nature of the art community, especially in terms of geographic distribution and ways of the community getting together in different avenues to share their knowledge and insights and really help each other in their journeys of using our data science.

So our next highlight is a new blog post on the R Consortium talking about the new R community in Ahmedabad, India that is called the Ahmedabad R user group or ARUG for short, and some of its journey to bring those in the life sciences industry in that region together to share their insights about r. And so the interview was with Sanket Sanjo Sanojia. Sorry if I didn't pronounce that right. He has many much experience in statistical programming, in data science in the clinical industry. And it was in about 2021 that he was meeting with some fellow collaborators and wondered how they could bring this growing community of our users in Amadaba together to share their knowledge, and, hence, they had the idea of setting up the ARAG group.

And it over time, they started to get together more, and then they actually, after a lot of planning, a lot of work to mentor new members and start the foundation so to speak, they had their first meetup earlier this year in July and this was a very highly anticipated event called r evolution shaping the future of clinical trials. Again, this is right up my wheelhouse, so to speak, where they had many different featured talks talking about the spectrum of where r is being used throughout the industry. All from, say, creating innovative Kaplan Meier plots or survival visualization, looking at ways of Shiny, you know, innovations with Shiny with different use cases, having a roundtable on the different trends in r and the ways to prepare for conferences in terms of getting the most out of that, and really just bringing people together, I think, is the key point here.

He talks about how they're leveraging platforms such as GitHub and Meetup to get in touch with the users, but in the end from what I've seen in this post it was a highly attended session and it looks like there is a lot of enthusiasm to to to level up each other along the way, wherever you're new to the industry and are are those that are wanting to give back so to speak some of their learnings. So the link the post has links to how people are interested in joining the group there are links to join on that but overall it is terrific to see our gaining a lot of momentum in this space and not just you know in my neck of the woods here in the United States but around the world whether in Europe or Asia Pacific or other parts of the world we're seeing R really take off to bring life sciences to another level so needless for me to say I'm here for it and I'm really great to it's really great to see these user groups spin up and really getting the community together.

Last but certainly not least, we're gonna close out the highlights, section today with a great use case in leveraging what is becoming a rapidly growing and very, you know, influential communication platform that we're all many of us are now using in our daily work, and that is quarto, of course. And in particular, when you are working in an organization and you have maybe you've done, say, a report or a slide deck and there's always, like, a few common themes or common elements that you want throughout these shared in your organization, what is the best way to get people started on the right foot so that you're all are kind of starting from the same template, if you will.

So this last highlight is addressing just that and it comes to us from Megan Hall, who is a technical product manager of the hockey group as Xelliss Analytics. And, coincidentally enough, that's also where my good friend, Tan Ho, is, is working as well as a machine learning and and software engineer and statistical data scientist as well. So credit to them for joining the conference. But Megan's, talk, which again I couldn't see live, but she has an accompanying blog post about how they have designed and deployed internal portal templates at Xelliss Analytics and some of the tips that you can take from this if you wanna go on this journey for creating internal resources around quartile.

So first of all, motivation for why you wanna do this, certainly for me, you wanna have consistency in, say, your organization's, like, theming of these different communication reports or slide decks, again, whatever have you. And you don't want every scientist or data scientist to create these by hand if you want to just make it easy for them to get started. So having a template I think is quite valuable and we've seen this also in the R markdown ecosystem quite a bit. Lots of templates have been shared in the community in respect to that and I think in quarto we've already starting to see that as well.

But, again, you can't always use the publicly facing one. Sometimes you have various elements that you wanna share only within your organization, Hence, the need to figure out how to build these internally. So what actually goes into a template? Megan talks about you want to put in probably some styling elements, whether it's CSS, custom theming, which again the quartal site does a terrific job of documenting how you can get one of these off the ground. So you might have, you know, say theming also associated with the visualizations you're doing. Maybe you have a set of core r packages that should be used in each report to help set up some of these more aesthetic elements.

She talks about having customized themes for ggplot2 and the GT package for creating tables so that the the scientist who's developing this report can just already have that snippet of code ready to go to load these internal packages that are giving these themes. So that's a great win right there for, you know, reducing manual effort. So she has examples about how they design a certain element to tell that a report is confidential and not to distribute it outside of their of their walls so to speak she talks about how they how you can go about customizing that element and where that actually goes in your theming and CSS files so that again you can just put that anywhere you like and if you're still you know learning the nuts and bolts of how CSS actually works trust me I'm no expert in this either she has a great tip about using the dev tools you know interactive element in chrome so you can get to the actual CSS for each element using its inspector or whatnot, and then on the fly experiment with different parameters such as like the rounding of corners around this box that she puts at the top of the report. So you can kind of demystify what can be, you know, quite quite, daunting I would say when you look at CSS for the first time. You're not really sure what these parameters are doing.

So really getting getting your hands on it in the browser itself is a great way to learn. She also points to resources like the Mozilla docs that are great for learning CSS as well, And so lots of lots of practical tips on that front. Well, it's one thing to create this bundle. How do you actually share it with the rest of your organization? Much like the R Markdown ecosystem, if you're in the you're in the r landscape like we are, you probably want to make an r package for it. And r packages work just as well with quartile templates as they have with r markdown templates.

So she has a great example with some visuals on how they have constructed a bundle where the package within its INST folder which is the part of a package where you can put ancillary or accompanying files that will get installed with the package itself when the user installs it on their system. She has folders for these extensions that hold the both the HTML and the LaTex or you might say PDF styling files so that both the printed version and PDF, so to speak, or the static version, as well as the web based HTML version have, you know, accompanying style files over its CSS, wherever it's custom LaTex code, along with the the quartile document that actually has the template itself.

And when the user utilizes a function, she calls it create Xeliss HTML for the HTML version. It will copy everything to their working directory and then get that template file set to go as a new file. And then the user can customize it as they wish, but they're already up and running with utilizing that template. And so she is, you know, gained inspiration from this from Spencer Sheehan and by proxy Tom Mach, who I also got to see at the conference. It was great to say hi to Tom, but she leveraged a lot of their previous resources that they've shared on their blog posts to put all this in action and then lastly in her post she has a nice animated gif to show what this actually looks like when the user sets this up a simple call to that Xelliss HTML function gives it a title and right there your portal document is is right there open with everything filled in.

What a great way to make this as easy as possible for your rest of your organization to leverage your innovative quartile templates. I definitely need to start doing this because I was just talking with Will about some day job stuff a couple weeks ago about, hey, we love using quartile for our slide decks. We, not the biggest fans of PowerPoint. At least I'm not. I don't think he is either. So we're thinking of making an organization template based in quartile for our slide decks because, well, it's another story for another day, but HTML interactive content.

Yeah. I think that's a little more engaging than PowerPoint stuff. That's a battle. That's gonna be a battle, so to speak, but one I want to take with quartle at my hip, so to speak, so that we can start leveraging interactive slide decks at my company. So this Megan's post here is very timely because I'm gonna use some of these principles and practice to to make all this happen. Well, much like I tell you every week there is so much more that goes into an r weekly issue. So we we will close-up here. I'll talk about an additional find that I have.

Again, somewhat related to the conference, but it was eye opening to say the least. So as I mentioned, I visited the shiny track, at at positconf, multiple times after my presentation, and there was a terrific session on kind of innovations with Shiny. One of the talks was from Keegan Rice talking about what are some what are ways that you can build, you know, user friendly data exploration, you know, paradigms in your Shiny application. And she, she was inspired by, you know, many of the principles that we talked about in the community quite a bit, but she shared kind of the what were the the boilerplate of how she created this live crime tracker as part of her daily work at the NORC at the University of Chicago.

And this application, my goodness, when you look at this you are gonna be blown away. This is amazing. An amazing application. You would not know it's shiny by looking at this. It actually has a lot of vibes similar to what I saw shared at the Shiny conference earlier this year by Appsilon. I'm one of the apps that, that that I was enamored by. Great organization, great use of interactive maps with tooltips throughout, and a great, you know, comprehensive documentation that's styled very clearly both about the tracker crime tracker itself and the methodology around it.

This is a great showcase of great user experience, you know, elements. And again, if you just shared it with somebody there is nothing here that, you know, screams shiny so to speak in terms of the typical UI layout. I don't exactly know exactly what's under the hood on this just yet. I just know that it is a very engaging experience and, knock on wood, so to speak, if things go well later this year, we may, dive into this with with her even more detail if I can spin up the shiny developer series once again because I'm hoping to have Keegan on for an episode that we dive into this much further. But in any event, if you want to be inspired by optimal UX design in a shiny app, I invite you to check out this live crime tracker part of the NRC at the University of Chicago by Keagan Rice.

Alright. As much as I'd like to talk about the rest of the issue, I'm gonna have to close-up shop here, so to speak. But as always, where you can go to find more information on our weekly, well, hopefully, it's one place to go, and that's our weekly dot org. That's where everything that's our central hub, if you will. That's where every issue lands on the on the front page as well as the archive of all previous issues in case you missed any in the past. There was a great issue last week where Mike and I couldn't record because we were literally at deposit comp, so you'll wanna check out the archive if you haven't already.

And, also, what's the best way to help the project? Getting involved. We have multiple ways to get involved. If you've seen that great new blog post package that just came through CRAN or that great resource that you saw throughout or maybe one that you created yourself, we're just a pull request away. It's at the top right corner, a little banner there. Click that. You'll be taken directly to the pull request template in GitHub. You can do it right in the GitHub editor. No friction required on that front. It's all marked down all the time. I've lived marked down. Who knows? Maybe in my dreams, I'll dream of markdown. Well, maybe not that far. Either way, I use markdown so much. Hopefully, it's intuitive enough for you to put in that link to that great resource.

And, also, we are always looking for additional curators to join our team. There is more information on that on the our weekly GitHub repository if you're interested in joining our team where we definitely could use some additional help, you know. People have real lives and we've lost a couple of curators along the way to their real life duties, so being able to have others join the team would be a welcome addition. Also, we love to hear from you in the audience for this humble little podcast. On the show notes of this podcast, you'll find a link to our contact page where you can get in touch with us directly on that front. And if you're listening to a mod with a modern podcast app like Podverse, Found, or Cast o matic. You can send us one little boost along the way and share your love for the show there.

And, also, I am on social media, you know, sporadically on the weapon x thing with at the r cast, but you'll find me more often on Mastodon where I am at our podcast at podcast index on social, and I'm also on LinkedIn as well. You just search my name and you will find me there. So, again, thank you so much for joining me in the solo effort today. It's never the same without my trusting cohost, but he'll be back soon enough. And again, huge thank you to all of you at the conference that said hi and also best wishes to many of the, you know, colleagues I met. There are more than a few of you that unfortunately have contracted COVID, so I hope you're on the road to recovery quite well.

I know in these times, these events, there's always at risk, but I'm thinking of you all And thank you again so much for the great conversations, and I hope you all recover soon enough. Well, that'll do it for this episode of our weekly highlights. Thank you so much for joining me today, and we or at least I will hopefully be back with another episode either next week or shortly thereafter. Until then, that's end of line."
"44","issue_2024_w_32_highlights",2024-08-07,57M 44S,"A realistic take on converting the NY Forest Carbon Assessment modeling pipeline to the tidymodels suite, and a review of R package development workflows in the Positron IDE. Episode Links This week's curator: Jon Calder - @jonmcalder@fosstodon.org (Mastodon) & @jonmcalder (X/Twitter) Converting New York’s Forest Carbon Assessment to Tidymodels R…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 174 of the Our Weekly Highlights podcast. This is the weekly podcast where we talk about the terrific resources and the highlight sections that are being shared along with much more content in this week's our weekly issue. My name is Eric Nantz, and I'm delighted you joined us from wherever you are around the world. We are in the month of August already, and time has flown by quick. So, of course, I gotta, you know, buckle up my virtual seat belt here as we get along the ride to an eventual conference next week. But, of course, I need to bring in my awesome cohost who I never do this alone, of course, because he's joining me here, Mike Thomas. Mike, how are you doing today?



[00:00:42] Mike Thomas:

Doing well, Eric. I am 6 days until my flight leaves, and I I can't, be more excited

[00:00:49] Eric Nantz:

to get to Seattle. That's right. In fact, yep. As as you've heard in the previous episodes, Mike and I will both be at Posit Conference Seattle, which means that, you know, usually during our, quote, unquote, recording time next week, I'll actually be giving a presentation around that time. So, you won't be having an episode next week, but, nonetheless, we'll make it up to you later in the month. But, nonetheless, we've mentioned this before, if you are gonna be in the area for POSITONV, please come say hi to us. We're gonna be out and about. I'm actually arriving pretty early because I'll be part of the Our Pharma summit that's happening on Sunday before the conference, and I'll be in one of the workshops or databases.

So I'll be around. Mike, you're getting in on Monday, it sounds like. So, yeah, we'll definitely looking forward to connecting with you listeners out there.

[00:01:36] Mike Thomas:

Please say hi. Yes.

[00:01:38] Eric Nantz:

Awesome stuff. Yeah. And And, again, I can't confirm or deny now. I have some sticker swag with me. I'm still trying to get stuff together. I still have to pack, so lots of things to to bring with me. But good thing we don't have to bring, you know, writing an article show ourselves. We got a handy curator team that handles that for the project. And speaking of going above and beyond, for the 2nd week in a row, our curator this week is John Calder. He really stepped in to help out with our scheduling for the rest of our curator team. So, again, many of you hopefully know this by now. Our weekly is a complete volunteer effort. So anytime we can pitch in and help each other out, it it it it's it's just so valuable to us. So our curator team always goes above and beyond. So, certainly, my thanks to John for stepping in. And as always, he has always had tremendous help from our fellow RM Wiki team members and contributors like all of you around the world with your poll requests and suggestions.

And, yes, it's been great to see the momentum behind, you know, the adoption of the tidy models ecosystem for many machine learning, prediction, and other pipelines in the modeling space. We've covered numerous, segments here in the podcast about some of the recent advancements on the internal tooling that's been happening over the years. And it's always great to see, members of the community start to adopt this to their existing workflows, especially in cases where they've had maybe some internal custom solutions.

And now they wanna see where does tidy models fit in terms of giving them advantages, reconstructing their modeling pipelines, and what that experience is like. So our first highlight today is doing just that. It comes to us from Mike Mahoney, who is now at the USGS as one of their scientists and biologists, which is the US geological survey for those who are outside of our US circle here. They've been doing some great work in the art community while they're tooling. But in particular, Mike is involved with a very important effort for the New York Forest Carbon Assessment, which in a nutshell is trying to objectively measure the amount of forest coverage within the state of New York and helping predict the changes in this coverage as part of the recent climate, Protection Act that was trying to minimize their use of carbon emissions from, obviously, fossil fuels and other things like that, by the year 2050, I believe. They want, like, an 85% reduction in that, which means that, you know, they're trying to take advantage of what, you know, forest and other plants can provide in helping offset some of the some of those, decreases.

So this summer, they've been working on version 2 of their automation pipeline and modeling pipeline of this assessment where, again, they've had some internal functions to help with the tuning and creation of these stacked ensembles. But now they wanna use now the broader ecosystem of tidy models to kind of bring all that together in one cohesive structure. Now, unfortunately, he can't share the actual data they're using for this ensemble project at this time, but the blog post does a terrific job of taking advantage of publicly available data from actually not far from your neck of the woods, Mike. They're taking advantage of tree canopy data, from online sources in the in the city of Boston from 2019.

And the goal of this post is to illustrate very similarly to how they're adapting their New York forest carbon assessment by fitting 2 types of models, the Mars, which is a multivariate adaptive regression splines, as well as gradient boosting modeling, which, again, these are highly popular in the machine learning and prediction space. So the first part of the post talks about how he assembles the actual prediction data, the outcome data, which, again, I think is very comprehensive. If you're into learning how to obtain these data, Mike definitely has you covered with all the data preprocessing steps, the various packages you need. And lo and behold, once you do some, you know, data preprocessing and visualization, you now have a tidy dataset with the outcomes of interest that he wants to predict.

So with that, now it's time for the model fitting. And so there they got the a cell values object, which is again, hosting the outcome data and the prediction data. One little nugget here right off the bat is that just like of anything in r, when you're doing a prediction model, you need to have some kind of formula object to help denote what are the relationship between the outcome and the predictor variables. And there's a handy little function in base r called df 2 formula, which I wasn't familiar with, which basically will be intelligent enough to determine if your data set has the outcome variable as the first column, and then the rest of the columns are prediction variables, you don't actually assume that structure, and then you don't have to write like the typical formula syntax of outcome, tilde, and then all the different combinations of predictors. It literally just takes that data as is and builds your form of your object right off the bat. So that's another one of those hidden base r nuggets that you see from time to time. It's always great great to see those.

Now in the tiny models ecosystem, there is kind of a stepwise fashion to how to produce these workflows. The first step is to register the recipe, which is kind of gonna be the building blocks for the actual fit itself where you simply feed in the formula and the dataset that contains your observations. Simple enough for the recipe, recipes package to do that. And then now it's time to fine tune the model specifications, which, again, one of the great things about the tiny models ecosystem is that it has dedicated packages to help with some of these things. Now that may not always work well, well, which we'll get to later on, but he outlines 2 specifications, one for the GBM model and one for the Mars model, which, again, depending on the model type, you may have different sets of parameters to specify.

And then throughout that, for many of the parameters where he doesn't know right off the bat, what are the best values of such as, like, the number of trees or the tree depth and the GBM side of it, Tiny models, of course, lets you tune for those parameters, optimize for those leveraging the hard hat package and its tune function. But you'll see in the code, it's actually littered throughout this model specification with no arguments. So it's trying to do a lot for you from an abstraction perspective, which, again, may or may not be always perfect, but we'll get to that in a little bit.

You've got those specification in the model now. Now it's time to set up the workflow, which is again coming from the workflow sets package where you can put in your recipe as well as your 2 model specifications and making sure you've got all that integrated together gives you this nested data frame back where you can see your 2 recipes in this case, the information inside. And then once you actually do the prediction, it'll capture the results as well. But what Mica's saying high praises of is that a lot of this takes a lot of code back in the days before tidy models, a lot of things you had to build on the fly.

Certainly, Max Schoon had offered the carrot package well before tidy models. Many people use that to orchestrate their flows. Still, you had to learn the nuances on how to stitch all this together. Tidy models is trying to abstract away a lot of that manual effort to stitch all this together so that you can have fit for purpose functions to define all this. So, again, we'll we'll come back to the benefits and trade offs of that in in later on. And then once it's ready to go, you've got to now start composing your samples, your re samples of the data, as well as tuning your workflow sets. In other words, finding what are those optimal parameters.

So with the workflow map function, which is gonna actually take that set of combinations of the recipe and the model fits. You feed in a certain met set of metric or parameters such as your space for the grid search. And then also some two arguments that we'll come back to later on, the metrics and the control argument, which is gonna help with specifying some defaults that can be surprising if you're not ready for it. We'll come back to that in a little bit. And then once you have that done, it's time to actually run the tuning and see what your best fit is based on your metric of interest.

So in this case, in this example, he's looking at the root mean square error to see which model or which set of parameters of the model fit best. And he's able to locate for each of the model types which particular model. It's got a somewhat ambiguous name of, like, preprocessing preprocessor 1, model 09 because it's actually fitting different models for each of these combinations, so it gives it a unique ID on it. But he's not interested in just cherry picking the best fits of these. He wants to use the ensemble technique, which basically is able to take all these model fits, figure out then with some analysis which are the, quote, good enough parameter sets that then we can he can use later on in the actual prediction side of it. So taking advantage of the information from all these different model types.

And this is a great advancement in tiny models ecosystem. They have a package called stacks, which is gonna be able to add all or literally stack together all these different model fits and then be able to see what is the highest weight in terms of the model giving the best performances and really get objective measures around that. So then he's got a nice tidy output here of the top 4 members contributing the best prediction outcome or prediction, I should say, power, you might say. And it is a mix of the light GBM and the Mars model for these different, you know, combinations of the preprocessing and the model fit itself.

And then he can take that and feed it just directly into the predict function. That's pretty neat. Just predict this ensemble object that he's created with these stacks, the dataset that has the amp predictor values, and you'll get a tidy data frame with the predictive values back. That, again, another, you know, less code solution to get your predictions out of that. And this is a pretty comprehensive start to finish flow on this. And I will say there were some little gotchas along the way that he want he is able to talk about in the next section of the post.

And Mike has a unique perspective on this, our author of the post, because he was an intern to the tidy models team years ago. So he's had some inside look on this, and that makes his, critiques here even more fascinating. So, Mike, why don't you take us through that?

[00:13:31] Mike Thomas:

Yeah. Mike was an intern back in 2022 on the tidy models team, and I have to imagine that while that must have given him a a huge leg up in converting their their legacy code into tidy models, 2 years in tidy models time must be like a decade, in terms of the amount of new functionality and packages and design choices that have been added to the ecosystem since then. I mean, we have survival analysis and tidy models now and that that was never on the radar back in in 2022, at least as far as I could tell.

And, you know, as we know that ecosystem in that that universe of of packages within tidy models has grown. And it reminds me a little bit of that recent blog post that we had about creating package universes. And, Eric, you and I discussed some of the trade offs that you have to consider when doing that. Right. And it reminds me a little bit about sort of the end of of Mike's blog post here. Because a lot of these tidy models packages work together. As Mike notes, you know, there's sort of 3 packages that work together just for hyperparameter tuning itself.

The tune package takes care of grid searching. This hard hat developer oriented package owns sort of the infrastructure around hyperparameter tuning, and then the DIALS package owns the actual grid construction. And when you think about hyperparameter tuning itself and trying to, you know, create these different methods, you know, within each of these these packages and having them work together, Well, not only do you have to do that for 1 modeling one type of model but I'm assuming that you have to create different object oriented methods for all of these functions across all of these packages for all of the models that tidymodels supports. And I know on the the tidymodels, you know, homepage, they have sort of a list of all the different modeling algorithms that they support tree based, you know, regression based algorithms, all all sorts of different stuff. And they there's a ton within that ecosystem that they have supported, but I imagine that once you want to add an additional model type, it's a pretty extensive process to be able to ensure that you can support that model type not just in one package but across all of these different packages that work together to create these modeling workflows.

So I am not a user of of scikit learn. I'll be honest. I've reviewed some scikit learn code in the past. I know it's it's very highly regarded in the Python ecosystem. I don't know this for a fact, but I have to imagine that the tidymodels team must have had the benefit of taking a look at, you know, what works well in in scikit learn and what doesn't work well in scikit learn when they went to, you know, sort of move from caret to tidy models and create this new framework. So I I'd be curious to see if Python sort of suffers from these same sort of of issues or if not, you know, how they're handled in scikit learn. Because I have to agree with Mike that this is somewhat of a pain point if you are doing some pretty hardcore machine learning and predictive modeling as Mike and his team are clearly doing. Right? Creating a lot of different types of models, trying to ensemble them together, trying to tune hyperparameters, and and do it in a way such that the code is as efficient as possible. Right? There's a function in here that I didn't even know existed from the workflow sets package called workflow map which I have to imagine is like a purr like approach to, developing workflows across a bunch of different models, hyperparameter tuning, and evaluating, and comparing those models, you know, sort of really in this this programmatic approach as opposed to hard coding things for each one of these models and then trying to compare and evaluate, the the outcome of these models separately. So I would advise you to to try to take a look, you know, I don't know how far down in the weeds that we want to go into to some of Mike's sort of specific, gripes, if you will. I think complaint is sort of a strong word that that's what he uses in the header here. But I think he's really just pointing out, you know, some of the things that you, as a Tidymodels user, the the deeper you get into it, will face as well. And calling out some of the things that worked for him in terms of workarounds, some of the things that that he learned is he admits that some of this stuff is is straight in the documentation, and some of it is not. And you have to sort of, you know, take a lot of time to figure out yourself. He he and this is super relatable. I think there is something that that was documented in a package here that Mike spent 26 hours, trying to trying to figure out before he was able to actually, you know, figure out what was going on here.

And and it had to do I think with the defaults in hyperparameter tuning and some of these workflows that we're failing extremely slowly, unfortunately, and weren't sort of, you know, bringing to light the errors, quickly enough to the end user. This is all to say at the end of the blog post that that they're still using tidy models because I think net net at the end of the day, Mike and his team believe that, you know, the the pros and the benefits that they've received from switching over to tidy models outweigh the cons. And I think like anything, you know, with an open source software, hopefully, some of the the complaints and the issues that they faced are things that will be, you know, resolved. And over the years, we'll move within the Tidymodels ecosystem to enhancing these things and making that user experience a little more easy. I've used Tidymodels, you know, many times before, really enjoyed. It's definitely a little bit of a learning curve from from Karat. I think you have to, have more of a sort of per like thought process, a higher level design sort of thought process in mind when you're leveraging, tidy models and understanding how all of these different packages like Parsnip, r sample, yardstick, you know, work together, at workflows to be able to, accomplish what you're trying to accomplish. But it is it is super powerful, and and I'm glad to see that the team is is still sticking with it. And, I'm this this is like a wealth of information around tidy models and is a great crash course, on if you are trying to get into the weeds of machine learning and R on some of the design choices that that Mike and his team made to to create these models and evaluate them programmatically.

It's fantastic. I'm not sure, Eric, if I've seen a blog post recently that goes into this level of detail within tidymodels for us. So, very welcome blog post. I think it's a great not only technical discussion but also sort of practical discussion, from a team perspective on on what's worked well for them and and what hasn't and where they're planning to go in the future. I was wracking my brain as you were walking through this, and I

[00:20:38] Eric Nantz:

don't recall one in in the recent, you know, months or probably even year of anything this comprehensive because there is a great mix here, again, of one of the points he he mentions towards the end is that this you you you may think you can kind of, you know, hone in on one particular aspect of tidy models, but he's saying that it really took him having this holistic view of how these different pieces fit together. That may be an issue for those that are kinda new to these, you know, suite of packages that have a cohesive API or opinionated way of integrating together.

Now as I say that, you may be thinking to yourself, well, that sure sounds an awful lot like the tidy verse itself. Right? I mean, certainly, they got inspiration from the tidy verse on a few things, But what I when I do data processing pipelines with the tidyverse, most of my time is spent with, I'll call, a core set of maybe 2 packages, maybe 3 at the most, like dplyr, tidr, and per to help with some, you know, mapping processing. Oftentimes, I don't quite have to get into the weeds so much as some of the other packages, but sometimes I do. And so it's always helps as you're building these cases or these use cases for yourself or maybe for your team to document these intangible kind of learnings that as comprehensive documentation might be for these given packages, it's how they're integrating together.

And, certainly, the tiny models team has done great work to put these freely available, you know, online books online about, you know, all the the different ways that tidy models can be used. Some, you know, Max Kuhn, Julia Silge, and others have been very front and center with that, and we highly recommend you check out the tidy model site to get links to those particular cases. I do think though that having a post like what Mike has done here touches on things that, again, are kind of on the more practical side.

And I have been victim as someone who uses HPC systems on a weekly basis. Oftentimes with jobs that won't complete in a day or sometimes 2 days, it can be costly when you thought you had a default set right, and then you find out after the fact you forgot to save that prediction result. You forgot to do that one little adjustment to p values in my case, and whoops, gotta go back to it. So there are some things that you can do to help minimize the impact of that, which I don't know works as well for the models. But when we tell people on our team is if you have a simulation pipeline and you wanna do, like, 10,000 simulations, you know it's gonna take a while.

You really only wanna do a few of them first to make sure you've ironed out all your connections, all your outputs you're saving so that you're not surprised after writing them over that amount of time. So I I I had the feels when I read that part of of my exposes. I've been there many, many times, but these are all things that, again, you kind of have to learn by doing, but then documenting your learning process is so helpful. And I do think there's going to be tremendous value for those that are adopting tidy models to their pipelines right now to kind of see, again, our literal real world usage of this and the and the, lessons along the way.

I'm confident that the tiny models team will take this hopefully constructive feedback here, and maybe we'll see some enhancements to, like, these more use case approaches, the documentation, and not just the, developer facing that you might see in the weeds of a package manual or when you go help for itune or whatnot or whatnot. You're not really getting the full picture at that point. So maybe we'll see improvements on that, but, again, this is the kind of the unofficial contract you sign when you leverage a suite of packages that, again, are meant to be coupled together tightly.

There may be cases where the abstraction doesn't give you the full store. You need to kinda get in the weeds a little bit like Mike has done here.

[00:24:52] Mike Thomas:

I agree, Eric. And that's a great call out to, the tidy modeling with our book I think if you are interested in tidy models or if you're stuck on something that can be a great resource. We'll put that in the show notes, but it's tmwr.org.

[00:25:06] Eric Nantz:

Couldn't be easier to get to. That's right. Yeah. And it should be on your virtual bookshelf or even your printed bookshelf there. It's a valuable resource. And, again, the the the ecosystem is always evolving too. There's always, like I think they do quarterly updates from time to time on the tiny models blog. I've seen Max and Emil and others, you know, do a great job of writing that up. But, yeah, what Mike has done here in this post is is tremendous value to the community. Alright. We talked about, you know, looking at things from a developer perspective and a user perspective. We're gonna put our dev hats on, Mike, a little bit because we're gonna talk about what has been a very, you know, well spoken or hot topic these days and how that applies to package development workflows.

And if you recall, it was a couple a month ago or so. It was kind of quietly put out there, but there is a new IDE authored by Posit called positron. And, again, the geek in me cannot ignore the fact that the name of my favorite movie ever is in the name Positron. So to take that for what it's worth, I have visions of the MCP talking to me right now. End of line. But in any event, this is really interesting to see the uptake on this, and I dare say we'll be hearing a lot more about this at Pasa Comp in a week from now.

But what we're seeing here is a post from, well, Stephen Turner who, coincidentally, when I started my our journey many, and I do mean many years ago, there were 2 blogs I discovered that helped me in my journey, especially as coming from another language like SAS trying to make heads or tails or what r was doing under the hood. Stephen Turner, the author of this post, was one of them because he wrote this terrific blog called getting genetics done, just was so instrumental in my learning journey with R, and it is just terrific to see him resurrecting this effort in a new in a new blog. But this post is one of the latest that he's put on this, I believe launched earlier in July. So, it's a huge thank you for Steven. And, you know, I don't think I've met you personally or or if I have, it must have been years ago, but you have been very instrumental to my journey of ours. So it's terrific to be able to cover one of your posts here in the highlights.

Nonetheless, what he talks about in his post here is his early adoption and user experience of positron to mimic what he's done in the Rstudio IDE and before that, e max of ESS over the years, and that is building in our package. What is the experience like in that space? So we're gonna dive into some of his, findings here in in the highlights now. And first of which is for if you're not familiar with how positron operates, positron is actually a fork of the open source version of visual studio code. They call it code o s s.

So when you look at positron, it's gonna look different than your studio ID, but that's by design because it literally is the Versus code shell with posits, you might call design choices on top to help bring some of that RStudio IDE functionality. Not quite all of it because this is a beta product, which we'll get to in a little bit. But he puts links to great resources about positron if you're new to it, such as the Wiki on their GitHub repository. Absalon's done a nice intro to Positron as well as Andrew Heiss, who's been frequently featured on the highlights, his experience of positron too. So definitely have a look at those after you listen to this, but let's get to the actual package development workflow.

So in our studio, what do we typically do? We like to create a new project in our studio to help house our package code, and positron brings their own spin on this as well. This is, again, one of their additions to the top of the visual studio code like experience where they let you choose 3 different project types, either a Python project, an R project, or a Jupyter notebook. So right off the bat, you've got a little wizard to guide you along the way. So, of course, he chooses the R project, and you've got, again, that familiar looking R project file created for you, which is just like what you would have in our studio. So there's already a familiar, part of your experience.

And then how do you actually create a package? Many of us are now using the use this package to create a package from scratch in that same directory. He does that. No gotchas there. He's got the scaffolding right off the bat in a package he's calling hello, and then he puts a simple function called isay. It's kinda like your hello world type, but just a little sampling of different strings in there. He writes that function. Now we get to some of the differences because there is a very convenient feature in in our studio ID that I use every time I make a new function for a package.

There is a either a keyboard shortcut or a menu entry to dynamically insert in our oxygen skeleton of the parameter documentation right there above your function with just a click or a keyboard shortcut away. Unfortunately, that's not in positron yet, so you're gonna have to write out the docs yourself. Of course, it's not too difficult. You'll get code completion, but it is just one of those conveniences that hasn't quite been replicated to the positron experience yet. But nonetheless, he's able to document his function and now comes the iteration. Right? When you're writing a package, you wanna develop your function, you wanna test that things are working, update the documentation, manual pages dynamically.

And what's nice about positron is you can import a keyboard mapping of shortcuts that will mirror very closely what you might have done in Rstudio's keyboard shortcuts. So you can you can import that in optionally, and then you can use that familiar, say, command shift or control shift d to populate the documentation manual pages on the spot, that works just right here as well. So that's gonna take care of, again, the the manual pages. They're gonna take care of putting the function name in the pages. They're gonna take care of putting the function name in the namespace for exporting so you don't have to do any manual effort on that front just knowing the shortcut or the command palette. You'll be able to do either one of those.

Another interesting thing is from time to time, we like to install the packages in our local environment as we're iterating on it. Before in RStudio, it would call the r command install function verbatim. In positron, it's actually leveraging the pack package. Pack colon colon local install, which again, I didn't know existed. So a little learning there. It's not a surprise that posit would use some of their tooling of the rlib suite of packages that they have. Excuse me. The rlib suite of packages to help automate some of these processes behind the scenes. So fair play to them.

Of course, as you develop a package, you're gonna see your fair share of warnings if you need to update, like, documentation names or other things like that or the license entry. You get all that in the console just like you would with RStudio. So no surprises there. You can run dev check, get the results there. You can build in your tests. We'll test that very quickly or use this to launch the test. It's gonna open that up right in your ID. Again, we're seeing a lot of similarities there. There is one other thing that's kinda missing from a development perspective that you might use from time to time is that for things such as the cover package to help you look at all your test coverage percentage with the functions you develop.

There is always a handy add in, r studio add in in the r studio ID that will let you kind of run that report quickly of a menu click. As of this recording, positron's not supporting r studio add ins yet. I think that's something that is being worked on because even in my visual studio code experience with developing our projects, I can get add ins on that, thanks to the efforts of Miles McBain and others from the community. So I think that's on the road map, but that's something to be aware of if you wanna adopt positron right away.

So from most of the, you know, actual workflow, package development looks pretty seamless with Positron. Now there are some additional things that are missing in the positron experience that I leverage heavily when I put on my Versus Code hat back for a second. That is remote container and the remote connection, functionality, meaning that I could have Visual Studio Code on a one system, but then have another server either on my local LAN or my local HPC environment or in the cloud that actually has the R process built into that, and then I can just farm out my computations on that, but look as if it's local.

That's not quite there yet in positron. They are working on that. That would be a game changer for me personally when they adopt that. But what was interesting, what Steven was able to do here is that little did I know, on top of writing great content about r, he's actually authored a package to help with getting a Dockerfile built from your package project called pracpack. Say that three times fast. But, nonetheless, this is new to me. I've only been familiar with the Docker filer package by our our friends, think r.

But this looks pretty nifty, and he even has a link to the paper about the package as well. We'll put that in the show notes, but he was able to actually, even in positron, uses pracpac package to create the Dockerfile with rmbakedin without any fuss. So at least he can get the Dockerfile going. He just can't do remote container development with that Dockerfile in positron. But, hey, it's great to see that he's able to get a Dockerfile on there. So if you wanna throw this into another environment and have those same dependencies from both the system level and the R package level ready to go. But he says the experience of that was very smooth because positron does take advantage of most of Versus Code's features such as syntax highlighting for batch scripting and other file types. So if you're in the multilingual environment, yeah, Positron is definitely gonna be a big help to that.

So the nutshell to me is that things are looking promising. But, again, this is beta. Be warned about that. But I do think that the seeds are planted. And if you're want to put on your speculation hat with me, Mike, Steven kind of has this in a comment in the post. It looks like a lot of the development energy is going to positron now, so we may have to, put a little toast out there to our friendly art studio ID because because I don't know if it's days or numbered. I guess we'll find out next week. What do you think?



[00:36:42] Mike Thomas:

Yeah. Maybe we'll find out next week. I know that some folks have raised that question and the messaging thus far, I think, is that our studio is gonna continue to be supported. You know, Positron's sort of for you know, there there's a lot of different levels of skill of our developers. Right? There's our beginners. There's a lot of folks that are probably are intermediate where they are consumers of our packages, right, and do a lot of their work and and ETL stuff and analysis and R, but maybe aren't developing our packages or or doing anything, you know, much more hardcore than than analysis. I I think that's a probably a large portion of the R community out there and, you know, those first two classes, they are developers and they are, sort of intermediate folks or our consumer or our beginners, I should say, and, our consumers mostly.

I'm not sure how much incentive they have to to move from our studio. I don't know if positron feels like a more welcoming environment than RStudio, but I'm I'm super biased because RStudio was probably one of my first ever IDEs. And then I I looked at Versus Code for a long time. It looked so scary to me when I originally, you know, booted it up for the first time and and tried to do some work in Versus Code. I had no idea where anything was but it's it's probably my bias from, you know, just being so used to our studio and and the layout there and and where the features are. So so I don't really know the answer to if, you know, someone starts out in RStudio versus starts out in positron, sort of who is able to make the best progress the quickest.

But it would be interesting to to see how that plays out, you know, for new users whether they're they're migrating directly to positron or in the future or or whether folks are still starting out with our studio. But, you know, as you said, Eric, you know, for for those of us that need some of these maybe niche features that we get in in Versus Code, you know, around remote SSH as you talked about so that you can work locally, or feel like you're working locally while actually, you know, SSH'd into some remote environment, that that's pretty powerful. I think that that's probably going to come fairly soon, to the Positron ecosystem. When I was looking through the issues and the discussions on GitHub, it looks like that one, is is fairly promising in terms of, you know, Posit's ability to actually get that incorporated.

The other one sort of that hurts me, you know, that we've talked about before is the idea of these Versus Code dev containers that allow you to develop as if you're in a Docker container, you know, while you're in a containerized environment. That's huge for us in collaboration on our team, but I fully understand that that is proprietary Microsoft code, that extension, that Versus Code extension. So there's not an easy way for Positron or any other IDE for that matter to be able to recreate that unless Microsoft someday decides to actually open source how they go about doing that. So I don't know which one's going to come first, whether Microsoft will open source it or Positron will incorporate it, but that's that's a big one for me that I would love to be able to see, incorporated. But, you know, again, these are these are niche things that for probably the majority of the R community may not necessarily care about. And, you know, some of the the trade offs here and the things that we're getting from Positron, you know, I I think are are huge benefits for potentially a lot of that intermediate class. So it it'll be interesting to see, you know, the adoption of Positron, sort of what takes place as it moves from beta to alpha, and what significant changes get incorporated for for quality of life for folks.

But, you know, one of the issues I I imagine that Pauseit is up against is accommodating so many different levels of our users. Yes. Right? Yes. It's tricky. It's tricky. So I don't envy, you know, what they're trying to do. It's a it's a large scale problem to solve. But I do think that being able, you know, the fact that Versus Code is open source and allows, you know, posit to sort of tailor it to whatever they want it to be is is powerful in and of itself and I guess another testament to open source. So we'll we'll see how things progress here.

I love the fact that folks are starting to dive into it and and pick up on the strengths and the weaknesses of it for the rest of us to be able to get up to speed early as we adopt it. So, you know, kudos to Steven as well for doing that.

[00:41:35] Eric Nantz:

Absolutely. And one thing I think about is, first, I do want to mention that I've used my fair share of attempts at IDs before our studio. They were hit or miss at best. I remember one in particular on the Linux system called rkward that tried to do a lot of things, and then others tried to make the Eclipse IDE try to fit with our projects. Oh, boy. That was gnarly, buddy. If you if anybody listening remembers that, give me a shout because I I can share stories of you about that. But I knew at that time, I was trying to learn it. But as, you know, R was starting to take more adoption in industry, it was gonna be a tough sell to get people to develop and those kind of IDs much less than a command line. I mean, hey. I love command line as much as anybody, but, it's not for everybody. Right? So when our studio came out, I mean, you can't underscore the influence it had on not just those getting new to r, new to data science, having this cohesive experience, everything in one place, so to speak, in your development journey.

But, boy oh, boy, did it help with adoption. It certainly did in my industry. So that's why I'm I'm cautiously optimistic that Positron will eventually, you know, hopefully be tailored in in more usability fashion to those use cases for those new to the language and who are not interested in, say, package development. They just wanna get their data science done, get their statistics model fits done, get that quarter report out there, and get on your way. Like, I think the bones are there. It's just gonna be a little while. But with this soft beta that they or that they rolled out, I think now we're they're getting their issue tracker is quite extensive now if I last checked. There's a lot on there, so it's a lot for the the team to prioritize.

But, yeah, we shall see. There is one little nugget more on the, speaking of open source, there's always the issue of licensing. Right? Little hidden nugget here in what Steven mentions at the end here, which I think we should call out here is that Positron is not using things like GPO, not using Apache. It's using what's called the elastic license 2.0, which you may have not heard about unless you're really familiar of all the nuts and bolts of software licenses. But let me read the the blurb that he calls out here that I think I may know the backstory about, but let me read it first. And he says, you may not provide the software to third parties as a hosted or managed service where the service provider provides users with access to any substantial set of the features or functionality of the software.

There have been vendors, I won't name names, that have bundled the open source version of RStudio into their, what I call, platform as a service offerings. And I've heard, unofficially, I won't put names on this, that Pazit was not too thrilled about this business practice. So it does not surprise me that they would go this next step with this fresh start to put this in there, but that may change where positron can actually be integrated. So I guess we'll see this space, but that's something to watch out for if you are in that in that kind of software, you know, provider space. So that was a little nugget I didn't expect to see, but we'll we'll stay tuned on that.



[00:45:09] Mike Thomas:

Yeah. It's interesting to Eric, that's why we have you on the podcast because you you know the backstory around all of this. You have the you know, environments like the the one that Steven works in, you know, environments like the the one that Steven works in where you may need to to host something like our studio server. Right? Mostly internally, maybe you're doing some sort of a consortium collaboration, right? And you you wanna stand that up in a way that, is easily accessible you know cloud hosted something like that to, a group of individuals maybe internally or externally. So I would love to see this license, like, say something like you're not allowed to resell, Positron in that fashion but if you wanna stand it up for for free and not make any money off of it, you know, in a similar fashion to how you would, you know, stand up our studio server, then then go for it.

But this definitely, you know, as as Steven mentions in his case, he's he's not a lawyer, but it looks like, you know, Positron's license may preclude the ability to do something like that. Yeah. That's,

[00:46:23] Eric Nantz:

that's looking more likely. Maybe, again, these will be things we hear about more in between sessions at at next week's Posikoff. But, if you're in tune to this space of those in the data science, you know, platform as a service area, Let me just say that if you're a fan of pizza in the US, you can probably guess the name of the company I'm thinking about. I'll leave it at that. I can't say that. But, nonetheless, this this was a very comprehensive post by Steven here. And, again, it's a pleasure to have him on the highlights, and I'm really looking forward to see how he continues to use positron and other efforts like this as I'll be testing the waters a little bit in my continued shiny development. There is one thing that I want to have that hopefully I can talk to deposit folks next week about.

Please, please make a NICS package for positron, please, then I'm I'm happy. Hey. It's open source. Pull requests are welcome. Yeah. I know. I I don't know if, Bruno, you're listening. Maybe we need to to talk about it. Maybe we need to strengthen numbers on this one.

[00:47:29] Mike Thomas:

There you go.

[00:47:30] Eric Nantz:

Well, now strength in numbers is also a way you can say, you know, what's the benefit of our weekly itself. It is the strength of the community that populates every single issue. So we'll take a couple of minutes to talk about our additional fines here. Now I'm gonna talk about something that I totally did not expect to talk about, especially in the the circle I operate in. We have a new package that's been released in the our ecosystem called Maestro positioned to create and orchestrate data pipelines in R.

I know a little bit about pipelines because I happen to work closely with the author of targets himself, Will Landau. So, of course, the first thing I'm thinking of is what is Maestro all about? So we'll put a link to this in the show notes of Khos, but at a high level, Maestro is definitely positioning itself in the realm of data processing. I don't envision it's trying to approach on the comprehensive ability of targets to be flexible in many types of analytical workflows that we often see in data science and operational automation efforts.

But the the nuts and bolts of Maestro are kind of fascinating where they're taking advantage of our oxygen tags in a function that you create for your data processing or your ETL processing to define characteristics such as the schedule of when the or pipeline will run such as, like, the frequency, what's the actual start time of that job, and there there are other decorators as well. And then once you have your functions decorated, you use some of the functions built into Maestro to build that schedule and then to execute it. So they do leverage other packages that help with, you might say, more of the high demand, you know, computation such as the future package and the FIR package. You wanna have that map like functionality for your running schedule. It looks like it can interface with those quite a bit, but I don't think this is an either or. I definitely when I saw Maestro get mentioned, I went immediately to their package site, and they have an article on their motivation for creating, Maestro.

And, of course, there is a section on comparison of Robert Paggins. I was pleased that they talked about targets on this because I was gonna be some I would have reached out to the office right away about if I hadn't seen this. But it looks like they are interested in seeing where Maestro and Targets can complement each other because, at least in some of the target critiques I've heard over the years, it's a little more difficult for these more ETL, especially the extract portion of data processing pipelines. Again, especially if you're interfacing with databases or APIs that you're ingesting data from, There are ways you can use targets with it. You just have to get a little more into the nuts and bolts of how you orchestrate your return objects and whatnot. So I'll be very interested to see if this ends up being a cohesive relationship or not. But with that said, if you're in this space of producing comprehensive data pipelines, Maestro may be worth a look.



[00:50:42] Mike Thomas:

Absolutely, Eric. I'm in very interested to see how that evolves. That sort of came out of left field. For me, that Maestro package wasn't something that I had had on my radar, but it's it's really, really interesting, you know, especially as we've been getting into a lot more targets projects lately, a lot more of these sort of pipeline, types of types of projects, so it'll be, you know, a potential new tool in our tool belt as well. I have sort of a different highlight that I wanted to call out. It is an interview with the R Consortium published and held with, Pharma Rug China organizer, Zhou Xu, who who spoke with the R Consortium about how he has grown the R community in China, you know, specifically in the pharma space.

I think it's something that we don't necessarily talk about enough is how international r is. One cool a couple cool facts about Joe is that he studied or he has his PhD in statistics where he studied in New Zealand, which we know is the birthplace of r. He worked at at Roche for, the past 4 years where he helped open source 30 software packages. There there's 4 that are called out here. I'm not super familiar with them, Eric, but you might be, you know, being in the pharma ecosystem, formatters, our tables, our listings in turn. Oh, yes.

Sounds like there's there's many more beyond that. This this our community in China has done a lot of collaboration and I think is involved pretty heavily in the pharma rug, pharma r user group sort of in general. And there's just a lot of great talks about how Joe has helped organize a lot of events, a lot of hybrid events, and how he's been able to pull those off using Microsoft Teams as well as, you know, hosting, the these hybrid events in person as well. So I I thought it was a really nice interview, for anyone who's looking to learn about, you know, organizing, our groups, you know, within their their neck of the woods that you might get something out of this interview.



[00:52:46] Eric Nantz:

Yeah. I'm really glad to see this because we are seeing a lot more momentum of our our colleagues in Asia Pacific regions adopting our in life sciences. This is hugely instrumental to making that journey hopefully easier for those that are that are on this. And, you know, very closely related to this, some of you know I'm one of the organizers of the annual Our Pharma Virtual Conference. One thing that we've always struggled with is how do we best accommodate our our fine friends over in the Asia Pacific region because we're mostly a US based conference in terms of time zones and the way we stream our our talks.

Well, I'm happy to say that, you know, alongside reading this post, if you are in the Asia Pacific region, there is going to be an r pharma specific event tailored to Asia Pacific. So we're gonna have a link to that. It's actually in our this week's our weekly issue that call for talks is open for that particular effort. We'll put that in the show notes just in case here too. Jonathan Caroll has actually been quite nice to to advertise that on Mastodon and whatnot. So, yeah, this is a wonderful time if you're in the life sciences space in in Asia Pacific region to to dive into this momentum.

I liken it to what we, you know, when our farmers first introduced in 2018. None of that had been built before. Now we're trying to expand the scope of this because we wanna take advantage and get everybody across the world a unique opportunity to share their learning and learn from each other along the way. So, yeah, gratifying to see the Arkansas team keep, keep on spotlighting these these great use cases. And, yeah, I'm very excited to see where the future goes here. And, of course, you can't just be excited about that. You gotta be excited about the rest of the issue. There is a boatload of additional content here following the full gamut of new packages, many of which we wish we could talk about here today, but there's only so much time in a day. But there's lots of great tutorials as well. I see a full gamut of spatial visualizations, tidyverse pipelines, even a great post by Nicola about creating typewriter styled images. I think that's great for my retro feels as well. Love to see that. So much more content there. Where do you go for it? If you don't know by now, you know now it's rweekly.org.

You bookmark that every single week. We got a new issue for you. And if you wanna contribute to the project, we rely on your contributions in the community. So, please, if you find that great new blog post, maybe you wrote that post, you find a great package, a great tutorial, send that to us via a poll request directly linked at the top right corner of the home page. It's all marked down all the time. Right? Markdown is how I live in writing my content, and as for some internal presentations having to go back to PowerPoint, it just doesn't feel right. I feel right writing a markdown in quarto or our markdown, and I'm gonna put my foot down on that. Hopefully, that becomes more of a trend in my industry, but I digress.

Either way, I'll markdown all the time so you can send your poll request right there. We'll be glad to merge that in for the week. And, also, we love to hear from you as well. We got a contact page in this, podcast episode show notes directly linked there. Did a low HTML hacking to get that together after we moved our podcast hosting. So hopefully it works out for you. But, also, you can get in touch with us on social media. I am on Mastodon these days at our podcast at podcast NSR social, also on LinkedIn. Send me a shout there. And, again, if you're gonna be at Pazikov, I will be there at this time next week for sure. So we hopefully hope to hear from you. And, Mike, where can listeners get a hold of you? You can find me on mastodon@mike_thomas@phostodon.org.



[00:56:30] Mike Thomas:

You can find me on LinkedIn just by searching Catchbrook Analytics, ketchb r o o k, to see what I'm up to lately. Or, Eric, as you said, find me in person next week if you're gonna be in Seattle. We'd love to chat with you. Yep. I'm gonna do my best to wear some kind of r related swag shirt every single day, so I'm easy to spot.

[00:56:49] Eric Nantz:

But who knows? Maybe everybody wearing r swag so it may not be easy to spot.

[00:56:53] Mike Thomas:

I'm gonna print up some rweekly highlights podcast, t shirts for us. Oh, yeah. Because it this is an audio podcast. People might not have a single clue what I look

[00:57:05] Eric Nantz:

like. Yeah. I still remember one time at a at a the first shiny dev conference, I was spending a question. Somebody looked over and said, hey. I know that voice. Yeah. So we're gonna get, I'm sure, our fair share of that. Nonetheless, we could blab around all day. We're always excited to talk about this stuff, but we got a close-up shop here. We got our day jobs to get back to, but we're very happy you joined us today for listening to this latest episode of our weekly highlights. And, again, we will not be back next week because we'll be at Pazacom, but we look forward to connecting with you all again with a new episode in 2 weeks from now. So until then, goodbye, everybody.

"
"45","issue_2024_w_31_highlights",2024-07-31,30M 46S,"Episode Links This week's curator: Jon Calder - @jonmcalder@fosstodon.org (Mastodon) & @jonmcalder (X/Twitter) Let's Talk About the Weather 2024 Shiny Contest Entire issue available at rweekly.org/2024-W31 Supplement Resources https://lorenzwalthert.github.io/precommit/index.html https://www.kenkoonwong.com/blog/llm-rag/ Supporting the show Use the…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 173 of the R Weekly Highlights podcast. This is the weekly podcast where we talk about the terrific resources that are being shared every single week at rweekly.org. My name is Eric Nantz, and I'm delighted you're joining us today. And I admit I look at the calendar at the top of my screen as I record this, and I cannot believe it's July 30th already. We are almost in August, which means my kids are almost in which might be a little bit of normalcy for those parents out there, if you can relate to that. But, nonetheless, we're here to talk about all things are in our weekly. And I don't do this alone as always. I'm joined by my awesome co host, Mike Thomas.

Mike, where did the time go?

[00:00:43] Mike Thomas:

I don't know, Eric, but, I imagine potentially like you you may be also scrambling, to put a presentation together for a conference in August. That's sort of where I'm at. These conferences have come up quite quickly. And, Yeah. It's crunch time.

[00:01:00] Eric Nantz:

It is crunch time. Yes. I'm gonna frantically put in the finishing touches on my upcoming talk about WebAssembly. Very excited for it. Having some initial good feedback, so I won't put any spoilers out here, but I got some good stories to tell with that with that effort. But, yeah, less than 2 weeks from now. So as you heard from last week weeks before, Mike and I will be there at Positconf, and definitely welcome for you to come say hi. And I cannot confirm or deny that I might have some stickers with me. We'll find out, but nonetheless, come say hi nonetheless. We're always all connecting to the listeners over at these events, and, yeah, this will be an exciting time.



[00:01:38] Mike Thomas:

Yes. Absolutely. I have a presentation not at Pazitconf, a different conference. It's on the topic of AI and you're gonna like this. My first slide is you might not need AI.

[00:01:50] Eric Nantz:

Good. Hit them quick with it. Really setting the tone. You should. You should. That that's terrific. Now I I but, yeah, who knows? And, you know, give or take here here and there, but there are some fluffy efforts going on in various industries. So, yeah, you keep it real, Mike. You keep it real. That's what we do. That's all we do. And what else keeps it real? Well, it's organically real in terms of the awesome content at our weekly every single week. And as you know, we have a rotating set of curators that pitch in on different weeks to help, assemble the issue.

And this week's our curator is John Calder, another one of our OGs, if you will, of the our weekly team. And as always, he had tremendous help from our fellow our weekly team members and contributors like all of you around the world. And our first highlight just happens to come from one of our fellow curators. And looking at this a phenomenon that unfortunately can occur when we grab our data from online sources and things get kind of uprooted from us, but the community comes to the rescue once again.

This blog post is coming from Jonathan Carroll, who again, on top of his awesome efforts of our weekly is always looking at new things to learn in his blog. And it's always a fascinating read. Well, he's taken a bit of a detour from his programming language exploits and other languages. He's gonna talk to us a bit about the weather in Australia, and he's actually looked at this for quite a while now. In fact, since the mid 20 tens, he's been grabbing weather data from Australia that's been exposed by the Bureau of Meteorology.

He says don't call it the bomb. I won't do that. But this has been keeping track of weather for a good while now. And but there's a bit of a bad news is that unlike other services, maybe you shouldn't be surprised about this. They may have an official API to download all this. So Jonathan would leverage his scraping skills using the various utilities that we've covered many times in this podcast and elsewhere. There are many awesome packages in order to help with web scraping in particular, like the Arvest package, but there's many others in this space.

Well, he's been doing that for a good bit. But there was a recent, mishap, if you will, where the service back in 2021, his function to do the scraping was not working anymore. No code changes. What gives? Right? Well, turns out as he did a little investigation in terms of user agents and other pits here, There was an official statement that was released. It says, and I quote, the bureau is monitoring screen scraping activity on the site and will commence interrupting and eventually blocking this activity on this site from Wednesday, 3rd March 2021.

Well, epic fail. Right? That's not good. And now, rightfully so, John's a little peeved about this because this is from a government site. You know, Australia, like other countries, we have taxes. We're open to pay the government from these kind of services. So oh, kinda throw your hands up on that one. Mike's biting his tongue here. I can tell there. Yep. Been yep. Yep. We've we've been there with these things getting uprooted from various sectors and government. But the community comes to the rescue once again, unfortunately, where Adam Sparks, who's also been interested in weather data from Australia as well, he discovered yet another site that was kind of filling the niche of what was happening with the bureau site called silo.

And he and Adam has built a new our package, which also is actually included in this our weekly issue called weather Oz and actually has an accompanying paper in the journal of, statistical software, the boots. So lots of lots of effort behind this, which helps have a compliant R package to grab the weather data from this silo site. Terrific. Okay. Back of business, John says. Now he can leverage this new package to have a very handy function called get stations metadata, which he's able to put in the station name and then which API to use. And by default, I believe it is a silo API.

And sure enough, you get a tidy data frame back of the various metadata associated with this. And once you get the station code, then he can actually grab more of the data itself. And this is where he started now updating his functions to grab, you know, the various metadata associated with temperatures and whatnot, longitude, latitude, lots of other metrics here. I'm looking at the glimpse of the data frame here. There is a lot going on here. So if you're a weather junkie or weather nerd, this one's for you. There's lots going on in this space.

So he was able to grab almost 50,000 observations over the last 135 years. So there you go. You got yourself some time series, if I do say so myself. So after a bit of tidying up, he decides, okay. You know what? I used to do some plots of this in the past as I was investigating some questions. Let me run these again. And sure enough with the new data and the tiny format, a little bit of g two code, and he's got a nice set of charts, Mike. Why don't you walk us through what he's trying to visualize here?



[00:07:21] Mike Thomas:

Sure. So it looks like John originally had tried to create these chart charts using a package called Bombrang, which I was not familiar but appears to be be superseded, and now we're transitioning to ggplot, and I really really enjoy the ggplot code that he's written here. 1st, he's taking a look at the daily maximum temperatures and as you said, Eric, there is a lot of data here. We're we're going back, from 2024 back to 18/89. So we have a scatterplot here with a beautiful curve on it, that shows, you know, has a different point for the day of the year, for each year between 18/89 and 2024 and what the temperature was on the y axis, and we have some some color gradients based upon the decade as well, which I I think is a really unique and sort of interesting way to, add this additional dimension to the data as well. And it produces this really nice curve really beautifully done in g ggplot. We're we're using Viridis, to accommodate the color blind folks out there. So I can't say enough about this. It has a great caption on it as well. One of the things that I really appreciate, that John does, you know, throughout some of the other plots in this, chart is the use of I don't know if you would call them, like, Lambda functions for for additional filtering. But within the ggplot syntax you can pass, starting with like a a tilde a dplyr filter statement where the data frame itself that's being used by ggplot is, you know, referenced by this dotx placeholder.

And there's multiple examples here in the code of how John goes about doing that and it's a really clean, nice syntax. It's probably something that I don't use enough. So if you are starting to get into, you know, a little more complicated ggplot visualizations, where in certain aspects of the plot or certain layers you you wanna use a portion, only a portion of the data and not the entire dataset, such as, in this daily minimum temperatures plot that John creates. He wants to highlight specifically in red so that these points stand out, you know, really really obviously to the user. All the observations, from June onward in the year 2024, so that you can see those really on top of the the rest of the Viridis, you know, gradiented, points on this daily minimum temperatures plot. And it's it's really simple syntax, I would say. Just this really nice dplyr filter statement and it's it's great that we have this beautiful concise interoperability sort of between ggplot and dplyr, to allow us to use those tools together to add these layers, you know, some that have a filtered context and and some that do not. Then we move into fastening as well where he has a daily maximum temperature plot, fasted by each month of the year. And one of the nice things here, again, is where we are, again, sort of calling out a specific point on each facet based upon a dplyr filter context where we're we're actually slicing the max, the the highest particular temperature across all 100 plus years, whatever that math is, between 18/89 and 2004.

And we're coloring that particular point red and we're sticking a geomtext label on top of it to let us know what year, that highest temperature in that particular fasted month took place in. So it makes it really, really easy to consume and take a look at this this chart to see the the temperatures, over time in a particular month, but then also call out the year that had the highest temperature in that month. It's it's really, really well done data visualization and he does the same thing, with the daily minimum temperature.

And one sort of neat visual trick is that instead of, highlighting those those, you know, maximum or highest, temperature dots with the color red, as he did in the maximum temperature side, he will, highlight those with the color blue because these minimum temperatures are the the coldest temperatures, that that took place or that the year that had the the coldest temperature in that particular month I thought that was that was really nifty and something that I probably would not have thought to do but it's really interesting. And the the captions here and really the attention to detail are are beautiful and it's just a one of those really nice data visualization, blog posts, Eric, that I know you and I really love and appreciate because I'm a visual learner. This stuff stands out to me. Most of the work that we do for our our clients, ends or or, you know, utilizes some sort of data visualization as well to get our point across because it's it's one of the the ways that we communicate data the most effectively. So if you are just looking to either up your ggplot game or just get a little refresher and and take a look at what, John has put together here and some of his ggplot work. I can't recommend this blog post enough.



[00:12:24] Eric Nantz:

Yeah. It's such a great introduction to a very concise EDA with a novel dataset too and very logically exploring these trends that he has seen as the years have gone by. Can we see, like, a seasonal type pattern in terms of the variation of these temperatures? And, yeah, he has noted that it's been a 2024. Apparently one of these lowest points was a negative 5 degrees Celsius. And yeah, that's a bit chilly. And, someone who who's lived in Michigan over my formative years. Yeah. I know how cold things can get. But, yeah, it's interesting to see. Again, the facets really show the story of how the spread tightens in the middle of the year and then spreads more out at the extremes or, I should say, the beginnings and ends of the year. Yeah. Really, really novel use. Like I said, that way, I'm the like functionality.

The key point there is these geomes in these data arguments. As long as you're getting a data frame back, you don't have to do this pre computed in fashion. You can do it in line, so to speak, in the geo, which again is great, especially if you're doing this EDA and you wanna iterate on this pretty quickly and with some pretty concise code. So I think it's a novel technique that, like you, Mike, I have not utilized this enough. So I have to take a take a bit of learning here as I revamp my simulation or visualizations of simulations that we're trying to do a better job of these days.



[00:13:51] Mike Thomas:

Yep. And you know me, Eric, you know, in terms of code review and things like that and collaborating with the team, I'm always trying to arrive at code that is as concise as possible while getting, you know, the point obviously across as effectively as possible about what it's trying to accomplish. And I I think this is a great example of doing that.

[00:14:12] Eric Nantz:

Yep. And and John never just stops there. He's got a a truckload of other amazing posts in his learning adventures. So if you really wanna get into the the nuts and bolts of other programming languages and how from an our user's perspective that he relates to them, there is a lot going on in his his blog. So if you haven't bookmarked it before, you absolutely should.

[00:14:36] Mike Thomas:

Yeah. And his session info, he's got the the OS.

[00:14:39] Eric Nantz:

The OS says pop, p o p with an Yes. Information on the pop up list. That's what I'm using on this very machine, PopOS. A Linux. It's a Linux distribution. It's, made by the vendor called System 76 who makes hardware dedicated to Linux. So this, box I'm talking to you right now is called a Feilio PC that they make right here in the US and Colorado. And I've had it for about 4 years. Yep. If not longer, actually. Yep. So Very cool. A fellow PopOS user. That's awesome. He's a he's a fellow Linux nerd. So that's awesome. Love it.

You know, Mike, I always wonder if someone was kind of missing this year as the months have gone by. Well, there is a tradition that certainly a shiny enthusiasts are very eager to see happen, and it is back. What am I talking about? It is the 2024 shiny contest run by posit is officially up and underway. And this blog post comes from us from the author of shiny himself, Joe Chang, as well as our posit, community manager, Curtis Kephart. And then the blog post is pretty short and sweet. If you've familiar with the contest before, there's not a lot of changes. But if you're new to this, this is the annual tradition where they are reaching out to the community and inviting them to submit their entries of their innovative ways of creating and and deploying shiny applications.

There are a set of requirements to be aware of is that both the data and the code behind the app should be open source publicly available. So, obviously, it goes about saying you probably don't wanna use your company's internal data for this, but that's neither here nor there. And they also invite you to deploy the application on this time around the recently launched posit connect cloud service, which I believe just went publicly available a month ago or so. In the past, that's been Shiny apps. Io. But if I'm, you know, that's the usual evolution of a enterprise and their software products, I must say. And that you store the code in a public repository that could be GitHub, get lab, that bucket, whatever have you.

And there is a set of judges, which I don't believe, you know, the names just yet, but there will be a set of judges that evaluate each of the applications based on a set of metrics. I speak a little bit of, experience on this because it was like a few years ago, I was a judge on one of these contests and my goodness, the submissions were such high quality. It was really tough. Great to see the applications, but, man, trying to find trying to judge these in a fair way when you just wanna say they're all great. Right? So it's a there's a set of judges to help compete with these metrics and a lot of nice awards at the end there with you are a runner-up or an honorable mention and the grand prizes. They have all the details there. Yes. There will be a bunch of stickers being thrown your way, and the grand prize winner actually gets a half hour private meeting with members of the shiny team. So my goodness. That's a that's an awesome opportunity in and of itself to go over your app and what you can do in the future.

So you may wonder where can I go to look? Well, I will say that but and the nice thing in this post is they've linked to the previous blog post where they talked about the winners and runners up all the way back to 2019. I still remember that you're very fondly of being like, okay. I'm all over this. I had fun creating a Lego mosaic app as part of a shiny contest of yesteryear. That was a that was a fun time. Of course, if you look at that code now compared to what I do now, just don't judge too harshly.

But, nonetheless, it's a great time of year. We're always excited to see what the community comes, comes to bear with this. So, yeah, if you're a shiny and a shiny enthusiast, this is this is a great opportunity to test your might if you will and see what you can bring into the community.

[00:18:53] Mike Thomas:

I agree here. I can I feel like there's so many more options in the shiny ecosystem nowadays and so many different sort of branches and paths that you could pursue as as you you know make your entry into this contest you know is it are you just going to have the the repository are you going to deploy your app somewhere is it going to be on shiny apps that I owe or is it going to be on shiny live right you know leveraging the web our framework are you going to use shiny for Python or are you going to use shiny for our or are you going to use that the teal framework for building shiny apps which is popular among folks you know in your space Eric and and farm on in life sciences and they did note that they are going to have a special recognition for developers shiny so if you were someone just getting started with shiny don't be intimidated by this contest actually embrace it because you could potentially get the the special recognition for for giving it a shot and I believe that that you will find if you are a new developer to shiny and you take place, in this contest and you participate in it, you know, one of the I think sort of where this is hosted is is the posit community? Correct. So I would recommend that you ask any questions that you have along the way within that Posit community forum. People are really responsive.

I hope and I would, you know, based upon my experience, I think that you'll find it welcoming. I think that you will find, folks pretty responsive to a lot of your questions, especially, you know, within the context of of this particular contest and everybody trying to put their best work forward. So, welcome everyone, 1 and all. It's put your submission in. I think the deadline is, Eric, when did we when does it look like the deadline is? Let me take a look. I should know that. Check this out beforehand.

Deadline for submission is September 15th at midnight, anywhere on earth. So you've got a little bit of time. It looks like we've got about a month and a half here, if my math is correct, based upon the time that we'll put this, this episode out. So get to developing.

[00:21:06] Eric Nantz:

That's right. And and certainly kudos to the shiny team and Curtis for working with, like like you mentioned earlier, this is the first time that we're really calling out the opportunity in the life sciences space to leverage the t o framework, which is making huge waves in our industry to help build these very comprehensive shiny ass of a modular structure that are tailored for reviewing the, you know, the types of clinical data that we deal with on a daily basis. So this is an awesome opportunity to see what you can do with that. And I've even seen others in the community that are not not necessarily part of pharma levers teal to do some really fun exploratory data analysis of a shiny front end. So, yeah, definitely, if you're a teal enthusiast, like, that's growing pretty rapidly. Yeah. This is a great opportunity to put your put your work out there as well. And certainly, like you mentioned earlier, for our friends in the Python space, Shawnee 1 dot over Python was just released. So another great opportunity if you've been wanting to test your mind with the Python side of things. This is a great place to do it. I always think in general, finding, a dataset or or a domain that you're interested in in and having an opportunity like this is such a great learning opportunity too. Because again, you're going to see not just your submission being put into the queue. You're going to see in real time as these submissions come in, there will be a post on posit community dedicated or a category dedicated to this contest and you'll see these start to trickle in and you get can get really inspired to see what what everybody's up to. So I remember going down to the wire on my Lego one and seeing just the, the, the high quality submissions and have admittedly a first time have low imposter syndrome. Like, oh my gosh, I can't believe this. But then again, I'm liking it to great learning opportunity and it's an, someone else will benefit from what you're putting out there. It always happens. So I remember getting some nice, comments on my submission, but then many others were providing questions and comments on these posts. So, again, very big emphasis on learning and having fun learning, I might add.



[00:23:17] Mike Thomas:

Absolutely. I can't wait to get my hands on keyboard and see what our team can come up with for a submission. I'm gonna hold myself to it.

[00:23:24] Eric Nantz:

Yeah. Yeah. Once the dust settles on deposit comp, I might have some more geeky opportunities to do shiny stuff with podcasting data again. We'll see. We'll see. But, yeah, we're on this podcast now. Right? And we wish we could talk about the rest of Arwek issue, but, there's always so much that we have time in today. So we're gonna wrap up here. Well, a couple of our additional fines that we saw in this week's issue, which again is always linked in the show notes of this episode. And I've always been, you know, trying my best to leverage, get effectively, especially with, you know, nice commit messages, you know, following, you know, standards that we maybe code reviews are gonna make easier if we follow the standards of one area that I admit I don't do enough of is there is a mechanism and get that will basically check before you commit run, I should say, a custom script that might check for various things, such as maybe number of files you're committing, maybe number of lines, maybe the type of commit message, maybe you have, like, a a very formal framework of it. You can build what are called pre commit hooks in your local Git repository to help be that kind of frontline check before you actually do the commit.

Well, there is a new package in your ecosystem called pre commit, which will let you author these pre commit hooks from the comfort of your r installation itself. And there is also, I believe, a little bit of requirement for Python as well. So this might be using reticulate under the hood. I'm not quite sure. But once you have everything set up, you're gonna be able to do, you know, a very nice, there's a nice package documentation site that we'll link to in the show notes going from the motivation of why you want to leverage this as well as some built in hooks that you can leverage such as for styling, such as making sure your read me is up to date if you're doing our markdown for your read me, making sure you're not leaving a browser statement in your code because who would ever do that? Oh goodness gracious. Who would ever push that into production? I don't know. My goodness. Where was this few years ago? But, yeah, there's a lot to choose from here, so, I may need this. What do you think, Mike?



[00:25:42] Mike Thomas:

That's an awesome resource, Eric, and I may need that as well. Yeah. That that could have saved me many many many times over so I need to check that out. I found a really really cool blog post called llama llama oh give me a sign what's in the latest IDSA guideline. And I could probably just leave it at that. That's such a cool blog post title but I won't. Right. This this one is authored by Ken Koon Wong and it is all about leveraging, you know, open source, I believe, LLM, RAG to be able to ask questions of the latest guidelines from the Infectious Diseases, Society of America and it is incredibly comprehensive.

It, uses Reticulate as well to interrupt R and Python to be able to to stand this solution up and lots of gifts, lots of content, it's an incredibly of blog post and if if this is something, that you might find interesting in using LLMs and and rag to be able to ask questions or or summarize a particular document or set of documents or or guidance that's out there such as, the the Infectious Diseases Society of America guidelines. I would highly recommend checking out, how can went about doing this. It's incredibly comprehensive.



[00:27:05] Eric Nantz:

Oh, this is incredibly useful too because in many enterprises, it's not just about trying to be, quote, unquote, future proofing your questions, but taking what you already have, whether it's these documents of, like, important information or metadata associated with, like, infrastructure or where you're designing experiments or whatnot. Being this is a great pose. I'll kind of walk through what is that process of feeding those documents in, creating under the hood what these vector databases they caused. So then the LM will use that as a source to help answer these questions. So this is a a very hot topic. And and and my industry is we deal with, you know, thousands upon thousands of study documents and patient documents and whatnot to be able to, you know, effectively put an LM in front of that to explore what we have.

Heck, you know what? Even in the podcasting space, there's been talks of trying to consume our show notes and having an LOM in front of that to maybe figure out, Hey, when did Mike and Mike and Eric talk about that, that, that new shiny package or that or that new, portal thing? You know? Boy, that would be the dream. Right? That little bot in front of that. I don't know. Just saying. Sweet. It's all the metadata is there, folks. This may maybe we'll help to build that, but this post is a good good way to get us started, I think.



[00:28:27] Mike Thomas:

Add us to the side project list.

[00:28:29] Eric Nantz:

As if it wasn't long enough already. What am I doing to myself? And on top of that, I'm trying to learn Nicks to boot on this. So my gosh. What what am I doing to myself? Nonetheless, no side project elsewhere here. We're gonna tell you about how you can contribute to rweekly itself. That doesn't have to be as in-depth as a side project. We are just a pull request away. If you find a great new resource, new package, new blog post, whatever have you, we it's all marked down that comprises the our weekly site. So if you know how to put a link in our markdown or markdown itself, you know how to contribute to our weekly. It's basically that easy. You have a template and a GitHub issue, or a poll request template to navigate you through the requirements. Again, very straightforward. We welcome all your contributions.

And, of course, we welcome hearing from you as well. As we mentioned in a couple weeks, Mike and I will be in Seattle for Positomp. But in the meantime, if you wanna get a hold of us, there's a few ways to do that. We have a contact page in the episode show notes when you download this in your favorite podcasting software of choice. You also if you're on a modern podcast app like Podverse or Fountain, you can send us a little boost along the way as well. We got details on how to do that in the show notes, and you can find us on the various spheres of social media circles.

I am mostly on Mastodon these days with atrpodcast@podcastindisc.social. You can also find me on LinkedIn. You search my name. You'll find me causing lots of fun stuff there potentially and on that weapon exiting from time to time with at the r cast. Mike, where can listeners get a hold of you? You could find me on mastodon@mike_thomas@fostodon.org,

[00:30:09] Mike Thomas:

or you can find me on LinkedIn if you search Catchbrook Analytics, k e t c h b r o o k. You can see what I'm up to.

[00:30:17] Eric Nantz:

Very good. I'm always every time I boot up in the morning, I'm looking looking for Mike's newest adventures, so I'm always pleased when I see that. Nonetheless, yeah, lots of adventures that we have to get on to the rest of our day. But, of course, we thank you so much for listening from wherever you are around the world. We really enjoy doing this every week, but we're gonna close-up the the proverbial mics here, and then we're gonna invite you to join us for another episode of our weekly highlights next week.

"
"46","issue_2024_w_30_highlights",2024-07-24,43M 25S,"Creating retro-gaming sprites rendered from the comforts of R? Yes we can! Plus an honest take on the utility of Github's Copilot Workspace in the context of package development, and taking the concept of code trees to another level with treesitter. Episode Links This week's curator: Ryo Nakagawara - @RbyRyo@mstdn.social (Mastodon) & @RbyRyo)…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode a 172 of the Art Wicked Highlights podcast. If you're new to the show, this is the weekly podcast where we talk about the latest highlights and awesome additional resources that are shared every single week in this week's our weekly issue. My name is Eric Nantz, and I'm delighted that you join us wherever you are around the world. It's hard to believe July is almost over, but, of course, we got a lot more great art content to talk about with you today. And I never do this alone as you know. So at the virtual hip right here on this split screen here is my cohost, Mike Thomas. Mike, how are you doing today? Doing well, Eric, at the virtual hip.

Only for a couple weeks until we get to see each other again in Seattle maybe? That's right. Yeah. Her her the countdown is on, so I gotta get all my bits sorted out and hopefully get that get that talk ready to go, but all all in good time. But, yes, the you might say the nerves are starting to hit a little bit, but again, it'd be great to see you again and see all the the wonderful peeps in data science and other sectors that frequent that conference every year.

[00:01:06] Mike Thomas:

Yes. I'm super excited as well. We have a ton of clients that are going to this conference this year, so I think it's gonna be a big one. I'm I'm almost surprised how many folks I I know that are gonna be there. So it's gonna be it's gonna be a party.

[00:01:19] Eric Nantz:

Yeah. Party as well. Yeah. You're gonna be high in demand, my friend. I hope they even get a few minutes of you after all that.

[00:01:26] Mike Thomas:

Quick short story is last time that last year when we were at Pawsit Conference, Eric and I met and it wasn't 2 minutes after, you know, we met, I guess, for the first time in real life that somebody came up and asked me to take a picture of you and them because you are the celebrity at this conference, Certainly not me. So Oh. Goodness. Goodness. Yeah. That was how it started.

[00:01:49] Eric Nantz:

That's yeah. That's how it always starts. Yeah. But you know, I think the tables will turn this time around. But, nonetheless, we're gonna have fun connecting with everybody. And, yeah, we're we're both gonna we're gonna have a lot going on there, but we got a lot going on here, my friend, with the mic in our hands. So let's get going here. Our issue this week was curated by Ryo Nakagorua, another one of our OG curators and longtime contributors to our weekly. And as always, he had tremendous help from our fellow Rwicky team members and contributors like you all around the world with your poll requests and suggestions.

As you may have heard in previous episodes, I admit the teas has input ever over like a few months ago that I think who needs that like game development engine, like unreal or anything. We can use our developer games. And there is yet another milestone in this workflow here that we're gonna talk about leading off this episode. And we are talking about the latest blog post from Matt Dre who has been on this quest to leverage not only tooling that he's creating, but also augmenting some of the awesome tooling that's being created by Mike Chang who you may also known as cool but useless on the social interwebs.



[00:03:01] Mike Thomas:

So probably the highlight of this blog post is that we know the full name of of Mike FC. Cool but useless. Right?

[00:03:08] Eric Nantz:

You are correct, sir. This may be the first time we've ever seen that spelled out. So I guess the mystery is over.

[00:03:15] Mike Thomas:

I know. I apologize to Mike if he was trying to, keep that from us for because I he did a pretty good job for a long time, until this blog by Matt has come along.

[00:03:26] Eric Nantz:

Yeah. You know, I I liken this to, you know, for those of you that follow pro wrestling, there are always that those days in the eighties where you didn't know that Hulk Hogan is really named Terry Beleja until, like, you were much older. Maybe it's just one of those reveals, like the gimmick the gimmick. The time's up for the gimmick, but nonetheless, we we know who you are, Mike, nonetheless. But, Yes. You've been you've been hard at work on these packages. So what Matt talks about in this post here is that he is leverage what Micah's created called the Nara package, which is basically super charging R's ability to produce raster based graphics, but tailor made to things like pixel art.

And he, and Matt what Matt has done is he's augmented the narrow package tooling with his previous package called rogue like, which if you recall from a highlight episode probably 4 or 5 months ago, was a way in r to create these, in essence, randomly generated dungeon crawlers all in text based format. So you get a nice ASCII art of the dungeon with the layouts, and it would respond to your key presses to have you as the player with a little, you know, maybe p symbol in the middle there going up, down, left, right, and then the randomly generated enemies or other artifacts would move along alongside you.

So he simply now merged the ability to do this, but now instead of the textual representation of those dungeons, he is augmenting some nice retro style looking sprites here. In fact, he's leveraging an open source framework, from a, username Kenny. He has what's called a tiny asset pack of all these different sprite arts that are 16 by 16 pickle pixels. So really, really nice, and if you ever played those RPGs of yesteryear, like, say, dragon warrior or final fantasy, yeah, these are gonna look right at home in those in those artwork.

So how does this actually work? Well, as I mentioned, there is a lot of the underpinnings have already been made by Matt in this roguelike package, But now instead of the textual representation, he is mapping that mesh which basically is a matrix of, you know, 16 by 16 or whatever dimension where each cell has either a movable space or an obstacle, the player or the enemy. And so that's already randomly generated up the front, but then that is being trans translated into these tiles that are being created by the narrow package.

So in the blog post, you see the textual structure of it, which again will look very similar to if you use Matt's roguelike package, but then that handoff is then translated, like I said, into this tile based board. And now you see the nice image going going under the matrix representation. It just looks literally picked out of a retro game. It it really is absolutely amazing what's going on here. And you can do all sorts of things of this. Obviously, you can make this as big or small as you want, but there is a lot more that is coming, and that's, tooling here.

He wants to make, you know, really a true loop of the game, which if you played RPGs or roguelike dungeon crawlers in the past, you know that as you move along the board, you get a random encounter with an enemy, do the battle, win or lose, rinse and repeat. But, of course, it's almost like an infinite loop per se. So he's looking at ways of having that true kind of gaming style loop in the back end here. And of course, like any RPG, you're gonna have it after your inventory at some point. Right? You gotta have those weapons, those potions, those antidotes when you get poisoned or whatnot. So, obviously, this is probably gonna be a huge rabbit hole if they choose to go down this route. But I am very much eagerly watching this. But, of course, the first major step is what the user actually sees.

So what's seen here with this, package that's Matt's created, he now calls a tile based. This is the start, folks. Like I said, who needs to pay for that unreal engine or that unity engine? Boot up your r console and go to town. Right, Mike? Absolutely. And it's you know, just reminds me of the old adage that

[00:08:02] Mike Thomas:

R is only a programming language for statistical analysis. Right, Eric? Can't can't do anything else well. Yeah. Not at all. Not at all. And this also throws me back to, I think these graphics are like equivalent to at least, you know, the the Game Boy Color at times of the world. This is reminding me of my Pokemon Blue game that I used to to play on long car rides quite often. Oh, yes. I think the graphics are are quite akin to that. It's it's pretty incredible that I think, as you mentioned, Eric, there's only like 4 they look like emojis, but I guess we're calling them, the these objects from a tiny asset pack which is some resource out there that has all of these different 16 by 16 pixels that you can use. I imagine that they're like Creative Commons licensed or something like that, which is why Matt chose to to use them and incorporate them and this whole entire game it looks like the graphics are just made from these 4 different emojis if you want to call that which is it's pretty incredible.

The idea that you know the the things that we're looking forward to here in the next iterations of this potentially could be, as you mentioned, a true game loop, a way to have some sort of an inventory system. The sound generation is really cool to be able to have some sort of a soundtrack to this game as well. I think I've seen some some more things from Mike FC coming out on Mastodon lately, if I'm not, if I'm not mistaken, around our packages that are doing some audio things. So maybe there's a potential chance that that could get integrated into this package as well but, you know, it's it's hard to do this justice in audio form. You really have to check out the blog post, see the visuals that are there, install the package from GitHub yourself, and try not to waste a couple hours, going down, you know, playing games in R. I I I dare you to try. And so this is this is really really cool stuff. I think it's a fun way to start off the highlights, this week, and I'm looking forward to what else is to come here. Yeah. The creativity

[00:10:09] Eric Nantz:

possibilities here are practically endless. And, of course, I as I'm watching this, I'm wondering, well, how would you be able to distribute this? Can you imagine a web assembly app that puts all this together? Good grief. I mean, I've seen, you know, maybe I shouldn't say too much here on audio, but I'll say it anyway. There are even on the Internet archives, some of these web assembly powered, you know, emulators of the classic arcade games that have long passed your IP windows, but it's all in your browser. It's like using native JavaScript to do it. Just imagine if, you know well, you know, we've been watching the WebAssembly space quite a bit.

I wouldn't put a pass either me or someone else maybe working with Matt or Mike to throw their hand at this and see what happens because just imagine how easily you could distribute something like this. Just mind blowing possibilities here. Absolutely. No. Sort of reminds me that that

[00:11:04] Mike Thomas:

the shiny contest is back. I probably should have saved that for my additional highlight, but I saw that, yesterday. I'm come across on my social media feeds, but it sort of reminds me of that the Appsilon app. I believe that was like the shark or underwater. That's right. The Shark Attack app. Yeah. That was awesome. That can be hosted on WebAssembly. Ivan from my team has recently, you know, published his first shiny live experimental app that's a card game. So, yeah, I think the the gaming Shiny Live, you know, crossover here is is gonna happen pretty soon.



[00:11:39] Eric Nantz:

It's not a matter of if, Mike. It's when.

[00:11:43] Mike Thomas:

Sounds like you found your next side project.

[00:11:55] Eric Nantz:

Speaking of trying things out, unless you've been living under a rock, you know that some of the biggest advancements in our world attack have been the use of generative AI and large language models to help with all sorts of things in our daily lives and especially in the world of software development. You've sure you've heard about efforts such as what Microsoft at piloted years ago with what's called copilot, which if you opt into that in your IDE such as visual studio code, also there are plugins to this with, say, our studio and whatnot. You'll get these, you know, auto completed like suggestions as you're typing out code. Maybe it's some boiler plate for a function call. Maybe it's helping flesh out some additional parameters or whatnot.

Well, there obviously this space is moving quite fast and our next highway today talks about some recent findings that have been explored by the Epiverse team on their explorations of what's called the copilot workspace initiative. So as I mentioned, this is coming from the epiverse blog. It's got a set of authors here. We got Joshua Lambert, James Azzam, Pratik Gupta, and Adam Kucharski. Hopefully, I said those right. They have teamed up here to talk about as they've been watching this space of how AI and LOM models are helping with development.

What would be the what would be the situation of trying to leverage this new copilot workspace, which is kinda taking what was mentioned earlier by me in this copilot initiative to the next level and not just auto complete code as you're typing, but actually take a set of requirements that are surface Savia GitHub issue and just see how to actually produce the code or produce a solution to the problem at hand. So they decide, let's do an experiment here. They teamed up with a group of professors and their organization to do 3 different experiments with this copilot workspace to see how it works in the real world.

The first experiment, and they tried to go, I guess, from easy to difficult as we go through this is say, they have a function in their r package that has been internal up to this point, but maybe they got user feedback and says, hey. You know what? That's a useful function. Maybe we should export that. So using their epi now 2 package, they looked at an existing issue that was, again, was already filed about, hey, this function called epi now 2_cme c m d stand underscore model function should be exported.

So they let the copilot workspace turn loose on this and it did get the job done, albeit in ways that probably aren't as intuitive to an R user. So let me explain a bit by bit here. The Copilot workspace did determine that, oh, you know what? We need to replace that keyword of internal in the r oxygen documentation of that function and replace that with export and also having to update the namespace. Now there were some little shenanigans here because apparently it does change the formatting one of their other function arguments.

I guess doing some text tidying up, which again, yeah, that's fine and all. But in the end, it did technically get the job done. But this Copilot workspace is not intelligent enough to understand how documentation is updated with the modern r tooling for package development, such as using things like dev tools document or, you know, more natively of our oxygen to the re reupdate the name space dynamically. It did it itself manually like you would maybe in the early days of package development. Package development.

So context itself, not quite there yet, but you can't say it didn't get the job done with this. It just did it in a much more manual fashion. So, you know, so far, you know, pretty promising. Let's take the difficulty up a notch, Mike, because now in the next experiment, they wanna add a new model to the package, albeit not too difficult from a complexity perspective, but this is upping the ante into what the Copilot workspace can do. How did this one fare? What do you think? Yeah. Not quite as well, Eric. They wanted to add what's called a simple epidemic model,

[00:16:21] Mike Thomas:

to the r package, that contains, you know, a bunch of different models as well. So they, I believe, created a new issue if I'm mistaken, to add a basic SIR model, it's called, with a couple sentences on exactly what they were looking for. What happened was get Copilot created a script, a an R script with a naming convention that followed the naming convention of the other modeling scripts that they had. This one was, you know, within the R directory which was good and, the name of the script was model_sir which again, you know, follow the the naming conventions that they've used in this epidemicsr package that they have right now.

And I think this follows a lot of sort of the the same things that I've seen, you know, over and over again with Copilot, with chatGPT, with some of this, it gets you close to the right answer and puts down, you know, some of the the the right things that you would want there, but not in the way that you would necessarily want that things organized, if you will. You know, so the code that was generated, you know, constructed that basic SIR model, used roxigeon 2 as as well, to document the code, but but a lot of aspects of the code, you know, didn't match what they asked for in that issue.

The code contained what they call some inadvisable coding practices in R, you know, what we like to call code smells, and the model it self, you know, followed the standard set of of differential equations that are solved using the the desolve which is one, that they had actually requested in their issue, but it didn't have any options to input, you know, things that are are really important to them like interventions, which is, you know, what they the the Copilot had suggested that it would actually include but but failed to do so.

The other downside is that they they use the require, dissolve they use the require function to import the dissolve action, function package, excuse me, in the body of the generated code. And as you know, Eric, when we are developing our packages, that's not something that you want to include in your function. Right? Yeah. The smell was stronger that one. Oh, my goodness. That one stinks to high heaven. And you know, we use a lot of utility functions like from the dev tools and they use this package if we want to leverage a new package within our our package in a right in a proper way we could do you know use this use underscore package dissolve Right? And that'll add that to our description file.

It might add it to the namespace where appropriate. And obviously, we could, request the use of that package in our roxygen, right, import or import from code decoration above that particular function. So that's a pretty bad one right there as well. So you know a a few different code smells here. I don't think that the function itself accomplished exactly what they were looking for that model to do as well as, you know, some of the kind of ugly best practice things. So we're we're starting to head a little bit in the wrong direction here as we get into experiment 3.



[00:19:44] Eric Nantz:

Yeah. And so talk about upping the ante a little bit, but, yeah, it's something that is very relatable to every one of us that are developing, you know, intricate code bases. They wanted to see if this Copilot works makes it actually do an intelligently driven code review of the package itself. As you know, as you get, you know, maybe new features put in bug fixes or whatnot, you get that, point of eventual re release, maybe an update you wanna release on crayon or whatnot. You wanna have that code review to make sure that all the things are looking good. You're being efficient with memory usage, efficient with coding best practices.

I always saw a little glimpse that, yeah, it may not be the best at coding practices. So what they are expecting to see is hopefully in this issue of asking it to do the code review, they would have a documented set with, you know, links to particular snippets of code or maybe things could be optimized or maybe even just asking questions about the code or whatnot. Well, bad news is it didn't actually do any analysis of the code itself. It basically regurgitated some of the changes that were already described in the poll request and looking at change log, I e, from the news file and just kinda bundling all that together as a narrative, which in essence means that it does a great job of reading the news that is not so much looking at the actual code than the thing changes that could be made to make the code better.

So this is where, you know, humans aren't being replaced on this one by by a long shot. But what I do appreciate is a, they gave this, you know, 3, again, realistic use cases and not all difficult. I would say that the first one, the first experiment is definitely the easiest one, and it did get the job done. Just not in the way of you as an our developer wouldn't carry that out. So I think the takeaway here is that as we as I agree with them and their takeaways here, there's a long ways to go. There are avenues of success here, but I think you as the end user definitely need to be vigilant on making sure that whatever prompts or requirements you're feeding into these are being accurately, you know, addressed in the results you get. And if you're getting code back, yes, you definitely should not take that blindly, so to speak. You need to make sure does that fit your overall paradigm of a code base, your stylist, you know, your style guides, if you will, your best practices for your team.

Obviously, there's much more work to be done to make these, in my opinion, more dynamic so that as it looks at the code base for a package or maybe even a set of packages that it can really grok if you will. What are the key paradigms that are governing that code base instead of relying on a whole bunch of additional sources of who knows where it gets it with respect to this effort. I think being intelligent enough about what's actually being done in the project is is the way to go For research software engineering context, like what this is based on, yeah, maybe some help, but in the end, sometimes you you just can't get away from doing some of this manually right now. But this space is moving fast. And again, I really appreciate their their attention to detail to put it through the paces.

But in the end, I'm not I'm not moves asleep over the fact that the AI bots are gonna take over package development anytime soon. What do you think? Me neither, Eric. You know, I I'm trying to be, like, open minded

[00:23:28] Mike Thomas:

about it and I think as long as you have the expertise to be able to to sniff out the good from the bad, running your code, you know, through Copilot for any of these purposes, can still be useful. Right? I'm not sure if I want it to actually physically make these changes, to my code in any way, but maybe, you know, it's not such a bad thing for it to provide me with suggestions. Right? There might be something if I ask it to do a full blown code review, there might be a little thing that it finds that I missed or or somebody in our team missed, because, you know, programmatically pouring through the the code, having a computer do that, might be able to catch things easier than, you know, we can catch with the naked eye.

Even looking at our our code for a little while. So I think I'm open to suggestions, let me put it that way, from Copilot and from the chat GPTs of the world, but I'm still in a place right now where I am going to to often take those with a grain of salt and, you know, leverage sort of my expertise over in what's coming back from those types of models.

[00:24:34] Eric Nantz:

Yeah. One use case I'm seeing more and more often in many industries or many organizations is the process of maybe refactoring from, like, one type of code base to another, especially when you're shifting languages. A lot of companies are turning to LLMs and AI to help with that conversion. I've always had a little bit of spider senses tingling at this because with that new code, will it look like somebody with competent skills and whether it's r, Python, JavaScript, or whatnot, will it look like they wrote it, or is gonna look like a hodgepodge of tutorials that have found online and trying to do all, like you said, these mix mash of different coding styles and practices. So I think we're still a ways there but I know that is a hot topic in many circles to see how fast it can get you to that next step And there may be cases where it gets you really close and you just have to maybe enough or 10% of your time to revise it. There may be other cases where it just gives you absolute garbage, you might as well throw it away.

So in any event, I think keeping an open mind and being realistic, I think are very important in these still early stages of this whole tech sector or this whole industry. But I think I think good things are coming. Just gotta use it responsibly, of course.

[00:25:54] Mike Thomas:

We'll see.

[00:26:04] Eric Nantz:

And then rounding out our highlights today, speaking of refactoring things, you may have a situation where there are some things you could do manually, but then as you do it over and over again, especially as you're dealing with a large code base, maybe not even the one you wrote yourself, you wonder there's got to be something that can help me get there even faster. So our last highlight today comes from a frequent contributor, Myel Salman, from her recent blog about her recent journey to look at all the names of functions under defined in a script or set of scripts and a new approach that is new to her and frankly new to me as well.

So this is fresh off the recent USAR conference where I'm seeing some of the videos of that come online, and it sound like it was a terrific event. Of course, I have a little FOMO every time I see that because I still haven't been to a user yet, but someday that will be checked off the bucket list. It just wasn't able to this year. But one of the talks that my old discovered that she didn't see live, but she heard about after the fact is, Davis Vaughn, from the PASA team talked about a framework called tree sitter.

What tree sitter actually is is a mechanism to parse code for somewhat like what we've heard in the past with things like abstract syntax trees and trying to parse either variable names, function names, or whatnot. Apparently, tree sitter is a c library and I guess maybe other libraries as well to help with that mechanism of parsing, you know, code intelligently. So her use case was a following is that she wanted to help out, finding wanted help finding functions in pat in, in the package eye graph. And if you know what an eye graph is, it's kind of like the standard bearer of doing network visual network, you know, data representation, which you can turn into network diagrams, you know, very much like tree setups are very intricate networks.

I graph has a long history. We've covered it before on the show and it is a massive code base. So she wanted to have a use case of okay what functions and I graph now there is a certain operator that I graph has. It's like a bracket or square operator. She wanted to see how many functions were only using within this special operator. So she had to literally go within this operator function to find all the utility functions throughout. The example she has in the post only has a couple of them just for kicks, but imagine there's, like, a whole bunch of them.

And she wanted to be able to she had to refactor these, but she wanted to or at least get to the point of refactoring them, And she didn't want to manually copy paste all these function names or discover them. So she has used things like XML parse data in the past. We've covered on a previous episode of our expirations without the parse, a complicated function. She wanted to see what it'll look like with tree sitter to do that similar operation. So she talks about her for use case here. She loads the package, and then she loads the parser for the r language and that reads in that function text or that script that has the function text.

And then she wanted to figure out, okay, where is the root of that tree that is now gonna govern all the child nodes of that function. So this gets a little in the weeds here, but there is a function in tree sort of called query that you can feed in kind of a snippet of code that you wanted to look for. And there is like a little kind of YAML like structure to it. You define what's on the left side, what's on the right side, and you give it kind of like the plain text like label of that, wherever it's identifier, a definition, and then you kinda give it looks like a regex kind of comparison about what you wanna match it to. This is all news to me. I've never seen the tree sitter syntax before.

But sure enough, when you run that and then you get a nested list back in r, that gives you the text of what it found versus the the actual text itself, the expression that's kinda more translated to the tree sitter notation of it. And then you've got you gotta step it away there. She didn't find the children functions yet, but guess what? There is more It's almost like you have to do a nested query to get to that point. You then do the similar kind of syntax. You're looking at the left side and the right side definitions, finding another query of the previous query and then she was able to find the names of these different functions, again, internal functions that are in this Square operator. And that's a huge list that after that point.

But then through some gripping to that, she was able to get all of those hidden function names in that square operator. There's about 5 or 6 of these in in intact, but this this all programmatically shouldn't have to look at all this herself. So you can imagine if you scale this up to, like, a 100 times this size with a huge r package, this might be a great way to kind of do this kind of, like, unique query language with TreeSetter to get to what you need. Admittedly, I have never even ventured near these rabbit holes before, but I could see for our legacy Go base. And, again, I stress maybe one that you yourself didn't write as you're getting familiar of it. Because I've looked at iGRAPH before, and, yeah, there there's a lot going on under the hood on that one. So I wouldn't know where to look with trying to pick out these internal functions.

Looks like tree sitter is something to to take a look at. And fun fact about tree sitter that I discovered after briefly looking at Davis's talk, this is being incorporated directly into posit's new IDE positron as part of its base for when you look at, say, the outline view of a script and getting those function definitions, which we've seen in our studio as well. But now I believe tree sitter is doing the heavy lifting to grab all those contexts around those functions. So it looks like tree sitter is here to stay for sure, but, what an interesting use case here to have to take a note of if I have to deal with legacy code based discovery in the future.



[00:32:34] Mike Thomas:

Yeah. Eric, I feel like Mal always brings us some pretty interesting use cases on the highlights. This is actually quite similar to a blog post that she authored a little while back where she did something very similar but, in XML instead using an XML parsed data R package, I believe. So this is if you wanna take a look at that blog post and this one side by side, I think it'll give you 2 different approaches to essentially doing the same thing. And it looks like there's about, you know, 5 or 6 different functions from the tree sitter package, that Maelle is is really leveraging here. And she does note that, you know, she went through a lot of different emotions as a beginner to this tree sitter package and not all of them were positive. One of them that I must have imagined taking a lot of tinkering with is where she does define that that sort of string that she's looking to to parse which involves, as you said, this this regular expression type of notation that we're looking at to try to, call out sort of the the particular function definitions that we're interested in here and being able to return that as a list that is is parsable.

And so I think sort of the the chief different functions that she's employing from tree sitter after she does that are the the query, the query captures, and then, this node text which I I think sort of turn this list that gets returned into something that's a little more easily parsable, if you will, within r and obviously at some point in here we're leveraging per because, we are out outputting a list, the the map character function, to be able to break things down into just this final, you know, simple handful of of sip 6 different names of functions, that she's interested in. There is one footnote in this blog post. It says, no, I have not installed positron yet. So my l is doing this all from the RStudio IDE at this point. Yeah. But it's interesting to hear that TreeSitter is obviously gaining some some ground. I'm not sure how new in and of itself the the TreeSitter C utility is but it's something that we're we're new to seeing I think in the R Eco system as there's recently been some other blog posts as as you mentioned that you know the folks at Positron are using it as well. Well. So it's very interesting to me. Obviously, you know, I I could see if we have some large code bases where you have to do some sort of, you know, profiling of that that code base or extracting of of particular portions of that code in a way that just really isn't, you know, useful to do manually, the old copy and paste method, and it makes more sense to do that programmatically.



[00:35:23] Eric Nantz:

Grateful to the fact that my EL has now given us 2 different ways to go about doing that. Yeah. And I'm looking at this tree sitter repo, and I will put this in the show notes if you haven't heard of this before. Like, we didn't hear about this before. It looks like a pretty mature project of a lot of modern bindings, nonetheless, and they're trying to be dependency free to start. But then if you wanna hook it in the rust, guess what? You can. Wanna hook it in the WASM or web assembly? Yes. You can. Even has its own COI to boot. So there is a lot going on under the hood with this, and the best part is you can use this wherever you're comfortable with. It doesn't have to be in positron as you said. This could be in any r session. You can put it as on Versus code or whatnot. So, again, what I appreciate is, yeah, being able to learn about these discoveries, but then to be able to use this in my preferred environment. So this will be great. Again, I think the trial with a low friction way to get start and there's an r package now that has been author. We'll put a link to the r package itself in the show notes that ties ties all this together. But, yeah, you never I guess you'll never look at your code the same way again when you're looking in the forest, getting the forest from the trees or whatever it says.



[00:36:32] Mike Thomas:

Exactly. Exactly. And that's a that's a good, tree related pun

[00:36:35] Eric Nantz:

here. Yeah. I know. I try I try I try. But, you well, you hopefully, you don't get lost then as the rest of this because there's a lot a lot of great content here. But as always, we put these in nice sections for you to digest, whether it's new insights, you know, uses in the real world, package updates, tutorials, or whatnot. There's always something for everybody here. So it'll take a couple minutes for our additional finds before we close out here. And for me, a reality in my industry, and I'm sure many others as well as they're building enterprise apps, maybe those that don't necessarily go outside the firewall, so to speak. But yet you have leadership, you know, stakeholders that are asking, hey. You built this great tool.

What's been the user adoption? You know, what where are areas that people are spending their time in the most? Not always questions I really want to have to answer, but if I do have to answer, I wanna make it as easy as possible for me to get those metrics. And that's where Epsilon has pushed an update, a major update to their shiny dot telemetry package version 0 dot 3 comes in with a lot of great updates here, including some very nice quality of life improvements, like actually checking whether it's an authenticated based app to get the user ID, if you will, of that session, which can be great as you're looking at different usage patterns or whatnot and to be more transparent about what you actually want to track. Because but if you don't wanna track all the inputs in your app, you wanna be able to wait to exclude them but not have to exclude them name by name, you can now do a regex if you have all your inputs name of a certain prefix in front or whatnot that you don't wanna include in, you can, you can throw a reg X that way too.

Other enhancements include actually tracking the errors that can occur in your app. And boy, that can be very helpful for diagnostics. Not that I would ever have an app that crashes. Wink wink. But also if you wanna take advantage of an unstructured database to put these metrics in or these uses patterns in, They now support from MongoDB, which, of course, is very popular for unstructured nested type data data representation. So lots more under the hood with that, but they also have updated their package documentation with 3 different vignettes all about the different use cases that you can have for shiny telemetry. So really kudos to them. Looks like a great package, and, yep, this is some idea with every single day. I roll a new app out, so I'll be keeping a close eye on this.



[00:39:07] Mike Thomas:

Me as well, Eric. That sounds really, really exciting. It's something that a lot of our clients are always asking for, right? You you finally get over the hump and build your your beautiful app and deploy it out to the world and then almost immediately, we get the question, oh, you know, can we get some user metrics on this app as well? So I'm excited to check out those new enhancements. An additional find that that I saw in the highlights this week are from, doctor Sophie Lee, who's the founder and director of s cubed, a statistician and educator. Now has a 2 day introduction to R with the Tidyverse course.

It looks fantastic. I'm seeing some, really, really nice, visuals here on this website which I believe are probably borrowed from, who who's the the person in the art ecosystem? I think she may work for observable now. Is it Alison Horst? Alison Horst. Yeah. Alison Horst that used to make the the really nice R, you know, types of of graphics and and imagery. So I see one of those here and that just tells me all I need to know that this is gonna be a fantastic, 2 day training here from November, or excuse me, September 24th to September 26th. So if that's something that that you're interested in or somebody in your team might be interested in, it's gonna cover everything from, R to R studio, data management, visualization, and and ggplot and EDA, and then some best practices for doing re reproducible research. So, you know, I think we cover a lot of in the weeds things sometimes on the highlights and I wanna make sure that we don't forget about those particularly new or or trying to, learn about ours. So this might be a good opportunity to to try to make that jump if that sounds like you.



[00:40:55] Eric Nantz:

Yeah. Fantastic resource here. I'm I'm looking at it. It's a definitely a portal based site and the the styling is fantastic, easy to navigate. So, yeah, yeah, kudos to her and the team. This looks like a fantastic thing to highlight here, and thanks for calling that out. And, boy, we love to call everything out, but, yeah, we're there's always so much time in the day, folks. But, again, that's why we put this link in the show notes. All the highlights you've seen, you've heard us talk about today and those additional resources are all in the show notes and also at arugia.org.

It's the easiest place to bookmark to find all these all these, terrific content and the back catalog of issues as well. And so if you wanna help the project, the best way to help is to share those new resources you found wherever you created them or someone in the community has created them. And it's just a poll request away, all marked down all the time. There's a link in the upper right corner. That fancy little octa octa con cat, whatever you call it is in the upper right corner. Just click that, take them directly to get help pull requests. You don't need an AI bot to fill this out. It is all marked down all the time. Very easy. They get started quickly. We have an issue template to get you up and running quite quickly as well.

And if you wanna get a hold of us, we have a few ways to do that. We have a contact page directly in this episode show notes. We are on all the major podcast providers, so you should be able to find us wherever your favorite preferred listening, preferences. And, also, you can get a hold of us on these social medias as well. I am at our podcast at podcast index dot social on the Mastodon servers. I'm on the weapon x thingy sometimes with at the r cast. I'm mostly on LinkedIn as well. Search by name. You will find me there. And, Mike, where can the listeners get a hold of you? Yep. You can find me on mastodon@mike_thomas@phostodon.org.



[00:42:42] Mike Thomas:

Or you can find me on LinkedIn if you search Catchbrook Analytics, ketchbrook, or you can find me in Seattle in a couple weeks. Shoot me a message if you're gonna be there and and would love to to chat all things are.

[00:42:56] Eric Nantz:

Likewise. Yeah. Yeah. Like I said, the time is coming close. So, yeah, not packing just yet, but that's not too far away, to be honest. And I always bring some tech gadgets too. Who knows? I might, maybe, I might bring a couple mics with me. I'll I'm just saying. I'm just saying. We'll find out. But, nonetheless, we're gonna close-up shop here for this episode of our wicked hot lights. We thank you so much again for listening to our humble little banter here, and we will see you back here for another episode of our weekly highlights next week.

"
"47","issue_2024_w_28_highlights",2024-07-18,33M 10S,"The world of web-assembly in R continues to move fast with key updates to the webrcli & spidyr packages, and what has us excited about the mapgl package for producing amazing spatial visualizations. Episode Links This week's curator: Batool Almarzouq - @batool664 (X/Twitter) webrcli & spidyr: What’s new Create a Compare slider widget Entire issue…","[00:00:03] Eric Nantz:

Hello, friends. Did you miss us? Yes. We are finally back with another episode of the Our Weekly Highlights podcast. This is the show where we talk about the latest and greatest new packages, blog posts, other terrific resources that are featured in the highlights section of this week's Our Weekly Issue. My name is Eric Nance. And yes, we had a little bit of a break, both self imposed and some unexpected reasons for the break. But nonetheless, we were coming back hopefully refreshed. And, for me personally, I was able to take the family on a little road trip to Niagara Falls early in July. It was a spectacular time. And if you haven't been in that area, if you have the means to do it, I highly recommend that as a future vacation. It was a, I drove there, so that was a long drive, but made it through in one piece, which is sometimes a miracle in itself when you have 2 kids that are angry at, you know, on highways going through farmland as the Midwest typically is. But nonetheless, back here in the humble boat here, and I'm very pleased as always to be joined from my awesome co host, Mike, who's had his own adventures both good and probably not so good lately. But, Mike, how are you doing? I'm doing well, Eric. I am as equally impressed in your r and shiny skills as I am in your skills taking your family halfway cross country,

[00:01:24] Mike Thomas:

with 2 young boys, in a car for for that long. So kudos to you in a successful vacation. I got some vacation time with my family, in Vermont as well over the 4th July here, our Independence Day. And, yeah, happy that we were able to have the break, but also very happy to, be back.

[00:01:44] Eric Nantz:

Absolutely. Yeah. I was I was thinking, yeah, just doesn't seem right. No. All is right with the world. It's a great recording session with you. And we've got a couple awesome highlights to talk about today. And this issue has been curated by Batool Almarsakh, another one of our longtime contributors to the rweekly project. And as always, she had tremendous help from our fellow rweekly team members and contributors like you all around the world for poll requests and other suggestions. So let's dive right into it. And our first highlight today is a callback from something that we put the spotlight on in episode a 156.

You can check out the back catalog for that. But, you know me, and if you heard the podcast recently, I've been espousing my, affection and really big enthusiasm for web assembly and web are in the world of our and web development. And there are a lot of different ways to spin this now. We've been historically talking about the amazing work that George Stagg at at Posit's been doing with the WebR package, the web assembly framework tie ins with WebR is just immensely powerful. But there are some other perspectives that you can take and really massage these tools to do some interesting use cases.

And way back in episode 156 earlier this year, we had talked about Colin phase adventures with building his own rappers on top of WebR and combining with no JS to kind of fill a perspective of how to efficiently expose functions and are from say in our package, what looks to be native functions and a no JS runtime. And so we got some follow-up on that in our first highlight today because, yes, Colin Faye is back at blogging his recent updates to his web r COI and spider utilities, encoded up in in JavaScript to help with this perspective of exposing these great R functions that you can compile with web R into a no JS runtime.

So what's new? Well, as usual, as you get things bootstrap for the first time and you start iterating on it, you see some ways to make things more efficient and also helping the new user get onboarded more efficiently. Their first of which is that the web are COI package combined with spider will help bootstrap a template file for you to start that kind of boilerplate integration of the R functions from a package into the node JS runtime. And he's been able to simplify the template a bit so that it's about now maybe what 10, 11 lines of code.

And he's hopeful that even if you're not an expert at JavaScript just yet, they'll be able to grok what's happening here. And I dare say he's got, he's mission accomplished on making that a lot more easy to grasp. I mean, he decomposes the explanations of the template in the blog post where there is a bit of setup to initialize these spider projects that's gonna help with the hand off, help with that integration, and then helping to import the r functions that have been bootstrapped into the directory where this project is being bootstrapped from. And then you can see then for a simple hello world example, you just put a prefix of our funds dot hello underscore world if that's the name of your r function, And there you go. Now you just called that r function from JavaScript.

And what's interesting about that actual execution of it is that this actually doesn't need our at that point to do the execution. It's all already been pre compiled

[00:05:37] Mike Thomas:

in the JavaScript. That's just bonkers to me, but it's amazing. Yeah. No. That that's that was probably the most interesting part of this whole article to me is that there's no R session running while the Node. Js app is running. It it's the spider package imports an R function converted to a native JS. So I guess that must happen beforehand.

[00:05:57] Eric Nantz:

That's my understanding is in that bootstrapping process as you initialize a project. I believe it's compiling all that in in that initial pre step. So then the Node. Js side of it when you're just executing these functions, it's treated no differently than any other JavaScript function you would run-in that. That's just, again, amazing to me. Mind blown. 101. Yes. And Colin does it again. We we should be used to this by now from him after all the GOLM exploits, but, boy, he always teaches us something new.

And then other tidbits in the post here that I think are gonna be great from a reproducibility standpoint and efficiency is kinda taking a page of how we do dependency management is making sure that when you bootstrap a WebR COI project that it will store the packages that are being installed in a packages dot JSON file so that then if you say move this to another computing environment or whatnot, as long as you version control that lock file, that JSON file, not to dissimilar what you would do with our end than the typical r setup, It'll bootstrap these these packages back down if they're not installed already. So I think that's paving the way for more efficient dependency management in this new workflow.

So he's got a nice schematic towards the end of the post where he kind of shows the workflow in a nutshell, which I believe is gonna be kind of the source of some additional documentation that he plans to author as he wants to have more than an end of one of users for this framework. He claims he's the only one using it. I highly doubt that. I'm sure there are others that are putting it through the paces. But he's got some other future items that I'm gonna keep an eye on for this is that in terms of reproducibility, he wants to make sure that when you initialize these projects that you can actually specify the web r version itself during that bootstrapping process. I mean, that will be, again, huge as web r takes more fruition as companies or organizations start to use this in critical pipelines just like with r itself. You wanna be to say, okay, I built this on web R version, you know, 4 dot whatever. I want to make sure that I can standardize on that for the future.

And another tidbit I'll keep a close eye on, we've seen George Stagg mentioned he's been plugging in ways to install package, you know, WebR compliant our packages from our universe. That's been a new thing that he and your own have been working on. Colin wants to make sure that in this framework, in the WebR CLI, that you can install WebR compliant r packages from our universe as well. That will be massive for opening the door for additional functionality, down the road. So sounds like he's not stopping anytime soon with this. So I'm very curious where he goes. And again, the perspective here, I think, is definitely important, especially as many in in software development, you know, data science pipelines, you may have a team that is writing very intricate, very powerful code via our functions and hence our packages.

And maybe they have the luxury of a separate team that's doing all the web interface layouts in their preferred JavaScript framework. This is tailor made for that scenario. The data science team can configure the business logic and r and doing all the powerful model fitting and everything. And then they just give this compiled with WebR COI to the web dev team. They're they they're no worse for wear so to speak. They just call it like they would any JavaScript function. I think it's gonna open a lot of possibilities for, again, using WebR and WebAssembly

[00:09:50] Mike Thomas:

in even more context than initially we realized. Are you insinuating that the web dev team won't be writing in Shiny? What? No. You know, I I think one of the really interesting things that's going starting to go on right now, and obviously Colin is one of the folks who's spearheading that effort, is we have pretty good frameworks for building, you know, what, Eric, you and I call, like, production grade Shiny applications, right, that have a have sort of a dedicated server, component to them. And now that we're moving on to to WebR and Shiny Live and things like that, that dedicated server kind of goes away. But, you know, up until till now, and I I don't think we're quite there yet, a lot of the, you know, things that you really need to go from, you know, just a a toy POC app to production, like, you know, environment variables and authentication and and things like that haven't really existed in in this new WebR and Shiny Live world, but it seems like we're slowly but surely starting to get there and maybe there is a day coming.

I don't know how how near or far in the future where essentially everything that we can do in in WebR, is equivalent to what we could do, you know, with the old, quote, unquote framework for for building Shiny apps with a dedicated server. And and as Colin notes, I believe it's in his WebR CLI utility that there is a share n, function that allows the user to copy 1, several, or all environment variables from node to the WebR instance, I think, which is is really interesting. It's really important. I don't really build any Shiny apps these days that don't, leverage environment variables for, you know, secure authentication to a database or or something like that.

So that's a huge, you know, hurdle that I sometimes need to need to get over when we're looking at new frameworks like this. And it looks like, you know, Colin's already thought about some of this stuff, to be able to help us out when we we do get to that point. And I have not dove too deep yet, admittedly, into WebR and and Shiny Live. But, yeah, besides seeing what others in the community, like yourself, Eric, are doing, and building, but I am pretty confident that it's it's not gonna be too far in my future where I'm I'm not gonna necessarily have a choice and and going to have to start, learning about that because I think that there are quite quite a lot of benefits here depending on your use case, to to leverage that that WebRamp framework. And it's just getting better and better every every month, every day, every week, it seems like.



[00:12:29] Eric Nantz:

Yeah. I always whenever I see in my notifications, I'm massing out a new post from George. You know, I'm always like, okay. Read it now because it's gonna be something that I think I'll be able to benefit from very quickly. Yes. I this is a new world to us as I've been, you know, thinking about framing my talk for the upcoming positconf, and I talk about our adventures of WebR and WebAssembly and your consortium pilot submissions. I've always been trying to think of an intuitive way to describe this paradigm shift. Maybe it's not so much a paradigm shift, but it is a traditional server side hosting process to what is now being exposed.

In essence, the user's web browser is the new engine. It's the new server. That's where everything's being conducted. And what are the benefits and trade offs of that? A lot of benefits that we're seeing. I mean, you've heard me mention on previous episodes or we're seeing in quarto, what we're seeing and be able to build these educational type materials showcasing the the native execution of our code so the users cannot just see examples, but literally play with it in their browser without that need of a shiny like server on the back end. That's still one of those use cases that I think is just beginning to really take fruition.

And I think it's just only a matter of time as those principles get built into what we're doing in data science pipelines that we've historically done with shiny and still, of course, who's the number one fan of shine wearing their t shirt today for goodness sakes. Of course, I love my shiny and to be able to combine it with WebR. I mean, that's still this intersection that I'm very passionate about and certainly to be abreast of this technology, abreast of what we can do in this space. Things of what Colin's doing here with WebR CLI and Spider along with the adventures that Bob Rudis has had on his various blog posts and what he's been doing with WebR. He's been just as excited about this as anyone else. So it's great to be well rounded into, like, the different ways you can spin this technology.

And, again, the end product, you're gonna be able to build these very sophisticated web our web assembly powered apps in many different frameworks. That's the key here. Right? We're opening so many doors for every teams to get into this. So, yeah, I'm it sometimes be it sometimes makes me shake my head just how fast this is moving. But once that initial hurdle got overcame last year, with bringing WebR to the r, you know, web assembly to the r process itself, the floodgates have opened, and I'm here for it. Me too.

And rounding out our highlights today, we're gonna dive into the world of spatial visualizations, which we touched on quite a few times with some of the amazing efforts in the art community to expose some very powerful frameworks in the mapping and geospatial space. And we got another highlight that is actually more of a function that's exposed into an overall effort that I think is gonna be very important for those that are looking to take advantage of innovations and web technology in the spatial visualization space into there are processes.

Certainly, this has a nice fit for shiny too that we may touch on as well. And we're talking about the mapgl package, which has been authored by Kyle Walker who has been featured on previous Arrukia highlights in the past. He runs, I believe, his own consulting company. We're gonna do a lot of spatial visualization analysis and visualization, and the map GL package. This looks really nifty to me because it is, Mike, many in the HTML widget ecosystem trying to get the our user and intuitive interface to an extremely powerful web based visualization platform.

And there are 2 of them to be exact that this package exposes. It's the Mapbox GL and the Map Libre GL mapping services. They age are slightly different, but yet they're very important use cases and map GL is a framework to bring that to the our user very efficiently. And, Mike, I don't know if you had a chance to look at their getting started vignette that Kyle has on the website. Holy moly. Some of the visuals he can produce in this are extremely powerful here. Yeah. It's absolutely incredible, and I've seen Kyle quite active

[00:17:09] Mike Thomas:

sharing the progress on this package, this mapgl package on, I think, Mastodon primarily, maybe LinkedIn as well is is where I've seen it, and the amount of new features and development that's going on in the packages is incredible. You know, it seems like they're they're rolling out functionality constantly. And one thing I think that strikes me, you know, it's a big takeaway from a lot of what Kyle has been sharing on social media is really the the performance, of this package and what you're getting from I think the underlying APIs, the Mapbox, GL JavaScript and Map Vibre GL JavaScript APIs.

If I'm not mistaken, those 2 API services are are paid. Maybe you can get a free free version, but you will need an an access token, I believe.

[00:18:00] Eric Nantz:

Yep. Yep. You need an access token depending which features you wanna utilize. I believe you have to pay for at least the first one, the map library one. I'm not so sure. I haven't played with it, you know, carefully, but I know to get started. And once you get the access token, you're good to go for a course set of features. I

[00:18:17] Mike Thomas:

think, you know, one of the things that folks doing, you know, pretty heavy geospatial data science, run into sometimes is performance because there's a lot going on on screen when it comes to geospatial data. The data can be large in and of itself and sort of heavy when your your data is representing polygons and things like that, not just individual points. And then obviously the the visual component as well. Right? Your maps are are complex visualizations and traditionally maybe not the the quickest visuals to render. But what we're seeing out of the the mapgl, package right now that Kyle his team have put together is some incredible enhancements, and it looks like those underlying APIs, though those services have have done a lot of work to try to make, these geospatial visuals as performant as possible. And one of the ones, you know, that we're calling out today in the rweekly highlights is this compare function, from mapgl.

It allows you to create a comparison view between 2 Mapbox GL or map libre GL Maps and what it'll do is it'll provide this sort of horizontal bar in the middle of the visual itself that you can drag to one side or the other, to be able to expose more of either the left map or the right map, and compare those 2 sort of side by side. This is something that, we have actually done, you know, quite a bit when we have sort of an overlay often of of the same, geographic area, but we're trying to maybe represent, you know, 2 different, you know, data sets on top of that same geographic area. So maybe 2 separate surveys in the same area, something like that, and you're comparing, one to the other. So you have those things side by side and you can go go left and right, 2 different simulations, you know, of, an ecosystem or something like that. That's things that we have done in the past leveraging that compare slider widget in the leaflet ecosystem and this seems to be pretty comparable to a function in leaflet called add map pane.

I don't know if there's a more recent one but that's the last time that I was working, with leaflet on exactly that that type of problem. We were able to create like a left map and a right map and the function that we used was add map panes. This looks, this compare function from mapgl looks quite equivalent to that. Again, you know, these maps look beautiful. The ability to drag left and right is something that, users of our apps that have done that when we've implemented it in leaflet have absolutely loved.

I I think it's probably one of the the the most favorite pieces of functionality that our end users let us know that they really like on our geospatial data science project. So it's pretty cool to see that also exist here in, the mapgl GL package. And, you know, from everything that I'm seeing, if you did wanted to migrate from Leaflet to, you know, this map GL package, it looks fairly straightforward. A lot of the syntax, a lot of the, arguments to some of these functions look quite similar. And obviously, the visuals at the end of the day aren't necessarily too far apart. So you may be able to to migrate fairly quickly and and pick up some of those performance gains that it looks like MapGL is able to offer or maybe there are just some aspects of MapGL that sort of make more sense, to you, to use, as opposed to, you know, the traditional leaflet or other geospatial packages. So if you haven't had a chance to check it out and take a look at everything that Kyle and his team have put together in mapgl, I highly recommend it because it is looking like quite a powerful R package that I I believe is gonna rise to the to the surface here pretty quickly within the r ecosystem.



[00:22:08] Eric Nantz:

Yeah. And then we'll put a link in the show notes to the the the vignette section of this package. Kyle's done a spectacular job of documentation here as somebody well, I was telling Mike in the pre show. I'm still pretty new to the spatial visualization space. This is definitely something if I was getting in this field, I would take a close look at because I always have a bias towards towards those packages that are putting attention to detail in that user experience of getting onboarded quickly. So on top of the getting started vignette, he's got a great section on how to efficiently use the wear layering system, which again, be very familiar to anybody coming from these special packages, the fundamentals of how to design your maps with mapgl.

And, yes, he does mention one of his original motivations was to plug this into shiny in the first place, and he wanted to put all this into a concise package that then could be sourced in the shiny quite easily. And he's got a whole vignette dedicated to that about the inputs that are being exposed. So, of course, Mike Mike and I know you can do almost anything you want with shiny once you get access to these inputs. So he's got a couple great, great little, teaser apps in in the vignette that you can get started with. So I think, yeah, this is something that we'll be we'll be paying close attention to in this space. It's all open source. So if you feel so inclined to get involved yourself, there's there's 7 issues out there that I'm sure Kyle would, appreciate some help on. So definitely check it out. Absolutely. So, again, links in the show notes and, yeah, I'm really impressed with what Kyle has accomplished here. And Kyle is just one of many in the r community that we're impressed with, and we're very fortunate to r weekly. Every single week features these great innovations from everybody in the community because this is a community project and this is where we'll take a couple of minutes to tow out some additional fines we've seen either in the issue or elsewhere. And from this particular issue, I do wanna I I had a couple ideas, but again, I'm I've been on a a kick recently with some of the great packaging that Bruno Rodriguez has been doing with the Knicks ecosystem. I'm not talking about Knicks here. I'm talking about a video that he's put into the issue.

Why as an our user, you might want to invest into using GitHub actions. So it's a great 20 minute video and it's, great if you're new to this. Because I think a lot of people may have heard about GitHub actions, but they just don't know where to start. They don't know what's in it for them. Bruno does a terrific job of summarizing it. I will mention, now this may be not so much applies to his video, more of my random exploits. The bugging GitHub actions can sometimes feel like you're in the wilderness without any line of sight, and I had that a couple times in the last couple of weeks where I was, troubleshooting, get a failure, don't know why, rinse and repeat.

The YAML is like the engine to the GitHub actions that there's a YAML file where you put in all the steps of your workflow and any associated parameters. So hopefully, the teams behind these different steps have good documentation and unfortunate that most of the workflows I was using were authored by EverPosit or other members of the community where it was pretty pretty well easily understood how to grok the options and stuff. But sometimes when you try to do things a little different, be prepared for a little debugging time. But once you get it working, my goodness, the peace of mind to say, okay, guess what? That's gonna run like once a week, and that's gonna run on every PR.

I'm doing this with shiny testing recently. I finally got it working. I was an achievement unlock with shiny test 2 and GitHub actions, but I finally cracked that nugget. That's just one way you could use GitHub actions. Well, like I said, lots of different ways you can use it from, like, you know, automated processes or scraping data from an online source. I do this with that fancy, podcast index dashboard that was a runner-up in the table contest recently. So I've been putting that, putting that through the paces quite a bit, but it's been a fun learning journey for me. So Bruno does a terrific job of highlighting that in his video. And I'm not sure if we touched on it last time we spoke because it's been a little while, but congratulations on your runner-up and in the table contest. Your your app was awesome.



[00:26:20] Mike Thomas:

They were all incredible. The winner of the Spotify app was absolutely incredible too. So great job by everybody who participated in that. It's really cool to see the community sort of come together for things like that, which is awesome. One of the blogs that I wanted to shout out is is one from Absalon this week, and it is on the power of transitioning to a verse approach in your r package development. It's, you know, it was a really interesting blog post about, you know, whether you should take the packages that you may have developed internally and create a sort of tidyverse or pharmaverse around them that would allow you to install, multiple of those packages at one time. And they talk about sort of the pros and the cons of that. Right? Obviously, if there are sort of breaking changes, in one package, then you might, you know, sort of be be hit by that a little harder if you are sort of leveraging this verse approach in a lot of your applications. But, they they call out companies like Apple that choose to introduce on purpose breaking features with every other OS iteration, you know, and they sort of effectively eliminate their need to support the legacy software that they create.

So it it sort of depend That hurts, but it's so true. It sort of depends how much you wanna support your your legacy software and your your legacy products a little bit. How, sort of what the benefits would be for your end users if they were able to, you know, install dot packages from a a verse as opposed to specifying each one individually every time. I have mixed feelings about this, but I can definitely see some particular use cases where it would make sense and and some particular use cases where it wouldn't make sense. So it was a really interesting read. It's, sort of a topic that I haven't seen discussed very often, so kudos to the Appsilon team.

And in particular, Fabian He for putting that blog post together. I will do sort of a self indulgent shout out as well. We had a R package that got a very significant update after 3 years, I think was the last release. It's called migrate for building state transition matrices. So if that's something that you you do, if you work in an insurance or credit risk or something like that, Maybe of interest to you, some pretty exciting enhancements there. Oh, congratulations,

[00:28:44] Eric Nantz:

Mike. That is awesome. 3 years in the making, That that's a lot of updates there. Yes. Yes. A lot of updates, a lot of

[00:28:52] Mike Thomas:

user enhancement requests that people had been very patiently very patiently waiting on that hopefully, we've gotten across the finish line now.

[00:29:00] Eric Nantz:

Oh, that's awesome. And I, yeah, this this resonates with me back in my dissertation research. I was looking at state transitions of these mark off processes. So, yeah, I can definitely see a ton of value into this. So we'll make sure to put a link to your updated package in the show notes because it is, you know, great job with the vignette, my friend. You are you're putting the attention to detail

[00:29:24] Mike Thomas:

The transition matrices for Markov processes, that's what it's all about. Sorry. We didn't sorry. We didn't make it sooner for you, Eric.

[00:29:31] Eric Nantz:

You know, you were just about 20 some years too too late, but that's okay. That's okay. I'm not I'm old enough. I don't need to say anymore. But, nonetheless, I'm gonna definitely check that out if I delve into that world again. But, also, we invite all of you to check out the rest of the our weekly issue. And, yeah, we were off for a couple weeks. So don't forget to check that back catalog too. There are a ton of great resources ever shared recently. Really some terrific highlights from the past couple weeks. So everything's available at ourweekly.org.

You'll You'll find a link to this issue right at the front page as well as all the other issues that we that we curate throughout the years, and it's been many years, and the project keeps on going, but it keeps going because of your support in the community. So, again, if you've authored a great new package or updated as another package, if you've done a great resource online that you wanna share, getting it on the issue is just a poll request away. All linked at the upper right corner of the r wiki dotorg homepage. It's all marked down all the time.

Markdown is where I live when I can. I do admit I've had a couple, instances this past week or 2 where I've had to go back to PowerPoint and it just doesn't feel right. I love making my stuff in markdown. It's just so much better. What can I say? I thought you're gonna say Microsoft Word. Well, that too, but not not as in-depth as the slide making nightmare that I've had to put myself through. But, I I went down a rabbit hole. Who's ready for story time with our

[00:31:00] Mike Thomas:

podcast? Bebe. I'm

[00:31:06] Eric Nantz:

in a story time here. I was trying to convert a PowerPoint to have, like, these placeholder boxes for putting, like, text of, like, you know, we call these 1 pager summaries or whatnot. It was I thought, hey. You know what that looks like? That looks like a portal dashboard with the cards, you know, just kinda neatly arranged. And I got close, my friend. I got really close, but they want PDF exports of it, and I couldn't quite print it effectively to keep the formatting. But had they stuck a web based formats, it would have been perfect. But ran into the dead end on that one, so them's the brakes.

I've been there. I've been there. Luckily, you won't find those, pitfalls when you contribute our weekly. We're not gonna switch formats on you. It's all marked down. It's staying that way. That's the wave of the future if you dare say if I dare say so myself. So links to that at r o g dot r o g. And, of course, you can get a hold of us in different ways. We have a contact page directly linked in the episode show notes. You can send us a shout out or a question or a comment. We love hearing from you, especially how you're using these resources in your daily work or your daily exploits and are. And, also, you can get a hold of us on the social medias. I am on Mastodon where I'm at our podcast at podcast index.social as well as sporadically on the Twitter weapon x thing with at the r cast, and I'm on LinkedIn. Just search for my name, and you'll find me there. Mike, where can the listeners get a hold of you?



[00:32:34] Mike Thomas:

You can find me on Mastodon at mike_thomas@fossadon.org, or you can find me on LinkedIn if you search Catchbrook Analytics, ketchb

[00:32:44] Eric Nantz:

r o o k. You can see what I'm up to lately. Yep. That's where I got. One of your awesome, package updates. So, yes, stay tuned for more great stuff from from you on that platform. And, yep, with that, we're able to dust off the cobwebs, I think, pretty efficiently this week. So we're back in the saddle again, and great to have you joining us or listening to this episode. And we hope to see you again for our next episode of our weekly highlights next week."
"48","issue_2024_w_26_highlights",2024-06-26,45M 56S,"The latest updates to the rayverse bring new meaning to smoothing out the rough edges of your next 3-D visualization, the momentum of DuckDB continues with the MotherDuck data warehouse, and the role nanoparquet plays to bring the benefits of parquet to small data sets. Episode Links This week's curator: Eric Nantz: @rpodcast@podcastindex.social…","[00:00:03] Eric Nantz:

Hello, friends. We're back at episode 170 of the our weekly highlights podcast. Gotta love those even numbers as we bump up the episode count here. My name is Eric Nance, and I'm delighted you joined us from wherever you are around the world listening on your favorite podcast player. This is the weekly show where we talk about the awesome resources that have been highlighted in this week's current, our weekly issue. And I'm joined at the virtual hip as always by my awesome co host, Mike Thomas. Mike, how are you doing today?



[00:00:32] Mike Thomas:

I'm doing well, Eric. I'm taking a look at the highlights this week. I see that there's there's no shiny highlights, so this couldn't have been curated by by you.

[00:00:43] Eric Nantz:

Oh, the plot thickens. Oh, yes. It was. It was my turn this week. Good. That's so sure. There is some good shiny stuff in there that we may touch on a bit later. But, yeah, it was my my turn on the docket. So as usual, I'm doing these curations in many different environments, Mike. You're gonna learn this as your, little one gets older. You gotta be opportunistic when the time comes. And I was curating this either at piano lessons or swim practices or whatnot, but it ended up getting curated somehow. So we got some fun, great post to talk about today. Never as short as a great content to draw upon.

But as we always say in these, in these episodes, the curators are just one piece of the puzzle. We, of course, had tremendous help from the fellow curator team on our weekly as well as all the contributors like you around the world who are writing this great content and sending your poll requests for suggestions and improvements. So we're going to lead off. We're going to get pretty technical here in a very have in many different domains that honestly you would least expect. But we're gonna talk we're gonna go to the moon a little bit here, Mike. Not quite literally, but we're gonna see how R with the suite of packages authored by none other than Tyler Morgan Paul, who is the architect of what we call the raverse suite of packages for 3 d and 2 d visualizations in R.

His blog post as part of this highlight here is talking about how to use r to produce not just images of the moon and other services, but very accurate looking images with some really novel techniques that I definitely did not know about until reading this post. So I'll give a little background here, but when you look at graphics rendering and you'll see this in all sorts of different domains, right, with, like, those CGI type movies or images or, you know, animations, of course, gaming visuals and whatnot that have to do with some form of polygons.

Little did you know, little did I know at least, that the triangle geometric shape is like the foundational building blocks of many of these computer graphics, which gets you really long ways, but it's not as optimal when you have to render a surface that is on 2 different sides of a spectrum. 1st, maybe something that looks completely smooth. No artifacts. No, you know, jagged edges or whatnot. Or the converse of that, something that has unique contours, unique bumps, if you will, such as, say, the surface of the moon with its craters and other elevation changes.

Now there are 2 primary methods that Tyra talks about here that can counteract kind of the phenomenon of these triangles not really being optimal for this rendering. One of these is called subdivision where, in essence, these polygons are subdivided further into micro polygons often smaller than a single pixel, which means we're blurry looking at something small here to help bring a smoother appearance with tweaking a certain parameters. And then on dealing with the converse of that, again, like dealing with bumpy surfaces or contours, you have what's called displacement of the vertices of these associated polygon to help bring this kind of artificial detail to the surface that, again, mimics what you might see in the real world with these contoured surfaces.

And Tyra says that the Raver suite of packages now have capabilities to handle both of these key algorithmic kind of explorations or rendering steps to make these more accurate looking visuals. And as I think about this, as we walk through this example here, I harken back. Again, Eric's gonna date himself here to my retro gaming roots where I remember very distinctly. It was in the, you know, early nineties, maybe mid nineties that the local arcade started having these racing games that were not those, like, sprite filled racing games like the yesteryear, like Out Run or whatnot.

I remember vividly Sega making a game called virtual racing, which was on the first times I had seen in an arcade really robust looking racing cars in 3 d type perspectives, but you could tell they were made with polygons. And they were smooth. Some were smooth, and yet some are really jagged as heck when you went out crashes or sparks, and you could kinda see if you, like, if you had a pause button in the arcade, you can see these little artifacts that appear that look like little geometric shapes. And in essence, that's what can happen when you render an image in the beginning, and he's got an example where he basically renders what looks to be, kind of like a cross between a sphere and a hexagon in a 3 d contour and then a, cube and then the r logo.

But when you render this first of a few parameters that he does with one of his, packages, you can see, yeah, they're little polygon shapes. They look like what I would see in that kind of first attempt at that virtual racing game. But then he starts to show example code, again, new to the ravers of called subdivide_mesh, this function which lets you define just how deep you do this subdivision. And he starts with levels of 2, and then he, you know, re renders the image. And now, in the second image, you look the left image.

Again, you want to look at this as we're talking through it if you have access to the screen. That first, ball on the left looks like the Epcot center or a golf ball and, on on the golf course. You can tell. You can see those little triangles splitter throughout, which is not what he wants. He wants to make this look completely smooth. So guess what? It's all iterative in these steps. He tries another subdivide mesh call with, you know, first trying to make things look a little more smoother without going new subdivision levels, and it doesn't quite get the job done. They still look, you know, a little bit off, but he's rounded out some of the edges to that aforementioned cube, but he's still gotta go deeper. And, basically, he finds this sweet spot of, like, subdivision levels of 5, which means this is upping the count on these little small triangle shapes that are composed in each of these images.

And going from the start to finish of this, that added many, and I do mean many additional triangles to each of these shapes as compared to what he started with. It's a huge increase, and, in fact, it was a 1024 fold increase in these number of triangles, which may not be the best for your memory usage in your computer, although I there are no benchmarks around that. But I know from my, you know, brief explorations and things like, what's a blender for rendering, 3 d and Linux and other software, that as you make these shapes more complex, yeah, your computer is gonna be doing a lot more work with that. So in the end of this example, he's got 3 versions of the the rendered scene, what he started with, and then one in the middle, which is kind of like a compromise of like the subdivision levels and kind of moving some of these vertices around, and then the last one of using like all these triangles at once.

The good news is is that this shortcut approach in the middle, you can't really see a visual difference of the naked eye between that and the more complex version with the additional triangles. But the key point is that, a, Ray the Rayverse packages support this, and you're gonna have to experiment with it for your use case to get that sweet spot of the smoothness that you're looking for. So that was a great example how you want to give this kind of smooth appearance without those artifacts fault coming into play.

Now, we turn our attention to the bumpier contour surfaces with, say, the moon as inspiration. And he mentions that this will be familiar to anybody who deals with GIS type data in the past with elevations. He has a nice render of both the moon image as well as, moderate bay where you can see the contours very clearly in the black and white images, And then he starts by drawing a sphere of what looks like the moon, but when you look at it, it just looks like a very computer graphic looking moon with, like, a shade on the left and light on the right, but you would not tell that's anything really.

So then comes the additional kind of experimentation of, okay, how do we first figure out what is the current displacement information that's in that image so you can get a sense on what's happening here and how many pixels are being sampled to help do this rendering by default. So there are some handy functions that he creates to kind of get into this metadata. And he notices that in the first attempt, there's only about point 0 9% of the pixels from that moon image are being sampled to produce this more artificial image, and that's why it's not looking like a moon at all at that point.

So gotta up the ante a little bit. Again, kind of like a iteration of this to figure out, okay, maybe I need to use more pixels to sample in this. It starts going from like 1.4% of them, Still not quite looking right as you look at the post. Ups it further, and he's got a handy function called generate moon mesh that he kinda ups the levels 1 by 1. And eventually, he gets to a point where, hey, we're now we're getting to what looks like the craters on the image where about 5 or 6% of the pixels are being sampled.

Still not quite there yet, but he ups the ante even more with this, levels. And then he finds what looks like, you know, a really nice looking moon where it ends up sampling about 89% of the pixels, which means that there is a lot going on behind the scenes in this image. And in fact, this would take up a lot of memory, And you can see there is kind of a there's a point of diminishing returns the more pixels you subsample here for this displacement. Not really a lot of change visually. So ends up, he finds a sweet spot between not going to the extreme of, like, 2,000,000 pixels resampled somewhere in between.

And hence, he's got a really nice visual at the end where you can see it it's getting getting much closer once he brings us back to the sphere shape. And there's a nice little phenomenon kind of towards the end where it looks like at each of the north and south poles of the moon, when you do it by default, it kind of looks like things are being sucked in like a vacuum. It's hard to explain. But then when he does the applying the contour more effectively, you see that it just looks like any other surface on the moon, nice little craters and whatnot.

As you can tell, I've been doing my best to explain this in an audio form, but what's really cool is that you can see kind of Tyler's thought process of when you have a problem he wants to solve with his raver suite of packages. These steps that he takes that go from start to finish, not too dissimilar to what I see many of the community do when they teach things like ggplot2. Start with your basic layer, start to customize layer by layer bit by bit until you get to that visual that you think is accurate given the data you're playing with. And in case a toddler's visual is here, to closely mimic what you might see if you put your telescope out there and look at the moon at night. So, again, a fascinating look to see the power of r in graphics rendering, which if he had told me this was possible 10, 15 years ago, I would have laughed in your face. But this is really opening the doors, I think, to many people making so many sophisticated images, not just in the 2 d plane but also in 3 d and really insightful walk through here. And you can tell when you read Tyra's writings just how much time and effort he's put into these packages to make all these capabilities possible. But yet with, you know, geological and very fit for purpose past.

So a lot to digest there, but, again, I think it's a very fascinating field.

[00:14:20] Mike Thomas:

Eric, I I couldn't agree more. I'm always fascinated by Tyler's work on this particular project. I think, you know, when the raverse sort of first came out, it was pretty revolutionary, I think, to the our ecosystem. You know, we had ggplot, and we could do, you know, a a lot in in ggplot and some of the other plotting packages that were available. But in terms of 3 d visualization, animations, things like that, you know, we never had anything like that as far as I know until Tyler came along. So these suite of packages in the in the Rayverse and in particular the ones that he's highlighting today and their new functionality are RayVertex and Ray render are pretty incredible.

I have tried to do some of this myself in the past. I think, I think you can expect your laptop fan to to a little bit as you try to do this. So get your GPUs ready, if you're especially if you're trying to add, you know, additional complexity and do some pretty in-depth things within these packages. But the the fact that we can do this at all is is absolutely incredible, and I loved the callback to sort of your early, you know, eighties nineties video game experience because that is that exactly what a lot of this blog post reminded me of as he leverages that subdivision levels argument in the subdivide mesh, function.

And you can sort of see how the the pixelation just starts to smooth out as that that number increases in terms of how many, subdivision levels are being used. And it's it's almost, you know, conceptually, like, following video game graphics from the eighties nineties to the 2000 as you sort of get away from these these triangles and the pixelation into something that looks, you know, much smoother, into the naked eye. You you really can't even see the pixelation at all. Great summary by you. I'm not sure if there's a whole lot more to add to it. It's an incredibly visual blog so it's difficult for us to do it spend a lot of time in, you know, 3 d visualization, I think you'll still find it fascinating.



[00:16:35] Eric Nantz:

Yeah. And it does remind me there is ripe opportunities that we often see shared on social media from time to time. They'll call it, like, accidental art when you're trying to do a neat plot, yet it doesn't come quite outright, but yet it's so interesting to look at. You wanna post it anyway. These things are ripe for accidental art, especially if yours truly is the one coding it up. But, I do think as you look at the evolution of these images, back to that racing example, it's like when I saw that virtual racing game back in the early nineties and, again, you could tell it was like Sega's, you know, first real attempt at making somewhat perform at Polygon renders.

And then it was like a couple years later, those of you who remember that Daytona USA racer game, it looked real. Like, everything was smooth. Everything was sharp. Everything is 60 frames per second. That is like you can have that same transformation as you play with these displacement, parameters and, again, the contour parameters. It's it's fascinating. It's absolutely fascinating. And what, who would imagine r could be an engine for graphics like this? And I've been following on social media kinda the other side of the spectrum where it's a slight tangent here, but Mike FC AKA Cool But Useless have been making all these packages that help me make sprite rendering even more easier in r itself.

And it's like, my goodness. We can make almost any type of game in r now. It's just a matter of time, really.

[00:18:08] Mike Thomas:

Absolutely. It's it's incredible stuff. Incredible stuff. And I do remember Daytona USA very fondly. How much fun was that?

[00:18:16] Eric Nantz:

Oh, lots of quarters of spending that one, and then, of course, the computer racers have just eat my lunch. But that was part of the design to get your money back in those days. So we're going to shift gears quite a bit here, to get into more of the data side of things because there are some really promising developments in some of the ecosystems that Mike and I have been speaking really highly about in recent episodes of our weekly highlights. And in our next highlight here, we're gonna go back to, the now getting a lot of traction, so to speak, the DuckDb paradigm, the DuckDb database that just got supercharged with a new plugin and a new capability of using a service that's new to us from the our side of things.

So and it happens to be called Mother Duck, and that's an awesome name. But, Mike, what is this all about?

[00:19:17] Mike Thomas:

So Mother Duck is a way to run as far as I know. It's sort of a whole cloud based data warehouse system, or platform, from the developers of DuckDb themselves that allow you to not only store data, in the cloud but to also run DuckDV itself in the cloud so that you don't need DuckDV installed on your own laptop. You just need to be able to authenticate into your Mother Duck account that you have, and you can run, you know, serverless cloud based analytics using DuckDb, but not needing anything installed on your own laptop which is is really cool besides, you know, the the packages to authenticate into it. So currently, and and, you know, one big component of this that that's noteworthy that I think we should touch on is DuckDV just released version 1.0.0 on June 3rd.

So this is incredibly exciting, you know, for the folks that have been following this project, for the folks that have been working on this project that we have a first sort of major, release of the project. And it's, you know, I think it's a a framework that we've seen, play out a few times, you know, in the data science ecosystem where a team will develop an amazing open source, framework or functionality. I think we've seen this, with the DBT package. If you're if you're, familiar with DBT software that I think allows you to do, more versionable ETL, They also have, I believe, a paid offering of that as well. I know that the team at Pymc sort of has some similar things going on, and it looks like the DuckDV team obviously has this incredibly powerful open source software that we can now use for doing some pretty incredibly efficient, ECL and and SQL querying on large datasets in, like, no time at all. But they've also built out, obviously, a, sort of privatized side of the business that allows them to to hopefully make money and to continue to, enhance DuckDV itself and that's this Mother Duck offering that we're talking about here. So there's pretty good documentation on, how to connect to your mother duck, service from Python but it looks like there's not necessarily any documentation out there yet on doing this in R.

Although the author of this blog post, Novikha Nakhov, notes that it's it should be pretty straightforward to do so, if you follow sort of the the Python framework and just adopt it leveraging, you know, what we know about how to connect to DuckDV databases using r. So that's what, Novica is doing in this blog post. He is outlining how you would go about connecting to a mother duck instance from our, leveraging our 441, on Linux with DuckDB version 1.0.0. It's pretty easy to connect, to, you know, a DuckDb database and in memory database and he has a nice little code snippet of how to do this, how to populate that database using a data frame, and then how to query it using SQL syntax. And, it also notes that we do have the new duckplier package that would allow you to do this in a more dplyr like approach as opposed to having to write a sequel string in your db get query argument.

But for the purposes of this blog post we're not going to explore Duckplier. So I Novica details the Python approach first to connecting to to mother duck, and really it's pretty simple. And this Mother Duck Extension actually comes native to the Python duckdb package. And it's just a little one liner essentially that allows you to connect to your mother duck instance. You'll automatically get a notification in the terminal telling you what URL you should enter in your browser to be able to authenticate to your mother duck service, and then you're essentially good to go from there.

In R, it's a little little trickier. The syntax looks pretty similar. It's a a one liner to try to connect to your your mother duck instance. But one of the first things that you have to do is actually from the terminal, I think write a little bit of code, to create a local database that's called md or shorthand for for mother duck. So the reason that you have to do that at nova deposits is that, the duck dp package for r doesn't automatically figure out that it needs to load this additional mother duck extension the way the Python package does.

So you have to sort of do that yourself but, again, Novica has all the code for us to do that here and it's it's fairly straight forward. You have to run a DB execute command to load this mother duck extension and then you're pretty much good from there, to be able to connect your mother duck instance and write your SQL queries that are going to execute in the cloud as opposed to locally. So it's a a fairly, you know, straightforward blog post. It's really really helpful, I think, for those of us that are exploring DuckDB and want to try out Mother Duck. I will note that there's a free tier of the mother duck offering that gives you I think 10 gigabytes of storage, up to 10 compute unit hours per month which is probably a lot, you know. I'm not sure, you know, we know how fast DuckDV queries execute. That's right. Most of my DuckDV queries even on parquet files, you know, that are gigabyte, 1 or 2 gigabytes in size will run-in maybe a second or 2, and there's no credit card required for this free offering.

So there's no reason to not try out this mother duck service if it's something you're interested in. And if you need to go up to, the the paid tier, it's $25 a month per organization, I guess not per user, which is pretty incredible. If you're an organization that wants to leverage this mother duck service, that gives you a a 100 gigabytes of storage and a 100 compute unit hours per month, which I, you know, unless you're doing some pretty crazy stuff, I can't imagine that you would you would hit that. So I think it's a very sort of generous price point that we have here, and, obviously, it's amazing that we have this free tier that allows you to to try it out first, if you if you want to and and maybe that's all you need as well. So pretty incredible. Great walk through by NovaCon on how to use this in R because up until now, I don't think there's been any documentation on how to connect to, the mother duck service from our and it seems like there there was a little tricky hack that he pointed out. That's definitely going to help. Those of us who wanna try this out ourselves, probably save us a lot of hours. So we're very grateful that he went down that rabbit

[00:25:57] Eric Nantz:

That's for sure. Ever since 1.0 was released. And I do recall, that's for sure, ever since 1.0 was released. And I do recall having to do some extension installation in my explorations a few weeks ago where I had to install a couple extensions, one of which was to help interactive object storage in AWS that didn't come by default. But, yeah, it's a similar syntax with what, Novica outlines here in this post. And, yeah, I guess, I'm interested in exploring mother duck to just see, you know, on a on a very basic basis just kinda how it plays in my workflows.

One thing I'm immediately thinking of, Mike, is that if you're an organization or you're a smaller or big company and you wanna leverage DuckDV as your database back end for things like shiny apps or whatnot. It sure does seem like having that hosted in a data warehouse would be an intriguing idea to help offset some of that storage footprint from you to that to that vendor. Of course, we were remarking in the preshow that, you know, you you wanna at least have, like, a couple, you know, plans, if you will, of where you host these files because, you never know what can happen on hashtag just saying.

But in any event, what's nice about this service, I think they have some examples I was perusing yesterday where they did hook up a couple I don't know. It was either observable notebooks or some other custom web apps with, say, the New York City taxi data and doing simple queries on that and some drop downs. And it was, as you said, blazing fast with these queries to get the results back to the browser and not making your browser do all the data munging itself. It's gonna, know, do that on on the server side. And it does look like they're also exploring interactions with, wait for it, WebAssembly too. So there's gonna be lots of interesting, I think, applications to play here. And, again, I'm intrigued to see to see where this goes in the future.



[00:27:59] Mike Thomas:

Absolutely. Me too. It's, an exciting space to watch. I've only dipped my toe in it. One really cool thing about the RStudio IDE is if you do have a dot SQL file in your RStudio IDE, every time that you save that file if you have a more recent version of RStudio, it'll re execute the query for you and show it up in in sort of the query output pane of our studio and that works great in duck DB as well I was playing around with it against parquet file, just editing my my DuckDV query and seeing the results come out real time without even having to, like, highlight the code and and click run or anything like that. It's pretty incredible.



[00:28:40] Eric Nantz:

All these save so much time when you add it all together. That is for sure. Did you say parquet files, Mike? I did, Eric. You sure did, man. That's a perfect segue to our last highlight today because, honestly, one of the building blocks that we have been, you know, espousing the praises of in many recent episodes is parquet as kind of this next generation data storage format. And, certainly, when you hear us talk about a lot of the use cases are when you have a collection of large data that you can do pretty, you know, sophisticated group processing with. So you can get really fast and and real time kind of analyses, being executed on it. But that's not the only use case because our last highlight here is, awesome update to the nano parquetr package, created by Gabor Society, a software engineer at posit. And his latest blog post talks about some of the key updates to this package and some of its use cases. So, Mike, why don't you walk us through what's new here?



[00:30:01] Mike Thomas:

I'd be happy to, Eric. You know that I am, want to sing the praises of the parquet format and everything around it. So very excited about this Nano Parquet. Our package and the 0.3.0 release, that Gabor worked on, I believe, Hadley may have worked on it as well and some others, on the the Tidyverse team at Posit. So one of the reasons that they created the Nano Parquet package is that, they wanted to be able to, you know, interop with Parquet files without necessarily needing, some of the the weight that comes along with like the arrow package because the arrow package does have a fair amount of dependencies on it and it can be a little heavy to install.

So one of the things about this nanoparquet package is that it has no package or system dependencies at all other than a c plus plus 11 compiler. It compiles in about 30 seconds into an R package that is less than a megabyte in size. It allows you to read multiple parquet files which is in a traditional sort of data lake, structure very very common, with a single function, you know, sort of like what we have in the reader package. I I don't know if this is little known or if it's well known but if you just point that to a directory instead of a file it will read in all of the files in that directory assuming that they have the same schema, and essentially append the data together into a big data frame. And that's the same thing that the nanoparquets readparquet function will do for us. It also supports writing most our data frames to, Parquet files and has a pretty solid set of type mapping rules between our data types and the data types that get embedded as metadata in a parquet file, which is, you know, again, another reason why, you know, parquet format exists and I think was another part of the motivation for this nano parquet package that it gets a little bit of a reputation, the parquet format, for only being for large datasets.

Like, you only wanna use parquet file format when your your data is big. And, I think what Gabor is trying to argue here in this blog post and that I would agree with is that that's not necessarily, you know, the case. And I think that anyone can make a case for leveraging the parquet for file format for storing data of of any size. And one of the, I think, big reasons for that is, the, you know, ability to not only have the data in the file but also metadata around the data including the data types such that when you hand that parquet file off to to someone else, they don't have to in their code the the way we used to do when we read a CSV and and define what the data type for each column that's coming in. You don't have to do that because it's embedded in the file itself.

So we have a lot more safety around, you know reading data frames and writing data frames and ensuring, portability, you know, not only between languages but between users essentially so that, you know, what you see is what you get. So I I think that's really helpful. You know, we know that Parquet is performant and typically a smaller size than, you know, what you would store in a traditional delimited type of file, which is is helpful as well. And, you know, there's additional things like concurrency, a parquet file can be divided into to row groups so that parquet readers can can read, uncompress, and decode row groups in parallel parallel as Gabor points out.

So just a lot of I think benefits for leveraging the parquet file format and I think this is maybe one of the first blogs that I've seen make the case for the parquet for file format for small data as opposed to big data. So I'm super excited about the nanoparquet, package because I think that there are some particular use cases, you know, potentially Shiny apps that I'm developing that are maybe reading a a smaller dataset where I don't necessarily need to, leverage the whole Arrow suite of tools and maybe I just need to to read that data set in, you know, one time, from a parquet file, and the nano parquet package might be all I need.



[00:34:23] Eric Nantz:

Yeah. I'm I'm with you. I'm so glad they give, you know, the spotlight not just to the the typical massive, you know, gigabyte gigabyte datasets. I think what Nanoparque is doing here, again, great for the small dataset, situation, but I really like these metadata functions that are put in here too because I think nano parquet could be a great tool for you, especially being new to parquet, getting to know the ins and outs of these files, knowing, like you said, the translation between the r object types and the parquet object types in the dataset. There are lots of handy handy functions here that maybe they are part of the arrow package. I just didn't realize it, but to be able to quickly have these simple functions that grab, say, the schema associated with parquet, grabbing the metadata directly in a very detailed printout of, you know, a nice tidy data frame of these different attributes. I think this is a great way to learn just what are the ins and outs of this really powerful file type. And, of course, Gabor is quick to mention some limitations for this. He does say he probably wouldn't recommend using this to write large parquet files because this has only been optimized for single threaded processing.

But with that in place, I still think this is an excellent way to kind of dive a little bit or really deep into what are the ins and outs of parquet. And then when you're ready for that transition, say, to a larger datasets, he obviously at at the conclusion of his post calls the Arrow project out as well as DuckDV out. So you've got it all it all leads somewhere. Right? You may your your situation may be that Nanoparquet is gonna fit your needs, and that's awesome. More power to you. And then as you escalate your data sizes, then you can still use par k. You're just maybe transitioning to a different package to import those in or the write those out. So I think it's it's a great introduction piece as well as I'll be honest. I'm putting on my life sciences hat for a second.

I would absolutely love to use parquet format as opposed to a certain format that's in that ends in 7 b that. You know what I'm talking about, those out there. This would be a game changer in that space.

[00:36:50] Mike Thomas:

I mean, the word parquet just sounds a lot smoother than, you know, SaaS 7 beat at. It it sort of sounds like, I don't know, r two d two or or I don't know. You know? Well, at least r two d two out of personality. Oh, goodness. Let's switch quickly and one acknowledgment, that they that Gabor makes in this blog post that I do wanna highlight just for the sake of talking about how small of a world it is out there in the data science data engineering community is nanoparquet is actually a fork of an R package that was being worked on called mini parquet, and the author of mini parquet that r package was Hans Nielsen, who is the founder of DuckDV.



[00:37:35] Eric Nantz:

Well, well, well, it all comes full circle, doesn't it? Oh, this is what a fascinating time. Awesome awesome call out. And I believe Hans will be keynoting at positconf this year, if I'm not mistaken.

[00:37:49] Mike Thomas:

You are correct. Yes. I As well as like every other data science conference, I think, this year.

[00:37:56] Eric Nantz:

He's been a busy man for sure.

[00:37:58] Mike Thomas:

Deservedly so.

[00:37:59] Eric Nantz:

Yeah. Lots of momentum here. And, again, it's great to see every every all these frameworks and are now being a great beneficiary to this amazing work and giving us the tools, multiple tools to interact with these these systems. However, like you said, we have multiple packages. They're not with DuckDV itself, not multiple packages that interact with parquet. You've got lots of choice here. And, yeah, the time is ripe as it does seem like we're we're moving on to, like, these newer newer formats, these newer database formats that the the sky is indeed the limit here.

And, yeah, thanks for riding that ship. I was about to go on more rants about another format, but we're gonna keep it positive here because this issue has much more than just what we talked about here in the three highlights here. I had lots of fun looking at the new packages, the updated packages, and much more. So we'll take a couple of minutes for our additional fines here. And, yeah, going back to the, sports theme for a second, now that we have a lot of interesting tournaments going on right now in the world of soccer and football. You might wanna visualize some of those, you know, head to head matchups, and I've got the package just for you for that one. A new package called brackets offered by Vera and Limpopo, who you might familiar be familiar with. She's been a frequent contributor to the shiny community and many talks and presentations.

But brackets is an HTML widget that lets you very quickly visualize the bracket of a tournament. And I could see lots of use cases in the world of sports analytics for darn sure, but the ever fascinating part of this package is that it was actually built during a course that Virla teaches about supercharging your HTML widgets, which is part of the analytics, platform that she recently joined alongside David Grandgen. So they've been both, hard at work on some of these courses about Shiny and HTML widgets. And, yeah, Brackets is one of the products of those courses. But, yeah, when I get back to doing more sports analytics in the future, I think I'm gonna put Brackets in my Shiny app just for fun alone, but I could see lots of lots of cool utility with that.



[00:40:20] Mike Thomas:

That's a great call out, Eric. And when I saw that, really, I guess, clean way to visualize, you know, tournament brackets, I was I don't know. I thought it was super cool too. So I'm I'm very glad that you shouted it out. I am not surprised. I did not realize that Virla is the one who put that together, and I'm excited to see more from that, especially, as we continue to dive deeper into a couple tournaments in my favorite sport, soccer or football for the rest of the world. A lot of bracketology going on these days so looking forward to more in that, area.

One call out, that I wanna make is a Frank Harrell's blog, Statistical Thinking, has a new slide deck, that he gave as a presentation at the International Chinese Statistical Association Applied Statistics Symposium in Nashville, recently, and the talk is titled Ordinal State Transition Models as a Unifying Risk Prediction Framework, and he presents a a case for the use of discrete time mark of ordinal longitudinal state transition models. If if we can pack all of those words into the same sentence and really the purpose there is is estimate estimating risk, and expected time in a given state, and and it's really, you know, in the vein of clinical trials in that area. But I took a look at these slide decks and, state transition matrices and models are something that I deal a ton with in, banking and financial services because most, loans banks have a particular rating assigned to that loan based upon, how risky it is to the bank, you know, the likelihood of that loan going bad, essentially, or likelihood of re repayment or or the borrower not repaying it. And, you know, they continue to to reevaluate that state that that loan is in.

You know, it could be monthly, it could be quarterly, it could be at some longitudinal time horizon. And then we want to take a look at how the volume within that loan portfolio shifted from one state to another, across a given time period. So I have in our package out there called migrate that is due for some updates, or due for a re release on crayon. We've had some work done on it recently but I just need to push it over the hump. So, I would check out these slides if you're interested in this type of stuff but mostly I'm calling, this slide deck out as a call to action for myself to get some work done on the migrate package and hold myself to it, out in the public here.



[00:42:57] Eric Nantz:

Hey. Our weekly has many benefits. Right? Making you do more work. That's awesome.

[00:43:03] Mike Thomas:

Yes. Right. But you're a great set of slides by Frank.

[00:43:06] Eric Nantz:

Yeah. Frank did a terrific job here, and and you you tell us a small world. I remember a lot of the the, you know, the mathematical notation and the concepts here because my dissertation very much had a component of theory around state transitions and competing risk methodology and whatnot. So I'm getting flashbacks of writing that chapter in my dissertation. It was not easy back in those days. But Frank has is one of the great thought leaders in this space, and, yeah, it's great to see him sharing this knowledge in many, many different formats and many venues. And there's so much more knowledge you can gain from the our weekly issue. Again, we could talk about it all day, but, yeah, I wish we had that time. But we're gonna invite you to check out the rest of the issue. Where is it you ask? Oh, you know where it is. It's atoroakley.org.

It's right here on the on the index page, current issue, as well as the archived all previous issues if you wanna look back to what we talked about in previous episodes. And, also, this is a community project. Again, no big overload company, you know, funneling down our vision. This is all built by the community, but we rely on your contributions. And, honestly, the easiest way to do that is to send a poll request to us with that great new package, blog post, presentation, resource, call to action. You name it, we got a section for you in the RN Week issue all marked down all the time. Just send us a pro request, and we'll curator of that week will get it into the next issue.

And, also, we wanna make sure we hear from you as well. We, of course, love hearing from you. We've had some great shout outs recently, and, hopefully, you keep that coming. The best way to get a hold of us is a very few ways. We got a contact page linked in the episode show notes. You can directly send us a message there. You can also find us on these social medias where I am mostly on Mastodon these days with at our podcast at podcast index dot social. I am also sporacling on x, Twitter, whatever you wanna call it with at the r cast and on LinkedIn. Just search for my name, and you will find me there. And, Mike, where can the listeners get a hold of you? Sure. You can find me on mastodon@mike_thomas@phostodon.org,

[00:45:20] Mike Thomas:

or you can check me out, to what I'm up to on LinkedIn if you search for Catchbrook Analytics, ketchb r o o k.

[00:45:30] Eric Nantz:

Very nice. So we're very much looking forward to that new package release when you when you get that out the door. Hopefully, that'll be, top of the conversation in a future episode. But, yeah. I think we've we've, talked ourselves out. We're gonna close-up shop here with this edition of our weekly highlights. And, hopefully, we'll be back at our usual time. We may or may not because this is a time of year where vacations are happening, but one way or another, we'll let you know."
"49","issue_2024_w_25_highlights",2024-06-21,42M 48S,"How the newly-released CRAN package deadline metadata inspired multiple learning journeys of the latest Shiny features with one of your podcast hosts joining the ride, a fresh coat of frontend paint to the amazing R-Universe, and the innovations R brings to forensic analyses of handwriting. Episode Links This week's curator: Ryo Nakagawara -…","[00:00:03] Eric Nantz:

Hello, friends. We're at back of episode 169 of the R Weekly Highlights podcast. Yeah. We're coming at you slightly a little later this week because, real life never slows down for either Mike or myself, but I'm delighted you're here. My name is Eric Nansen as always. It's great fun to talk about all the greatest, highlights that we have in this week's our weekly issue on this very podcast.

[00:00:25] Mike Thomas:

And I never do this alone. I've got who's been on an adventure of his own, Mike Thomas. Mike, how are you doing today? Hanging in there, Eric. Hanging in there. We're, yeah, like you said, a little late this week because real life happens, but we're still trying to get it out there for the people.

[00:00:40] Eric Nantz:

Yes. We're getting it out there. And, yeah, we we've been taking some mild inspiration from a certain team in Canada that's now dragged the uncertainty from Florida to game 6 of the Stanley Cup finals. So if they can come from a 3 hole deficit, we can knock out another podcast today, I dare say. I like that analogy. Yes. Yeah. It it works for me today anyway. So what also works is our weekly itself is a project where we have a rotating set of curators for every single week. And this week, it was curated by Ryo Nakagorua, another one of the OG's of the Arruki project itself.

And as always, he had tremendous help from our fellow Arruki team members and contributors like all of you around the world with your awesome poll requests and certain suggestions. So we lead off with an issue that if you ever have released a package on CRAN, you are probably familiar with at one point or another. Whether it's your package or one that you depend on, is sometimes on the CRANS system, there are packages that may fail certain checks. Happens to all of us, and then there may be such a failure that the CRAN team says, okay. We've detected this, failure.

Now, you have until a certain amount of time to fix this. And that can be scary for a lot of people, myself included, especially if it's a package that is a very important part to my workflow or is a dependency of an important part of my workflow. But you may want to know just at a glance what is really happening in this landscape. And this first highlight covers that and another bonus feature to boot that's come into R recently where Matt Dre, another frequent contributor to Rwicky highlights in the past, has created 2 very neat interfaces.

And the first of which we're gonna talk about is this grand deadlines dashboard. This is very interesting, and the way this has been generated is that you may not know this, but in the base version of R, there is a handy function called crannpackagedb where this is literally grabbing an online source of data that the CRAN team maintains about package metadata. And you may be thinking to yourself, well, wait. Isn't this based on my current installation of r? No. No. No. This is a publicly available dataset that is updated regularly. Not quite sure the frequency of the updates, but there were 2 new fields that were introduced recently in this data frame. And in Matt's post, he shows first the typical metadata. You get, like, the package name, the version, and the maintainer, but there are 2 additional columns. The first of which is called deadline, and deadline is giving, as the name suggests, the date that the CRAN team has given that package author or that package itself to fix whatever issues have been flagged in their check system.

And you sure enough on the CRAN page, you could go to each package's website. You could then see the notes of these check failures on that package site itself on CRAN. But what Matt has done here is he took this opportunity to learn, become the latest and greatest in shiny development, which, of course, will please me in Mike's ears quite a bit. He has leveraged bslib and some pretty nifty ways of constructing the UI to create a very attractive looking, very fit for purpose deadline dashboard with individual cards as in the BSweb system that give at a high level the number of days remaining until that deadline, or, unfortunately, it could be the days that have passed since that deadline.

And you can see that very nicely. You have color coded boxes here, so you could quickly grok in the order. It goes from the ones that have they're running the longest in their update up until the ones that have more time. But you could scroll through this and see very quickly then which packages have been flagged, and it'll give you a direct link to the detailed check failures that you can navigate to directly from this page. Really nifty interface here, and he's not alone in this. Actually, Hadley Wickham himself has also built a similar report, albeit it's built with what it looks like an automated portal kind of GitHub action deployment, but it's got similar metadata associated with it. So you got 2 great sources for this information.

And I got wind of this not so much from Matt's post, but from a post he did on Mastodon as he was experimenting with this. And I pull up this interface, and I'm thinking to myself, oh, this is nifty. When I pulled up the interface, it was running on shiny apps. Io. It is a shiny app after all, but, immediately, in my head you wanna guess what was popping in my head, Mike, as I thought about what this could be good with?

[00:05:55] Mike Thomas:

Shiny Live WebR.

[00:05:57] Eric Nantz:

You got it. I'm on the the Shiny Live web train as you know from my recent efforts on the ER consortium pilot. So I wondered, I wonder what why maybe that wasn't done. Well, sure enough, it went to the GitHub repo, and I'd see that Matt did indeed attempt this, but it wasn't working. So this got me thinking here. You know what? I've been in the Shiny Live game for for a bit here with this other project. Maybe I could have my hand at trying to get to the bottom of this. Hence, my rabbit hole. So it's a little bonus, dev story time of Eric here. I pulled it up, cloned it on my local system, and I tried to look at the errors. And Matt did, have, like, a a general version of this error and an issue on his issue tracker on GitHub.

And I noticed that for whatever reason, the shawnee live app could not download that aforementioned CRAN database from that handy function inside of r. Why is that? This gets really interesting because, apparently, there are security controls in place in your web browser for accessing certain versions of URLs behind the scenes due to what's called the cores or the cross origin something or another. I'm not as in-depth with this, but I have seen posts from George Stagg in the past on, like, the WebR issue repository or the Shiny Live issue, tracker repository saying that you have to be careful about how you download files. And in fact, in web web r, you have to use the download that file function more often than not to actually download something, and you have to hope that the browser is able to access that URL because of these built in security protocols.

So I noticed that even I tried to download the file directly after I scanned the source of this, CRAN package DB function, I still couldn't get that RDS file. It just would not register. Then I went down another rabbit hole to figure out, I wonder if others have experienced this with RDS files. I wonder if it was just specific to that or, you know, just shooting throwing darts at a dartboard at that point. I stumbled upon another issue on the WebR repo where somebody put up a clever workaround to use this kind of openly available domain, like, referral service where you can transform a URL to meet these core security requirements.

It's kinda like when you have in, certain, you know, web lingo, you can put in a base URL, and then one of the query parameters is like the URL that you're actually interested in. There is such a thing as this that I stumbled upon in the issue tracker, and I'll put a link to my poll request in the show notes where you can get the the full details of this. But it's so I I plugged in this kind of more modified URL, and sure enough, I could get the RDS file from the CRAN site once I made this modified URL. So I thought, okay. I'm in business now.

So in the in the source code of Matt's dashboard here, I now have a handy function to detect if the app is running in a web assembly process. And that's credit to Winston Chang from the WebR repo to basically detect if we're in what's called the web and scripting kind of functionality and web assembly. Again, I'm still learning all this, but it's kinda like a a simple true false if you're in it or not. So then if I'm in it, I know to use this custom function that I developed to modify the URL with download that file directly and then import that into r as a typical RDS file. Whereas if you're running this in a traditional r process, you can use that function that Matt shows in the post as is. No no big deal there.

And so so once I figured that out, I thought I'd solved everything then. But, no, there was one other kink here, Mike, and, you know, it never stops sometimes. WebR is currently still using our version 4 dot 3 dot 3. And Matt has, as he's actually covered, I believe, in one of his recent Mastodon posts, in version 4.4, now there is a handy function in BESAR that we've actually covered called sort underscore buy, where you can sort a day frame very quickly. It'll be very familiar to any of you that use the dply or arrange function.

Well, guess what? In the web assembly version, that function doesn't exist because that's running 4 dot 3. So then I have another trick where if I'm running in web assembly mode, I use the more, you might call traditional order function in base r with the data frame referencing the variables directly. Again, dusting off my, you might say, our development skills from pre 2015 on that, but, hey, Gradle was useful again. Right? So once you put both of those things in, then the web assembly version actually works.



[00:11:06] Mike Thomas:

So I felt I felt pretty vindicated, so I'm I'm geeked out. I'll send that PR to Matt. He merges it in. He recompiles the app, and now it's on Shiny Live. So woo hoo. Little dev fun with Eric there. That was that was a good time. Eric, that's awesome. I think, Matt, as he notes in his blog post, was super grateful for your help. I don't know when you sleep. I don't know how you do this stuff. I don't know how you figure it out and keep up with the shiny lab stuff, but, you are clearly leading in this space somewhat. If I remember correctly. You might even be giving a little bit of a talk at Pawsit Conferences here on this exact topic. So it sounds like you were the best the best man for for the job, so to speak, on this one. And, the the end result of this app that Matt has put together is is excellent. So I know he's grateful to you for that work that you put in and that pull request that I'm I'm taking a look at right now. It's it looks like it's full of rabbit holes that you went down. So Yeah. That that short narrative doesn't do it justice, but it was there. Every bullet point is like a a rabbit hole in terms of how I'm interpreting it. So that that's that's fantastic.

And the app is the app is great. It's really cool to see the the Shiny Live use case here. I know that that's something that Matt was trying to tackle here and was also trying to up skill a little bit on bslib. So the the app that came, out of it looks looks really nice. I think, you know, an alternate approach if you didn't care about, you know, going the the bslib route and the shiny live route would be you could potentially, have like a GitHub actions script, right, that would reach out to the the CRAN database that doesn't need, you know, Shiny Live or anything like that, you know, to be able to run and collect that data, maybe store it as an RDS file on a nightly basis that gets refreshed in the the repository

[00:12:54] Eric Nantz:

and then, you know, create a a GT table or a reactable table or or something like that. Right? And maybe that sounds sort of like what Hadley That put together. That's basically exactly what Hadley has done. Right? So it's it's great to see both of these approaches side by side, though. And and as Matt says, this is an awesome learning opportunity for him, and I, of all people, love going down rabbit holes for learning opportunities

[00:13:16] Mike Thomas:

in our ecosystem. Awesome learning opportunity for me as well just to read through this to be able to see the repository and see how it all came together. It's fantastic. And as you mentioned, one of those other columns that comes back from that, CRAN database, function in BaseR that we have is is DOI. And a DOI is a digital object identifier, and it sounds like CRAN has recently been tagging on these DOIs to, most, if not all, R packages that are on CRAN. I'll have to check if the package that I have on CRAN have a DOI right now.



[00:13:51] Eric Nantz:

Yeah. I haven't checked mine yet, and I'm not sure if they've gotten to everyone just yet. But, boy, I've seen a lot of excitement about this on on the social sphere, so to speak, with, being able to use this as a very important, you know, reference for somebody's, you know, development and research journey. It's it's terrific here. Absolutely terrific.

[00:14:11] Mike Thomas:

Yeah. And I imagine as, you know, I am not the expert on DOIs, citations, things like that. But I I think this is a way to be able to track, you know, how often this DOI gets used out there in the wild and and really that the purpose of that DOI is for citation purposes. Right? For someone else to someone else to be able to to not only, you know, cite your work in their paper but if they stick that DOI somewhere on the web, I believe that you'll be able to sort of track, where your work has been used. And it sounds like these DOIs can be either project specific or like blog specific, I think, or article specific, something like that, as well as perhaps, user specific, individual specific DOI. I'm not sure how you would, you know, essentially have a profile that that has multiple DOIs under it, right, for all the different articles that you have or different R packages in this case.



[00:15:07] Eric Nantz:

Yeah. I would love to hear from listeners because I know a lot of package authors will put what's called their ORCID identifier in the package manifest if they can link all this together somehow, but it's a fascinating time to as you said, I think this is a first step to getting some real tangible metrics on package usage, in in whatever domain people are interested in. And, what Matt's done here is really awesome because he has a handy little function, right, that called get cran DOI badge where, you know, basically, on your GitHub repos or in your readmes, you you often see these awesome little badges about, like, the version status or if it's on crayon or whatnot. Now you get a very attractive DOI badge free of charge, right, from this function. Really, really nice stuff here. Yes. The big shout out to Matt for making this very easy

[00:15:58] Mike Thomas:

for us, via his his Badger package to just stick that in our our readme and have, you know, another great badge to stick on top of our our packages. So that's that's awesome that he's made that so easy for us, and I'm excited to dive into the world of of DOIs, especially as it relates to the r packages that we have out there and see if I can add some badges this

[00:16:21] Eric Nantz:

week. Yeah. I've been, slowly getting on that badge craze, but I see some other prominent developers have, you know, quite a few of them. Like, how do you do that? Because it's all still kind of foreign to me. But, hey, this is this is, quite handy here. And, this is catching, you know, it's catching, you know, you know, a lot of momentum here because on on this particular feature alone, Dirk Edebutel, who, of course, is famed with RCPP development and are on Linux, he's created now a simple fetcher to his Littler script pipeline, which I use a lot of my Docker, files for installing our packages when they're using the rocker base images. Littler is kinda like a short COI like interface to r to say install a package from CRAN, install a package from GitHub. Apparently, he's also got now a way to fetch those DOI, information directly in Whitmer as well. And then another fun, you know, time is all together is that Mike FC, I'll be, also known as cool but useless on Mastodon and Twitter.

He's trying to make a list of all these kinda like CRAN metadata dashboard, you know, situations. And now Matt's positing, oh, wait. Maybe we need a a search that aggregates all these together. So it's like inception overload with all this metadata

[00:17:40] Mike Thomas:

scraping. But, hey, I'm here for it. This is this is terrific stuff here. Me too. Me too. Shout out, Matt. Great blog post, great walk through. I think this is a fantastic sort of I don't want to call it simplistic but it's a a pretty straightforward thing that he's trying to accomplish and and being able to to have this very straightforward use case for Shiny Live makes it it really consumable for me to understand what's going on.

[00:18:17] Eric Nantz:

And we just mentioned, Mike, how yeah. What Matt's produces very aesthetically pleasing, and we're gonna go from simple to what is a very intricate system that just got a really big face lift to the end user perspective and a whole lot more. What we're speaking about here is the very influential and very powerful Our Universe project, which just got a very, you know, welcome revamp to its user interface and user facing functionality. So this next highlight comes from a blog post from the architect of our universe himself, your own oombs, who talks about, guess what? The web interface for our universe has a brand new front end, and this is not not a trivial front end by any means.

It's actually based in the express JavaScript framework with some server side rendering that, Jerome says has been greatly helpful to getting results appearing on the our universe page much more quickly, much more efficiently. I even did a little, GitHub investigation last night in this repo, and it's even using mongodb as well of all things. My goodness. Talk about supercharging your UI and back end processing. But, obviously, this is an audio podcast. We can't really show you over in this little chapter marker. I've got a, image of it, but we'll describe kinda what he is most excited about in these updates. And what I saw when I scanned this earlier is that now this the UI of it, it's gonna be very much optimized to multiple, you know, resolutions wherever you're looking at this on, like, a a phone screen or on a big monitor. He's done a lot of attention to detail to make the page very responsive, which we all kinda take for granted now when we build our shiny apps with bsweb or other frameworks like that. And as I mentioned, server side processing for a lot of these rendering results. So your client browser isn't having to do as much of the heavy lifting as it's pulling this information from the our universe services.

And he's also made a little bit of rearrangement to where you see the information on the owner going on the left sidebar now. And then, also, each package now gets a dedicated section to we just talked about earlier, citation information, which I'm sure the DOI, framer from CRAN may or may not play a role there. And also some handy contribution charts that are showing now proper time stamps and kinda capping out at 12 contributors in case you have a package that gets a boatload of contributors and not overwhelming the graph. But, he's also been able to optimize the table representation of package help pages.

He looks like the HTML of the help pages has been greatly enhanced as well, And he's also made some rendering of the vignettes inside a package look a little more native to a web browser and not just kind of, like, pasting in text from you can tell on different source. It all looks very cohesively constructed here. Really nice stuff. We invite you to check out the post for, obviously, links to the information. And he was very careful to keep backward compatibility here with the various URLs at different parts of our universe such as the packages, cards themselves, the articles.

You're gonna still get those same URLs. So don't fret if you're a package author on our universe. I'm wondering, oh, no. I passed this link to, like, my team or or in my, you know, Twitter or Mastodon, shout out to you. You'll still be able to use those same links. But, again, really, really polished interface here, and it just goes to show you that it seems like your own never stops on his our universe development adventures, but really really awesome UI enhancements here.

[00:22:17] Mike Thomas:

I couldn't agree more, Eric. You you know, when our universe originally came out that the UI to me that we now had around, you know, these individual package sites was like mind blowing that we had something like this now. Right? Instead of having to go to, I guess, GitHub and take a look at the the read me or or the package down site. Right? This our universe serves a lot more purposes than just that and thinking back to what the UI looks like before this facelift, this one no offense to the old UI but like because that blew me away, but this one has blown the other one away.

So this is just much much cleaner. The navbar looks fantastic. It looks like they have, you know, these different cards. Reminds me very much of like a bslib card type of situation that we have going on here that breaks out the section content with really nice headers as well as footers, which are are fantastic and and then it all the readmes, and the vignettes the way that they have rendered now I think everything is just a little bit cleaner which is fantastic. I'm looking at at one of my own for my package migrate and somehow, up here I have a link to my or Yaron has a link to my mastodon account next to my name. I'm not even sure I ever supplied that.

I must have at one point in time because I imagine that he's not doing that by hand, but it's it's you have to see it for yourself and and take a look at the new Our Universe face lift, you know, search your favorite package, take a look at what the site looks like. I need to start installing packages from our universe. I need to to shift over there from some other package managers because I think there's a lot of advantages to this being sort of a one stop shop for, you you know, package installation in my local work, package installation for for web assembly purposes, right, as well. Yes.

So it's it's a pretty incredible resource that we have as as well as all the documentation, citation, you know, and all sorts different things that we have around all the different packages, that are now on the our universe. So incredible resource. He's another one where, like you, Eric, I I have no idea when your own sleeps but I'm very grateful for the work that he's done for the community.

[00:24:35] Eric Nantz:

Yeah. Like I said, I inspected the, GitHub repo for the new front end and my goodness. If I could under if I was able to understand even 10% of all the JavaScript magic he did there, I'd be a lucky lucky person. But, yeah, I was even pulling up the I now have 2 packages on our universe, albeit they're both related to the, podcasting 2.0 ecosystem. But, boy, those pages look fantastic. Like, man, my goodness. I I I feel good about passing this off to somebody and showing it off to to friends and stuff. This this is top notch. I love it. Just love it. Me too.

Sometimes, I'll catch myself saying when I read maybe some terse documentation. I'm not gonna say it's inspired by real life scenarios, but, you know, take that for what it's worth. And I'll kind of flippantly say to myself, who even wrote this stuff? Well, you know what? We're gonna answer that question literally in a much different context in our last highlight today because there is, of course, even in this advent of technology and most of us using a keyboard or a touch keyboard to write, you know, in the majority of our communication, there are still many instances where we are handwriting material.

And there is a very important domain of research in forensics that are looking at more data driven approaches to helping identify potentially from a source of handwritten materials, maybe they're letters, maybe they're short notes, maybe they're whatever have you, I'm trying to use, you know, awesome algorithms to help link maybe certain writing sources to a particular author or a particular, you know, writer. This last highlight is fascinating to me in a world that I definitely don't frequent in the world of forensics, but there is a major update to an r package called handwriter.

This is maintained by Stephanie Reinders, and she is part of what I believe is the Iowa State University, the Center For Statistics and Application in Forensic Evidences or CSAE for short, mouthful, but they've been doing a lot of research in this space. And in particular, this r package that we're referencing here is a first step in a bigger pipeline that they are in the process of open sourcing, although they've got some manuscripts to show for it, where this package handwriter will take a PNG screen grab, if you will, of any handwritten content, and it's gonna apply some handy analysis methods to help identify certain features of this writing. And this is it's all about rabbit holes. I can only imagine how much research time is gonna spend into this, but I'll give a high level of the pipeline because there's no way we could do it all just this year, but it's got 4 major pieces of this pipeline.

First of which is that let's say you are scanning a handwriting reference that has colored ink or whatnot. It's going to turn that image into literally great black and white or gray scale first, and then it's gonna take, you know, the the stroke width of each of the writing. And there's, we have a link in the show notes to the package site itself where they have a good example using literally the word c safe written in cursive where they will take this reducing the width of each of those strokes to 1 pixel.

That's fascinating too. And then trying to detect, they call this the break phase next to compose it into small pieces, which in 2 of you might think are the individual letters in this writing. And they do this with a little graph theory to boot. They'll put nodes and edges that correspond to these different points in the in the detection of these units or breaks. And then they'll actually take measurements of the distances between these breaks, distances between the different letters to help figure out then, okay. Can we are there certain properties of this writing that then if they get additional sources, if they get similar writing style, you will see different versions of these or similar versions of these metrics as compared to, say, if you and I, Mike, wrote, like, the same, you know, same, like, short story on 1 page on a one piece of paper, it should detect our writing styles to be quite different. And I venture to say you probably have better handwriting than I do do because my handwriting's terrible. But, nonetheless, this package should be able to detect that if we gave 2 PNGs of our different sources that it would correctly identify us as being different riders.

So as I said, this is part of a larger, research pipeline that they also talk about in this, in this, package down site of how they integrate this with additional analytical tooling using some Bayesian methodology, some classical k means clustering to help really with a larger set of writing to be able to put this into distinct groups so that then you could link maybe potential sources to the same author. So as you can imagine, where this would really be important, especially in the world of forensics, is if there is an investigation and maybe one of the pieces of evidence is a writing that maybe a suspect had and then trying to link that maybe to that actual person as part of the evidence. I'm just speculating here because, again, not in my domain, but you could see how important this is in the role of forensics. So the fact that r itself is able to drive a significant part of this pipeline is absolutely fascinating to me. So when I pulled up this page the overnight, I I was I wasn't sure what to expect, but, boy, I can see that this is very important research.

And, again, I I started to think it analyzing my handwriting because I'm not sure maybe I'd break the system on my bad handwriting. But nonetheless, this is, a pretty fascinating domain and certainly a big credit to that team, at CICE for really, putting all this out there. And if you're interested in this space, yeah, I would definitely recommend checking out the handwriter package.

[00:31:19] Mike Thomas:

Well, Eric, I have seen your handwriting before and let's just say I am glad that we now have an r package for me to use to figure out what the heck you're saying.

[00:31:33] Eric Nantz:

Oh, that cuts deep, Mike. That cuts deep. I'm just kidding. I'm just kidding.

[00:31:39] Mike Thomas:

One thing that warms my heart more than anything else in the world is seeing universities use R to this extent and create R packages to just further, you know, the specific research that they're working on. The fact that this Center For Statistics and Applications in in Forensic Evidence, c safe at Iowa State University has clearly what I think, you know, chosen to leverage r as a as a tool to to push their research in their field forward, it it makes me so happy. And like you, Eric, this is a domain that I literally know nothing about. Probably my extent of learning anything about handwriting processing in R is the old MNIST dataset, where you're trying to classify, figure out which number someone wrote down. You have this 60,000 row dataset of all the that you you might use. So if you have done that before and taken a look at m MNIST, this is gonna blow your socks off, especially if you are in the space of doing any sort of image processing or handwriting analysis or or stuff like that. There's a great and maybe we can link to it in the show notes, The the documentation here is fantastic and it's all in a GitHub repository as well. It is a GitHub Pages site which is fantastic and it is, you know, well beyond any package downside, let's put it put it that way, that I have ever created.

So it's it's absolutely beautiful. I'm not even sure that they're necessarily using PackageDown. I know they've got some CSS, some custom CSS going on and things like that in a docs folder just sort of in the main branch, but it's it's an absolutely beautiful really informative site. It is a fantastic example of really, you know, including as as much documentation as there is functionality and and covering all your bases. There is a how to guide which I would recommend being maybe one of the first places that you can take a look at the functionality of this package, you know, in terms of word and then just a few quick functions to be able to process that image and and take a look at the results And the depth of the analysis that, you know, techniques that are encapsulated in this package is incredible. As you mentioned, Eric, it's, you know, plotting all sorts of different vectors and and directions as well as, you know, going all the way into to Bayesian analysis too, I think is included in this package. I saw some commits that that have stuff to do with Python support as well.

There's all sorts of documentation in here for whether you're working on a Windows, additional information for Mac users as well. So it's it's a fantastic resource. Again, it really warms my heart to see a university adopting R this hard and really putting a ton of best practices And

[00:35:03] Eric Nantz:

And we probably shouldn't be too surprised, this influential research is coming from such an esteemed institution as Iowa State because this is also the university that once had the esteemed Di Cook, presenting as a professor at the university and graduates such as the, the, the man that needs no introduction, Yway Siya, graduated from Iowa State. So, yeah, this is a a powerhouse in the world of our the art community and research. So credit to them for keeping innovation going.

[00:35:36] Mike Thomas:

I shoulda known.

[00:35:38] Eric Nantz:

The more you know, Mike. The more you know. And, speaking of the more you know, there's a lot more you can know by learning and reading the rest of this our weekly issue. There's another fantastic, roundup here that Rio has put together with awesome packages updated or brand new and awesome great blog posts and many others. We're gonna take a couple of minutes for our additional finds here, and we're gonna go meta on ourselves a little bit here, Mike, because what I teased about maybe a couple weeks ago, a listener got in touch with us, Yan Chohan, who has put together a GitHub repository of using R to scrape the metadata associated with this very podcast, rweekly highlights. And we'll put a link to that GitHub repo in the show notes, but it's a very clever use of scraping XML, which, of course, is the language behind the r Wiki highlights RSS feed just like any podcast.

And he's got a handy, little bit of repo set up there where you can see then a little screen grab of the plot of episode duration over time. It has a nice annotation apparently when I ramble quite a bit back in, sometime in in 2022 which, who would have thought that. Right? But it's a great little time course. And, again, using openly available data as part of our RSS spec. So it sounds like he's got a lot more planned in this. So, again, huge credit huge credit, to Yan for putting this together, And we'll definitely be watching this space because I always love it when people can use awesome, you know, uses of our to analyze things based on what we're actually producing. That's that's not that's top level stuff.

So, Mike, what do you think about that? I couldn't agree more. That's awesome.

[00:37:25] Mike Thomas:

For his sake, I certainly hope that the XML is not too bad. I have been in situations where the XML structure is great and easy to parse, and I have been in situations where it's the complete opposite. So I I certainly hope, that it's it's nicely structured and really flattered, I guess, in a way. Appreciate the the work on that and excited to continue to watch, what he's able to to to come up with there. Absolutely. Any, football or soccer fans out there as we call it in the US, you may know that there's a couple big tournaments going on or at least at least one going on right now and and one starting up shortly. The the Euro Cup is going on right now and the Copa America starts very shortly. And Luke Benz has put together a really cool article, with a great what looks like GT table kind of heat map, of the expected, you know, probabilities of winning the tournament as well as some statistics like offense, defense, overall rating, things like that, for both the Euro 20 24 and the Copa America 20 24 tournaments, leveraging a a Bayesian bivariate Poisson model, I think, to model the likelihood of winning the tournament. So that is is really really cool work. I always enjoy sort of the sports analytics stuff. Sometimes that's the easiest way for me to learn new things because it's, you know, a use case that I have a lot of passion for. So, awesome write up. Appreciate Luke's work on that.



[00:38:53] Eric Nantz:

Yeah. I do spy. Like I said, a GT table front and center in in those, summaries, and it looks absolutely fantastic. I'm sure that would make the author of GT Ritchie own a very happy to see that. That is top level stuff. In fact, I don't know if, Luke, if you if you had a chance to submit that to the table contest, but that looks like a serious, you know, top contender entry my if I do say so myself, really. It's so much fun to use sports data to learn, you know, different techniques in the in the our ecosystem.

So when I maybe get some more downtime again, I'm gonna get back to my world of hockey analytics and start turning loose because that was one of the very first types of data I analyzed way back in the early history of the art podcast as I was doing some fun stuff with ggplot2 and scraping some CSVs. There were some from a Google group of years gone by. Oh, I can't reminisce too much.

[00:39:49] Mike Thomas:

One of those days. But in any event, yeah, great great, great analysis of the, the world football, if I do say so myself. And if you can't get enough, you know, forecasting the the Euro 2024 tournament, there's another blog by Akeem Zales that's also on the highlights this week, and that is a machine learning ensemble approach. So if you wanna take another look at the same problem and a different approach to it, check that one out as well.

[00:40:13] Eric Nantz:

Oh, that is awesome. And, I shouldn't be too surprised that these made it because, Rio is a very, very big soccer fan, one of 2 huge soccer fans. I I should say 3 of yourself included on our our weekly team. So, yeah, representing Mike's representing that. You won't be able to see that on the screen, but I I see it. So awesome stuff. Awesome stuff. Well, yeah, we could go on and on, but we're gonna have to wrap up here as RealLife is calling us again. But we will leave you with, of course, how you can get in touch with us as well as the project itself. First of all, our weekly is driven by the community. We produce a Florida community, but we live off of your great poll request, your suggestions for additional resources to highlight on every issue. And that's all at our weekly dotorg, handy little poll request tab in the upper right corner, all marked down all the time. We dare say if you can't learn our markdown in 5 minutes, someone in the r community will give you $5.

I won't say I got that, but he he knows what he's talking about. We'll put it that way. And, also, if you wanna get in touch with us personally for this very show, you have a lot of ways to do that. We have a contact page in the episode show notes in your favorite podcasting player. And if you're on one of those modern podcast players like Podverse, Fountain, Cast O Matic, many others in this space, you can send us a fun little boost along the way and show us your appreciation there. And also, we are on the social media spheres. I am getting a little bit more on Mastodon these days, with at our podcast at podcast index.social.

You'll also see that I did a little, plug for a table submission I just have at the table contest. We'll see what happens. But in any event, it was a fun to put put my hat in the fray on that contest. Also, I'm on the Twitter x thing at at the r cast, and I'm on LinkedIn. Just search for my name. You will find me there. Mike, where can listeners get a hold of you?

[00:42:14] Mike Thomas:

You can find me on Mastodon as well at mike_thomas@fostodon dotorg, or you can find me on LinkedIn if you search Ketchbrook Analytics, ketchb r o o k.

[00:42:27] Eric Nantz:

Very good. Awesome. Well, we we we powered through it. We're gonna hope for the best in our next adventures the rest of this week. But until then, it's been great fun to talk about all this great art content with all of you, and we will be back with another edition of our weekly highlights next week."
"50","issue_2024_w_24_highlights",2024-06-12,59M 20S,"A thoughtful perspective on why it's not an either/or situation with popular data processing paradigms in R, another case of being kind to future you with your Git commit messages, and satisfying the need for speed in the evolving geospatial space. Episode Links This week's curator: Tony Elhabr - @tonyelhabr@skrimmage.com (Mastodon) & @TonyElHabr…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 168 of the R Weekly Highlights podcast. We're so glad you're here listening from wherever you are around the world. And this is the show if you are new to this podcast. This is the show where we talk about the terrific resources that have been highlighted in this week's our weekly issue, all available at rweekly.org. My name is Eric Nantz. As always, I'm delighted that you've joined us wherever you are. We have a lot of fun things to talk about, but this is a team effort after all. And I got my partner in our related crime here, so to speak. Mike Thomas is here. Join me at the hip. How are you doing today, Mike?



[00:00:40] Mike Thomas:

Doing pretty well, Eric. Not to violate any HIPAA laws and knock on wood. I am currently trying to dodge pink eye in our household so far. So good, but, I've I've heard that it's it's not fun. This is our first go around with it. So please pray for me. But We, we we have you in our thoughts for sure as somebody who's had a whole smorgasbord of

[00:01:03] Eric Nantz:

illnesses, viruses, things, and that and the kiddos, daycare days. Yeah. You,

[00:01:10] Mike Thomas:

keep strong, my friend. Thank you very much. But, Eric, how was your weekend?

[00:01:15] Eric Nantz:

Yeah. You know what? It's been a while since we did this. We're gonna dust off a segment I used to do in my in my live streams. Who's ready for story time with our

[00:01:25] Mike Thomas:

podcast? Beybey.

[00:01:30] Eric Nantz:

So it's story time with Eric now. So let's, buckle up for a few minutes here. So Friday night, I get a little opportunity to do a little, you know, open source development work on some of this podcasting database stuff. I've been pitching in with a podcasting 2 point o project. And recently, Dave Jones, who helps spearheads a lot of that effort, gave me access to some more lower level JSON files and object storage to help analyze some of the the data that's going into their database as kind of a more real time access of it. And these are these are actually massive JSON files. In fact, one of them is over 200 gigs worth. So I figured, okay. You know what? To help, you know, minimize the bandwidth cost just for once, I'm gonna download a snapshot of those locally on my, you know, somewhat beefy network storage down in the basement where I have, like, a group of 4 hard drives networked together, and that's where I do all my backup. So I figured, you know what? I'm just gonna throw it on there. That way, I can, you know, work with them in a prototyping way like any good citizen would do with bandwidth and everything like that.

So that was like, earlier in the week, and then Friday night rolls around. I'm gonna do a little hacking on it again, just kinda get to know those files a bit. Go to the path. My, laptop mounts. This is like a Samba share. But I I go to and it's run Linux, but Samba is the protocol. So in any event, I go to grab one of these files or always read it into R. It's kind of like a prototyping. And then I get a path file not found. And I'm thinking, what's this about? I know I downloaded earlier in the week, and it's alongside a bunch of my other kinda our open source, like big file stuff. And I go into the SSH session to my network storage that's running Linux. And I navigate to that area, directory not found.

Oh, no. Oh, boy. So I do a little poking around, and I realize that's not the only thing that I found. I am now not able to see some very, and I mean very important family media files, which if you guess are pictures and videos that I've recorded over the years. No. No. No. No. No. No. No. No. No. No. No. No. Now now I when I set this up many years ago, I did put in a couple backup strategies, one of which was to send my pictures over to Google Drive, albeit in an encrypted way so that I didn't feel like I was feeding their machine learning algorithms for image recognition and whatnot.

And so I quickly dust off my notes on how to interact with that. I was using a service called Duplicate to get that. And luckily enough, I did get the majority of my pictures back, albeit they stopped working after 2023, which was odd because I didn't get an alert on that. So that's one downside. But what was not on that cloud backup were the videos. So back to this internal setup, I have what's called a pooled file system of all these drives put together. And then there was an weekly job I would run that was kinda like RAID. If you ever heard of, like, how network drives are backed up in, like, a mirrored way, RAID is one way to do it. This is a different take called SnapRAID where it would, every week, funnel everything into what's called a parity drive.

And that way, if the worst happened, I could restore from that parity drive and get everything back. So I figured, oh, yes. I I did run that job. Right? Now I go to where that file would be stored. It's like this big compressed file. But then my heart sank because the last run of that, apparently, was in 2020, and I didn't know that. I still ran the restore anyway just to see what happens. And sure enough, I've got videos and pictures and other important files up to that point. So I am missing now 3 almost 4 years' worth of important content. So, of course, now I investigate, can I mount this drive somewhere else?

Can I just take it out of this network storage and put it into, like, an enclosure, connect it to my laptop, see if I can somehow salvage the files from there? That's when I realized there's the problem. The hard drive is making this sequential kind of buzzing kind of sound and cannot be recognized by any OS. That screams hardware failure. So I have now sent this to a drive recovery vendor. I'm waiting to hear back for the evaluation if they can get everything back. But if they are able to get it back, I hate to say it's not gonna be cheap to get it back. But this is a very harsh lesson, folks. If you have a backup strategy, make sure to check that it's actually working.

If you learn nothing else from this 5 minute diatribe, look at your backup strategy. Make as many notes as you can because future self will need it when you absolutely positively least expect it like little old me here.

[00:06:55] Mike Thomas:

Eric, I'm so sorry. For our dear listeners out there, I asked Eric how his weekend was, and he would not tell me because he wanted to get my live reaction during the recording. And my live reaction is is despair, but I am almost, also not entirely surprised because I feel like story time with Eric probably sways a little, towards the the bad as opposed to the good if we're taking a look at the distribution of sort of sentiment around story time with their stories. I am sorry to hear that. I hope that that hardware vendor can can help you out some way somehow and it's not super expensive but, yes, that is, I think something we all struggle with is is backing up and and storage of especially, like, family media and stuff like that. I don't have a good strategy for it.

Yeah. Oh, that that hurts my heart. That hurts my heart because I know how much work you've invested into building out, you know, your your hardware and software infrastructure in in the best ways possible and using the the best practices. So,

[00:08:03] Eric Nantz:

I'm so sorry to hear that. Oh, I I appreciate that. But life is so ironic. Right? Of all the drives to fail, the one that has a majority of these family files, files, that had to be the one. I've got other nonsense on there that I could live without. It's just out of convenience, but it had to be that one. So on top of, yeah, this vendor crossing my fingers, I will not say who they are because I'm not sure if it's gonna work or not. So I'm not giving any free advertising here. I hope this vendor actually pulls it off. But the other lesson is I was kinda living on borrowed time with some of the infrastructure around this. So I am building as we speak. The parts are coming in the next couple weeks. A brand new server that's gonna have a much better strategy around it. So there you go. That'll be some rabbit holes.

I does not surprise me at all. Best of luck with that. Yes. Thank you, sir. And, you know, one way or another, I'll I'll, share my learnings. They're they're this machine will double purpose. It will not just be a storage piece. This may be a piece where I can do some local Weaver machine learning stuff, AI stuff, or whatnot because I'm gonna I'm gonna spec it out pretty well for those workflows and media production. So it'll be a fun journey nonetheless. Just not one I was quite ready to do until Sure. This, certain event. Well, once you, install the Microsoft operating system and and turn the the Microsoft recall switch on, you'll never have to, you'll never have to worry about backing up anything ever again because it'll just watch you all the time. You know what? Yeah. What better peace of mind than to send that over to whatever they're training on? Oh, my goodness. For those of you who aren't aware of what we're talking about, that, Microsoft kinda got in a bit of a kerfuffle when they announced this feature, if you wanna call it that, where they would have a background process on your win Windows installation literally every, what, I don't know, 30 seconds.



[00:10:02] Mike Thomas:

Oh, no. I think it's less than that. I think it's, like, every second or every other second. It's gonna take a picture of what's on your your laptop screen.

[00:10:11] Eric Nantz:

I wow. I mean, just wow. It is is hopefully, some of you may think, yeah, that might have some disasters associated with it and many reasons. So they are rolling it back a little bit in terms of how they're rolling it out. But, nonetheless, the the cat's out of the bag, as they say, of what they've been planning for for this. So I think, more to come on this side of it. But, yeah, of all the things I did not expect to see, that announcement was one of them.

[00:10:40] Mike Thomas:

Likewise. Likewise. It just makes me wanna install Linux instead.

[00:10:44] Eric Nantz:

No. You're not alone. There's a lot of people making the switch after that kind of news, and Apple hasn't quite done themselves any favors either with some of their decisions. But Onto the r stuff. Yeah. Yeah. We've, digressed enough. But, in any event, we don't have hardware failures to talk about in these highlights. Thank goodness. And they have been curated this week by Tony Elharbor, another one of our awesome, curators on the rweekly team. He's been around for quite a couple of years now. And as always, he had tremendous help from our fellow rweekly team members and contributors like all of you around the world with your awesome poll requests and suggestions.

And we lead off the show with really a testament and a glimpse into the vast variety that's available to us with the nature of open source and especially some of the fundamental issues that can occur, not just from a development perspective, but also from a social and other type perspective as well. And this blog post and our first highlight comes from Kelly Baldwin, who is a assistant professor of statistics at Cal PIE, That is California Polytechnic State University for those that are outside of the US and may not know where that is. But in any event, she leads off with framing this discussion on 2 paths, if you might say, in the R community with respect to a lot of the ways that we handle data manipulation, data processing, and what has been perceived to be these separate roads traveled, so to speak. So she frames this post in light of Robert Frost, the road west traveled story.

And she also is upfront that this definitely has some opinions. Hey, Mike and our full opinions on this show too. But I, as we go through this, I definitely see exactly where she's coming from, and I think it's definitely an important discussion to have. What we're talking about here are 2 major frameworks that have kind of, over time, become pretty mainstream in terms of the art community's efforts for data manipulation. 1 is the tidy verse, which is well known by this point. Of course, the development of the tidy verse has been led by Hadley Wickham and his team of engineers at Posit.

And then we have data dot table. Data dot table was around well before the tidy verse. Data dot table, for those that aren't aware, had its first major release in 2008. Yeah. That's, got a dozen in the history books on this, And it was originally released by the core developer, Matt Dowell. And there's a link in the county's post for a nice video of what inspired they are dot table in the first place. And it you know, from near the beginning, they do that table has been so influential for many in the industry and academia, all walks of the life, so to speak, in the r community for respect to its very great performance and importing large data files.

It's got its own, you might call, DSL or domain specific language for how you conduct queries and whatnot and how you do, like, group summaries or filtering or creating new variables. It's got a syntax that is unique to it. And once you learn the ins and outs of it, it can be extremely powerful. And then fast forward about 6 years later, dplyr, the one of the cornerstones of what became the tidy verse, was released in 2014, again, authored by Hadley Wickham. And this, a lot of people point to this as the start of the tidy verse because of this new, at least new to many people at the time, way of performing these, you know, fit for purpose functions that use the data frame as, like, the main input such that you could combine this with the Magritter pipe and do all these nice kind of flowing analyses of data manipulation, you know, new variables, summarizations, group processing, and whatnot.

So with this, you now have, you know, at at this is Kelly Podsits here, about 3 major choices for your data processing. You could stick with base r. Of course, base r, it's got, you know, plenty of ways to do these type of summaries, data dot table, and dplyr. And they're all going to look quite different from each other. There has been a bit of a misconception over the years that data dot table is almost directly linked to base are. That's not quite the perception over the years that it was so perception over the years that it was so tightly coupled with base are that it was somehow going hand in hand with that. No. No. No. That's that's definitely not the case. It's got its own, you know, framework. It's got its own syntax. It's got its own paradigms.

And again, in my opinion, choice is great in this pay in this space. Now, sometimes the kinds of choices, well, which one do I go with? Well, we're not going to always use a cliche, it depends, but a lot of it does actually depend on what you want out of your data processing pipeline. But this shouldn't be a situation where you feel like you have to pick 1 and stick with it forever. Just like any project, you should have the ability to choose the right framework for your requirements or for your job, so to speak, and what's important to you.

But it's one she also talks in this post is that, yeah, sometimes people feel like they have to be loyal to a certain package through and through no matter what. No. We all greatly respect developers, but in the end, the choice is ours for what we choose in our data processing needs in this case. And you shouldn't feel bad about going from, like, a little bit of base r to, like, data dot table or maybe the tidy verse and kinda as you wish to be able to mix these together. You're not confined to one framework or the other all the time. This is entirely driven by choice here.

And so she recommends that let's be pragmatic about this. Let's not try to get into these, you know, you might call camps or tribalism of having to really be gung ho about one framework and you're gonna fit this, you know, use this hammer on every job, so to speak, no matter what. Let's be pragmatic about it. But then she gets to the rest of the the next part of the post where she talks about kind of what's the reason that this is coming about, that she wants to, you know, put this out in the universe, so to speak.

Well, in 2018, Mike, there was a certain event on our well, at the time was one of the leading spaces for kind of social type discussions in the art community on Twitter back when it was called Twitter. There was what she calls the great Twitter war of 2018, and take that for whatever irony you wanna read with that. But, Mike, just what in the heck for those that weren't around? What was this all about here?

[00:18:17] Mike Thomas:

Yeah. There were some, accounts on on Twitter and some folks that I guess decided to for whatever reason at that point in time get get fired up about whether or not you should be using data dot table or dplyr. It actually led as as Kelly, I believe, points out, to some positive conversations, between some of the the cooler heads and actually maybe the the more important folks on both sides of the fence including Matt Dowell and Hadley Wickham, I believe to the, development of the dtplyr package which is sort of the best of both worlds where you're allowed to write dplyr syntax that actually translates and executes data dot table code under the hood. So you you get your dplyr syntax, but you get your data dot table performance, which was was pretty cool. But, you know, 2018 on Twitter was a little bit of a different time.

Your timeline was actually relevant to the people that you followed and the type of content that you wanted to see as opposed to nowadays. I don't think, I see any posts from any of my followers on there but I I guess I'll I'll digress. It it was, I would like to say, a better time for the data science in our community and it was a nice place to to come together as opposed to now. We have a few disparate places to to try to do that, Macedon being being one of those places. But for whatever reason, you know, most of the discourse, most of the time was great.

And in 2018, a a few people decided to start, you know, throwing stones from the data dot table and tidyverse sides of the fence at each other, for some reason. But I I I think most of the the cooler heads out there either either stayed away and and just sort of watched with their popcorn and how silly everybody was being, or, you know, tried to, try to add their their 2¢ and make the conversation a little little more productive as I think some folks did. But, you know, that's that's social media, I guess, for you. And,

[00:20:21] Eric Nantz:

for better or worse, Mike, for better or worse.

[00:20:24] Mike Thomas:

Exactly. I think there was a lot of better, but that was one case of it it being worse. And, yeah, you know, I love the approach that that Kelly took to this blog post and I couldn't agree more. To me, not to draw too much of a parallel to, like, like, politics, but you don't always have to be in necessarily one camp or the other and just, you know, blindly believe and and associate with one side or the other. From a project to to project basis, data dot table may make more sense for your project in one project and then the next one that you go to tidyverse may make a whole lot more sense to use and that is okay.

Everyone listen. That is totally okay. You know, now DuckDV maybe, you know, the better solution than either one of those for your next project. Who knows? Like the thing that we love and that Kelly loves about the ecosystem is that we have choices and and that's incredible. And you should use whatever choice you feel makes the most sense to you, at the time and for the particular project that you're working on. So, in my opinion, you know, this is a great reflection. Very happy to hear that, there's been some grant funding towards the data dot table project that's going to allow that open source project to progress. That's something that we don't see a lot of the time where open source developers and community actually get get funding for their work and and for all the hard work that has gone into this project because, you know, if you take a look at the history of data dot table and what we're going on going on 15 year or yeah. Something like that. Right? Going on 15, 16 years of the data dot table project.

There's been an incredible amount of work that has has gone into it and helped it evolve and get to the place it is now. So it's it's fantastic to see a piece of software have that type of longevity and, you know, it it looks like that's only going to continue.

[00:22:21] Eric Nantz:

Absolutely. And, Kelly, we'll be talking a lot about this topic more in-depth at both the USAR conference coming up in July as well as PASIT conference. So I have a talk about these themes. And what's interesting here is, as you mentioned, there is funding for this recent effort and, well, through, you know, a a coincidental, situation. I was able to hear more from Kelly herself in a recent meeting, but this funding is coming from a National Science Foundation grant, that was carried out, I believe, ill, back in 2023.

In fact, we may have covered parts of this in our weekly highlights way back when the launch of what they call the data dot table Ecosystem Project. And this is being headed by Toby Hocking as well as Kelly's involved with this as well. We'll have a link to this in the supplements of the show notes here. But this is meant to not just look at data dot table from literally just the source code perspective, but to help fund the ecosystem around it, which is both technical and infrastructure and social and making sure that it is sustainable for the long term.

Certainly, Kelly is gonna be speaking about these themes in her upcoming talks, but there is one huge difference. And when you look at the history of data dot table as opposed to the history of tidyverse, is that let's be real here. Who's funding the tidyverse? It is a dedicated vendor, ieposit, helping to pay engineers full time, open source engineers to work on this. Data. Table does not have that. It is enthusiastic But like many projects in open source, it is built on the community to help drive that innovation, drive that continued development.

So I know part of the motivation of this grant is to help make sure that that ethos is sustainable for such an important piece of many, and I do mean many in the community that rely on data dot table for all sorts of uses. I mean, I just spoke of a couple of them, but I always scratch the surface. And I've had, like I said, great success with certain pieces of it in my workflows, and I don't feel ashamed about that. Like, Mike, you said, Mike, I wanna choose what's best for the job. And by golly, I'm not gonna have any regret because in the end, I gotta get my stuff done. Right?



[00:24:55] Mike Thomas:

Exactly. Exactly. And it's like, I can't say enough. It's amazing that we have these choices. It's amazing that these choices are so robust and so powerful, and so well supported. So, you know, hats off to sort of a great perspective, I think, hereby Kelly. Fortunate enough to I believe have met Kelly at last posit conference. I think she was in our our shiny workshop which is really cool. That's right. So she she is absolutely awesome and I'm very happy to see this blog post from her.

[00:25:33] Eric Nantz:

Well, we're gonna go back to the development side, Mike, a little bit because our next post is gonna talk about ways that you can at least help future you even more even when you're already buying into proper version control. And what we're talking about here is a latest post from Al Salmon who, of course, has been a frequent contributor to our weekly both as a curator and now contributor to many great highlights in this podcast. And she speaks about why you should invest in not just doing version control, but helping future you by making what she calls small and informative get commits.

Because I don't know about you, Mike, especially even now, sometimes when I'm in a tight bind and I realize, oh my goodness, I have about 10 files that I've gotta get committed now because the time's running out. I, I've I fall to temptation, hit all the checkboxes, hit stage, just say, get this a prod in my commit and be done with it. Yes. It still happens. Yes. I know better. And that's why this post is gonna help you figure out why what I did was such a big mistake. Because let's say you get to a point in time and you're either doing a code review, maybe a colleague's doing a code review for for example, and they see a line in your code.

And they're thinking, well, Eric, why did you do that? Like, what was the motivation for that line? You don't have a comment around it because at the time, you're just trying to fix maybe fix an issue or something in your head, and then you didn't put it into prosperity and via commit or a comment. Well, of course, we get you can go back and look at the history of your commits. Right? And in fact, there is a concept in Git. They call it Git blame, but I I'm with my I'll hear. I hate that terminology.

It's really not blaming anybody, but it's better to be called git explain. I like the way she phrases that because then in a platform such as GitHub, you could have the view of the code itself, I. E, the file, but then next to it, see which commit message related to that change. And you might see in that example that I gave that this mysterious line was part of like this massive 10 file commit and you just have something that says, get the prod or just get it done, get out of my hair or whatever you wanna call it.

Well, that's not gonna help very much. But had you done small informative commit messages where you do, like, one file at a time or maybe a set of related changes even if they're spread across multiple files. That get blame methodology can be very helpful for you to go back in time and figure out what were you trying to fix. Were you trying to add a new feature? What was the motivation for that? A bit of like a a way to get a lens into your development journey of what happened there. So, again, if you do what I said was a bad thing to do and just put all these files in one big commit, you're not gonna get any real benefit out of that.

But like I said, had you done a small informative message of that particular change, and again, it might be one file, it might be related files, it's gonna make it so much easier for both you and collaborators to figure out just what was your motivation for that. So that's one reason to do it. But there is another reason, especially if you realize that over time, maybe it's only like a month later, maybe it's a long time later, Something is not working, and you've got to figure out just where it started to not work.

That's where Mike Sheehyah takes us onto another concept and get that I don't do enough of, but I can definitely say if you're behind on certain situations. Why don't you take us through that? Yeah. So my Elle talks about this tool that I admittedly

[00:29:42] Mike Thomas:

do not use, this this function I guess you might call it that get offers us called git bisect. And what git bisect allows you to do is to try to identify the particular commit in history where your software was was working. And it it'll make you try some commits to help pinpoint, you know, which commit exactly introduced the bug that you're now observing and it sort of breaks your code, into 2 sections sort of before, everything was working and then after sort of everything fell apart. And it will assume that all commits are up to that particular commit that you're bisecting on were good, and it's gonna make you sort of explore, commit by commit the the the following ones to to figure out exactly, what introduced the breaking change. And it's it's a really nice feature that sort of allows us to I don't know. It almost feels like the debugger in a way in in R where you're able to sort of go commit by commit very slowly and figure out exactly which one is the one that introduced that that breaking change. And and hopefully, if you made small commits, we're able to identify that git commit that that broke things as, you know, this small diff that that in Mel's example shows, you know, 2 changed files with 3 additions and 2 deletions.

If we're very unlucky, it's going to have a message like one of those that Eric writes occasionally and I have never certainly ever ever written a commit like that that says, you know, commit a bunch of files and it's at, you know, 6 o'clock on a Friday and the the diff shows a 145 change files with 2,600 editions and 22100 deletions. We we certainly hope that's not the case but but maybe it is. But GetBisect can help you get to that point. You know, one nice additional thing that we have, in our ecosystem now is is both unit testing and and services like GitHub Actions and and formerly Travis to when you do make a new commit, you can test, you know, you can execute your unit test to make sure that everything continues to work, after that new commit has been introduced and and this is certainly something that I I think everybody that uses Git can be guilty of is is not making small enough commits. I have sort of this long standing issue where I know that I need to make multiple changes to the same file, and I will make the first change and forget to just sort of commit that small change and I will make 3 different like conceptual changes within the same file.

And now I either am at the mercy of, you know, trying to undo those particular, you know, changes number 23 and just revert back to to change 1 and and make my commit message small and and related to that. Or try to write some 3 part commit message, that, you know, sort of tries to explain that the 3 different changes that took place but it's it's not good in either way is is really ideal or easy. So the best thing that I could possibly do is to stop after I make that first change to the file, make my commit, and move on to the second change. But but sometimes, you know, when you're in the zone, it's hard. It's hard to stop and and lift your fingers up off the keyboard and and remember to do that instead of just knocking everything out at the same time. But Git blame is and it should be called Git explain. I agree as well. I think it's a fantastic tool when we do have to go back and, you know, try to figure out sort of sort of what went wrong, what is the the status of, our our software at the point in time that that things went wrong, what what does our repository look like, and how can we sort of pinpoint where things went wrong and and what led to that. I think GitLab is a fantastic tool. I admittedly don't use it enough, but I do know the people that do use it and rave about it, and find it very helpful to their workflows. So it's it's one that I think my Elle is, inspiring me to dive a little bit deeper into and to start using a little bit more. So I I certainly appreciate the overview here. I I appreciate, the introduction and to get bisect as well because I think that that is gonna be a new handy tool that I'll be able to use in these, you know, tricky, situations where where something breaks and it's not trivial to pinpoint where it took place.



[00:34:19] Eric Nantz:

I really like your your relation earlier to the concept of a good bicep and debugging your code because it made me think of a debugging technique that Bicep kinda mirrors a little bit in that if you have a long r script, wherever it's a Shiny module or any any data processing thing, for example, and you know there's an error somewhere, maybe it's a function or whatnot, but you just don't quite know where it is, A lot of times, what people will do is they will comment out or start putting the browser at, like, a halfway point, and then they'll see, oh, wait. Is the error happening before this or after this? And if it's happening after somewhere, then you know that the rest of the code above it is working. And you kinda go in this I don't know what the proper term is, but you kinda go half by half or a certain percentage.

And that's where get by site kinda starts you off with is you're gonna find that known point, at least that your conjecture is, and then you tell it, okay. Oh, yeah. This one works. I'm going to go to the next part, the next part, the next part. But none of this none of this is going to help you if you don't follow the advice from the start on these fit for purpose, you know, you know, targeted commit messages. And, again, yeah, Mike, I you've you've always been a pro with this. Like, you you don't need any advice here. But little old me, I I did some shortcuts even this year that I'm not proud of. So myel gives me a reality check that, you know, sometimes I need from time to time because I'm not perfect. What can I say? But if you wanna practice some of this in the confines of r, Nael has made an r package, which I'm going to butcher the name once again, of superlipopet or something to that effect where you can practice get concepts in r, albeit with your get client at hand to practice things like BISEC. Practice things like, you know, commit messaging or restoring or branching and things like that. So if this is all novel concepts too and you're wondering what's an easy way to kind of practice this about risking the destruction of a production code base, I recommend her pack as superwhippulpit.

That's the best I can do. But that's a that's a great package nonetheless.

[00:36:39] Mike Thomas:

That's the best I could do as well. And that it's my first time taking a look at that package, and it's incredible. It's fantastic. Allows you to to practice all sorts of git commands including git bisect, which is is really really cool. I think that's really powerful because it's very difficult to practice Git in a in a sandbox, I guess, unless you're spinning up a brand new a brand new project, but then you don't really have any history to play with. So this is a great great reference here.

[00:37:06] Eric Nantz:

Yeah. And there's one other tidbit that I've seen both her mention, although be a not so much mentioned specifically, but then I see others in the community that I follow follow this trend as well is that at the beginning of a commit, they'll have a short phrase, maybe a one word phrase that describes the intent of that particular commit. It may be fix or feature or chore or things like this. I'm trying to opt into that a bit more because that's a handy way if you're looking at your history of your commit messages to isolate very quickly to distinguish between a new feature type of commit versus a bug fix commit versus, oh, I gotta rerun that r oxygen documentation again, regenerate the man pages as like a chore kinda commit because it's a chore to do. I guess that's where that naming comes from, Mike. But, have you have you done any of that in your commit messages?



[00:37:59] Mike Thomas:

I haven't. We have some folks on my team who are are just fantastic developers who I have noticed recently do that, very much so in in all of their commits and are very consistent about it and it's, sort of a practice that I'm still maybe getting used to reading and and trying to decide whether that is something that we should try to adopt globally across the team, or not. You know, I think the idea of of creating issues and branches that reference those specific issues and, you know, obviously, your commit messages are are on that particular branch sort of lets me know exactly what is being accomplished here with maybe up without perhaps needing, all the way down to sort of the commit message level of granularity, letting me know sort of what's being worked on.

But I I think on some particular projects, especially, maybe where it's not as clear cut of an issue branching workflow and maybe we're we're making commits to a particular branch that are across a wide variety of, of types and use cases, you know, both development work, bug fixes, chores, things like that. I think that that might be possibly a great idea. So, it is something that I have have seen. I'm just not sure if I am all in on it yet, but I don't see any drawbacks to it.

[00:39:22] Eric Nantz:

Yeah. I've I've been seeing it enough where it's like I'm gonna start paying attention to it. And maybe for my next project, I'll give it a shot. But like anything I get, it's not just the the the mechanisms of doing it. It's the discipline to keep doing it. And, again, if nothing else to take away from this, be kind to future you, be kind to future collaborators, Small fit for purpose commit messages will help you. Even if it takes a while to get used to, you'll thank yourself later. And, yeah, that goes with testing as well. Don't ask me why I think testing is a good thing because I've I've been bit by that too. I'll be a nut with hardware failures.



[00:40:01] Mike Thomas:

When AI does finally get good enough, I will try to create a Mael hologramrubber duck on my desk that can just yell at me when I'm I'm doing the the wrong software development practice because I need this advice constantly and I'm very grateful to all these blog posts that she puts together to to remind me to, stay the course and continue to incorporate those best practices.

[00:40:28] Eric Nantz:

Unlike some other things in AI, that is one that I would welcome to have next to me monitoring me through many points in my dev journey.

[00:40:36] Mike Thomas:

Me too.

[00:40:53] Eric Nantz:

I'm rounding out our highlights today, at the risk of dating myself with a reference here. I feel the need for something, Mike. You know what I feel the need of? The need for speed because it doesn't just apply to one aspect of data processing. It also applies to the way we consume data coming from these robust APIs and combining that with the very vast evolving ecosystem of geospatial data and performance. Our next highlight has the intersection of all these and definitely made me do a few double takes in terms of the metrics that we're about to talk about here. In particular, we're talking about this post that comes from Josiah Perry who has been a long time member of the r community, and his tagline lately is helping r be built and used better in production.

And his blog post here is his journey on making what he calls a ridiculously fast API client in r. And his day job is working for a company called called Esris, who is specializing in putting on these certain services to query geospatial data from many different development frameworks. I believe they have some host of services as well. Well, as I'm guessing as part of his daytime responsibilities, Josiah has released a new r package called arc just geocode, all one word, the big name here, which is an r binding and our interface to what they call the ArcGIS world decoder service.

And he says that this looks like an official package from his organization, Esri, in this space. And in his knowledge, up to this point, he's got some metrics to prove it. He claims that this is the fastest geocoding library available in the entire our ecosystem. In fact, he's claims that our just geocode is about 17 times faster when you do what's called a single address geocoding and almost 40 times faster when doing a reverse geocoding lookup as compared to other packages in the community already in this space.

So let's dive into this a bit more. His first reason for why this is important is that some I took for granted until a recent project, I talked about the onset with some of this podcast metadata JSON data is that parsing JSON data is not always fast, my friend. Not always. Many of us are using JSON Lite, the, you know, almost tried and true package offered by your own ooms for, consuming and writing JSON data in r. And it's it's it's been a huge help to me. Don't get me wrong. But it does have some performance issues, especially when dealing with long text strings in your JSON data and then bringing that conversion back to an r object and the other way around.

Well, guess what? Another language that starts of r comes at a rescue in in Josiah's initiative because there is a rust crate, that's kind of their term for packages, called SERDAY or SERD, I'm not sure if I'm saying it right, that handles both the serialization and deserialization of structures and rust. And guess what? It is very tailor made to JSON data too. It just expects you at the onset to help specify for your given JSON type data you want inside your your session, the specific attributes of each variable or each field.

So in his example, he's got an address structure, if you call it, with all the different fields that would come out of this JSON data. And most of them, big surprise, are strings, but it'll give you the ability and rust to really determine what's supposed to be a string, what's supposed to be an integer, different types of integers. You can go, you know, really specific on all these needs. Now that's great and all, but how does that help with r? Well, there is the package called extender, which is kinda like the RCPP, if you will, of Rust to r.

We'll let you tap into a Rust crate and bring that into r and be able to go back and forth with our functions to basically call out to these lower level Rust parts of the paradigm. So this, Serde JSON crate is a massive speed improvement in general for importing JSON data, serializing JSON data over the community counterparts as he attests to in this post. But that's not the only part. The other part is a feature that I should have known about in my research in HDR 2, but it just went over my head until this post, is that we often use HDR, HDR 2, and this is the regeneration of that, if you will, by Hadley Wickham to help in r perform these, you know, requests to API services. I get request, post request, and whatnot.

And, usually, I do these requests kind of 1 at a time, if you will. Like, there's a get request. I'm gonna do that with specific set of config parameters, bring it back down and whatnot. Well, how in the world did I not know about this function, Mike? Rec perform parallel,

[00:46:29] Mike Thomas:

which lets you do it in more than one time. Oh my gosh. Where have I been, Mike? Did you know about this? Neither did I, Eric. I had no idea that this was part of the h t t r two package now, that we have the ability to send, I guess, paralyzed request to an API pretty easily and maybe point that function towards a connection pool or a few different nodes, few different workers, to be handling requests concurrently. This makes me think of, I don't know, potential integration with the crew package or something like that.

Perhaps this is something that you could do on your your own machine and and leverage, you know, the the ability to parallelize your own machine and and send out a few different requests, at the same time which is is pretty incredible. And there's this Twitter thread that Josiah puts, earlier on in the blog post where he's, you know, he's he's showing off the fact that he was able to geocode, 2.6 1,000 addresses in in 3 and a half seconds. And and somebody asked, in as a reply to that particular screenshot that he put up, they said, isn't geocoding always bottlenecks by the server?

And, that is true. But I guess when you have multiple servers to span the job across and we have this beautiful function request perform parallel that I imagine brings it all back together at the end. That's that's a fantastic utility to be able to, you know, speed up, significantly and and this is something that I've dealt with in the past as well, geocoding. It takes time because you're typically going address by address and, you know, a lot of projects you'll you'll want to be geocoding your entire data frame which could get, you know, could get particularly long you know thousands tens of thousands hundreds of thousands of observations that you need to geocode so it's really interesting to me that we have the ability, you know, not only to leverage the speed of Rust, for this serializing and deserializing of JSON objects which is time intensive, but also, being able to just break the job out into across a few different, nodes here which is is pretty incredible, something that I didn't know about. Obviously, Josiah says that you need to be a little bit careful when you are doing anything like this especially when you're sending sort of paralyzed paralyzed requests at a different service that you don't perform, an accidental denial of service attack, you know, which is where you you make way too many requests all at once to the service and bring that down. So you don't wanna be the person that does that. So so certainly be be careful and perhaps, you know, use a small number of workers.

If you're just geocoding a couple thousand addresses, you don't need to spin up a few hundred workers or something like that to be able to do that just just spin up a couple and and maybe wait those extra few seconds that it's going to take, to be patient between, you know, spinning up a few workers and and many many workers. But it's really interesting because you will get, you know, these this great sort of orders of magnitude returns in performance, when you do add compute to the problem here as well as, you know, leveraging Rust for that that quick deserialization, of of JSON data. One thing that I will also shout out just because I was curious about this and, you know, we're finding all of these new, faster programming, software utilities out there when it comes to to data.

And there is a DuckDb extension as well for handling JSON data and reading JSON data and so I would be interested to see how that compares to what Rust can do for parsing JSON data as well.

[00:50:16] Eric Nantz:

But both very fast, I'm sure. Yeah. They are. I'm experimenting that a little bit too actually in my DuckDV adventures and I'll definitely come back with my findings as I get that more polish. I'll be at the hardware catastrophe, put a stop on that for a bit. But, nonetheless, I'm really intrigued by the concepts that, Josiah, parts in this post here. And then at the end, he recommends additional packages in the community that if you're still not quite ready for the plunge to Rust for some of these efforts, if you're not ready to build, like, an internal JSON parser that depends on Rust, There are some additional JSON related packages in the r community that definitely take a look at. He recommends the RCPP SIMD JSON package as a great way for parsing data as well as the y y JSON package that comes from the affectionately known cool but useless on the our community. He's got a great package called y y JSON and also JSONify, which I don't have as much experience with. But you might wanna take a look at each of those too and see it for your parsing needs or writing serialization needs. If that just gets you good enough without while being able to stay in the confines of of R directly.

But, nonetheless, this is great learning for me personally as I'm starting to deal with JSON data more and more in my open source work as well as my internal day job stuff where we just built an API binding package. But we're running this in the day to day use with, patient level data set where there's, like, multiple visits, multiple, obviously, patients in a treatment group, and it's basically doing these get requests to this custom service 1 at a time for each of these calls. My goodness. I could do the rec perform parallel and probably speed that up a bit. So I'm gonna have to take a look at this and hopefully not make my IT admins angry when I start to paralyze this.



[00:52:14] Mike Thomas:

Definitely do it on a, development cluster to start. That would be my recommendation.

[00:52:19] Eric Nantz:

Yes. Because, yeah. That that was another, story time from years ago when when I, was too greedy with multi cores, and I took down an entire HPC cluster. Don't be that person, everybody. Just don't be that person. I remember that story. Yes. Who lives in infamy. But, what lives in good ways, of course, is the rest of the our weekly issue itself. Tony's done a terrific job as always he does with the rest of the issue. We got so much we could talk about, but we're gonna have to wrap up here with our additional fines over this issue.

And, Mike knows and many of you know I am a retro gamer at heart, not just with retro games, but also I have fond memories of going to the local arcade and putting some quarters on those awesome pinball machines that waste a lot of hours on that. Well, there is a fun post that combines things like, geospatial data, API calls, and some handy visualizations from Rasmus Bath. He calls this the public pinball machines per capita looking at where pinball machines are most distributed around the world. And and as post surprise, surprise, the United States is in the lead with this, but Europe is not far behind in some of their visualizations.

But his post goes into how he attained this data, loading the packages. And guess what? It's coming from an API just like everything else. He's got some, you know, really elegant code to do this. Really modern code here. They was using HTR 2 as well and some great handy visualizations built in ggplot for the distribution of these, pinball locations and some word clouds too. So really, really fun stuff here. And, yeah, it makes me get the itch to get some quarters out and find that retro arcade to, you know, knock out some pinball action. So that was fun. But, Mike, what did you find? Very cool, Eric. I found an oldie but a goodie. I think at the the recent,

[00:54:21] Mike Thomas:

our conference that's held in New York by that under Lander Analytics hosts, there was a panel discussion about your favorite R package. And I believe it was, who's the author of the data science Venn diagram there? Drew Conway. He was being asked what his favorite R package was and and I think it was one, it was called iGraph and I think it's one that's been around for a long long time, for doing, I believe, network analysis type stuff in, r. And I saw on the highlights this week that r igraph has crossed the 2.0 threshold.

So it's it sounds like, you know, similar to data dot table that this is another package that has had quite a life to it, continues to be very very well maintained, and it's it's just awesome to see a package like that be able to have the longevity it has.

[00:55:13] Eric Nantz:

Yeah. IGraff, I have tremendous respect for. It is the engine behind many, and I do mean many, of the r packages that provide some kind of network visualization or processing. My exposure to this has been using some custom eye graph bindings inside a widget called Viz Network, which I have a kind of a love hate relationship with, but I've done enough customization to wrangle it to my particular needs. But, yeah, iGRAS been around a long time, and it is prevalent in additional languages as well. But, yeah, huge congrats to that team for the 2 point o release.

And I know, yeah, my whole biograph stays around for a long time because it is a fundamental pillar in these, network visualizations. So awesome

[00:56:01] Mike Thomas:

awesome call out there. I grabbed 0.1 released in 2006. Even older than the data that table.

[00:56:08] Eric Nantz:

My goodness. Yeah. As if I didn't feel old enough in this post. Good grief. What are you what are you doing to me, man? I I I need to get a youth injection or something. But what makes me feel youthful is just seeing what's happening in this community. Again, really great developments happening. Really great showcases of data science, you know, providing good to everybody involved. So where can you see this all? Where we've been talking about here, so many of our great wings are available at rw.org.

Many additional findings here, many additional developments of packages. We need to touch on all the newer packages out there. There's so much happening in these spaces, but we invite you to bookmark that site if you haven't already to check it out on a weekly basis. And also, we love hearing from you in the community. There are many ways to get in touch with us. You can send us a contact or message on our contact page, which is in the episode show notes. So, again, keep those comments coming. We love to hear from you.

You can also, in the modern podcast app, such as PowVerse, Fountain, Cast O Matic, Curio Castor, there's a whole bunch out there. You can send us a fun little boost along the way if you want to send us a message without any middle thing in between. And no harbor failures on that one either. But and the other way you can contact us is on social media as well. I am mostly on Mastodon these days with at our podcast and podcast index on social. I am also on LinkedIn. Just search for my name there. You'll find me there and sporadically on the weapon x thing with FDR cast and a special little shot. I didn't get to it in my stories time. But I had the great pleasure of having a, little dinner with, the author of GT, Rich Yone, who was in town for a local conference. So great fun talking with him, all things are, and someone he's up to. So, Rich, if you're listening, thanks for coming into town even for a brief couple of days. A lot of fun learning from you. But, Mike, we're gonna listen to get a hold of you. That is awesome. I am so excited for POSIT conferences here to be able to see some see some community faces. But, folks, until then, can get a hold of me on mastodon@mike_thomasat

[00:58:20] Mike Thomas:

fostodon.org or on LinkedIn if you search Ketchbrook Analytics, ketchb r o o k, you can see what I'm up

[00:58:28] Eric Nantz:

to. Yes. And, hopefully, I don't see any messages about any failures in your infrastructure. Let's call it that. Let's keep keep our fingers crossed. But, yeah, just don't do what I did. That's the best advice for today. But in any event, we thank you so much for joining us wherever you are and wherever you are in your our journey. We hope that we're a little bit of a help along the way. And one of the best ways also to help us out is just share the word. You know? If you got value out of this podcast, we'd love for you to share it around, tell your friends, and always great to expand our audience for those that are getting started with our we think our weekly is one of the best resources for anybody in this community. Am I biased? Absolutely. Yes. But, hey, I speak I speak the truth here, I think, in any event. We're gonna close-up shop here, and we'll be back with another edition of our weekly highlights next week.

"
"51","issue_2024_w_23_highlights",2024-06-05,35M 15S,"How vintage features in R could introduce chaos in your quest for a tibble & data.frame function, and the awesome potential of integrating custom parameters and conditional processing in your next Quarto workflow. Episode Links This week's curator: Jon Calder - @jonmcalder@fosstodon.org (Mastodon) & @jonmcalder (X/Twitter) Make your functions…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 167 of the R Weekly Highlights podcast. This is the weekly show where we talk about the awesome resources that are shared every single week on the R Weekly issue. My name is Eric Nantz, and I'm delighted you joined us from wherever you are around the world. And, yep, we are in June already. It's hard to believe, almost midway through the year. So as usual, I can't do this shindig alone these days. I am joined by my awesome cohost, Mike Thomas. Mike, how are you doing this today?



[00:00:33] Mike Thomas:

Doing pretty well, Eric. I've been paying attention more and more to your favorite sport, hockey. I know your red wings aren't in it, but, Canada could have the cup coming home for the first time in a long time. Shout out Connor McDavid's goal the other day. That was incredible if you haven't seen it, for any sports heads there. And, my my local Boston Celtics are are in the basketball finals so that's pretty exciting over here. But other than that, it's all it's all data science all the time.

[00:01:03] Eric Nantz:

That's right. And, yeah, the the the nation of Canada is gonna have this kinda carrot dangled in front of them once again for the first time since almost 2006 for that team. Who knows? Who knows? They're going against a mighty tough Florida team that was there last year, and they got an education, and they've been on a mission ever since. So it'll be a it'll be a fun one to watch for all you hockey fans out there. But, yeah, congrats to your Celtics. I think it is their year. I would have put a lot of lot of stock into what they've learned over the years, so it should be a fun final. We'll see. We'll see. I gotta

[00:01:38] Mike Thomas:

see, any of the r, you know, sports probabilistic, you know, models

[00:01:44] Eric Nantz:

out there. I gotta I gotta see what they're saying. Yeah. It's good. I think it's gonna be kind of a toss-up depending on how hot that, certain Luca gets in the in the Dallas side. But it's gonna be gonna be fun to watch nonetheless. But in any event, we're gonna hear talking about all sorts of things of our data science today. And our curator for this episode or this issue, I should say, is John Calder, another great contributor in our Our Weekly team. And as always, he had tremendous help from our fellow Our Weekly curators and contributors and members like you all around the world with your awesome poll requests and suggestions.

And we lead off today with, a great post about if you've been using, say, the tidyverse align your data science analytics and other work, and you've been starting to harness those principles and maybe a new package at your organization or making some functions that are consuming datasets as their main input, this first highlight today is a great primer for how you might wanna make your functions a little more compatible, so to speak, with the main building block of the tidyverse, data processing object type, albeit the Tibble.

And this post comes to us from Hugo Gruson who is a research software engineer in epidemiology. And he leads off this, very succinct post with motivating the situation. 1st, a bit of those here that aren't aware, the Tibble is what you might call a reimagination or or a slight update on the typical data dot frame object and base r, which, of course, holds your rectangular style data. The TIBL does a little things under the hood to not so much replace data frame, but, add some either safeguards or more kind of quality of life enhancements on top of it. The biggest one that's obvious to you, especially you've been a veteran of r for a few years, is that if you have a large data frame and you print that in your console, oh, you're probably gonna have a wall of text just spinning down all throughout because it's gonna print that whole thing.

Whereas a Tibble has a modified print method, whereas I'm gonna show you the first, like, 10 rows or so and then the names of the columns. So, hence, it's a good way to kinda get a quick glance at your dataset without overloading your console. That's just one of the minor differences. But we're gonna talk about some differences under the hood that could affect you if you're writing a function that takes a data frame or a tibble as input. And in case in point, there are, there is a big difference here. And that's the first part of this post here is that when you subset for a column and, like, a particular column of observations in a data frame, When you look at that under the hood, you will get a vector right away if you use the bracket notation of, like, the name of your data frame, the opening bracket, a comma to say get all rows, and then either the name of the column or the index position of the column, you will get a vector of those observations right away.

With a tibble, not so much. With a Tibble, if you do that kind of notation to grab a column out of it, you're actually gonna get a 1 column Tibble back instead of a vector. That can be a bit of a problem if you're gonna make a function that does some kind of, like, single column selection. And that function, like in the example here, a mean function is expecting a vector of observations, not a 1 column data frame. So his advice in this first example is that you can't rely on that single column subsetting having that default behavior of a data frame of what we call drop.

Drop meaning, like, drop the data frame class and go straight to vector. Don't don't hedge your bets on that, so to speak. So in his example, he explicitly mentions or explicitly states a parameter in that bracket style subsetting of the column with drop equal true. Then you're guaranteeing that no matter if it's a data frame being fed into this function or a tibble, that you will get that one column back as a vector of observations, a vector of numbers instead of that 1 column data frame. I have been victim of this in the past when I was writing some packages that albeit didn't use Tibbles in their main package code, but then I was using Tibbles as a way to put some nice kinda convenience functions on top of the pipeline.

And that drove me nuts when I just wanted one variable instead. And then suddenly these functions I was doing that was doing, like, p value derivations was just spitting errors at me because it was getting a data frame inside. Like, I couldn't believe it. Sure enough. I was I was running into that little mini inferno about the drop the drop situation. So sage advice here from Hugo on that because I think that trips up a lot of people when they're starting to make functions that are using Tibbles as an argument instead of just the data frame itself.

But that's not all, Mike, because there's another concept that, admittedly, both you and I have never really paid a lot of attention to, but that could wreak havoc on your function as well.

[00:07:24] Mike Thomas:

No. You're exactly right, Eric, and and I just wanna go back to to your topic really quick. I think anybody who's ever probably ever developed in our package has been bit by this this drop equals true versus drop equals false behavior, you know, where with a sort of plain vanilla data frame, you're getting a vector out and with a Tibble, you're actually getting a data frame out, instead of, you know, the vector of the values of the column that you're you're looking for. So if you haven't been bit by it yet, it's this is one to, watch out for. And and another one to watch out for, Eric, as you're alluding to, is the concept of partial matching, which I must have seen, you know, somewhere along in my journey but I do not remember this and it's blowing my mind that this is even a thing.

And what we mean by partial matching is when you're using the dollar sign operator on a data frame to extract a particular column from that data frame. And we'll use like empty cars as an example here. If you say empty cars dollar sign s, it will pull out the speed column because it's going to use partial matching to try to guess which column you were referring to. And you don't necessarily have to specify exactly the name of that column to get it out. To me, that's mind blowing. The the fact that we can do that, I'll be totally honest with you, I don't think I really like that. It does not feel safe to me. I'm surprised that it it still exists. I'm assuming it still exists and, you know, our 4 dot 4 dot o.

You know, that the latest release, may maybe it's 441 that's out there now. I imagine that this was something that was implemented in the early early days of r, you know, thinking that that maybe it's a good idea. We wanna make, you know, this language and the syntax as user friendly as possible. But to me, oh, man, it gives me the heebie jeebies and and makes me feel like there's a lot of more opportunity for disaster to strike here than there is for efficiency to be gained. But, you know, the the idea along this section of Hugo's blog post here is is definitely do not rely on this partial matching, idea. I don't know too many people who who do rely on it but if you do, it's something that you should certainly watch out for. You can actually set some options if you'd like in your environment to ensure that that partial matching, you know, can't be can't take place.

And one of these options is the warn partial match dollar argument, which you can set to true in your dot r profile file, for example, which will essentially ensure that you get a at least a warning, that a partial match is is taking place and that warning will appear in your console. You can do the same thing. You may wanna do the same thing in your your test that folder so that your your unit tests throw an error, or a warning at least give you that flag if there's partial matching going on that you're not necessarily expecting, which I think can be a great idea.

And if you you do really need to support partial matching, obviously, this partial matching concept is is only implemented on the the vanilla data frame side and not necessarily implemented on the Tibble side, but you can, manually handle it. And Hugo gives an example here of using the the care match function, c h a r match function, to be able to sort of fuzzy match, partial match, a column name, that you're looking for and sort of have the same exact user experience as you would have with this partial matching against a a pure vanilla data frame, within a Tibble environment. So there's a nice little code chunk here if you're interested in doing that. But, again, you know, it's this is just my opinion only. You know, we're full of hot takes on this podcast and my hot take today, I guess, is be careful of any partial matching that's going on. And I think this is a great blog post in general to just warn you about some funky behavior that can take place between data frames and Tibbles that that you may or may not have been privy to. And it's it's definitely something that could easily, easily trip you up in a script that you're writing or a package that you're writing. So it's great stuff to look out for. Hugo also alludes to and provides a link to a longer vignette that contains all the differences, between Tibbles and data frames. And I think data dot frames and that, vignette is from the tibble Tibble package itself.

One of the fun pieces about this blog is, that it Hugo continues to use, the phrase compatible, which, you know, is a really nice pun that that rounds out this blog post, I think, beautifully. So a great top to bottom, summary of these sort of 2 funky behaviors between data frames and Tibbles. And I I definitely think it's a good reminder, warning to those of us out there to to be on the lookout, stay vigilant for, these differences.

[00:12:34] Eric Nantz:

Absolutely. And even had I known about partial matching back in my very early days of of using r back in grad school, I probably wouldn't touch over 10 foot pole. I am just one of those people that on the side of caution, on the side of verbosity and specificity. And I'm gonna put the full name of that variable no matter how long it is because in the end, a, there are many ways in programming, not just in ARB, but in programming in general, to just whack yourself silly with, you know, user inflicted issues already.

Last thing I need is partial matching to wreck havoc. And I do think it's kind of a you might say, I don't know, relic's the right word, but it's a it's a it's a snapshot in the history of r and s itself because let's think about what was not around when r and s were built. We didn't have modern IDEs and modern frameworks that would support auto completion in an intelligent way. Like, if you're looking at a TIP or looking at a data frame and you wanna get the name of that variable and you start typing the name of the data frame, dollar sign, and start typing the name of that variable, Our the modern editors like our studio and even some of the plugins you get in Versus code and whatnot will let you auto complete the name of that variable. So you don't have to very rely on partial matching in those situations because the ID is gonna give you a helping hand to help complete that that full name. And also within the tidyverts itself, packages like dplyr, tidy r, and many others under the hood, use a package called tidy select that will let you grab related variables based on maybe, like, the prefix of their name or maybe their position or other other ways to intelligently search for these. And, again, what you'll get back is the full name of the variable. It's just a convenience way for you to get there.

So, certainly, I think the modern tooling and the modern principles in the tidy verse hopefully make it that you don't have to support partial matching. But if you have some legacy projects where you do, this post has definitely got you covered on those as well. And then, also, Hugo throws some interesting nuggets at the end here with respect to testing if you have to really rely on data frames and Tibbles being, you know, jointly supported in your function, he has some ideas and some resources that you might wanna use to help make that testing more efficient.

He has some plugs for both the Patrick r package, which lets you do parameterized tests with test that, which I didn't know about, as well as this auto test package. And that is basically a way to help scrape your r packages, like example code for, like, your functions. And then intelligently or dynamically, I should say, kinda change inputs that kinda do some, you know, test generation based on that. So 2 little nuggets there if you wanna really be robust with not just equipping your functions for the support of tibbles and data frames, but also helping yourself as a package author to test for these differences in a more systematic way. These two packages could help you out quite nicely there. So lots of knowledge dropped in the short post here.

And rounding out our highlights today, we've got another great showcase of the power of the quartile documentation engine with a k a use case that'll be very nice for those of you in the educational sector or building maybe internal our trainings that wanna leverage this novel technology. And we have this highlight coming from the esteemed Nicole Arany who is a frequent contributor to the rweekly highlights in the past. And she talks about her solution that combines the concept of parameterized reporting in quartile with dynamic and conditional logic inside your quartile document itself to hide or show certain pieces of elements. And the use case here is as she wanted to create exercises and tutorial material for her students to work through in a workshop session. She's actually taught many workshops that have been shared in the art community. I still wanna thank Nicole once again. You know, I thanked her many times before for awesome workshop and machine learning at last year's our pharma. She did a tremendous job with that, and she built up the materials using quartile, so timely here as well. But in case of this example that she's talking about here, she had a set of goals that she wanted to achieve here is that the the document would have a series of questions that the users would have to, you know, write some r code or use r to help answer.

And then a companion document to this that would have not just the questions, but the solutions to the exercises as well. Again, very, familiar concept to anybody in the educational sector. And this approach would not just work for web based formats but also work for static formats as well, such as PDF output. Hence, if you have to do hike a paper version for every reason, I've don't ask me about why I've been looking at static outputs recently. We'll save that for another episode. But in any event, the first part of this post talks about what are parameters inside a portal document.

If you used Rmarkdown before, this will actually sound pretty familiar to you because it works basically the same way. In your YAML of a portal document, you can have a field called params, and then inside it, you can have a series of 1 or many of these what look like key value pairs, only they're variables that you might give it a name like year. And then in colon, after the colon, you give the actual value of that. So you could define as many of these as you like. And then this sup this is supported both with the knitter engine as well as the Jupyter engine. So no matter if you're on the r or Python side, you can use parameters just as easily in each framework.

And this has been a hugely helpful technique, especially if you're doing automated reporting, maybe on a ETL or another processing pipeline where you've got the same report, but it's changing like a subset of the data or it's changing a key variable, but yet the rest of the outputs are gonna be generated in a similar way. Parametized reports are a great way to, make that happen. So in the case of this tutorial example, she has a parameter called hide answers, and the default of this is true, meaning that in the document, it's gonna be one document.

But here's the kicker is that in the document, she has put what a what we call these fenced identifiers around the type of output that will only be shown if one of these parameters is set to true. So she has this in a very interesting way with both the answers being shown as well as if you're doing PDF output to only show when the output format is is PDF in the case of when she renders it. But this is all combined very nicely in her quartile chunks of the code where she's gonna dynamically put in whether to put in these fence identifiers of what they call content hidden based on an inline r code snippet, where if that parameter of hide answers is true.

This is genius here. I love this technique. I, in fact, I was literally thinking to myself, how could I do this of a portal dashboard to do certain styling elements or certain pieces of the card, you know, styling if I had a parameter that was, like, longer length than another one. This is the key to make that happen. An inline little snippet that says, if params dollar sign hide answers, then put in this fence div of content hidden. And then you can still have the answer in one place. It's all in one document. She didn't make separate documents for this. Hence, we've we're applying the the dry principle here. Right? Don't repeat yourself. Don't make a separate document just for just for one snippet. This is this is terrific stuff. And then in the end, you see an example where one printout of the screenshot is asking the question and then another printout with hide answers false is showing the actual r code to help achieve that answer.



[00:21:34] Mike Thomas:

Yeah. It it's pretty incredible. I would say that Nicola is probably one of the leading voices on all of the different gymnastics that you can do with with quarto and especially when it comes to, you know, not repeating yourself, making things as reproducible as possible and making things as easy on yourself as possible. And this is a another fantastic walk through and I I do just wanna before I forget, thank Nicola for these many quarto blog posts which have honestly helped me and saved me hours, in a lot of the quarto work that that we do at Catch Brook.

So in the conditional content section of of her blog post, she leverages, again, one of these sort of fenced divs and you're able to specify, you know, when a particular format is being rendered to take this action. So the best place to take a look at this is is in the blog post but within this fence div, she uses the dot content, hidden argument followed by the when format argument and specifies when format equals PDF, then the content is going to be hidden. So it's a pretty powerful little utility that we have in quarto to be able to take in a conditional action based upon, the type of format that's being rendered to. So very very useful, I think especially for folks who are rendering documents to different formats because let's be honest, you know, sometimes when you're rendering to HTML, some element, of your quarto document is going to look really nice and when it gets rendered to PDF either it's going to break or it's just going to look terrible because you know they're 2 completely different formats.

Maybe you're taking advantage of you know some HTML content that you know really only belongs on a web page and doesn't belong on a static document or or vice versa when it comes to PDF. So this is a really powerful, little argument within this fence div that we can create that I think is excellent. And then maybe the last thing that I'll point out is, you know, that there are multiple ways to render your documents. One is from the command line, which is, you know, utility that that I'll use quite often as well. But if that render call gets a little bit lengthy, you know, for example maybe you have a few different arguments that you're trying to specify.

You know, one maybe being the the output file name if you wanna rename that file, something different than the naming convention that you used for the dot QMD document. Maybe that's something that you wanna specify or if you have parameters, you can use the the, params flag of that command line call. Alternatively, you may wanna use, the quarto r package itself and leverage the quarto render function and specify these different arguments, you know, a little nicer than this this really wide, command line call that you might have to be making, if you are trying to set these different flags and specify these different arguments and output file names and things like that. So at the end of Nicole's blog post, before the additional resources, she has 2 code snippets for how she rendered, you know, the questions that she created as well as the answers document that she created in these 2 different, our calls using the the quarto render function from the quarto r package which you may be interested in in taking a look at. And the only difference between the 2 is the execute params, execute params, argument is specified such that, you know, for the questions hide answers parameter is set to true and for the answers, the hide answers parameter is set to false.

So that second time that she knits this document, the answers will be displayed. So, she'll create, you know, 2 different documents here which is is fantastic and a great, you know, top to bottom run through on how she went about doing this, all the different considerations and options and fence divs and parameter options that we have in quarto to make things as as flexible as possible, not only across, you know, 2 different outputs that she wants to create but even multiple document types. So great great walk through.



[00:25:41] Eric Nantz:

Yeah. And, like, echo your your your gratitude and thanks to Nicole for these terrific resources because I was able to use some of her previous materials as well that combined principles that we see in this post here for my recent, our consortium submissions working group, efforts where we have a document that we have to send as part of this bundle that we transfer to regulators. We call it an analysis dataset reviewers guide or ADRG. And I wanted to try something different with this. I could've taken the the typical route and just do a do a Word document, type it all for being was like, no. No. No. No. We're in 2024. Let's do something better here.

So it's a portal document, but it's got 2 formats. Because as much as I'd love to be able to submit the HTML format, we can't really put that in the bundle, so we do a PDF as well. But we do have conditional logic throughout some parts of the document that we know that for PDF, it's not going to quite render as nicely with the default font size. So I'll do some code chunk updates based on the output type. And then that way, I have a preview version of the ADRG linked on, you know, an HTML static site hosting area. And then the reviewers can look at that, you know, as as they give me comments and whatnot. But then the actual bundle that we transferred to them, which hopefully be this month, we will be able to supply the PDF version, and it's gonna look, you know, very good, very polished.

But, again, the source is all from one place. We just have conditional logic on top, but still preserving the majority of that source going to both output types. So I do think this is this is a very novel concept, and parameters can you know, we're just scratching the surface of what is offered here. At the end of Nicole's post here, she has doing some terrific additional either blog posts from Mike Mahoney or JD Ryan's recent parameterized quartile reporting workshop as you gave for our ladies a while back. There's some great additional learning material, and I can't stress enough how awesome it is to have, like, a data frame of, like, parameter values in each row, maybe a different configuration of it. Just do a, per walk of that. For each row, run portal render. You got your outputs all generated within about 10 seconds. I did that yesterday, and it was a thing of beauty, my friend. Just a thing of beauty. Music to my ears, Eric. I I love nothing more than the per walk over that that nice little data frame with all your different parameter configuration. So thank you.

Yes. So so once you combine all this stuff, it's like, yeah, I always equate it to being like a a chef that can actually cook because I'm not a chef that can actually cook very well. But with programming, I like to think I can mix and match these quite a bit but I'm certainly not alone. We're learning from the call and many of the other thought leaders in this space here. And it never stops learning when you look the rest of the art weekly issue, and John has done a tremendous job for the rest of the issue here. So we'll take a couple of minutes to talk about our additional fines that caught our attention for the week.

And going back to the concept of Tibbles and concepts of, like, the building blocks of the tidy verse, There were some some great posts that were illuminating a concept in tidy yard that I did not know was possible. There are a couple posts here from Lael Lecri on the use of a Tidyr function called packing versus the concept of nesting, which we often hear about more fully with respect to creating your Tibbles. And if you're like me, when you first saw this issue or thinking, packing? What is that? Well, you can think of packing as, like, a slightly different take on how the data frame is being collapsed.

Nesting is kinda like doing a row based collapsing. You're gonna have maybe a set of columns that become their own table, but it reduces the number of rows in your data frame as a result. We often do this in group by processing and whatnot. Packing is kinda taking the opposite approach. It's gonna keep the same number of rows, but then it's gonna make a table out of various columns, and then you can unpack it to make it wider again. And so there these two posts are gonna talk about the comparisons of the 2, when you might wanna use 1 versus the other. And in particular, I mean, I still have a lot to learn in this area here, but she did eliminate some interesting examples. If you have jason type data, why am I want to look at packing versus nesting and those concepts? So again, just an illustration that I always think I I know most of the tidy verse, but there's always something I don't know. So we're all we're all learning together on this. But, Mike, what did you find? So I found that the Geo Aero package, I think, has its first release on CRAN. It's version 0.2.0,

[00:30:39] Mike Thomas:

which I am super excited about because I am a huge proponent of, the arrow package and the parquet file format, and we are getting some utilities in R to be able to wrap the geoparquet project, which allows you to work with GIS data that's stored in in in parquet format for just huge efficiencies over, you know, traditionally, you know, large heavy datasets when we talk about geospatial data. One more blog I wanna shout out is a blog from Athanasia Mowinkle which is called Customize Your R Startup Message. And it's a really nice little walk through about how to give yourself, a little message if you'd like, when your R session starts up each time. So a really cute little fun one for anybody interested.



[00:31:27] Eric Nantz:

Yeah. We we need a little variety in life. Right? I mean, I have all my my friends in the Linux community that do these fancy shell prompts and start up messages for their servers. I'm like, I need some of that for my artwork. So yeah. Well, that post is gonna gonna have me cover there. So yeah. And and awesome. Congrats to the Geo Aero team because, yeah, we're seeing immense immense, you know, innovations in the world of parquet with, spatial data. So this is just another tip to the hat, and, yeah, what a great time to be part of that ecosystem.

And it's a great time to see the rest of our week as well and for you to get involved as well because this is not a project that's just run by a few individuals on the whims. This is based on the community, for the community, and by the community, if you will. So the best way to help us out is to contribute to the issues directly via your suggestions for new resources, new packages, new tutorials, new workshops. We are fair game for all that. That will help your community. How you do that? Every issue at rwicky.org has a link in the top right corner for the upcoming issues draft. You can just click that, and you'll be taken directly to the poll request template where you can put in markdown syntax, that great link, that great resource that you found, all marked down all the time. We live marked down. We love it. Hence, Hence, some quartiles have been great with markdown as well. So all just a poll request away to get your resource in the issue.

And, also, we love hearing from you and the community as well. We have a few ways to get in touch with us directly. One of which is in this episode show notes. We got a link to the new ish contact page that you can send us a message on there. And I did hear back from a listener that wants to do some interesting, I guess, meta analysis on our past show notes. I'm gonna, you know, give that's gonna be exciting. So I'll definitely get in touch with you about that. And, also, you can also send us a fun little boost in the modern podcast app if you're listening to, say, paverse, Cast O Matic, fountain, many others in this space. So you can send a little boost directly in the podcast app, and we'll get that message directly to us about any third party middle person in between.

And speaking of podcasts, I guess I had a busy, couple of months recently because I was on another show recently hosted by my good friend Barry at Pod home talking about my adventures of podcasting and data science, and I'll have a link to that in this episode show notes. That was that was great fun to join Barry on that on that episode. So I will link to that from the about podcasting show. That was great fun. And, also, you can get in touch with me on the social medias these days. I am mostly on Mastodon.

We've got our podcast at podcast index.social. Doing a lot of fun dev work now behind the scenes with the podcasting 2.0 stuff and getting real gnarly with JSON data. It's been a fun learning adventure to say the least. I'm also on LinkedIn. Just search for my name. You'll find me there as sporadically on the weapon x thing with at the r cast. And, Mike, where can the listeners get a hold of you? Sure. You can find me on mastodon@mike_thomas@phostodon

[00:34:43] Mike Thomas:

dotorg, or you can check out what I'm up to on LinkedIn if you search Ketchbrooke Analytics, ketchbrook.

[00:34:52] Eric Nantz:

Awesome stuff. And, yeah, it's been a nice tidy episode that I think, judging by the fact that I have kids at home, I gotta get myself out of here. So we're gonna close-up shop here in this episode of our Wriggle highlights, and we thank you so much once again for joining us from RevAir around the world. And we hope to see you back here for another episode next week."
"52","issue_2024_w_22_highlights",2024-05-29,50M 17S,"The recent patches in R that pave the way for a future object-oriented-programming framework to accompany S3 and S4, a treasure-trove of open spatial data ready for your mapping visualization adventures, and a collection of tips for the next time you refactor your testing scripts. Episode Links This week's curator: Jon Carroll -…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 166 of the R Weekly highlights podcast. If you're new to the show, this is where we talk about the latest happenings at our weekly every single weeki highlights podcast. If you're new to the show, this is where we talk about the latest happenings at our weekly every single week with, take on the latest highlights and other other great finds that are in the issue. My name is Eric Nantz, and I usually say every week as we usually do it. But, the universe kinda gave us a sign last week that maybe it was time for us to take a little week off. So I think I think we're better for it. But in any event, we are back this week, but I'm never alone on this. I am joined by my awesome cohost, Mike Thomas. Mike, how are you doing today?



[00:00:38] Mike Thomas:

I'm doing well, Eric. Yep. We did, record a great or we did, I shouldn't say record. We have a great episode last week. We just we just missed, the record in case you can't tell. But this week, we have pushed the button, so let's hope that it makes it out there to the universe.

[00:00:56] Eric Nantz:

Yes. And, as Mike can attest to, when he and I connected this morning, this was the very first thing yours truly did. So sometimes, yeah, the universe says may have gotten too comfortable before. So, you

[00:01:10] Mike Thomas:

know, the the bots can't automate everything. Mike, AI can't automate all this either, unfortunately. So Nope. That's right. Just add it to the collection of lost episodes out there. Highly Yeah. Valuable.

[00:01:20] Eric Nantz:

Luckily, it's still single digits, and we hope to keep it that way.

[00:01:24] Mike Thomas:

That's right.

[00:01:26] Eric Nantz:

But, you know, it's not lost. We got another fantastic issue to talk about today, and our curator this week was our another longtime member of the rweekly team, Jonathan Carroll. And as always, yeah, tremendous help from our fellow rweekly team members and contributors like all of you around the world with your poll request and other suggestions. And it's fun when you can lead off with, you might say, straight from the source, if you will, because we have a blog post straight from the r development blog itself on some of the latest patches and enhancements that have been made into base r to support a new frontier of object oriented programming systems.

And if you're coming into base r, you may have noticed that there are 2 built in object oriented programming systems right off the bat with no additional packages needed. These are called s 3 and s 4. They both accomplish similar things, albeit they go about it a bit differently. But you will find that, for instance, a lot of the tidyverse packages have been built upon s 3, whereas other packages in your ecosystem, especially those around the bioconductor side of things, have links to s 4. And we've covered in previous episodes of benefits and trade offs of each of those.

But ever since about a year and a half ago, there has been a new effort, a joint effort, I should say, between the Arcore team as well as developers of, say, the tidyverse and other community related packages on a new object oriented programming system that is now called s 7, which right now is available as a standalone package that you can import into r. But with the advent of s 7, it has provided the R Core team a time to reflect, if you will, on how to maybe help out packages like s 7 in the future become part of the base r installation.

And they've identified a few key gaps, if you will, of ways to make this journey of incorporating into base r a bit easier. And this blog post in particular talks about a set of 4 different patches that have been landed ever since our version 4.3.0 that are going to pave the way for an easier onramp of s 7 and perhaps other packages in the future to take advantage of some of these principles that have been built into R for quite some time, but to help give a more friendly interface around these. We'll do a quick recap of each of these 4, but we're gonna, you know, buckle up a little bit. We're gonna get pretty technical here. The first one, probably the most technical here, is the concept of how dispatch works with respect to different object types and different operators.

If you don't know what dispatch means in this context, it is really the concept ascending a generic, which is another word for function that's supposed to have, like, one name such as plot or summary, but take different actions depending on the type of object being fed into it. And so it'll say, hey. If I wanna do a summary of a data frame, it's gonna do, like, a quick five number summary of each, you know, variable in your data frame versus for a model object. It's gonna give you those model fit convergence statistics, those likelihood statistics, and other information.

And typically for s 3, a generic will use the first argument that is passed into that that that that method call to help determine where it dispatches. S 4 kinda takes a more general approach. It could be any number of arguments in that call that help determine which method gets dispatched to. S 7 is somewhere in between. It has this intricate kinda double dispatch system And even I'm not quite sure how that part works, but, luckily, for the rest Even I'm not quite sure how that part works, but, luckily, for the rest of this explanation, we really need to know that because now since version 4.3, they are now helping the situation where it's not always just like a summary or, you know, other function like that. A lot of the operator functions in r, such as like the plus or the multiplication or many of these others, actually are built on 2 arguments. Right? You don't see it, but the left hand and the right hand side of these operators are like the 2 arguments to the actual function itself.

Well, before this patch, it would always if there was ever a mismatch in the types of arguments that are in these kinda operator type methods, r would warn you, hey. These aren't the same type, and it wouldn't dispatch anything that was based on those. Now since 4 dot 3, there's a new function built in the base r called choose ops method which is gonna let an object type declare that they are in fact based on a combination of different methods for that argument pair, not just one method. Where this is really gonna come into play is the situation where you have a pretty complex method that wants to do, like, these operator methods that is meant to do a unique thing, but be able to take an account that these argument types going in could be more than one type.

And they use an example from reticulate in this piece, which I think is quite interesting here. But for those aren't aware, Reticulate is the RPAC is a interfaces with Python under the hood, and, hence, it's going to find a way for your r code to go to the appropriate Python method. But especially in the space of machine learning and other methods like that, sometimes these operators, they might wanna build a custom, you know, method on. They're gonna have different things inside each of the left and right side, if you will. So in the example they have in the post, it's based on TensorFlow where you might have an interesting object that's based on a tensor, tensor itself, which is a Python object, but you might wanna add to that, like, a simple array in Python or whatever.

Whereas in for for r4.3, it wouldn't know what to do because these are 2 different object types. But now with this choose ops method, reticulate can say, okay. For this unique type of combination of an array and a tensor object, dispatch to this particular method. Now you may never really encounter this in your day to day development, but as a package author dealing with some more novel and more intricate class systems, this is going to be a big win for many people. So it's going to pave the way for more flexibility in these situations of building your own method on these, what you may call classical operator type functions.

And speaking of those classical operator type functions, there has been one that's kinda had a unique, you might say, history and where it's invoked. And that, in particular, is the multiplication surrounded by percent signs. If you've done any, like, math in r or maybe in your graduate school or over class training, you probably use this when you had to multiply matrices together if you're learning the ropes of linear algebra and the whatnot. Prior to our version 4.3, this only worked with either s 4 objects or base r objects.

Well, guess what? Now that in 4.3, there is a way to have now a new generic category called matrix ops where it's gonna have this built in operator as one of its first citizens of it, so to speak. But package authors and developers of new object class systems could help put in new methods into this. And they say in the future, within base r, they're going to fold in the cross prod and the t cross prod, functions into this family. So they do admit this is meant for backward compatibility, but it's also intending this to let other packages have to do with operations that are more unique in this situation such as s 7 in the future, be able to do the appropriate classification of these methods.

And, yeah, we're we're not quite done yet. We got 2 more to go. One of these is also filling a need that can be inconvenient or maybe it was a bit difficult in the past of trying to figure out for an object type what class it inherits from. There is a function in base r called inherits where you put in as the first argument the object itself, and the second argument of the name of the class that is meant to check if it inherits from. Well, a lot of these class names that for an s three standpoint were kind of built in as an attribute and not really meant for human consumption. It was not meant to be very syntactically, you know, very easy to understand, apparently.

Well, now in the inherits function ever since version 4 dot 3, you can now have arbitrary objects as that second argument that you're gonna use the check for inheritance of that first argument, which means you don't have to memorize or go in source code to figure out what is that name of that s 3 object type that you're or class type that you're checking from. You can now feed in a logical name of this or an object itself. So the example they have in s 7 where they define a class called class x, they give an object called class x of that new class, and then you can now use that as a second argument instead of a string trying to guess what that class type is.

So that's also meant for use of s 7 to get into base r eventually, but there's no reason other packages can't inherit that same idea. Again, similar to the Python side of it where you might have in your reticulate package, build down reticulate, maybe a new class name that can be a lot easier to type for the user. Last but certainly not least, if you're familiar with s 4, if you ever explore that, the way to get into what we call slots of these objects in your creative s 4 has been the at sign. It's kind of been like the dollar sign to me when you would use that in, like, your data frame, dollar sign, name a variable.

In s 4, it's been quite similar. You would have your object that was built in s 4, use the at sign, and then you would start typing in, like, the name of either the method or the slot that you want to get more information on. Well, this never worked for anything besides s 4. Well, now, ever since 4.3, now that can be used with s 3 dispatching as well. I still don't quite know in my day to day when I would would actually use this, but they do give justification for this by saying that s 7 is taking bits of s 4, albeit these more formal definitions of slots and other formals that are going into these class types.

And now, if you have that muscle memory of using s4 and this at sign to get into these different slots, you're going to be able to use this of s 3 and s 7 now as well. Again, not sure when I'll use that day to day, but, again, many of these changes are paving the way for an eventual future state of s 7 becoming part of the base r installation. So I think it's one of those things we're gonna see the fruits of this labor more down the road. But if you ever had questions on what our our core project is doing to get this state ready. This blog post is definitely for you, especially if you want a primer on how these are gonna make your life at developing a new object oriented programming system or building a a PONCE S3 or the new S7.



[00:14:16] Mike Thomas:

These are definitely, features that you wanna keep your eye on as you're developing your source code for that. Yeah, Eric. This is this is pretty exciting stuff and it's interesting to me to see how Posit and the RCCOR team are coming together on this initiative as well as some others, you know. It's really cool to have that, collaboration out there between, you know, the brightest minds in the some of the brightest minds in the R ecosystem. And if I understand it correctly, I think the the road map here is that this new framework is currently out there as an R package called s7 for developers to, today if you want to, download, install, and kick the tires on it for a while.

And then once the team feels that the package is stable across a wide variety of use cases, I believe the idea is to bring s 7 into base r. And I don't know if there's any specific timeline on that that we saw in the blog post. Not not yet. It looks like it's still got some work to do. Yeah. So I I would imagine that it's gonna be a function of how much use this s seven r package gets, and how many folks out there can download it and and try it out, and how many edge cases, they can find before they they finally feel that, s 7 is is stable enough to bring into base r. But, this is all, you know, I think extremely exciting stuff for those who are trying to, you know, sort of push the boundaries of what R can do. I I think, a lot of the power behind, this object oriented programming approaches is actually in making sort of the the end user's life easier and making the UX for, you know, our users that maybe don't necessarily need to know or care about, you know, methods, classes, object oriented programming, make, you know, their day to day jobs easier. Just like you said, Eric, we have these these generic functions, you know, summary. You can call that against a a data frame and get your your quantiles for each column, or you can call it against the model object. And each, you know, for for most authors of of our packages that contain predictive modeling frameworks, they all typically implement their own sort of summary generic or or print generic as well against the model objects that those packages offer. And the ability to sort of do that and custom tailor what the end user is going to see on screen is is really cool. It's really powerful. And for those of us in our early in our our journey or even, you know, intermediate in our our journey, the fact that we just sort of get that nice functionality for free, is is fantastic. And it it makes everybody's life easier. I think it contributes to some of the reasons why R can be a friendlier language for some folks to pick up. So, you know, the fact that we're getting some additional, you know, extension functionality, you know, beyond s 3, what s 3 can do in this new s seven implementation, you know, particularly with with sort of that second argument idea, I think is going to go a long way to continuing, to help, you know, those hardcore r developers make, everybody everybody else's life a lot easier. So we're very appreciative of that.



[00:17:27] Eric Nantz:

Yep. Exactly. And I'll admit my journey in OOP systems right now has been on a package, r 6, because I'm doing some shiny extensions as we speak. But it has been on my bucket list, so so to speak, to get into s 7. And our our good friend, John Harmon, has been doing some work of s 7 as well and some of his package development exploration. So it's it's on my to do list. But as you know, Mike, there's only so much time in the day.

[00:17:52] Mike Thomas:

I'm in the same boat, Eric. I have some grandiose ideas still sort of on the s3 object oriented side, but, we have this this credit scorecard package that leverages traditionally just a a GLM, a logistic regression model, but it's very possible to stick these scorecard models or wrap them around, other modeling types like XGBoost and something black box. But the methodology for for doing so, is is quite a bit different. So I'd have to to stand up a lot of different, I guess, object oriented approaches to ensuring that these generics that we've written, you know, fire correctly. So I have lots of ideas and plans as well, but it's only so much time as you said.



[00:18:46] Eric Nantz:

You know me, I love it when we have more open resources for both developers and users to kinda test their visualization muscle on, so to speak, and really generate some awesome insights. And our second highlight is highlighting just that. It comes from Kyle Walker who is a director of research for a winyan, if I'm pronouncing that right, as well as he has, apparently, his own consulting company for data science. And he highlights area that I did not know about until I read this post. So, again, I'm always learning when I read these.

But, there is a project out there called the overture maps datasets initiative, which is a project not just from 1, not 2, but multiple big tech giants in the tech sector such as Microsoft, Amazon, and Meta, along with others to help produce both open and standardized geospatial datasets. My goodness. Oh, this this looks really fun, and it's meant for developers to be able to use in the applications that they develop. Well, of course, nothing wrong of using this in r as well. Now how will we pull this off on the r side of things?

Guess what? These datasets are written in what format, Mike?

[00:20:06] Mike Thomas:

My favorite format, Eric, parquet.

[00:20:13] Eric Nantz:

Exactly. We've been harping on parquet for many months now on this show, but these datasets have been shared as objects and s 3 storage on AWS for public consumption with parquet. And just how can you import these into r itself? Guess what? The arrow package is your friend here because the arrow package will be able to import not just these parquet files that are stored locally on your disk. They can read from remote locations as well, such as these s three locations and not just read from them. It's not like you're, you know, behind the scenes, like, downloading in some temp area and then somehow importing all this into your system.

No. No. No. No. These are kept remote, and this is really important example that Kyle outlines here because he found a unique data set with respect to building coordinates or topologies, and this data set has, get this, over 2,350,000,000 That's a b. Building footprints around the world. Mike, have you ever had your hands on a data set this large?

[00:21:22] Mike Thomas:

How how big is the New York taxi dataset?

[00:21:25] Eric Nantz:

Probably not that big. I don't think it's that big. Yeah. I think that's in the millions, but, boy, we're talking about 1,000,000,000 here. So you can imagine if you put that in your system, yeah, it's gonna probably blow up and smoke fire or whatever in your laptop or PC of choice. So you don't want that in your home memory. Guess what? IRL comes compatible with dplyr functionality, so you can use this to query for just the parts of data you need. And in particular, in the example that Kyle walks through here, he has an example of filtering the coordinates based on San Francisco, I e the state of California where he was able to grab the actual coordinates, from, I believe, another package to get the coordinates, right, the bounding boxes.

And then once you have that well, of course, we're visual. Right? We wanna see what this looks like. The the package called rdeck, which is a wrapper on deck GL, gives you a way to have a 3 d based visualization of these buildings. And in the post, the code itself, really and really straightforward. Really not too much going on here other than setting your initial view and centering the coordinates, doing some zooming parameters, and then adding the layer, I. E, the Polygon layer for based on that data, the building topology from San Francisco and then being able to basically feed in variables for the various attributes, much like ggplot2 in a sense. And lo and behold, if you're looking at this in your modern podcast app, you'll see in the chapter image marker, you got a really nice looking topology layout of San Francisco that looks pretty darn cool if I dare say so myself.

And so Kyle was curious. It looks like the u United States portion of this data has a little more fleshed out, building data, so to speak. He thought, what's it look like in the rest of the world just to kind of test that, you know, hypothesis out? He did the same kind of code, but instead of going to the US, he went to Sydney, Australia to see what kind of topology he would see here. And you do see that there's only maybe looks like about 15 or 20 buildings that are represented here, which obviously we know Sydney is a big city. Right? So there's gotta be a lot more to it.

But this just goes to show this iterative nature or might say the early stages of this initiative. So perhaps there will be others in the community that start contributing more data to the Australia side of things or other parts of the world. But in any event, you've got access to this really innovative set of data. You do not have to blow up your hour session and import it in. Parquet is, again, we've been harping on this before. It's opening up a ton of possibilities for you as an R developer and R user to be able to do some really great insights without having side of side of things later on, this data set this set of data looks like a great source to draw upon. Yeah. Eric, I couldn't be more excited about this project. In the past, I've

[00:24:45] Mike Thomas:

leveraged typically the Google suite of APIs for geolocation types of of activities. Think if you remember my Connecticut, COVID 19 at home test spotter shiny app that I I shared on the shiny dev series, we use the Google Places API to allow users to search for particular places, in in the US and have, you know, that autocomplete that you're familiar with seeing, if you're ever been on Google Maps and ensure that the the particular place they selected is is a recognized location in Google's giant database. And it's a fantastic API, but there there can be some cost associated with that depending on your volume.

And this new project, that's, as you mentioned called called Overture which is this collaboration between these powerhouses, Microsoft, Amazon, and Meta to open source these large geospatial datasets is is really exciting. And I I there talk about 5 different, particular geospatial datasets out there. One being administrative boundaries. 2, land and water features, building footprints and roof prints, and then points of interest which I think are are like that places dataset that I was talking about, places API.

And then last is transportation rare layers, like roads and public transit ways. One other feature that I hadn't, seen before is this deck.gl, library which is a relatively new, I think, framework for visualizing these very large datasets. And as you can see in the blog post, the output of, the deck dot gl maps is is beautiful. They have great visuals there, very modern looking, really really cool. And that rdeck package is just this great wrapper for the API that, as you said, the code here is is quite minimal. It's pretty incredible. There's some data prep work that goes on to to grab counties and to filter down, the buildings within a particular bounding box coordinates that we we care about, using, you know, the dplyr and arrow combination as well as the the sf package.

And then there's this one call, a few lines. I think it's 1, 2 arguments really, to the rdeck package in order to sort of build that base map and then leverage this add polygon layer function on top of that to, really stick your polygons on top of there that are going to draw, the the different shapes on the map that you're looking for, all these different buildings and their their heights and the gradients and scales and things like that. And and that's about it. It's it's pretty incredible. It should feel pretty straightforward and and familiar to those who have done any geospatial analysis before.

So the fact that we just continue to get these higher and higher performance tools for all of our data science work is super exciting. A great blog post, great walk through. I used, you know, you you we've talked about this a 1000000 times on the the podcast before and my love for Parquet k as well as the arrow package. I recently used DuckDV for the first time last week finally. I had a 25,000,000 by 50 column SQL Server table that I wasn't about to to try to write summary queries against because I I tried to do that once and it it did not end well. Let's just put it that way. So I was able to to write that out to parquet using a a chunked approach, and the new I think, ADBC, that database connection for, for the Arrow framework and wrote that out to Parquet, and then I use DuckDV to query this this parquet file that ended up being, you know, I think around 1 gig for these 25,000,000 rows by 50 columns sequel server table, and the the queries that I was running with DuckDV were taking like less than a second.

It was it was insane. It was incredible. So I can't I can't sort of recommend these frameworks enough for, you know, high performance, data analysis. And this is a fantastic use case of bringing a few different tools together, in terms of some of the Tidyverse ecosystem, the Arrow project, some of these other data visualization libraries, and then this new deck dotgl library, which is incredibly exciting. And, like I said, sort of the outputs here are are really visually appealing and nice. So it's a fantastic blog post, to to have out there. So thanks to Kyle.



[00:29:07] Eric Nantz:

Yeah. Absolutely. In fact, I've even just thought of a a use case for this myself in a rather geeky project where I'm starting to interface more with some interesting podcast metadata, and some of it is the location where downloads are occurring from. You know? Maybe I could use this to map out the downloads of our weekly highlights on a nice little map, so to speak. So I'm, you know, cover me intrigued then, and I I still think parquet is becoming the new CSV in a lot of our data science circles, and this is just yet more more credence to that. So, yeah, really, really fun to learn about the innovations in this space as well.

And last but certainly not least on our highlights section today, we, in this, quote unquote loss episode last week, we we had a great discussion on ways that have been explored to help automate some refactoring of tests. Well, we're gonna make good on it because we have part 2 of that series here. We're gonna talk about the highlights here, and this comes to us from Al Salmon who, of course, was a former contributor here at rweekly and now is a research software engineer with rOpenSci and rhub and synchro as well.

And this is part 2 of her quest to refactor test files in one of her packages. And, again, last in her her post last week, which, again, is on last week's our weekly issue, she had a fantastic, albeit quite in-depth system of using a combination of parsing our code and XML representation of code to find calls to certain expectation functions and be able to swap them on the fly by how having to do the infamous control f find and replace for all these hover instances. Well, we're we're getting a little bit more, human readable here, so to speak, in terms of automation versus, you know, some principles that I think on paper they make complete sense.

But, again, you got to put it into practice. Right? And, I already have one project where I put maybe half of these in practice, and I'm gonna go back and put the rest in practice. We'll kind of hit these 1 by 1 here, but a lot of these are gonna relate to not just test, but also your base r code as well for your package or other routine. Make variable names that actually make sense, and this also applies to your test suite as well because it may be tempting if you're, like, doing a similar expectation by just varying, like, 1 or 2 things, kinda reuse those variable names over and over again.

No. No. No. No. No. Don't do that. It'll make the bugging a lot worse, especially if most of those expectations pass except for one thing here and there. So having variable names that make make more sense to you when you read it, don't be afraid to make them verbose. I like long variable names if it helps my cognitive function. And so don't worry about, you know, trying to be with that. Again, you are probably the one that's gonna read this the most. So be nice to future you on this. And speaking of variable names, you, again, may be tempted to use the same ones across different tests if they're all doing the similar type thing.

You know, you might wanna change those up a little bit. So in her example here, she's got a function that has, like, 3 different modes of complexity, and she changes those inputs going into the expectations based on that mode, if you will. Easy, medium, and hard versus, like, x x x for all of them. So, again, tailor it to the way you as a developer can help move your debugging when these expectations are not passing. And then as you're developing these expectations, you may think about, oh, when I wrote this package, I had, like, all my objects created above, and then I put them, like, my processing functions below that or whatnot.

Maybe for testing, you don't wanna do that. Maybe you wanna put the the objects you're creating related to the expectation that it's testing with and put those groups together instead of, like, all your objects in one part and then all your expectations in another part. Again, some of this is subjective, but, again, when you're debugging things, it may be much more straightforward to have these logical groupings of the object and expectation with each other instead of, like, everything in the preamble of your test and then every expectations all below it. So, again, these are all nice enhancements, but she's not done there. She's got a lot more here, Mike, that I think you could definitely relate to as a as a developer as well.



[00:34:06] Mike Thomas:

Yeah. Unit testing is a a funny thing, Eric, because some of the sort of best practices that you might think of in terms of how we like to author our code, can sometimes go by the wayside or or not necessarily make as much sense within a unit testing framework. And, you know, her her last sort of section here that she talks about or as you were discussing before is maybe not, you know, create all those different objects and then run your sort of expectations, you know, against those different objects that you previously created, but keep everything sort of logically together. So if you're about to execute a test using, you know, expect, equal or expect greater than or something like that, you should have the object that you're using in that test or objects that you're using in that test as close to that as possible.

And and I think that that makes a lot of sense. You know, when I'm reading some source code of developers that I, you know, admire and and try to mimic, I think that that's something that I see, you know, quite often in their their unit tests within their test that folder is that everything is is sort of chunked up nicely into different sort of logical sections where they they have a per very particular thing that they are trying to test and, you know, they have that that very much segmented in one location.

However, you know, sometimes you're trying not to recreate the wheel all the time as well and there may be opportunities maybe at the header of that file before you start executing any of your unit tests to create a particular particular object that you're going to actually pass and leverage in a lot of the different tests that you're running. So there's some trade offs here. A lot to think about. I I again, I say this I feel like every time I'll author a blog post but I feel like these topics aren't something that's talked about very often. So it's it's really nice to have someone out there who is talking about, her opinions and best practices, you know, most of which I I sincerely relate to. But these are a lot of things that I've had to learn the hard way, sort of, by trial and error, over the years and not necessarily had a lot of content out there to follow.

So very grateful for Mel for for putting this together. And and maybe the last, point that she puts out is is try to use a a bug summary and not just maybe the issue number that you're solving as your test name. You know, if you go back and take a look at a test, you know, where sort of your test that, first argument there is just the string that says, you know, bug number 666 is fixed as Mel uses for an example. She certainly prefers that you you write that string to say something like, you know, my awesome function does not remove the ordering. You know, sort of, maybe providing the summary of that issue as opposed to just the issue number itself. So that when folks are going back to to understand it and look at the tests that have been written, It's very easy for them to be able to see exactly what's taking place in these tests, what has been solved, what maybe still is open and needs to test around that. And she recommends, you know, use that summary as that string, that first argument to your test that call, but but maybe you can have a comment within that that has a link, to the particular bug report or that git GitHub issue, so you have sort of the best of both worlds there. So a great great, blog post here again by Maelle. I really appreciate her covering a a topic that I think doesn't get enough attention, and I definitely learned a lot here. Yeah. Me as well. And I've been actually,

[00:37:48] Eric Nantz:

in the past couple weeks, I've been bolting on some really interesting unit tests for a shiny app I'm about to put in production. But guess what? As much as it may seem like a paradox, Mike, 90% of those tests have nothing to do with shiny. It's about the business logic, which again is a pattern that may not be as obvious as a Shiny developer. But guess what? Because the business logic is just based our code if your summaries or visualizations or over, you know, in this case, some Bayesian modeling.

These are all things that we can take principles that Miles outlined here and as well as her previous posts on developing best practices. So I have her blog on one tab. I have the r packages tab on or open on another tab. The r packages book by Hadley Wickham and Jenny Brian. And so these have been greatly helpful. And, yes, there is some shiny test to match. I got the end there for what I call a more end to end user focused view of it. But, boy, if I have confidence in the business logic and I organize these tests such that I know exactly where to go if something fails. And sure enough, I had some failures with a survival object outcome where I was like, wait. Why didn't it work for binomial and continuous? Why not for survival? Sure enough. I was doing a dumb thing in one of the inputs and the processing functions. But the expectation covered it, and I knew exactly where to go. So it does help future you. You've just gotta try it once. I promise.



[00:39:13] Mike Thomas:

I couldn't agree more, Eric. Please please please write unit test for your business logic. It's the most important place to have them.

[00:39:20] Eric Nantz:

Exactly. So I've got, members of of our developer team helping with some of those business logic. I gotta kinda help with the summary and visualization side of it. But in the end, we've got over 40 of these tests, and right now they're all passing. So yay for now. Hooray. You know you know what passes, though, as well. It passes the, the r curiosity test, if you will, is the rest of the rweekly issue because we got a lot of great content for you to continue your learning journey on with new packages, new blog posts, great new resources for you to dive into.

So as usual, we'll take a couple of minutes to talk about our additional finds here. Speaking of learning, this is definitely gonna be relatable to those who have children here in the US have started their summer breaks. Right? They're, they're not using their, their course web pages now to get homework and whatnot. But most of those schools here in the US build their learning systems on something called canvas. It's a learning management system. Mike, you're gonna know about this in a few years. Trust me on this.

And most of their most of the teachers will use this to help bring out assignments or, you know, get in touch with their students, add resources, and whatnot. Well, if you are a maybe teaching a, who knows, maybe a stat or math course, and you wanna use r in it, but you wanna help use r to help automate some of your management of these resources, well, there is a package for you. This package is called vv canvas. This is a package to automate tasks in the Canvas LMS, and this has been authored by Ewin Tomer. And this is actually part of what was new to me, a verse of package is called the Vusa verse, which is a collection of our packages tailored to student analytics with respect to teachers using these in their courses and and other systems.

So this package basically builds upon the Canvas API If you wanna use r to help automate certain tasks, like creating, you know, assignments or finding metadata of assignments and whatnot. Looks really interesting if you're using this system with respect to your teaching. And as usual, as I say to many people, if there's an idea, there's typically a package for that. Well, little did I know there will be a package for the campus learning management system. So that was a new one to me. So maybe if you're one of these teachers that are on your quote unquote summer break and you wanna supercharge your, learning pipeline for administering your course and you use R, maybe this is for you. I have no idea.

Unfortunately, as a parent, I can't leverage this API. They don't give me the keys for that. But, hey, if I was ever teaching, I would. I

[00:42:10] Mike Thomas:

have heard of Canvas before. You're right, Eric. I'm not quite there yet, but I am sure that sooner rather than later, if Canvas is is still around, I will be engrossed in it, once my daughter gets to gets to school. So very good to know. Very, very good highlight. I've got a couple for us. I will wanna shout out Stephen Paul Sanderson who has a bunch of little bite sized, posts which are are just fantastic little nuggets. One is called how to remove specific elements from a vector in r. He walks through, approaches using baser, d player, and, data dot table, I think. He has a a little, blog post how to split a, number into digits in r, which is fantastic. It's actually a use case that I had with with DuckDV, which has fantastic, function to be able to do so to split a a particular value by a delimiter into an array.

And then it has a second function for picking the element of the array that you want but that's just a a tangent, based upon some rabbit holes that I had to dive down recently. So I want to shout out, Steve Sanderson for these great bite sized posts. Then I also wanted to shout out Epsilon for this contest that they are having, to win a, I believe, professionally designed HEX logo for your r package. You have until May 31st to sign up for this. So you have a couple more days left. You have to provide some details about your r package, package, name, purpose, functionality, and some links and, briefly describe the design that you're looking for, design concept, any preferences that you have for the logo, and you could win a professionally designed hex logo for your r package. And if you've seen any of Absalon's, I think newly sort of rebranded or redone, hex logos, I'm thinking about the tapper package which is, I believe is that on the Shiny for Python side? That's their Yep. Rhino That's their kind of rhino like wrapper. Yes. For building modularized, Shiny for Python applications.

These HEX logos look super clean, super modern, really really cool. So if you did win this, I have no doubt that that the design that your R package would get for its hex logo would be be very fresh, and would be, you know, sort of way ahead of of most of the competition in the R ecosystem. So very exciting stuff and and thanks to Absalon for running that contest.

[00:44:38] Eric Nantz:

Yeah. Because, Mike is designed a hex logo easy for you.

[00:44:43] Mike Thomas:

Not necessarily. I spend way more time than I need to spend but it's it's maybe not necessarily, the time that it takes to actually do the work, but maybe the the finickiness that I have trying to get, you know, what I want in my head, sorted out.

[00:45:03] Eric Nantz:

Yeah. Exactly. And so I've I've massive respect for anybody that it can have such, you know, in, you know, such amazing design skills. When I look at these examples on the blog post here. Yeah. I, I got absolutely nothing on that on that route. So all my internal packages do have a HEX logo. They take me a long time to get across, and then in the end, they just well, you won't see them, so you'll never know. But, basically, I just find some random open source image. I use column phase hex make shiny app to throw it on there with a little metadata, and then I'm done. I'm like, you know what? It's an internal package, so be it. But, yeah, if you are, as as Mike you said, a package author and looking to have a great HEX logo, yeah, this contest is definitely for you. So you got a couple days left to sign up for that.

But what won't have a deadline here is our weekly itself. It does not stop. Right? I mean, we are continuing, you know, hopefully for a very long time with sharing these great resources for you. And, certainly, when, your trusty hosts here know how to hit magic buttons, we come with a new episode for you every single week. But, of course, we love we love hearing from all of you too. So one of the best ways to get help with the project, first of all, it would be looking at r o q dot o r g. Looking in the upper right, there's a little poll request button right there for the upcoming issue draft. So if you see a great new package or great new blog post that you think should be highlighted or or be part of the next issue, It's just all markdown text. Very simple. Just give the hyperlink.

We give a nice little contributor guideline there right in the in the template so you can, you know, fill out that information. Again, very straightforward. If you know Markdown, you know how to contribute our weekly. It is just that simple. And as well, we'd love to hear from you as well. There are many ways to do that. We have a somewhat new contact page because yours truly forgot that we moved to a new host that my previous contact page was gonna be null and void. So I've got a new one up. The link has been updated in the show notes, so you'll get to that right there.

That's actually some custom HTML in that, by the way. I'll be found a template online to do it, but credit to our podcast our podcast host called Pod Home for giving me the keys to be able to do that. So that was kinda cool in any event. You can fill out that as well as if you're on a modern podcast app, such as PowVerse, Fountain, Cast O Matic. There are many others out there. You can send us a fun little boost along the way right directly in your app itself, and these modern apps are gonna be where do you get to see those fancy chapter image markers on top of the chapters themselves so you can kinda get a little visual cue every time we dive into another topic.

But, of course, we are also available on these social medias. I am, Mastodon mostly these days with atrpodcast@podcastindex.social. And my quest to bring r to the world of podcasting 2 point o just hit another milestone recently. I have now open sourced a new package called 0p3r, which is a front end to the OP 3 API service by John Spurlock that gives open podcast metrics. And, yes, now I can visualize our weekly highlights metrics with my same package with a little deep fire magic. So more to come on that as I dive into to more on this, escapade that I'm on. So, that was, that was from my interview, podcasting 2 point o. Adam Curry himself said, yeah. Yeah. It'd be nice if we did that OP 3 stuff in your dashboard. I'm like, duly noted. So new package as a result. Boardroom driven development nonetheless.

But I'm also on Twitter, x, whatever you wanna call it with that the r cast sporadically, and I'm also on LinkedIn. Just search for your my name and you'll find me there.

[00:48:54] Mike Thomas:

Mike, where can the listeners get a hold of you? Eric, that is super exciting about your OP 3R package. Congratulations. I'm excited to to check it out and then install it, from GitHub and see what I can play around with. And it's a it's an API wrapper? API wrapper. Once again, I'm in the world of APIs now. Gotta love it. Gotta love it. I'm excited to to see how you went about, wrapping an API and and learning a little bit more about that because I know there's a few different ways to that folks go about doing that, some best practices out there and now that we have h t t r 2 and things like that. So, it looks like the thumbs up sounds like that's that's sort of the approach that you That's exactly what I built it upon. I've got some APIs. Very wild. Got some APIs I need to wrap. So, I will be taking a hard look at your repository. Thank you for your work there.

For folks looking to get in touch with me, you can do so on mastodon@mike_thomas@phostodon.org Or you can reach out, on LinkedIn if you search for Catchbrook Analytics, k e t c h b r o o k. You can see what I'm up to lately.

[00:49:58] Eric Nantz:

Very good. Very good. And, yep. So luckily, all things considered, I think we've got one in the can here. We're gonna hope so anyway. But, thank you so much for joining us wherever you are, and we will be back for another edition of ROV highlights next week."
"53","issue_2024_w_20_highlights",2024-05-15,49M 15S,"An aesthetically-pleasing journey through the history of R, another demonstration of DuckDB's power with analytics, and how webR with shinylive brings new learning life to the Pharmaverse TLG gallery. Episode Links This week's curator: Sam Parmar - @parmsam@fosstodon.org (Mastodon) & @parmsam_ (X/Twitter) The Aesthetics Wiki - an R Addendum R…","[00:00:03] Eric Nantz:

Hello, friends. We're back of episode 165 of the our weekly highlights podcast. If you're new to the show, this is the weekly podcast where we talk about the latest resources, innovations that we're seeing in the our community as documented in this week's our weekly issue. My name is Eric Nantz, and I'm delighted you join us wherever you are around the world and wherever you're listening to. And as always, I never do this alone. We're in the middle of May already. Time goes by fast, but I'm joined by my cohost,

[00:00:32] Mike Thomas:

Mike Thomas. Mike, where's the time gone, man? It's crazy. I don't know. It is crazy. And, yeah, we gotta do the stats on how many episodes we're at together now at this point, Eric, but it's it's gotta be a lot.

[00:00:44] Eric Nantz:

That's right. And now would never feel the same without you as is, I was thinking that you're sick of me. Not quite. Not quite. Not yet. You ought to you ought to give me more rants before that happens. But in any event, yep, we have a lot of great content to talk about. Before we get into the meat of it, I do wanna give a huge thank you to the, pod father himself, Adam Curry, and the pod stage, Dave Jones for having me on this past week's episode of podcasting 2.0, where I was able to geek out with them and, or as the chat was saying, nerd out, the, our language itself and the use of our end quartile to produce this fancy podcast index database dashboard. That was apparently more of a reality check than what Dave was expecting in terms of data quality of the podcast index database. But it was a ton of fun, And, apparently I've given him some things to follow-up on, which we may be touching on in one of our highlights here. But again, thanks to them for having me on, and I've been anointed the podcasting 2 point o data scientist. So I'm gonna take that and run with it. Pretty cool. Add that to your CV, Eric. I'm super stoked to listen to that episode. Today, I got a long car ride this afternoon and I will be tuning in. So,

[00:02:01] Mike Thomas:

very very excited to to hear about that.

[00:02:04] Eric Nantz:

Yes. And I'm actually gonna make sharing that even more efficient in this episode show notes. I will have a link not just to the episode itself, but to an actual clip of the episode with my segment on it. Again, one of these great features you can give in podcasting 2.0. So that's just a tip of the iceberg or some of the things that dev community is up to. But, nonetheless, we got some great art content to talk about here and our curator this week's for this week's issue was Sam Palmer, another great contributor we have in our our weekly team. And as always, he had tremendous help from our fellow our weekly team members world with your awesome poll requests and suggestions.

And, Mike, we're gonna I guess guess this first high is gonna be a kind of a trip down memory lane, but I'll be at a visual aspect of it in ways I totally did not expect until we did research for this show. Oh, yeah. Exactly. So our first highlight today is coming from the esteemed Matt Dre who has been a frequent contributor to our weekly and past issues. And in particular, he's been inspired by a resource that admittedly little old me did not know about until now, but is called the aesthetics Wiki, which in a nutshell, what this is is a Wiki portal, but it's a very visually themed Wiki portal where it's got a collection of images, colors, links to musics and videos that help give you a vibe, if you will, of a specific maybe time period or a specific feeling in mind. And I met when I looked at this and I was going coming through some of the, you you know, the top Wiki pages, there was one about the y two ks issue from way back. And, I could I could definitely got the memory triangle. I'm looking at that Wiki portal, but I'm also getting vibes. And so this is gonna date myself quite a bit, so bear with me here. But, yeah, I had cable TV growing up and one of the, guilty pleasures I had was on VH one watching the I love the eighties, and I love the nineties series because it was such a fun trip down memory lane for each of those years of those decades. And I will still stand on my soapbox here. And at the eighties nineties were the golden age of video games, so take that for what it's worth. But in any event, I I definitely got those same vibes here. And apparently, Matt was inspired to think, you know what? On this Wiki, there's something missing.

It's the R language, of course. What could we do to visually depict certain, I would say, major stages in the evolution of our, for respect to this kind of theme on the aesthetics Wiki. So I'm gonna lead off with probably the most appropriate given yours truly is the old timer on this show with the first period that Matt draws upon is called base core. And the picture on the blog post definitely, gives you some vibes of there's a lot going on under the hood with our, but it's, you know, kinda scattered around. It it's it's got a lot happening.

And he talks about some of the bullet points that inspired the imaging here because of if you know your our history, it actually came in the mid nineties. It was derived from the s language and then to our from Rasa Hakka and Robert gentleman and then base core as we know it was founded in the year 2000. And if you have old r code laying around from that time period, I eventually gonna have 3 major operators in there one way shape or form. Bracket notation for referencing rows or variables, dollar signs for referencing variables inside a data frame, and the tilde's for your linear regression model fits and whatnot.

He also mentions that if he was gonna put this Wiki together, he would have a link to the, writing our extensions PDF, which I've combed through a few times in my early career, but it's, not for the faint of heart if you're if you're new to the world on that. But, yeah, the the the vibe of it is very, you might say, utilitarian, kind of mundane a little bit here and there, but the potential is there. So I really like what he put together there. And of course, in the in the blog post, it's in our post after all. He's got a code snippet that looks straight out of my early r classes and graduate school where you've got the assignment operator referencing rows with the double equal sign in the brackets, the dollar sign for referencing variable names and FL's aggregate, you know, bay from base are, and then finally ordering it by height. In this case, it's a data set of star wars worlds and characters. And in the end, you get a nice data frame of the average heights for those living in Tatooine or Naboo and others.

So I feel seen when I see that because if I look back in my old land drive here in the basement, I have lots of code that looks just like this. But our many other developments have happened. And the next period that Matt talks about will be something that I think will look pretty familiar to most users now. He calls it the tidy wave, Mike, and I would say it's anything but mundane.

[00:07:28] Mike Thomas:

Yes. No, Eric. That was a great trip down, you know, the the late nineties, early 2000 of our the aesthetic or the fashion, that Matt matches to the the base core, time period is cardigans and a wired mouse with a ball in it. Man, if that doesn't sum up base score, I don't know what does. But on to the Tidy Wave as you'd say, he he talks about the history sort of starting in 2,008 and then later being popularized, mostly through the Tidyverse, sort of wrapping up a lot of, this different tidy functionality in 2016 when that was originally released.

The key visuals here are the mcgritter pipe, the data placeholder as a period or a dot, and then the tilde, not being used for formulas but being used for Lambda functions in this iteration here. The fashion choices are Rstudio, hex stickers, and rapid deprecation.

[00:08:33] Eric Nantz:

Where did you get that idea from?

[00:08:36] Mike Thomas:

Every package is experimental. I feel feel like, you know, with the the little badge that it has, during this particular time period, but a lot of fun. Right? Absolutely. And you can see that the code snippet that we have here is leveraging a lot of dplyr verbs like filter, select, left join, mutate, summarize, and arrange. And the code arguably is a little easier to understand what's going on from from top to bottom and left to right. Looks a little more SQL ish than what you got in the base core, time period.

And that brings us to this last new period, Eric, which Matt is calling v4punk.

[00:09:17] Eric Nantz:

I love this because this is one of those situations where we are seeing another big evolution, but it's so fresh. May not fully grasp it yet. But the visuals of this have big roots into one of the most major developments in the our language itself of the last few years. And that is the introduction of the base pipe operator, which, again, one could argue that the tidy verse popularity you just summarized was a direct influence on this feature coming in alongside other languages having similar mechanisms.

And in 2022, it was released fully, in support of our the the base pipe. So that is, of course, one of the key visuals here along with the underscore, which is not just for variable name separations between words, but as a data placeholder, similar to what you mentioned earlier with the period and the and the previous summary and a new notation for Lambda functions, which, admittedly, I'm still wrapping my head around, but it's the slash with the parenthesis afterwards. I admit, Mike, I have not actually written that one out yet. You're gonna have to pull the tilde for lambda functions out of my cold hands, so to speak, but I am warming up to it. It's just taking a while.

So the fashion here is this one's really funny. It's, you might call evil mustaches and posts on Fostodon and memes up the wazoo on various blogs. There there's a lot a lot going on here with the pallet being, you might say, very, you know, unified and pleasant, a rainbow pallet. And his nearest aesthetic that he finds on the Wiki that would correspond to it is and, again, I feel seen when I see this vacation dad core. Apparently, that's full of dad jokes and other visuals that those are kids probably put your kids through embarrassing moments or whatnot. Not that I've ever done that. But, the sample here is gonna look very similar to the sample you saw in the tidy verse sample above, but substituting for the base pipe, substituting for the new data placeholder and the new Lambda function notation.

But guess what? This is monumental, folks, because now, a very important feature in many languages is now in r itself. And who knows what the future holds here? But in any event, it's gonna be a wild time. I'm sure as more companies adopt new and later versions of r and say version 4 dot 1 and up going into production systems depending on your organization. But it is, a fast moving space and who knows what the rest of the future looks like. But, again, I can't stress how monumental it was for the r language to adopt this operator after this many years. It does show that the team is not resting on its laurels. It is definitely keeping up with the time, so to speak.

So it was this was this was a fun fun new take on the evolution of r itself, and, I can't wait to see what's next.

[00:12:24] Mike Thomas:

Yeah. Eric, it's pretty wild that in that last code snippet that uses entirely base r, looks so strikingly similar to the prior code snippet that was using all dplyr, verbs. It's it's pretty incredible, I guess, where we've come, you know. I don't know if this is a cherry picked example totally because, you know, you this works great until you get to a particular base art function that you wanna use that that maybe its first argument, right, isn't necessarily what's being piped into it. I know there's there's some placeholders to help you you get around that, but I do think that that maybe there's potential use cases that the Tidyverse can handle better than what we have access to in Base R, but, you know, there's also use cases here as Matt clearly demonstrates where you can just leverage what's straight out of Base R, and it's it's pretty wild. It's a pretty wild time to be in our developer.



[00:13:23] Eric Nantz:

Yeah. And even at the end of that last snippet, we see hot off the our version 4 dot forward presses the use of sort underscore by. And, again, we'll be very analogous. So we see in dplyr with the arrange function. So you're right, Mike. It is very mind blowing to me that after all these years, we have this capability that I think for those that are new to the language. And again, there will be opinions on this across the spectrum. So just take the Eric's opinion here. But when you look at the readability of this from a person new to the language, outlining all the steps that are happening in the state of manipulation, transformation of new variables, summarizing all that in a group format and calculating these derived statistics.

I know for those coming from other languages, certain ones with proprietary name to it. This syntax is gonna feel more at home than what we saw in the in the good old days. But then again, I still have that old cold for a reason. It's always fun to look back. That's right. And next on our highlights very, hot and much talked about duck DB format for databases and how that compares to what you may have done in the past in tidy verse pipelines within particular, the deep wire package. And this highlight today is coming from Dario Ridicic who is a senior data scientist at Neos, and he's published this on the Epsilon blog talking about how you can take advantage of DuckDV to enhance your data processing pipelines in your ecosystem.

We have we have definitely mentioned DuckDV before, but if you're new to it and in particular, I mentioned being on the podcast in 2 point o show on Friday, I offhandly mentioned duct d b as someone else looking at for some of the podcast database and I, I had, Dave, scratch in his head a little bit. I'm running, oh, I gotta follow-up on this. So, Dave, this is for you. If you and those also that have not heard of DuckDB before, but this is a database system, but there are there are definitely different classes of database systems.

One of which is the more traditional, there's a client interacting with a server process somewhere. And this is where if you heard of MariaDB, MySQL, Postgres, sequel and others. These are very prevalent in the industry, but they often have a pretty heavy footprint, yet they are quite valuable in in the industry sector and other sectors as well. And then you have what are called in process database systems. These are ones that you can run directly on your computer as a file format, so to speak with where the data stored. And then there's some layer in your system is gonna interact with that.

And the most historical one that's had the most popularity and still does to this day is SQL light. And that's where you just have to install a compliant front end on your system, have a SQLite database file on your system. And then you're able to read that into your preferred language. Duck DB is in that same space. So it does share some similarities of sequel light, but there are some interesting differences that may be worth your time to look at as you're thinking about, you know, database evaluations for different formats is that DuckDb is designed for data analysis, and the abbreviation here is OLAP for online analytical processing.

This is massive if you're gonna have situations where in your database, it's not just querying for the existence of particular rows or existence of things. It's actually performing analytics on the fly much like we do in the tidyverse pipelines a lot where we're gonna aggregate by groups. Gonna derive mean values or or or medians or other statistics. And probably have to do that with complex, you know, indexing as well. DuckDV is tailor made for those situations. And so the other selling point of DuckDV these days that it is first completely open source, and that's of course a win for us and the data science community.

And it is relatively dependency free, which means that it can be run on a variety of systems and architectures, including web assembly, which is some I'm very intrigued to buy for sure. Now it's not all roses and unicorns. There is some mention of disadvantages for DuckDV. I think it's more limited depending on your use case here. It's probably not the best for constant writing to the database file. But for most of my needs, I'm not usually writing a lot concurrently like I would say for an IoT device device or other digital device that might be hammering an endpoint multiple times per second. Most of the time in my day job, I'm analyzing these data from source and doing more complex operations.

And you're not gonna get, like, the distributor set up like you would with a client server model for, say, postgres or Spark or other ways that you can have a database layer that's networked across multiple nodes on, say, a a VPS provider. But, again, you're in a different use case already in that point. So and the upper part that is to keep in mind, it is still a fairly new project. They certainly had a lot of progress, but it doesn't have quite the maturity at a sequel light. But with that said, there is a lot of advancements happening in the community here, and I do think DuckDV, especially the integrations, what we can do with web assembly technology and other systems, even as lower scale as a Raspberry Pi, I think are really thought provoking on what you can do with this. But as an r user, you may be thinking yourself, I'm very comfortable with dplyr.

What's in it for me to move to DuckDb?

[00:19:48] Mike Thomas:

Yeah. So in this benchmarking that that Damien, puts together here between dplyr and DuckDb. I believe he used, tried to aggregate about 38,000,000 rows that was spread across 12 different parquet files. And if you're not familiar with the parquet file format, we've talked about it a lot on this podcast. It is, in my opinion, sort of the new CSV. It's it's columnar storage that makes analytics, very efficient. It compresses the data in a way that makes your files, these parquet files, much smaller than if you were storing them as something more traditional like a text file or a CSV, but we have plenty of other other links and episodes that that discuss that.

So Damon used the, taxi data, the the very famous yellow taxi, the dataset, to be able to create sort of this large 38,000,000 row dataset in 19 columns, that takes a a little less than 1 gig of disk space, I think 630 meg, he said. And that's, you know, quite large for trying to do some analytics on natively. So one thing that he was able to do with with dplyr was to be able to, create this dplyr pipeline just using, 3 verbs here, 3 or 4 verbs. It looks like mutate, where he's creating a couple different date type columns, the ride date and the ride year, using lubridate to be able to parse those things, from a date time column. He's filtering, for only ride years in the year 2023, And then he he's grouping by these particular dates and taking a look at a few different median statistics, like the median distance, median fair amount, and median tip, as well as the the ride count, which is just a simple count. And he reproduces that same exact logic, using dplyr verbs again, but the second time it's against this this dplyr table connection, that connects via duckdb, the the DB connect, DuckDb driver, against these parquet files. Again, you're able to just have DuckDb sort of sit on top of and interact with these parquet files as opposed to DuckDV, being the database itself. You do have both options similar to SQLite. You can actually create a DuckDV database file if you would like. But you can also just use the DuckDB engine to operate against parquet files, which is what is going on here, and I think is is probably what you'll see more common out there in the wild these days and act in terms of, data storage as opposed to actually storing, you know, your your data itself as the stuck DB database.

He also demonstrates that you can if you didn't wanna write the dplyr code, you could write, an entire SQL statement that does this exact same thing, within the DB GetQuery function, from the DuckDb, package which would allow you to, you know, express your logic, all the CTL logic in SQL as opposed to dplyr. But now here's sort of the big results here. We're trying to look at this this little dplyr chain and how it performs, in in Raw dplyr as opposed to using the DuckDV engine under the hood. The dplyr approach takes almost 15 seconds to do this aggregation, which results in a data frame that I think is, they're they're showing about 16 different, observations here, 16 rows across these, 4 different, statistical columns that get computed, and then the DuckD b, as opposed to the 15 seconds that it takes in deep prior, the DuckD b approach takes less than one second.

So it's it's pretty incredible. Damien knows that they have, triple checked this statistic and that the numbers are correct, and DuckDb here is almost 20 times faster than dplyr, which is is pretty incredible and right that, you know, a lot of times when especially, you know, in the context of, Eric, a lot of what you and I work on in terms of Shiny apps and trying to make those UXs as seamless, frictionless as possible, you know, moving some of that ETL from something like, you know, raw dplyr or base our code, to leveraging, you know, DuckDB or Arrow can create these these drastic orders of magnitude gains inefficiency that that really make that user experience, tremendously better than it would be otherwise. You're not having to have people, you know, continue to wait around for something to happen for these ETL chains to run.

So this is a great great walk through, great use case. I would also recommend maybe a couple other, functions out there and some pieces of documentation out there. I think there's actually a function from the arrow package called to_duckdv, which allows you to sort of start with an Arrow object, but leverage the DuckDB engine, on top of Arrow to make that pipeline even faster and to allow Arrow and DuckDV to integrate seamlessly together, which is is fantastic. There's also a blog post sort of on that same, vein on the DuckDV website. I just realized it's from from 2021, but it had been, you know, getting shared around on on Twitter recently, which is called DuckDV Quacks Aero, a 0 copy data integration between Apache Arrow and DuckDb. So maybe I can share that in the show notes as well.

But it certainly seems like, you know, this is sort of the way of the future in terms of ETL. It's certainly the way that we're doing a lot of stuff at Catchbook right now for our ETL pipelines, and it's it's making a huge difference for us. So it's a really exciting time to be able to leverage and see all of these, really nice ETL data processing improvements and tools that we have now.

[00:25:40] Eric Nantz:

Yeah. And there was a situation actually came up at at the day job last week when, there was a user that thought that was Deepfire just having really bad performance on a certain situation. And, of course, they tried to get more details because we don't wanna just vague request like that. But, apparently, they were analyzing a 7,000,000 row dataset, which, yeah, if you're just gonna ingest that in native our sessions without a helper like duct DB or even SQL life for that matter, it's going to be a long time to produce aggregations and summaries on that kind of set. And it didn't help that this set was in SaaS format, but that's another story for another day. But in any event, imagine if that dataset is getting split up into parquet files or intelligently by groups, And then you take the same kind of code that was demonstrated by Damien over here and to be able to run that very efficiently or Dario should back. Try that again. And imagine if you take the same code that Dario puts together in this post with those parquet files again, like you said, correct me. My I feel is that the new CSV, There's a reason I linked to those as extract downloads in my podcast index dashboard. I didn't wanna do CSVs anymore trying to push this as the other communities as well to look into this. But you are gonna get tremendous gains in these situations with with DuckDV on the back end. And one addendum that I would be interested in trying on, maybe I will when I get some spare moments, is it was, what, a week or 2 ago on this very podcast, we had covered the release of the Duck plier package, which again is a very similar vein to what Duck DB package itself is doing here, but it is a even more of a direct one to one replacement for dplyr.

It is still newer, so we're keeping an eye on its development. But you could probably leverage the majority of that same code into a duct flyer pipeline, and I would venture to say you're gonna get those same performance benchmarks because it's all Duck DB under the hood anyway. But we'll definitely have a link to Duck plier as an additional thing to explore here. But I see, again, massive potential in the spaces that you and I operate on in both of my day job efforts. But now, as I mentioned in my open source efforts of this podcast database, my portal dashboard kind of takes a quote unquote easy way out where I've through GitHub actions, I'm doing all the processing outside the dashboard and the dashboard is simply referencing them directly via RDS files that are being downloaded on the fly.

I am waiting for the day where I can swift flip that up to do web assembly, DuckDV in inside that, and then I can do the real time querying, so to speak, in that dashboard as a Shiny Live app instead. It's coming, Mike. It's coming. I just need the time to do it.

[00:28:44] Mike Thomas:

And speaking of WebAssembly, we have one last highlight to wrap up the week that I think might just be on that topic.

[00:28:51] Eric Nantz:

So, yeah, it's not only on that topic. It is literally in my same industry about ways that I've been envisioning for ever since I've heard about this technology, where I think the potential is, and we are seeing that here. And in particular, this last highlight is coming from the pharmaverse blog and authored by Paolo Rucci, who is a data scientist developer at Roche. And for those unfamiliar, the pharma verse, if you're in the r community, you've like you heard of things like the tidy verse and whatnot. Well, this is a collection of packages that are helping make clinical pipeline analyses, data processing that we routinely do in life sciences much easier to do in the our ecosystem as ours now taking a much, stronger footprint in the life sciences industry.

Well, one of the suites of packages and the pharma versus, you know, getting popular is the nest packages and others to help produce what one might consider the backbone of what we call a clinical submission to regulators for a given study, and that is the production of tables, listings, or graphs or TLGs for short. Traditionally, these have been done in a certain proprietary language, but that's not we're gonna talk about here because r has made tremendous strides in this space. And as a way to get new users or to show off the potential of what's possible with the pharmaverse in this in this, part of the pipeline, The pharmaverse has produced for a while now the TLG catalog, which has been a snippet for the common table listings and graph types that we often do in submissions, but with the r code to actually produce those and a snapshot of the output.

Well, what, Paulo was talking about in this post is they have now integrated WebR, hence WebAssembly, into this portal site to not only show the code, but to let the users run the code themselves, change things on the fly, experiment with the existing syntax. My goodness. This is just a massive win for learning in this space. I am geeked out about this already. They had me there, but as the game shows will say, but wait, there's more because another popular part of the pharmaverse has been the teal framework, which is a way to bring interactive data exploration with these common summaries into a Shiny application with a set of reusable modules and packages to help with different analytical types, different parts of the processing, but teal is a great way to ingest these clinical data for review in your given systems to explore for new insights and whatnot.

Well, guess what? With the advent of shiny live, Powell is also plugged in a way to run a teal application tailor made for each of these table summaries inside that same website, folks. This is massive. I cannot underscore how enthusiastic I am about this because think of the doors this is opening. Right? Is that there will be, many in life sciences that are new or newish to our for a lot of these clinical analyses. Anything, and I mean anything we can do to make this easier for them to transition, we need to take that and run with it as fast as we possibly can. This TLG catalog, I think, is amazing first step in this vision. So I whoever you're in the pharma industry or not, just take the ideas from this. Think about how this could be used in your use cases for whether you're teaching new users of r or you're trying to demonstrate to your leaders what is possible when you mirror the technologies of open source and web assembly together. This t o g catalog is a massive vision. I or a demonstration of this vision that I think can be applicable to many people. And, yeah, when I saw this, I'm like, I'm IMing some people at work. I'm like, you need to see this folks. This is amazing. So I'm definitely gonna be pursuing this very closely. But, again, what an excellent showcase of WebAssembly, quarto, and the pharmaverse all in one place. Absolutely massive, Mike. Can you tell I'm excited?



[00:33:15] Mike Thomas:

I can tell you're excited, and it's hard to do it all justice, on audio format, but there's some great gifts as well that Pavel has, included and implemented into this blog post that really demonstrate exactly what's going on. And I don't know if the users sort of caught it, Eric, but what this means and what's being showcased here is not only the ability to create this this sort of shiny app, right, using web assembly and web are sort of in the browser, But also allowing users to be able to edit the source code for this app and actually change the underlying, you know, structure of this app within the browser itself, and actually see the changes in that app underneath the code immediately, which is fantastic.

No needing to install any dependencies on your machine. You don't even need to have R installed or Rstudio installed. This is all taking place in the browser, which is is absolutely incredible. I think, you know, again, it it just lends itself to, less friction for those trying to get started with our easy learning, the language language and adoption of the language. Hopefully, for those coming from from different languages to be able to to get started and and go from sort of 0 to 1 with the only requirement being an internet connection that they need, which is really incredible.

The other thing that I would say is, you know, what this this whole, sort of WebR framework and WebAssembly framework where it has sort of my head spinning now at this point which is something that we talked about, I think, on recent episodes but I don't think it is quite fully implemented yet is the idea for, you know, packages that we develop and these pack beautiful package downsides that we create with them, for the examples that we have for the different functions, in the reference section of our packages to be able to to actually be run-in the browser instead of users having to install that package and run those examples, you know, within their own our session.

What I saw recently, and I can't remember if it's the most recent version of quarto that got released or if it's the most recent version of our that got released that had these capabilities, but you can now use quarto or QMD files to create package vignettes, which is probably sounds like a very subtle thing, but my hope is that now that we're able to to leverage quarto for authoring, packaged vignettes which I'm super excited to move to just because I love quarto. My hope is that maybe that leads the way or blazes the trail for the integration of of WebR into these vignettes as well backed by maybe the shiny live quarto extension and allowing us the ability to to do what I just described So, you know, like you said, Eric, and like we've brainstormed, the possibilities here are endless with all these advancements in in WebR and WebAssembly.

And you and I I'm not sure if you had met him before but but at least I was fortunate enough to meet Pavel at, Shiny Conf last year and and maybe he'll be there again this year, but, a brilliant data scientist and a very, very well done blog post from him and a great way to wrap up the week.

[00:36:36] Eric Nantz:

Yeah. We we were able to connect as well at that same conference, and I've I've you know, I've had multiple conversations thinking about potential of this new technology, and that's why I'm so excited to be part of this space right now with my, our consortium effort with that pilot, Shiny app submission and WebAssembly. I think we're on to something here and, that that indeed what you mentioned with quarto, it is in as of quarto version 1.4 is indeed the version Atlanta support for package vignettes, and I cannot wait to try that. Even if it's a internal package, if I can pull that off, yes, that again is a massive win for learning.

Even as something as I'm gonna call trivial, it's not so trivial. I have an internal package to import SAS data sets into our bet backed by Haven, but with little syntactic stuff on top. Imagine just having a a a vignette powered by quartile to help demonstrate what that code looks like and some of the post processing functions I have in store. But, yes, I think in in our industry, we we are ready to embrace technology like this. Admittedly, we still have a long ways to go to see where this impacts, like, actual submission pieces, hence what I'm exploring in this pilot.

But when you put this in front of people, I don't know how you can't be excited about the potential of this. And like I said, no matter what industry you're in, I hope you take some inspiration from parts of this because this is this is the time to try things out. I think this is that evolution in history that we're gonna look back on maybe 5, 10 years down the road and wonder how did we live without this.

[00:38:16] Mike Thomas:

I hear you. I agree.

[00:38:19] Eric Nantz:

You know what else is hard to live about is our weekly itself. Right? I mean, if our week had been around when I was learning our oh, I would have learned so much faster. But we can't turn back time. But at least you can start with the present and check out the rest of the issue that Sam has put together of more excellent resources. So it'll take a couple of minutes for our additional finds here. And, yes, I am very, you know, much a fan of the web assembly frame, but I haven't forgotten about my explorations with container environments.

And that's where this another mind blowing moment for me from this issue is that James Balamuda, who goes by the cultless professor on the various social handles, he has been the pioneer of that web r extension for quartile and whatnot, but he has put a spectacular demonstration on his GitHub organ profile here that has a, custom dev container setup, which for those aren't aware of dev containers are a way for you to bootstrap a container environment in versus code or get hub code spaces. And he's got one that's actually 2 of them that are tailor made for both building portal documents and with newer extensions such as the web r extension and the pile died extension if you're doing Python based, web assembly products.

But also these dev container features have one that works in the traditional GitHub code space, which will look like Versus code in your browser. Again, you as a user don't have to install anything with this on your system. It's all in the cloud. And wait for it. There is a dev container feature for Rstudio server. Let me say that again. There is a dev container feature for Rstudio server. If you recall maybe a year or so ago, I was exploring my workflow for these dev container environments for all my our projects, and I would spin up RStudio container via custom Docker file. And I would, okay, get from the rocker project, add these dependencies, add these packages and whatnot. And, yeah, it quote, unquote works, but it is a bit of, manual setup nonetheless, even if you clone my repo.

Oh my goodness. Do you know how much easier this is now with just one line in that devcontainer JSON file he can have in our studio environment in the browser, not on your system, all powered by the GitHub Codespaces. Oh my goodness. I am playing with this as soon as I can.

[00:40:54] Mike Thomas:

That is super exciting, Eric. I can't wait to play with it as well. You you know, I I don't need to say it again that I have been bitten, hard by the dev container bug, and we do all projects now on our team, within dev container environments just makes collaboration and and sharing that much easier. So this is a great great blog post, touching on that subject again. I can't get enough of that content, so that's a great shout out as well. Another thing that we do a lot of, so this spoke to me quite a bit, was that there is a new release of ShinyProxy, version 3.one.0.

And for those who aren't familiar, ShinyProxy is a platform for hosting Shiny applications. It's it's clear fully open source, but you you host these Shiny applications, heavily via Docker such that each user, when they click on an app on your your ShinyProxy platform, it spins up a separate Docker container for that user, and that allows you to to leverage things like Kubernetes fairly easily, and it does allow for some, you know, additional security. You're able to deploy your Shiny apps, as as Docker images, which is is really nice in terms of reproducibility as well. But, you know, ShinyProxy is something that that we leverage pretty heavily both internally and for our clients. This was exciting to me and I know that there's some other folks out there that, leverage this project as well as it's it's one of, the the multiple Shiny hosting options that you could have. And the new features in ShinyProxy, 3.1 are, as opposed to what I just said, you know, when a user clicks on an app, it spins up a a brand new separate container for them. In 3.1.0, we have the option for some pre initialization of that container or sharing of that container across multiple users. So, sometimes when a Docker container sort of is is spinning up depending on how large that image is, it can take some time to spin up and users may have to wait, you know, 5, 10, 15 seconds, something like that, for that container to spin up and for them to actually be able to see their application. So 3.1.0 provides some opportunities for pre initialization of those containers and actually sharing, containers across users such that an already running container can be leveraged, by a second user.

The other, big update is that, you know, up to now, ShinyProxy supported 3 different container back ends. That was Docker, Docker Swarm, and Kubernetes. Now there is a brand new back end, AWS ECS, which is pretty exciting. I think AWS ECS, allows you to or or maybe, they take care of some of the server setup, and scaling that you might have to have rolled your own, if you're using Docker, Docker Swarm, or Kubernetes. So it's nice that that abstracts it away from you. But as Peter Salomos notes in his, hosting data apps blog that that recaps this 3.1.

O release. You know, one downside of of AWS ECS is that in most cases, it takes more time to start the container compared to other back ends. But now that we do have this this pre initialization and container sharing capabilities in 3.1.0, we might be able to reduce that startup time to something that's that's even less than a second, if we are able to take advantage of those new features. So, there's some additional stuff in there, but but really exciting, for myself and maybe those others in the community that that leverage ShinyProxy.



[00:44:28] Eric Nantz:

Yeah. These are fantastic updates, and I can underscore the importance of that pre initialization feature because sometimes with whoever you're, you know, sending this to, you might have to, have to think about, okay, what are the best ways to enhance user experience. Right? And loading times is unfortunately one of them for better or worse. So be able to have that ready on the fly, I think, is massive. And for all the DevOps engineers out there that may have been, you know, what we call an r admin that's administering shiny proxy or maybe working with IT to support that. What I really appreciate, as you mentioned, Mike, with the ECS, you know, new back end implemented, If you're wondering how that might fit for some of the automation, like deployment pipelines, they linked to in this post a, a repo that has an example for Terraform, which is a very popular dev ops, you know, system configuration platform for automation. So I I I have a few HPC admins in my day job that would appreciate seeing that because that's Terraform all the things with Chef. So this might be something that if we do shiny proxy in the future, I'll be sending this their way. But, yeah, really, really exciting to see these updates for sure.



[00:45:43] Mike Thomas:

I have somebody on my team who's big into Terraform. It's not something that I've stepped into yet, but it seems pretty awesome for sort of just writing maybe kind of like a a Docker file of your pipeline in terms of your entire setup. You know it's fairly easy to read, easy ish to read, and it's sort of just listing out the instructions of the infrastructure that you wanna set up. It's pretty cool.

[00:46:06] Eric Nantz:

Yeah. There's a lot in this space that I've been, you know, trying to learn up on on my time. I've gone kinda far, but I've been learning up on Chef and Ansible and a little bit of Terraform, but, there's only so much time in the day. But, nonetheless, there is so much more we could talk about in this issue, but we would be here all day if we did. So we'll just invite you to check out the rest of the issue at rnp.org along with all the other resources that we mentioned in this episode as well. It's all gonna be in the show notes.

And by the way, if you are listening to this show on one of those more modern podcast apps like pavers or found, you may have noticed we've actually upped the production quality to show quite a bit where now you get not only the chapters, which we've always done, we've got chapter image, images that will directly link to each of the highlights that we talk about. So I think that's pretty cool if you're taking advantage of the modern tooling here and also a big win for accessibility, each episode now has a transcript directly generated into the show, which, ironically, even Apple has adopted recently. So you should be able to get that in many of the podcast apps that you listen to this very show. And, we love hearing from you and the audience as well. But, of course, the art weekly project itself goes with the community. It's powered by the community.

So if you see a great resource you wanna share with the rest of the art community, we're just a pull request away. It's all at rwiki.org and click in the upper right for that little GitHub icon, and you'll be taken to the next week's issue draft where you can put in your link. And our curator of the week will be glad to merge that in after a review. And, also, we'd love to hear from you as well directly. There are many ways to do that. The fun way, of course, is if you're on one of those aforementioned podcast app, you can send us a fun little boost along the way, and we'll be able to read it directly on the show. And, it was fun to be on the Friday show because I I did get a few boosts sent my way because I was part of what they called the valued split at that point for that segment. So lots of fun shout outs about the r language and whatnot and are nerding out on that side of it. But, also, you can contact us on the various social media handles.

I am mostly on Mastodon these days with at our podcast at podcast index dot social. I'm also sporadically on x Twitter, whatever you wanna call it, with at the r cast and also on LinkedIn. Just search for my name, Eric Nance, and you will find me there. Mike, where can the listeners get a hold of you? Sure. You can find me on mastodon@mike_thomas@fostodon.org.

[00:48:38] Mike Thomas:

If you search for my name on LinkedIn, you will probably find way too many Mike Thomases and not be able to find me. So, you can see what I'm up to if you search Catchbrook Analytics instead. That's ketchb r o o k.

[00:48:53] Eric Nantz:

Very nice. And, yeah. That will wrap up the episode 100 and 65. We had lots of fun talking about these highlights of all of you, and we definitely invite you to come back again next week to hear us talk about yet more our community goodness of our weekly. So until then, happy rest of your day wherever you are, and we will see you right back here next week."
"54","issue_2024_w_19_highlights",2024-05-08,49M 6S,"Our take on the important conversations spurred by the recent R deserialization CVE, how simulations may save you from cracking open that probability textbook, and recapping the exciting 2024 Shiny Conference. Episode Links This week's curator: Colin Fay - @colinfay@fosstodon.org & [@ColinFay]](https://twitter.com/ColinFay) (X/Twitter) Everything…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 164 of the Our Week of Highlights podcast. This is the weekly show where we showcase the latest highlights and awesome resources that you can see every single week and the Our Weekly issue at rweekly.org. My name is Eric Nanson. Yeah. I'm on demand. I'm sounding a bit more like my normal self. So thanks to all of you for persevering through that last week if you're listening to that. But we have a lot to discuss today, and I'm not going to do this alone. And there's no way I could do it alone today because I have my awesome cohost with me, as always, Mike Thomas. Mike, are you ready for a jam packed episode?



[00:00:38] Mike Thomas:

I'm ready, Eric. The highlights are heating up. It's it's heating up here on the East Coast.

[00:00:43] Eric Nantz:

And I don't know if you wanna hear this or not, but my Boston Bruins are heating up as well. Yeah. You know. Yeah. Yeah. At that point, you know, when you get to game sevens, they're a coin flip. But, yeah, once they got through that pesky, Toronto Maple Leafs, now they're gonna they're gonna give those, dreaded Panthers a run for their money, it looks like. So fun times ahead for you. We'll see. We'll see. Well, revenge in action because last year, they, they got a rude awakening and 7 games from them. So maybe the revenge factor strikes again like it does in pro sports. But, yep. But we don't have any revenge to conduct against each other. We're good friends as always. But, of course, this issue is, of course, not built by us. It is built by the R weekly team. And in particular, this issue was curated by the esteemed Colin Fay, of course, the architect of all things GOLM and many of our shiny awesome packages.

But, of course, he had tremendous help from our OurWeekly team members and contributors like all all of you around the world. Well, if you had to guess where we're gonna start, it's probably not so much a mystery because this has been making the waves for the past about 2 weeks now in the R community and even other tech sectors outside of R itself. We are, of course, talking about the recently disclosed critical vulnerability exploit or CVE that was disclosed with respect to the serialization of particular r data types with the RDS format.

And we had a little bit of a mention of this last week because it was literally late breaking as we're recording in last week's episode. But if you're not familiar what we're talking about here, it was about, about near the end of April, April 29th, to be exact. A vendor called Hiddenware, who I've not heard of before this particular story, had this had disclosed the vulnerability called CVE 202427322. I don't expect you to memorize that, but all the links are in the show notes. But this is describing that an exploit that you can utilize in previous versions of R up to version 4.4, where apparently this has been patched, where you can inject arbitrary code to be executed into an RDS serialized file through means of clever uses of the headers inside that file and taking advantage of concepts like PROMISE for evaluation and the like.

We won't read that post verbatim here. You can look in the show notes for it. But as I expected, when this broke, I thought there would be some additional either fallout slash follow-up from the community, especially for those that are in tune with both the intersections of data science and infosec or information security. And this first highlight is one of those responses we're gonna dive into along with a bunch of related resources. And this post comes from Ivan Krivlov, who I believe has been featured in previous highlights.

And his post succinctly titled, everything you never want to know about the r vulnerability, but shouldn't be afraid to ask. I think that's an awesome title because this is really going to break it down piece by piece in terms of what this practically means in terms of our day to day for the usage of R and just what is going on under the hood. One thing that we're going to say right off the bat is that this specific instance in this CVE is just one example of how potentially, and I do mean potentially, someone could exploit certain features of the R language to, you know, do malicious things on a on a somebody's system and whatnot.

But these are features of the language itself, and R is most definitely not alone in having these features in place. We'll get to that in a little bit. But in essence, Ivan starts off the post with kind of some background on the RDS and R data file formats. Where they are, their job is to directly represent the state of an art type of object. And I mean really represent it. In essence, serialize it such that it can be completely reproduced on another system or another installation of R via the loading functions for that particular object.

Now these objects do, by nature, often contain code that needs to be executed on the system related to R itself. A good example that he calls out here is a lot of those model fit objects that you might pass around based on a linear regression fit or whatnot. It's gonna contain code VSA s three methods or others under the hood to do certain things with, say, the data that was supplied in that model fit, looking at model assumption, you know, model metrics and whatnot, that's by design, folks. That's always been there.

So, yes, what this vulnerability really is is just finding clever ways to inject additional execution of code in these places, in these serialized objects so that you could take advantage of it. But if you really wanna get really deep it, it's not that easy. He does have examples talking about the anatomy of a particular vulnerability like this, of how you might have to get into the source of R itself and how it serializes objects and try to interject things in between if you want to do something really nefarious.

But in the end, this is all things that have happened in other languages as well. And that's where he leads off with, at the end, you know, our friends in the Python community, this is something they've been dealing with for years with respect to an arbitrary serialized object called pickle, often used in machine learning model fits, prediction modeling in general. There's nothing stopping a nefarious person from injecting Python code. He got somewhat hidden in that object where if you don't know to look for it, you might miss it.

But I'm going to take a step back here and think about an example that I've been telling people as they've been asking me about this. We live in this era of digital communication. Right? And what is the most you know, one of the most direct means of communication these days is the good old email message. Right? You often have text and email, but you might also have something else with it called attachments. And I know in my organization, they put us through very rigorous training to say, did you get an attachment with someone you trust?

If not, you might wanna think twice about opening that up on your system. And that's where I think the CV is bringing the light maybe something that I don't know if glossed over is the right word, just hasn't been talked about much, is that these serialized data objects have always had this capability. The question is, do you trust the process that's building it and who is sending that to you? And with that, on top of Ivan's post, we've got other terrific resources here that I'll put in the show notes that really dive into the practicalities of what the CV really means.

In particular, Bob Rudis has an excellent post on his blog, and he is very, opinionated on this. Although, frankly, I share his opinion. This probably never should have been a CVE in the first place. Again, this is not a unique problem to R. This is the language operating as, quote, unquote, its design. It doesn't take away our responsibility as end users to be, you know, critical of when we're receiving these data objects, verifying the source. And in particular, he's got a proof of concept in his GitHub repository that will let you scan these data objects to look for potential vulnerabilities or potential nefarious code injections inside.

That's just one example. And another great example linked to is Josiah Perry just has a very brief or to the point video on his YouTube channel about how you can take advantage of these exploits as well, again, using fundamental features of the language that are not gonna go away anytime soon, nor they should of you know, executing arbitrary code in s 3 methods and other, you know, methods that you get when you load a data object into your memory. And then a couple other things before I, well, glide down here is that the concept of CVEs in general, while, of course, I acknowledge they are hugely important in the realm of security, there's always more than meets the eye about this. So I found this great interesting post from Daniel Stenberg who has a interesting take on the CVE process and some of the nuances behind it in light of disclosures back a few years ago with respect the curl utility and other frameworks.

Just something to have in the back of your mind as you see this reported in the general tech sector of these outlets that aren't really going really deep into this story. And that's what we're trying to do here. We want you to give you the resources to do more follow-up on your own and make your own call about what this means to you. We're just sharing what it means to us. And then lastly, if this whole data serialization topic is new to you, like it was to me even just a year or so ago. Another link I'll throw in the show notes is Daniel Navarro has an awesome blog blog post, easy for me to say, about how data serialization and r really works, in particular, the RDS format. So you can kinda see the nuts and bolts of how this works. So my takeaway is I think this v CVE, while I don't really agree with how it was disclosed and highlighting a feature of a language itself that's not in and itself a vulnerability.

I am at least you know, happy is probably not the right word, but I am glad that it's bringing to light the the discussion that we've had internally in my company, but also others in the community about really being responsible about how we're using these data that's supplied to us or being built by these other processes. So I think that's good that's good discussion to have. I've seen it on Mastodon. I've seen it on LinkedIn and other places. So maybe it is that wake up call that the community needed that the art language has grown enough in popularity

[00:11:20] Mike Thomas:

that now we really have to think critically about these methods that could potentially be used again for nefarious means. But, Mike, I've rambled enough. What's your take on all this? No. Eric, I share a lot of the same thoughts, and it's I'm glad that you pointed out Danielle's, blog post there because that was one that came across my mind when I was reading, you know, all the different blog posts around this topic because really what underlies the issue here is, you know, serialization and deserialization of of our objects and how that works. And I think a large, you know, sort of, point to projects like the the Parquet project was to try to get away from some of that serialization and deserialization of language agnostic or or yeah, language specific, I should say, you know, formatted objects like RDS files, like pickle files, and to try to be able to store data in more of an agnostic way, that doesn't involve serialization that allows users, to be able to to to work with that data no matter, what language they're in. But obviously, we don't have something like that when it comes to something like a model object, right, or storing, you know, pretty much anything else, you know, in an RDS file or and I believe on the Python side in a pickle file, you can pretty much store anything you want in those files, which is incredibly useful, when it comes to doing things like machine learning and and having to, you know, deploy a machine learning model. Right? You want don't wanna have to retrain that model, as part of your production process. You wanna train that model, store it as a some sort of single object that makes it very easy to make predictions, to load and make predictions against. And that's where a lot of the times we'll use RDS files or or pickle files on the Python side.

But, you know, I certainly share your sentiment that if nothing else this CVE brings to light, you know, the point that you need to trust the sources of your RDS or your pickle files, completely. And, you know, if someone I I thought of this this sort of use case here that I think I've seen before, you know, in, like, a GitHub gist, you're you're trying to solve a problem. You're you're diving deep into to GitHub and to Stack Overflow, places like that, and you finally, you know, see somebody who has a reproducible example with some code, but in order to run that code, right, they they've also attached a file. There could be a CSV, could be an RDS file, a pickle file, something like that. And you you need to have that file to be able to run their code. Right? They weren't able to create, like, a full reproducible example for for whatever reason just based out of code where they they sort of simulated that data. You they're providing you this file that you're going to need to run that gist or run that example.

And maybe in the past, you know, depending on where that file was was located, depending on, you know, who the author is of that that gist. If it's a repository of someone, you know, well known that you trust, maybe it's in the Pandas project or the DeepLiar project and it's posted by, you know, somebody from somebody from Posit or or Wes McKinney or something like that, maybe you're you're likely to trust that file and to download it and run it against that code. But if it's, you know, that that gets into a very sticky situation. And I think it's just a good time maybe to remind your team in situations like that that they need to exercise extreme caution when opening and opening any file. Right? And and the the open source community, I think relies on a lot of trust for better or for worse. Right?

And, you know, I guess this is just a good time for us to sort of step back and remember that, you know, there there could be bad actors out there and that we we have to deal with a lot of skepticism, in order to ensure the safety of the data products that we're creating and deploying to production. So, you know, like I said, if nothing else, you know, this is maybe a good time to to do that step back and to think twice. You know, admittedly, when this came out, you know, the the fact that that NIST, which I believe is, like, the government, entity, National Institute, Institute National Vulnerability Database is the NVD, place where this got posted to. And NIST is the big, you know, cybersecurity risk management, you government entity out there. So whenever you see something come across NIST, you know, and I'm thinking of, like, you know, a lot of the organizations constantly watching what NIST is publishing.

So if this came across their desk and, you know, NIST is sort of blasting this this new R vulnerability, It's something that we need to pay attention to and we need to engage in communication with those stakeholders to to really bring all sides of, you know, the the discussion to the table so that we can sort of understand, you know, exactly how this impacts us, what what's the messaging that needs to take place, What are any changes, if any, you know, that need to take place? And, you know, I think, again, sort of some of the the blog posts from from Bob's, sort of some of the blog posts here that we're we're reading from Ivan, you know, may just reiterate the the need that the the messaging, and the conversations, and the reinforcement of, you know, best security practices is is maybe more important here than actually making any changes to your R infrastructure.

So admittedly, you know, when I I did see this initial blog post come across from Hidden Layers and, you know, this new entry in the national vulnerability database come across, it was, you know, it was sort of big scary news a little bit. I put out a LinkedIn post, you know, talking about that we're we're sort of advising our clients to up to to our 4.4.0 for new projects that they have and just to exercise that caution on RDS files or RDA files or any sort of, you know, pickle type file that, you know, you don't know exactly where it originated from. I I think those two pieces of advice are are still, you know, solid pieces of advice But in terms of, like, going backwards and and blowing up any processes that you had in place that maybe don't have anything to do with with the RDS or RDA files or something like that. I I don't think there's a need for that, at all. But obviously, a lot of opinions on this subject. I'm glad that we're getting a lot of opinions on this subject and folks are taking the time to really dig in here, but but this is sort of a a great blog post from Ivan to provide that perspective. It comes with a lot of links, a lot of citations, and I sincerely appreciate his work on this post.



[00:18:16] Eric Nantz:

Likewise. And, yeah, there's been there's been a a few others as well, and Bob's been quick to acknowledge others in his blog posts that have really been looking at these issues, you know, for a long term. But now ever since this came out, they were looking at even more. So Conrad Rudolph's another one. I call Davydov is another one if I'm saying it correctly. They've been on Mastodon looking at different proof proof of concepts of how this exploit could be utilized. But, again, the message is is that the language itself is still doing what it expected, that that it it hinges on this type of feature in terms of, you know, serializing data objects as efficient ways of transferring and whatnot.

But I do like your your callout about other formats that are taking shape in the community, such as parquet and the like. And with respect to putting on my life sciences hat for a second, there was questions I would get about, what does this mean in terms of if we use r for clinical submissions and whatnot? Well, guess what? It's very strict guidelines with respect to regulators that we transfer data in a very specific format that is not a serialized object. They have very rigorous procedures in place to eliminate the hint of any file being transferred that is, quote, unquote, executable in that sense. So we wouldn't even be bothering with these formats in a clinical submission as as of now.

But the other piece that I thought about, and I may mention this at the end of the episode, is I've been been working on a side project with a portal dashboard and GitHub actions that does duplication analysis but, and also a data quality check. But then I'm sending extracts of, like, each of the, quote, unquote, failed quality checks in 2 file formats, RDS and parquet. But the key point is that if someone is wants to inspect that themselves and maybe they are our user as well and they wanna see the RDS file for that particular step of, like, looking at duplicate IDs in a database or whatever.

1st, they got a choice of which data format to pick from. But second, if they wanna see how I produce the RDS file, guess what? It's all in the open. Right? That's the nature of open source. I have all my my my GitHub action, our script, all in there that's run every week, and they can see for themselves what algorithm I'm using, how I'm serializing at the RDS, and, hence, transparency when it is available, I think, is paramount in these situations. So I think, you know, leaning on that but also leaning on the practical, you know, you know, practical, you know, thinking process, thought process of, you know, being critical about your data sources and then being transparent to your customers or your other colleagues about how this is being produced.

I think that is being brought to the forthright foreflight here, so to speak. And I hope the discussion keeps going in this direction and not just tunnel vision on this very specific CVE itself because that's missing the point. This when we bring back the point to the general use of open source, the use of these serialized formats in general, again, nothing new in this ecosystem. It just so happens that it got popular enough that it became the light. So I guess that's that.

[00:21:38] Mike Thomas:

Yeah. Or if, in Bob's take, you know, I think there's some sort of a conference coming up and somebody may have been looking for a little clout, heading into that conference. So who who knows? A lot of a lot of different possibilities out there, and I would definitely, sort of recommend that folks take the time to read up on this across all these different perspectives and and form their own opinion and and figure out how to, best communicate this and and make any changes necessary within your own organization.



[00:22:16] Eric Nantz:

Well, Mike, let's lighten up the mood a little bit, shall we? Because, we're gonna we're gonna go back to school a little bit. But I hear there's a way we can even circumvent some of those hard math details in our next highlight here. And this is coming from Andrew Heiss who has been a frequent contributor to the our weekly highlights in in many episodes ago. And he has this great post on how you can take advantage of a method I've been utilizing in my day job for years, of simulation to help calculate probabilities where you might have had to rely on those, you know, famous or infamisimally or prospective math formulas that you might have seen in grad school. So what are we talking about here is that this is motivated by situations where maybe you're asked or you're teaching a concept that has to deal with some very, you know, intricate concepts and probability.

I don't know about you, Mike, but I remember in my stats or math coursework, whenever you got the probability, you always were hit to that problem of you got, like, 1 or more of these urns of red, green, or yellow balls or whatever. Nice. And you gotta pick pick which what's the probability of getting, like, at least one of each color or 2 colors and not that one and whatnot. That's where Andrew in this post starts with, you know, a little refresher on how to use common metrics to represent the pool of choices from these balls and these earns and choose k, you would often call it. So the formulas are all there.

You I remember in the old days, I used the old TI 85 to calculate all this. Shout out to all the TI 85 nerds out there. We may or may not put games on it, but that's what I'm sorry for another day. But in any event, guess what? You can do this in r itself. There's the choose function. Right? There's choose, and you can say out of 40 balls, choose 5. What are the number of possibilities you can get there? And then, of course, you can do that for your answer. It gets a little more complicated when you get to more nuanced questions of maybe at least one red, blue, or green ball. Then you can see in the post here, there's lots of different combinations they have to sort through in your numerator and denominator.

And, yes, you can use r for it, but, again, you've got you've got a lot going on there. Well, Andrew mentions way back in his training in 2012, he had a professor that kind of mentioned a comment at the end of one of his lectures that, you know what? You could get a similar answer as what you're doing in this math probability through simulation. And, of course, I'm sure if I had heard that at the time, I probably would have had that light bulb moment in my head too of like, hey. I wonder if I could circumvent some of this gnarly math formulation. And sure enough, you certainly can.

So Andrew dug up some old some of his historic r code. Again, credit to him for being transparent on this of a 4 loop that he created to simulate that earned problem of choosing at least one color probability of choosing at least one color out of that. And it's a pretty intuitive for loop. Right? You'll see it in the post, but when you read it, you grok what's happening there without the need of those fancy combination formulas. Now where does this mean practically? Well, he was inspired by a post he saw on Blue Sky of someone asking in their household.

They have an n o six. They have all their birthdays inside a 6 month interval. How likely or unlikely is that in practice? That's an interesting birthday problem, if I dare say so myself. So the rest of this post goes into all the different ways that we can address this. And one neat way to address this first is taking advantage of a unique geometric representation to see just how this shapes up in terms of how the months are related to each other. And he mentions creating a radial chart of sorts, taking advantage of a newer GEOME that's supposed to ggplot23.5.

And there's code here in the blog post to do this. I believe it's called GM radio or if I recall correctly. Yep. Oh, cord radio. Yep. Yep. And there's some great visuals here, for his household where these birthdays fit birthday months fit in this spectrum. And if they're in a 6 month window, they're going to be all within a half circle radius of each other or half circle on the on the perimeter, if you will. My geometry is rusty, folks. Bear with me here. But it gets even more gnarly of that when you start thinking about, well, now let's try to generalize this a little bit. And what happens in the case where a birthday is from a previous year, but it's still within that 6 month window? How do you navigate representation on that?

So he's got some examples of how you do a clever use of additional segments to go from the previous year to the January 1st date of the new year and visualizing that. So there's great code in ggplot2 that walked through how he is able to accomplish this visualization and how it handles birthdays outside that 6 month scope and wealth within it. And like I said, with years overlapping and whatnot, but that's just the visualization part. Now, the actual calculation of the probabilities at hand, this is where I've always said dates, times, time zones are hard. And this is another situation here because you can't quite get away with representing the birthdays on this perfect circle because in calendar years, we got different days a month. We got the leap year to deal with and whatnot.

So there's a lot of custom code that's needed to derive things like the distance between the the earliest date and the latest date, accounting for year transitions, trying to transform this into a pseudo circle representation. There's a lot of interesting code here that's doing a hybrid of, you know, per map reducing and arithmetic with respect to these birthday dates, sometimes having to multiply time spans by 2 and whatnot to make this happen. And then once you get through kind of the algorithm side of it, now you gotta actually perform the simulation.

And, again, taking into account leap years and whatnot. So he does come up with a function that takes advantage of some of the algorithms he develops early in the post to start simulating this. And, again, at this first glance, from a uniform type setup, assuming all birthdays could be, you know, occurring equal on equal days in the calendar. On the surface, that seems like a a gen like a reasonable assumption. Actually, not so much. And this is where at the end of the post, he discovers that there are sources from the CD and the Social Security Administration here in the US that actually keeps track of daily births in the United States from in fact, in 538, that portal had a story in 2016 about the patterns in this. And guess what? They put their CSV files on GitHub that they use to assemble all this. So guess what?

Now instead of assuming that it's a uniform distribution of these birthdays falling on any of the 365 days of the year, we can actually sample from this known set of prior birthdays to get a more accurate representation of that distribution in in daily practice. He does this nice visualization of the kind of a heat map here showing that there are indeed portions of the calendar where birthdays are more common, such as, like, not many are current around Christmas and New Year's. Yeah. I wonder why, but that's an interesting thought exercise for everybody.

But it it's right there. Yeah. You can see it's clearly not uniform. So he's able to update his probability functions to utilize this type of distribution approach to sample from here. And then in the end, we get a nice little bar chart at the end or a lollipop chart, depending on how you wanna call it, where you look at, depending on the household size, the decrease in probability that all these birthdays occur in a 6 month window. To no surprise, when you have less number in the household, like 2 or 3, although, actually, 2, looks almost a 100% confident that you'll have birthdays that fall in the 6 month time span. But as you get the 3, 4, 5, 6, you start to see that kind of exponential decay, if you will, of seeing how unlikely it is to when you have a larger household, such as, like, 6.4% for an 8 person household.

But in the end, what is not in all the functions that he produced here to derive this answer? No call outs of probability formulas. No calls to that choose function. It is all based on a rigorous set of simulations. I admit there's a lot going on in this post here, but in the end, the approach is what I wanna take away from here is that when you're in doubt to find that maybe close form solution or that neat probabilistic, you know, derivation or formula calculation, simulation is your friend here. And with the computer technology we have and modern systems and whatnot.

You can get a great approximation to these answers even for fun problems like this birthday problem. So, again, felt like I was going to school here, but it was a great way to tie in multiple things that I do in the day to day. But explain in Andrew's a really great,

[00:32:04] Mike Thomas:

technique here. Well, I felt like I was going to school too, and I think it's fitting because I believe Andrew is a professor and he is also a machine because, this is a a lengthy phenomenal blog post with, you know, looks like it might be authored in in quarto or something like that with fantastic narrative, inline code, all these beautiful gg plots. You know, that's sort of what really stands out to me. It's just really the attention to detail on a lot of these gg plots as well. And I too, Eric, like you, really wish that I had been taught probability and statistics, with the concepts of of simulation, instead of the more theoretical approach that I was taught with. I think my grades probably would have been a little bit higher in that class if that had been the case, but, you know, what can we do now? It eventually clicks fortunately, and I imagine that Andrew's students are probably pretty grateful of his very practical and applied approaches, to explaining and demonstrating concepts like this. I I too thought it was pretty interesting, the idea that, birthdays maybe are not sort of normally distributed.

Although, it looks like the difference between his simulation when he made that assumption that birthdays are normally distributed across the year, versus, you know, when he went to using the data from 538. The differences there are are pretty marginal, you know, with his household size of of 6. You know, assuming that there was a uniform distribution of birthdays, I think his simulation returned like a a 17.8% chance of all 6 of those birthdays occurring within the same 6 month window. And when he used that 538 data to simulate from, the percentage actually jumped or the likelihood actually jumped from 17.8 to 19.3.

So a a little bit higher probability with 6 folks in your household that they will all have a a birthday within sort of a same 6 month window. I I thought that was, you know, fairly interesting. And really, it's the approach here. It's the practicality. It's the the tools that he's giving us to to execute the same type of analysis that I think are super useful. So I would recommend that that anybody who is maybe, having a difficult time or or just needs a refresher on, you know, sort of basic probability and statistics via simulation, check out this blog post. It's fantastic top to bottom.



[00:34:38] Eric Nantz:

Yeah. And I dare say it's also kind of a little gateway, if you will, to the concepts of Bayesian statistics as well with this idea of you may not know in real life what that distribution of outcomes or that set of data you're dealing with. When in doubt, you'll take what you do know, try to use that as prior information. Again, I'm using this on purpose, but these are things that again, when I was in school, I didn't know heads or tails about Bayesian probability. I was over in the Bayes theorem, which we just glossed over in 1 week, and then suddenly I'm off to the next convergence stuff.

That is a fascinating world that we often dealing with with respect to the multitude of data sources we have. And it is very difficult to assume one type of specific recipe, if you will, for how these are being created in the real world. So simulations combined with Bayesian methodology, I think they put so much in the hands of you, the data scientist or statistician, to come up with meaningful answers in light of not knowing what is the real source of truth here. So, again, it's a great it's a great, you know, pseudo, you know, gateway to to that side of the world. And, yeah, I probably would have had a little better grades if I had something like Andrew as a professor, but we don't have that time machine yet, Mike. And, I dare say we ended up pretty well nonetheless, but, you know, hindsight is 2020.



[00:36:02] Mike Thomas:

I think so too, Eric.

[00:36:14] Eric Nantz:

And lastly on the show today, we're gonna talk about we referenced this a little bit last week, but we had the good fortune, both Mike and I, of attending the recent, shiny conference, 2024, that was held by Absalon. And our last highlight today is a little, very nice and insightful blog post that they had that they wrote earlier this week on their particular highlights from the conference. And trust me, as someone who's been on the other side organizing these type of events, it is always great to reflect back, and I think what they share is what I share as well.

The content that we saw at Shining Conf was absolutely amazing from top to bottom across many different spectrums of innovation on Shiny itself the respect to doing data for good, which is a track that was led by our good friend, John Harmon. And then our shiny life sciences track, I had the pleasure of leading that track and having a lot of great content as well and amongst much more about Shining and Enterprise as well. They first mentioned, you know, talk that we actually talked about in the highlights last week, the tailoring Shiny for modern users. There's a shout out here in the post about that.

You gotta listen to last week's episode for our take on that particular talk. But, also, one of the keynotes that gave a lot of attention there were a few keynotes, actually. But Joe Chang, of course, the CTO and author of Shiny himself, talked about the async interest session concurrency in Shiny with extended task. This has been in the works for a long time, but when the recording comes out, you'll definitely wanna watch this talk because if you've been down the promises train that was talked about years ago in one of the, Rstudio conf keynotes, You know that wasn't for the faint of heart. This is really trying to put async within Shiny apps in a much more accessible way to developers.

And there's a lot to come in this space, I'm sure, as more teams adopt this. As well as, of course, the keynote that I was fortunate to recruit, George Stagg from from the Pasa team to talk about reproducibility of WebAssembly and WebR. And boy, oh, boy, there's some nuggets at the end of that talk. And when they come to fruition, oh, boy, the game is going to change in respect of reproducibility and package versions and the like. There's some big, big stuff happening in this space. So that was really fun to have charts talk about that as well. We're also fortunate to have Pedro Silva from Appsilog, give his keynote on the past and future are shiny and and so much more. And, apparently, there is an announcement that they're gonna have a new open source package in the works with respect to, I believe, something to do with Rhino.



[00:38:58] Mike Thomas:

That's, that's a tease here. So I think it's already I think it's about a time between when we're recording today and when this blog post came out, I think it's already out. It's a new package called Tapyr, t a pyr, from Appsilon, which is sort of their Rhino approach to Shiny for Python. So it's building modularized Oh. Shiny for Python applications.

[00:39:21] Eric Nantz:

Nice. Nice. So they they put the t's out and, apparently, it was announced after the fact. So, yeah, congratulations to them for that monumental achievement. And there were some fun stuff in in the conference as well. In fact, they had a little are which package are you quiz, which is a fun one to take. I remember taking that, and I I actually forgot my answer already. But I think it has something to do with data modeling or something like that, which probably isn't too surprising for me. Although I thought it might be a shiny thing, but I got gross. I'm a I'm a complicated case. Do you remember what you got on that score, Mike?



[00:39:55] Mike Thomas:

I don't quite remember what I got. I might have missed that, I might have missed that game, but I would be very interested to see what I would come up with. Yeah. You have to try that out after this episode for sure. It might be a more complicated case, Eric.

[00:40:08] Eric Nantz:

Yeah. Yeah. We're we're we're we're, we're complicated folks, aren't we? But, but in the end, this was, I believe, the first of the shining conferences that had the multiple tracks. So, again, there's always pros and cons of that approach, but in the end, really excellent content nonetheless. And, hence, when the recordings come out, you might be able to catch up with what you missed. But, overall, that was a fantastic event, very well attended, and the engagement from the audience was top notch,

[00:40:37] Mike Thomas:

from top to bottom. So, Mike, what are your impressions of what you saw there? Yeah. That was fantastic. I mean, you covered a lot of the the ones that were big highlights for me between Joe Chang and and George Staggs. Another one that really resonated with me because I really appreciate someone who is actually trying to take an applied approach to quote, unquote AI was Tanya Casarelli's, best practices and lessons learned. I guess it was a real world use cases for AI in Shiny, which was just a fantastic talk and great to see a practitioner out there actually, you know, putting, you know, fingers to keyboard to spin up solutions that are helpful, for her clients with, you know, AI and Shiny as opposed to just a lot of those vendors out there that are just sort of yell AI at you with, with no sort of, you know, execution plan behind it. But I'll digress, you know, it resonated a lot with me because I have to give a very similar talk on applied AI use cases at a conference in August. So I may study up on on her materials again and rewatch it in on the recording in the RingCentral platform which folks who attended, have the ability to do.



[00:41:54] Eric Nantz:

Awesome. Yep. And like I said, watch out for those recordings. But, yeah, there was, I think, something for everybody. And, again, excellent, job by Epsilon, and congrats to them. Hopefully, they're getting some well deserved rest after that because I know after the our pharmas that I've helped organize in the past along with an excellent team, we're like, okay. We can exhale now, and then give us a couple weeks before we think about the next year. So in any event, what you do have to think about is where do you find content like this every week? Well, it's our weekly dotorg, of course, and we got more than much more than what we just talked about here. Collins put together a fantastic issue, and we'll talk about some additional fines over the next couple of minutes here.

I'm gonna I'm gonna I'm gonna give a little plug to Collins organization, ThinkR, on their blog. They have a great blog post. If you ever felt a little intimidated or don't know where to start with after you find that awesome JavaScript library online, but yet you're not quite sure how to put that into your Shiny app, they have a great, example here about taking the Sweet Alert 2 JavaScript library and putting that into a Shiny application, which to no surprise is powered by Golem. So this is a great, you know, compliment to what you might find in the existing, like, engineering production grade shiny apps book that I think R and Colin Fay have authored.

If you want a kind of a quick take and you found that Sweet Alert 2 utility, how do you bundle the dependencies together? How do you inject that into your golem app manifest? And how do you build, you know, easy functions on the other side of things to call that JavaScript and passing the data back and forth. It can definitely be very helpful to see how this works in the day to day because in the end, you're probably if you're in the shiny space for more than a little bit, you're gonna be asked by a customer or maybe even yourself to bling it up a little bit, give a little more pizzazz with that awesome utility. And you're like, wait a minute. There's no package for that. What do I do? Well, this approach is a is a great way to call that out. And I didn't know there was a new Sweet Alert 2 library. I've been using Sweet Alert via Dean Nitelli's shiny alert package, so maybe I need to throw him a note to see if he's interested in something like this. But in the event, as a shiny nerd, that was right up my alley.



[00:44:10] Mike Thomas:

Me too, Eric. That was a great find. One that was also right up my alley was a blog post from Athanasia Moical on the different IDE's that she uses. And, she flip flops sort of back and forth between RStudio and Versus Code in a way that is resonates very much with myself. She's Rstudio. Typically, for anything R related, if there's a a git conflict, you know, that's when she'll spin up a Versus Code window to be able to solve, those different git conflicts. And then for just about every other project or language, she'll use Versus Code. And there's just some great comparisons across the different features in RStudio, Versus Code. And also, you know, she talks about her her journey, you know, in her early days. She actually started out in MATLAB, as well as, like, the the Vim editor.

And it really compares, you know, the features that she experienced along that journey, to the different tools that she was using across the different tools that she was using. And, it's a great rundown, resonated a lot with me because I think there's a lot of folks out there that are, you know, seeing some some of the different ways that you can do different things across different IDEs, even though a lot of the times it's it's just a matter of of preference. You can pretty much accomplish all the same stuff, depending on on which IDE you're in. It might just be, you know, different keyboard shortcuts. It might be, you know, different options to to set up, and and click on. But it resonated a lot with me and for any folks who are trying to navigate which IDE is best for them on their project. This might be a nice blog post to read.



[00:45:49] Eric Nantz:

Yeah. And being, in my opinion, having the awareness of each of these out there, but then knowing what are the best use cases for your environment or your projects is hugely important. I have an aspirational goal. I want to be just as fluent with using, say, the combination of Vim or Neovim with the r plug in as much as I am fluent with RStudio and Versus Code. That's my aspirational goal. I don't know if I'll ever get there, but, boy, I'm gonna keep trying more than ever. I've seen it. It's it is cool. It's very cool.

Yeah. There's I've seen some screen cast by some good friends, and I'm like, how did you pull that off? Whether it's VIM or Emax or stuff. Some good cred there, I must say. But, you know how you get good cred, you read the rest of our weekly, I tell you, because it's gonna give you a boatload of additional insights that we couldn't cover today. So, again, if you don't know where it is, I'm gonna tell you again, it's rweaker.org, and we love hearing from you in the audience. We have a few ways to do that. We have a contact page in the episode show notes, Also, we have a modern podcast app, like, say, Poverse, Fountain Cast O Matic, CurioCast. So there's a bundle of others. You can send us a fun little boost along the way, and we'll get that message directly sent to us. And we'll be able to read it right here on this very show.

And, also, we are sporadically on social media as well. I am mostly, I'm masking on these days of at our podcast at podcast index dot social. Speaking of podcast index, yours truly has a good fortune of joining Adam Curry and Dave Jones on the podcasting 2 point o show this Friday to talk about a quartile dashboard that I built with our quartile to visualize the podcast index database duplication analysis and quality checks with the point blank package. So if you wanna hear me talk about that, tune in this Friday around 1:30 EST.

But in any event, I'm also on the, LinkedIn as well and also on Twitter with at the r cast.

[00:47:48] Mike Thomas:

Mike, where can they find you? Eric, if I was in charge of editing this podcast, I would insert some celebratory applause. That is awesome. I'm very excited to listen to that that episode, and I'm sure that the folks, working, you know, on that project are are super, appreciative of the work that you put into building that quarto dashboard. Is that publicly available for folks to check out? It sure is. Oh, man. We'll put a link in the show notes. Put a link in the show notes. I would love to take a look at the final final product. I think I've seen some draft iterations as you've been working on it but I'd love to see the final products. That's awesome. Folks can find me on mastodon@mike_thomas@phostodon.org or you can check out what I'm up to on LinkedIn if you search for Catchbrook Analytics, k e t c h b r o o k. That's where you'll be able to to check out what I've been up to lately.



[00:48:37] Eric Nantz:

Awesome stuff. And, yep. So we're gonna wrap up here on this edition of our weekly highlights, and, hopefully, you've been able to take away some good insights from us. We knew that this was a very important week for our show because of the big story that was at the top, but, certainly, give us feedback if you enjoyed the discussion or if you have other things you want us to discuss. In any event, we'll close-up shop here, and we'll see you back here for another episode of our weekly highlights next week.

"
"55","issue_2024_w_18_highlights",2024-05-01,36M 55S,"Why R 4.4.0 may reduce your trips to a certain kind of stack overflow, a call to update your favorite Shiny application code snippets, and how the steller ASTHOS Profile Shiny dashboard has your hosts blown away and fighting the urge to refactor their applications UIs! Episode Links This week's curator: Eric Nantz: @rpodcast@podcastindex.social…","[00:00:03] Eric Nantz:

Hello, friends. We're back of episode 163 of the Our Weekly Highlights podcast. This is the weekly show showcasing the latest innovations and terrific resources that have been highlighted at ourweekly.org. My name is Eric Nanson. If you couldn't tell already, your your trusty host here is feeling a little bit under the weather here, but I'm gonna power through this because it's always exciting to talk about awesome art content with my trusted cohost right here virtually, Mike Thomas. Mike, I hope you're doing better than I am these days.



[00:00:34] Mike Thomas:

I am, Eric. My allergies are in full bloom. Had a little bit of a tough time on the the golf course on Sunday, both allergy wise and golfing wise. But, we'll not talk about the latter. And but everything else is is going pretty well. I'll tell you what also bloomed was engagement on my LinkedIn post on dev containers. So keep gonna keep rolling on that topic. It seems like it's something that people enjoy hearing about. Yeah. I'm really excited to see see how much that took off, and I am

[00:01:07] Eric Nantz:

super thrilled that it's helping your team, you know, enhance your work flows. And, you know, once you once you go that route, it's very hard to go back because it just makes things so much easier to get effective collaboration in your dev environments because ain't nobody got time for package mismatches and all that fun jazz. Right? No. And system dependencies, system libraries, and stuff like that not being installed.

[00:01:30] Mike Thomas:

Yeah. It's it's really changed the game for us from a collaboration standpoint. It it makes collaboration pretty much frictionless, which is amazing.

[00:01:39] Eric Nantz:

And, yes, we are here to talk about our latest issue. And let's see who was that curator. Oh, yeah. It was me before I got this bad cold. So luckily, I was able to knock out some awesome resources here. Really terrific stuff we're gonna be talking about today. And as always, I have tremendous help from my fellow curators and contributors like all of you around the world. I will be honest. There weren't any poll requests this time, but luckily, many of you have submitted your websites to rweekly.org, so you're able to grab that content dynamically and put it into this issue.

So without further ado, here we go with a very important highlight to start with because version r4.4.0, codenamed puppy cup, has been released as of last week. And as always, it's always a big event whenever a new version of r has been released, and a great summary of the big changes has been put together by Russ Hyde, who is a data scientist at Jumping Rivers. And Russ leads off this post on the latest updates of 4.40 with a very interesting example on a concept that I don't take as much advantage of, although it can be very important, especially in situations where you don't quite know how to get to the right answer, so to speak.

And this example is, of course, based on recursion. And, again, this is a paradigm often used when we're trying to get a close approximation to a type of solution. But, again, the path to that made me unclear, so to speak. So we may have to approximate running a function over and over again and then maybe build some either stopping rules or other criteria to say, how close are we? Are we close enough? And the example in the post that Russ puts in here is having to do a finding zeros in a mathematical function, such as, like, the sine curve and whatnot.

He has a nice example of, like, if you're gonna build this function from scratch, how you might how you might approach it. He is quick to mention that there is a a built in function in R that would be better for this, but he calls this a bisect function and it's got some mathematical operations under the hood. But at the end, you'll notice it calls BISECT again right at the end of the code. So this is gonna keep running until it hits the right criteria for, say, getting that answer in a tolerance limit. Now beforehand, you might not know how many times this is gonna have to be called. You hope it's not very much, but it could be it's gonna be called quite a bit, perhaps 1,000 or even 1,000,000 of times.

And what happens when that happens? You fall victim to a Stack Overflow error. And there's probably a generation of listeners on this show who may think a Stack Overflow is a website to get programming help, but this is actually a CS term when you have these function results that are built on top of this stack, usually based in c or whatnot. But in r, you may have encountered this unwillingly if you accidentally put recursion in your paradigm, and it just ends up blowing up the stack, and then r will literally crash saying, you know what? I got too much in this stack. I don't know what to do with myself kinda thing.

In version 440 of r, there is now a new function called tail call, which is gonna be basically a drop in replacement for that last call to your function at the end of the original function where instead of, like, doing, in this case, the bisect call again at the end, you're gonna put bisect into this tail call function. The motivation for this is that r is only going to keep track of that last call that ended up knee being necessary for the solution instead of all those intermediate calls beforehand. Hence, you could have a recursive function ends up being used, say, 1,000,000 of times, but that call stack is only gonna retain what it needed at the end, so to speak.

Now I admit, I don't do recursion very much, but I can definitely see how useful this will be in situations where you're probably gonna need some additional iterations before you get to that elegant solution that you're trying to solve. So they do mention it is experimental because this is a brand new function, so, hopefully, it'll work well for your use cases and recursion. But certainly a welcome feature for those of you in the approximation realm. And I deal with this a lot in simulations as well, so that will be a very nice usability enhancement for my recursion workflows.

But of course, there is much more here, too. And also new to R is the introduction of a nice and elegant way to deal with those null values and conditionals. And, Mike, why don't you tell us about that? Yeah. This is something that's been in Arling for quite a while now,

[00:06:33] Mike Thomas:

that I've seen, you know, in a lot of source code that I've taken a look at online but maybe not realized, and and haven't used it myself either and and not realized that it's not in base r. So it's this operator and if you want to write it, it is 4 characters, percent sign, pipe pipe percent sign. And essentially what it does is it evaluates the left hand side of that pipe and it says if that left hand side is null return what's on the right hand right hand side of the pipe. So it's it's a sort of a shorthand for is dot null, essentially for evaluating maybe conditional, if statements, flow control type statements, and just making, your code a little bit more concise that you don't have to write out that full is null, you know, else statement if you want. So especially if if you're someone maybe developing in our package or something like that where you are trying to test to see if a particular element is null and if it is, you want to return maybe a message or a different object or something like that just because you're trying to program defensively, which is something that we all try to do, in all of our work, I think this this shorthand operator could potentially be your friend there and may reduce the need to import Arlang, although I haven't written an R package where I don't have to bring in our where I haven't brought in Arlang in quite a long time.

But maybe it'll still be useful, you know, in your local scripts if if you don't need Arlang as well and you just wanna have this particular operator handy. So very excited to see this operator come into Besar. I think it's, I think it's a good a good decision by the ArcCorp team. I know it's probably not a trivial thing to add, you know, an operator to base r, and that's probably, actually a fairly big decision at the end of the day. But I believe that its use is is so widespread probably at this point that it makes sense for them to have done that. So that's a a nice little inclusion that we have in our 4 dot 4. One additional thing that is introduced as well is this new shorthand hexadecimal format, which I guess is common in in web programming. And and you can think about, you know, specifying your your hex codes for your colors. Right? Maybe in your ggplots or something like that. And most of the time, if we're we're trying to specify a particular hex code, we have to write it out all 6 digits out, you know, except maybe in the case of, some hex code that has a repeating pattern, you know. I think of black and white. It's just f f f and and 0.

Right? Sometimes you don't need to write those all the way out. But, I guess the the shorthand allows you to translate a code like 112233 to just, you know, pound sign 123 or hashtag 123 for the kids out there. So it's interesting, that that we now have this shorthand textadecimal format. There's some additional, improvements around parsing and formatting complex numbers if you're someone that leverages complex numbers in your R code, I can't say that that I'm one as well. And then probably the the last one which may be arguably the most useful is this new sort by function, sort underscore by. And it allows you to essentially sort a data frame typically based upon 1 or more columns, that you could specify in a list if you have multiple columns or you can just provide that one column to the sort by function.

And, you know, I I think it's it's really interesting that we have that now. It's it's going to make, everybody's life easier to to sort of data frame in base r,

[00:10:18] Eric Nantz:

by 1 or more columns. So nice little handy improvement as well. Yeah. There there is much more to it. So Russ links, and we'll put a link in this as well to the full news file of the the all the updated changes. And he also concludes we're we're just talking about containers earlier, Mike. A great way for you to try r 44, if you don't wanna put it on your system right away, is to leverage Docker containers. Then he's got a simple example of pulling that, into into your system, and it's very easy to do, 2 lines in in each of those cases.

And you may wanna look at this sooner or later because I you you can't write this you can't script this, so to speak, but literally the morning after I released our weekly, this updated issue, A news item that's been making the waves, and we'll probably have a lot more to say possibly next week on, is that for possibly the first time ever, there has been a critical vulnerability exploit discovered for the r language with respect to potential malicious code that could be inserted into what's called the RDS file format, which for those that aren't familiar, is a way for you to serialize our objects into a binary type format that's tailored to what R can use to import and export.

Now there's a lot you know, I think there's a lot of information that's still going to be shared, without throughout the community on this. But if you're in an organization or you help consult with an organization, it may be worth informing them about this in case they wanna update their r version of 44 now that that's been released. And that's important because r44 event has been set to fix this vulnerability. So it would only be previous versions of r that could be potentially vulnerable to this. Like I said, I imagine we'll have more to say about this in the coming weeks because there's still a lot to digest here, but that is something for awareness. If you were you're on the fence about updating, that might, push you onto that edge sooner than later.



[00:12:20] Mike Thomas:

No. That's a great call out, Eric. And, yes, I I agree that I think we'll probably be hearing more about this story and maybe it's Genesis within the next week. Perhaps, something that we can talk about in more depth in the highlights next week, but I I guess it's good to see that this has been, you know, detected. This has been, you know, run up the food chain where it needs to be run up to and folks are investigating and it sounds like it's been remedied actually in in r4.4. So I think now it's it's sort of about spreading the word, maybe doing some sort of, you know, post mortem analysis to see if there's any other places within r that could potentially be impacted by this vulnerability or the way that r sort of serializes and and deserializes, data and, it's it's very interesting you know to to see this come across.



[00:13:21] Eric Nantz:

And for our last couple highlights here, we're gonna visit our friendly, old Shiny Corner here because we got 2 terrific highlights having to do with Shiny itself that we'll touch on here. And actually, both of these are fresh off some presentations that were shared at the recent Appsilon Shiny conference. So lots of momentum from that. And first, we're going to dive into a very nice kind of usability enhancement for you as you bootstrap these Shiny apps and you want to take advantage of the latest and greatest best practices.

And in particular, we're going to talk about a post by Garrick Aiden-Buie, who is a software engineer on the Shiny team at Posit. And he mentions in this post that a big initial focus in his 1st year with the team has been, you know, enhancing the bslib package, which Mike and I have been singing the praises of for over a year now, I believe, of this package. But for those that aren't aware, bslib kinda started off as a way to bring in the modern bootstrap styling into your Shiny apps without it affecting the Shiny core package itself. So this is an additional package that you'll load with your Shiny apps. And with it came some really nice new takes on layout functions, such as page sidebar and other, layouts that are very well tailored for dashboards and much more, as he says. Well, it's one thing to be a SLIB out there. It's one thing to have these examples out in the community, you know, say, with the package vignettes, the package website, and whatnot.

But there has always been a feature that many have taken advantage of in their development environments to quickly get Shiny apps up and running with a traditional sidebar layout, and that's the use of snippets. So in in RStudio Workbench or RStudio IDE, for example, there are a handful of built in snippets, including one to get a Shiny app off the ground. But you'll notice that this Shiny app is using the traditional Shiny package itself with its sidebar functions. And what Garrick does in this post is shows you how to quickly update that to take advantage of bslibs page sidebar layout so that you can get started right away with using bslib in your app. And it is super helpful, especially if you're going to be building these on a routine basis and you want to be able to not have to refactor your code after you get that initial snippet off the ground and say, oh, wait. No. I I wanna pivot the BS flips thing for this. So great. Yeah. Now you can have this snippet added in your Rstudio or update your snippet, I should say, in your Rstudio IDE's available snippets.

But there is more. This was an intriguing thing I've seen from somebody at Pasa themselves is that he also calls out a simple way for you to update or add a new snippet in your Versus Code snippets as well, which I know has been used heavily in a lot of Shiny app development these days. We in fact, at the recent Appsilon Shiny conference, we had a lot of people spreading enthusiasm about using Versus Code for more complicated setups. So Garrick has an example at this post of how you can add this snippet that looks very similar to the one that you would add in our studio's snippets feature, just a little bit more quoting around it. And let's say JSON, you know, syntax. But once you have it, then you can get that same functionality in in Rstudio or Versus Code. So take your pick. You're gonna have a nice snippet that you can use to get yourself started with BS Lib right away, right on the right foot.



[00:16:57] Mike Thomas:

Yeah, Eric. That's a great find, and snippets are something that I feel like I was very late to the game taking advantage of. But nowadays, you know, especially when I'm creating, like, small reprexes or examples or stuff like that for clients, you know, the the little Shiny app snippet that I get in R Studio is so much faster. It probably saves me a minute or 2 of actually creating that that UI in that server and loading Shiny at the header. So this is fan and I haven't really explored the concept of, like, creating your own snippets, but this is just a really nice simple walk through by Garrick on on how you could go about doing this in either RStudio or Versus Code. So it's definitely something that I need to start taking advantage of a little bit more because it is such a time saver. It's such a no brainer.

And as Garrick sort of outlines in this post here, it's easy. It's it's not difficult at all to get up and running with. So, fantastic walkthrough by Garrick. And, you know, just like you, Eric, I am all bslib all the time when it comes to Shiny apps these days. So having some shorthand that actually sort of skeletons out the bslib app as opposed to having to edit what gets returned from the old Shiny app snippet, to conform to a bslib type structure is just going to additionally save that amount of time and and make my workflows more efficient.



[00:18:20] Eric Nantz:

Yeah. You're you're right. I've been late to the snippet game as well, but I think to myself, what are on top of, like, getting the app, you know, boilerplate going, I have a bunch of widgets that I use every single time in my apps. In particular, I've always been a big fan of the ones such as picker input from Shiny Widgets, for example. I should have a snippet just to get one of those off the ground very quickly. I mean, there's you could take this to many different levels. But, yeah, anything to save you time to get this off the right way and then kind of reducing that manual effort of maybe just copy and pasting that snip that thing from one app to another. Oh, I gotta change the input and the ID and all this jazz. This can be a very efficient way for many aspects of Shiny development to to streamline your workflow.



[00:19:05] Mike Thomas:

Absolutely.

[00:19:21] Eric Nantz:

And our last highlight for today, again, we're gonna stay in the shiny corner for this. But Mike and I both agree we are blown away in very positive ways from what we're seeing in this summary here. And this is actually a presentation that was given at the aforementioned Appsilon Shiny Conference. And this was given by Lindsay Jorgensen, who is a director of business intelligence at Athos and as well as John Coene, who, Mike and I know very well, has been a prominent member of the shiny community all of these years.

He actually is the CTO of the Y company that he cofounded earlier this year. So that's a new venture on his part. But they co presented a presentation called Tailoring Shiny for Modern Users. And the motivation for this was in this organization, as ASDOS, they have, which stands for Association of State and Territorial Health Officials, they usually they conduct on a regular basis a longitudinal type census every 3 years, and they started this back in 2007 to help get some information with respect to potential funding and other projects that are happening at the state level for these different agencies.

The results of these are very instrumental, apparently, for monitoring trends, advocating for additional funding, informing their future strategies, and much more. So lots of data to deal with. And, of course, what's a good way to deal with these data? Let's throw this into a dashboard for end users to review. And the previous generation of this dashboard was built with Tableau where everything was in a kinda one interface layout, And certainly it looked functional. I never used it, but it definitely looked functional.

But everything, like I said, is in kind of the single view and may or may not leave a little bit to be desired. Well, fast forward to the way they approach refactoring this with usability in mind and design in mind. So they took a step back and they modernized the user interface approach with this, I'll call, 3 column type layout, Where on the left, they've organized the inputs and not just, like, throwing all the inputs top to bottom in a a huge sidebar they have to scroll down on to actually see everything.

They've sub you know, they kinda compartmentalize these in different unique tabs at the top to get you focused on a few key inputs at a time and be able to click through these, different, like, tabs or organization so it's not overwhelming you right away. And then in the middle is like the main content. In this case, it's a lot of interactive additional context for that result. Maybe it's links to additional information or other information that would be useful as you're reviewing those visual results. And they have this kind of unified approach for the different tabs representing different analysis, you know, results all within the same application.

But, Mike, I when you look at this, there is almost no way you can tell this is a shiny f. This just looks absolutely fantastic. And on top of that, they even looked at accessibility as a first class citizen, so to speak, and how they built the colors of this as well. There's just so much going on here. It's just amazing stuff.

[00:22:47] Mike Thomas:

This is one of the best designed, like, websites in general, you know, information based websites, dashboards, whatever you wanna call it that I've ever seen, shiny or not. It's it's so clean. It's so thorough. It's incredibly thorough. You know, just some of the concepts that get employed here around displaying information and and trying to minimize clutter, you know, when there's sort of long explanations they cut it off after after 4 lines with an ellipse but have a hyperlink to to read more where that will open up, you know, sort of a longer form screen to to read more context on that text. They have definitions which are, you know, expandable, and nicely collapsed. Like you said, these these Highcharts driven charts are are beautifully tool tipped on top that that have a really nice, really nice encapsulation of all the information that they're trying to display. One of the coolest things for me, Eric, that we talked about preshow a little bit was on some of these, charts pages, you know, they allow you to sort of add filters dynamically where you can, you know, click a single button that says add a filter, you select, I believe, the column that you want to use for filtering and then you select the different, value or values, within that column that you want to filter down the data to and and that, you know, results in filtering the information that gets displayed in the chart. But instead of having, you know, a bunch of different filters on screen already across all the different columns that are probably represented in the underlying dataset, that would probably clutter the screen with the idea that maybe the user only wants to to leverage 1 or 2 columns for filtering context.

You know, don't create that clutter right off the bat and allow the user to select as many filters as they want. It's a few extra clicks, but I think I'm sure that they paid a lot of attention to user experience and really got to know the audience that's gonna be using this this app and understand that, you know, maybe they're only using a couple filters, and and maybe it's it's certainly gonna change from user to user, then it would make sense to to go with that approach. But, again, you know, that approach just makes everything super clean on screen and it I'm still blown away. I think I'm probably gonna it's gonna be very difficult for me to navigate away from this app today and actually start doing my work, as opposed to just playing in here, and desperately, you know, begging John and Lindsay for the source code behind this app.

But, it's it's one of the most beautiful apps I've I've ever seen. So I'm very grateful that they, you know, at least shared this app with us. I I think if nothing else, you know, and we don't get access to the source code or anything like that, the design ideas behind this are are just fantastic. And as I told you earlier, Eric, it just makes me want to go into all of our Shiny app projects and just immediately refactor them and bring in some of these principles because I feel like our work is light years away from what they've put together here. It is the the best way that I can describe it is just so clean.



[00:26:02] Eric Nantz:

It's so clean, so organized, and they're it's the perfect balance of making sure the user is seeing what they need to see first, but then letting them opt into additional context, additional information without thinking like, oh, they might want this. I'm gonna throw it on there. I'm gonna throw it on this other side. I'm gonna throw it on this footer. Like, it it will just drive you nuts. And, yes, I have seen many dashboards that were built with Shiny or things like Power BI that just become so cluttered with everything in your face, so to speak.

This, the organization at the top nav bar, but then within these sidebars, not overwhelming us of all the inputs at once. I mean, these are it is just it takes so it it just means that if you pay attention to detail on this early on, I mean, you're just gonna set you up for so much success for a wider range of customers or audience for your app and really taking advantage of these modern principles. Are quite overwhelming. Even though they're meant for statisticians, I don't think a statistician should have to suffer, so to speak, from information overload. So there are definitely techniques from here I wanna take into my production apps. Now the one thing they have open sourced, is this concept of what they call the data stories feature, where in the app itself, you will see this gallery of these different kinda more long form articles, but they're very nicely organized, kinda like a scrolly teletype setup a little bit. But then as you scroll, you get this nice interspersed set of narrative with the aforementioned visualizations and really a great way to kind of break apart visually, so to speak, these different sections. So there is an example repo for that that we will link to in the show notes if you wanna take advantage of that feature in your development. But, yeah, I'm gonna have to fight the urge to refactor all my apps now. But I do think I will refactor some of my apps. Just kidding.



[00:28:01] Mike Thomas:

I'm in the same boat, and it looks I this is deployed on Shiny apps dot io. One of the interesting things is that there is a, you know, extension on that Shinyapps.ioURL, where it's backslash profile. Makes me wonder if they're using some sort of a multi page approach here with brochure or maybe John didn't John have a a project called Ambiorix or something like that that Yeah. He does. That's right. Was leveraged here to have that sort of multi page possibility. So that would be interesting as well to see if there's any of that employed. But for how big this app is and where it's deployed on Shiny apps dot io, it's quite performant.

So I'm sure they paid a lot of attention to trying to minimize the overhead on the on the back end as well. So very, very impressive.

[00:28:49] Eric Nantz:

Yep. And, I'll I'll put this out there, and I don't know when it will be yet. But I did, I was very enthusiastic about what I saw here, and I did get in touch with Lindsey, and she does seem receptive to being to joining me on a future shiny dev series episode. We're hopefully be able to dive into a lot of the technical bits on this in the future because I think it's a excellent story to tell, not just the the journey to get here to these principles, but actually how she pulled it off. So stay tuned for that. I don't know when yet, but you will know when when it's ready for that.

But there's a lot more to stay tuned for because the rest of the Wiki issue has a boatload of additional resources, whether it's new tutorials, new packages, and updated packages. And I dare say this might be my biggest package section update yet because I got a lot to choose from here and a lot of great innovations across all different areas of of our and we'll take a couple of minutes for our additional finds in this in this segment like we always do. I've been, you know, a very big proponent, especially on my interactions of HPC systems, being on the command line, so to speak, you know, getting down and dirty with my data, quick inspections, you know, like Ed or tail of the last few rows.

And I've I've been, you know, very enamored by some of these tools in the Linux community that let you profile performance of your system. There's something called HTOP and VTOP, etcetera. You can see, like, in real time your system performance and these nice line charts and stuff. Well, if you want that similar situation with R, guess what? There is a brand new pack that's released to CRAN called plot CLI, which has been authored by Klasheuer. Hopefully, I'm saying that right. And like I said, it just hit CRAN recently.

And this is going to take a very traditional type ggplot and convert it to a special object that you can print in the terminal and get nice coloring and symbol representation, which if you can't get to, like, say, a POSIT or RStudio IDE or a Versus Code instance, this might be a great thing to do if you're on that lower level, so to speak, with that HPC system and wanna look at a quick visualization. So I'm really excited about that. And, Mike, what did you wanna talk about? Oh, very cool find, Eric.

[00:31:05] Mike Thomas:

You know, not to be, morbid here sort of at the end of the show, but I think it it certainly warrants talking about, there is Fritz Leisch, passed away recently, and and Fritz was a member of, really the the core our project team. He helped cofound CRAN, if you can believe that, with Kurt Ornic, and that was, I think, around 1997. Yeah. He worked at Vienna University of Technology, and then moved to the University of Munich. I was a professor in computational statistics and became the head of that department actually in 2010 before moving back to to Vienna in in 2011 to to to work at the University of Natural Resources and Life Sciences.

So a very impressive career. One of his also additional contributions, I mean, this this almost sounds sounds made up, is that he actually developed the sWeave system that allows you to combine our and LaTeX into a single document which obviously is sort of, you know, the the core engine and how the Knitter project sort of came about and was built on top of, and now we have quarto sort of on top of that. So and I'm imagining that that also predates things like Jupyter Notebooks. So that idea of, you know, creating sort of reproducible research that encompasses both your your code and your narrative and your figures and your tables and your output like that, is is largely, I have to imagine, due to Fritz's work which is incredible.

You know, they also note that that led to, you know, the ability to create our package vignettes which obviously is something that's super powerful, I think, in the our ecosystem because it provides us with a way to give end users documentation that is does not necessarily exist or is not as thorough, or or beautiful, in my opinion, as what some other languages offer in terms of documentation around their packages. So, you know, he they say that in his, professorship, he he taught generations of of students, at bachelor, master, and PhD level, introduced 100 of, R users to to proper R development workflows, and he is going to be extremely missed by the our core team and the our community, and we're incredibly grateful for his contributions to the our ecosystem and really allowing us what allowing us the ability to do what we do on a daily basis. So, definitely some sad news, but I guess maybe a time to reflect and and be grateful for the legacy that he leaves behind.



[00:33:53] Eric Nantz:

Yeah. Yeah. Thank you for that, Mike. Very well summarized. And, again, our our deepest condolences to his family and close friends and, yeah, the rest of the our core team. But I I mentioned this before either on this or my, older podcast. My dissertation was written in s sweep. It was revolutionary to me how I could literally without having to do the old copy paste method, I could get that plot, get that table into my LaTeX file. And just think of what's been built on top of that paradigm, as you said. That that cannot be understated, and we're really fortunate that we've been able to take advantage of these ideas, that that he's created, since the very beginning of our basically. So it is so many so many innovations that he's brought to the project that brought to the community.

It yeah. I definitely am very appreciative of everything he's accomplished. And, you know, it's always a it's always a time to reflect back and really appreciate what we have. And the innovations have to start somewhere. And he was one of those instrumentals to get the R project going. So definitely thank you for mentioning that. And also, you know, we always love to hear from all of you as well with respect to the R weekly project and this very podcast here. And again, one of our favorite parts of this job is to be able to showcase what all of you are building in this community and extending some of those great ideas. And if you want to get in touch with us, there's a few ways to do that. You can use the contact page directly in this episode's show notes. And also, if you want to contribute to our weekly itself, we have a link to contribution in terms of a poll request to the upcoming issue draft. You just click the right hand corner at the top. You'll get directly linked to GitHub for a poll request template, all marked down all the time. You know how it goes. I don't have to keep saying it anymore.

And if you have a modern podcast app, such as, Podverse, Fountain Cast o Matic, and CurioCaster, you can send us fun little boosts along the way to give us a little encouragement and show your value of liking the show. And, also, you can get in touch with me on social media these days. I am on Mastodon at @rpodcast@podcastindex.social. I'm also on LinkedIn under my name, Eric Nantz. You'll see, tweets are posted from time to time and sometimes on that x thingy at @theRcast. And, Mike, before my voice completely goes haywire, where can the listeners find you? You're hanging on pretty well, Eric. Yeah.



[00:36:19] Mike Thomas:

You can find me on mastodon at mike_thomas@fosstodon.org, Ketchbrook Analytics, k e t c h b r o o k. There's too many Mike Thomases out there, so you're better off looking that way. I'm gonna need to recharge my batteries now, so we're gonna close-up shop here on episode

[00:36:41] Eric Nantz:

163. We thank you so much for listening, especially to my horrid voice here. And, hopefully, I'll be back to normal shape and as well as to talk to you about awesome new resources for our next episode, which is coming up next week."
"56","issue_2024_w_17_highlights",2024-04-24,38M 50S,"Bringing interactivity to a staple graphical display in the genomics space, how one team is taking the box approach to sharing and developing modular R code, and a set of intriguing benchmarks with the newly-releaed duckplyr that have your hosts thinking of many possibilities. Episode Links This week's curator: Jon Carroll -…","[00:00:03] Eric Nantz:

Hello, friends. We are back of episode 162 of the Our Weekly Highlights podcast. This is the weekly show where we showcase the latest, awesome resources that you will see every single week in this particular week on rweekly.org. My name is Eric Nantz, and I'm delighted you joined us from wherever you are around the world. And, yes, I am never doing this alone as you know by now. I am joined at the hip here virtually by my awesome cohost, Mike Thomas. Mike, how are you doing today?

[00:00:30] Mike Thomas:

Doing well, Eric. It's a good time to be a Boston sports fan. The the Celtics had a big win in the playoffs recently, and we have playoff hockey going on as well. And, I think the the Bruins took down the Leafs earlier this week or over the weekend.

[00:00:44] Eric Nantz:

Game 1, they did, but it was a tough one last night. Oh, I missed it. Yeah. Well They fell last night? Yeah. There was a there was a one goal difference. But, Boston and Toronto, they don't like each other, and this is probably gonna go the distance if I had to guess. So buckle your seatbelts wherever you are. But, of course, you had to bring that up. My beloved wings lost on a silly tiebreaker to not make the playoffs. So I'm still wincing from that. But, nonetheless, we're not gonna we're not gonna, you know, complain too much here because we got some fun rweekly content to talk to you about today, and this issue has been curated by Jonathan Carroll, now one of our long time contributors to the rweekly project. And as always, yeah, tremendous help from our fellow rweekly team members and contributors like all of you around the world with your poll requests and suggestions.

And we lead off with a visit to our visualization corner once again, and we're gonna talk about what is a very common and fundamental visualization that we often see in genetic and biomarker analyses and give it a little interactivity boost to make the experience even better for your collaborators. And this post comes to us from Thomas Sandman, who is a computational biologist. And he leads off with this situation where he wanted to share a very, powerful visualization called volcano plots to a collaborator.

Now, what is a volcano plot, you might ask? Well, this is the plot where you can look at if you're comparing, say, 2 different groups in your experiment or your design. Maybe it's, in the case of life sciences, it might be a treated set of patients versus a controlled set of patients. And with these biomarkers, you are measuring boatloads of genetic quantities such as gene expression for their genetic microarrays, you might call it. And then you're gonna do some inferences on these. But you're often gonna have 1,000 upon 1,000 of these genetic markers.

So you don't really know, you know, right away which one of these is gonna show a, you know, positive or negative trend either way. So the volcano plot's job is on the y axis. You will see often a log adjusted p value of the inference method. And then on the x axis, you'll see the fold change of the gene expression, which is like a ratio of the gene expression values from 1 group over the other. And, again, the shape, as you see, this does look literally like a volcano erupting on each side. It's a very, very intriguing visualization.

I remember being blown away about that many years ago in my previous life as a statistician analyzing gene expression data. But, nonetheless, you can create these, as you can guess in R, quite, quite efficiently with the, always, you know, ready at your at your visualization hip here of ggplot2. And he has a great example after downloading some, gene expression data to do this volcano plot where you see, as I mentioned on the y axis, the log transform p values. And on the x axis, the fold change, also log transform.

And he colors it by the different groups in the experiments. And you can see above a certain threshold, the red dots with the the gene expression fold change going in the positive direction versus the blue dots going in negative direction. Now that's great, but what if your collaborator or yourself wants to get more information on individual points in this? That's where interactivity comes to play. In particular, you can opt into interactivity quite efficiently with the g g I raf or g giraffe, depending on your pronunciation of it. That package has been mentioned quite a few times on the highlights over the years. That package is authored by David Goel, who is also the architect of the Office verse, family of packages that we often see in our enterprise work. But ggiraffe, the way that works is that all you have to really do is replace the geom point in your red regular ggplot2call with the geom point interactive function.

And with this, you supply a tooltip where when the user hovers over a point, you can supply metadata, if you will, on that particular point. And in the case of this plot, Thomas is supplying the p value, the log fold change, and the gene name itself. So, again, you can quickly see as you hover over that plot right in the blog post each of these, you know, specific genes and their fold change and p values. Now that's pretty good in and of itself. That might get you all the way there. But this is where this gets pretty intriguing, actually, because what if this plot is gonna be part of a larger report? I remember making these reports routinely with our markdown or even the predecessor to that s sweep in the in the early days.

But these plots, when you add these interactive elements, can take up a bit of size in terms of the HTML that's used to produce these plots when you add these tooltips in. So this comes now the very clever part of this post where Thomas is combining g g I f or something else and making the plot even more efficient for these reports. What what kind of trick has he done here? This this is pretty interesting where he combines the gg raster package with the ggiraffe

[00:06:20] Mike Thomas:

package, and, you know, essentially, what he's able to do here is to leverage this function called GeonpointRAST, to actually initially plot the points on the chart that you will see, but those points are not interactive. It's this, raster rasterized image which does not encode the position of each point separately. So it's sort of less overhead on the page in and of itself. And then on top of that, he overlays transparent interactive points for the significant, genes. And you could see that's where he leverages the Geon Point interactive function, but the the the alpha there is set to 0 such that there's no color.

You can't actually tell that those points are plotted unless you are hovering over them. So it's a very clever way to, I think, reduce the overhead on the page. And the use case here would be particularly if you, had a lot of plots on the same HTML page and it was, you know, slow maybe for users or, you know, the tooltip wasn't responsive because, you know, there's a lot of overhead potentially on those pages. So I thought this was super super clever. I see some use of, the poor man package to do some filtering, which is interesting as well to only show tool tips for the points that meet a certain significant threshold, which is really interesting. So sort of the grayed out portion of the bottom of the graph, you're not gonna get tool tooltips for, because, you know, the idea would be that users only necessarily care about those that are at or above the level of significance.

So a really really clever use case here for doing that. I think the code is very familiar to those who have experienced with ggplot. It's it's really not a very big step to go from ggplot to the ggiraffe package. And this gg raster package is one that I had not necessarily leveraged before but, it's very very very interesting to me and very, I don't know, kind of incredible how well these two packages work together, the gg raster package and the ggiraffe package, for being able to essentially, you know, plot the the points visually, sort of, in a static way on the page and then adding the interactivity, in a way that, you know, does not duplicate, the effort. So, great, great blog post top to bottom. I think for those either in the, you know, just interested in data visualization, I don't think you necessarily have to have the background in life sciences.

I certainly, you know, am not super well versed in in gene expression data or anything like that, but I still took a lot out of this blog post. So a great job by by Thomas and, great start to the highlights this week. Yeah. It's a really clever way of inter of, blending in these different elements, and it's also underscoring,

[00:09:16] Eric Nantz:

trend we're seeing in almost any industry you can think of. HTML for reports is the way for the future. Right? So anything we can do to optimize the display of these, to be able to share these in self contained ways, and to minimize the footprint, so to speak. I mean, let's face it. I can in the before times, well caught. Did you ever wanna up you know, open a PDF that's like 30 megabytes? Yeah. Probably not your most, pleasant experience. But being able to have these reports done efficiently, I'm really, really impressed by the things we can that are at our fingertips.

And, again, you as a a developer and R and programming these analyses and visualizations, you're not really stepping outside of a comfort zone here. You're just leveraging 2 additional packages that are in the same vein as your ggplot 2 type syntax. So it's a great way both, you know, from a cognitive standpoint to opt into this, and the result is absolutely fantastic. I've always been very impressed by ggiraffe and kind of its place in the visualization community. It often doesn't get enough love, but I think, you know, posts like this are definitely gonna show just just what it's capable of.



[00:10:31] Mike Thomas:

Absolutely, Eric. And I wish that you would tell my clients that HTML reports should be the only, sort of format that they should be interested in, but I'll digress.

[00:10:42] Eric Nantz:

Oh, don't get me started on the whole slide stuff. We're still in a PowerPoint world on that one, but, let's let's keep it positive, Michael. Let's keep it positive. And coming now speaking of, you know, efficiencies that we can gain with our code in general, our next highlight here has a very interesting take on how they're approaching modularity and their codevelopment across different projects and within the same project. This next post here comes from Owen Jones, who is a data science engineer at the UK Health Security Agency.

And this post is all about how his team has been leveraging the box package as an alternative to other means and r of distributing and hosting reusable code, code bundles, modules, whatever you wanna call them, and to gain efficiencies in their projects. So he mentions at the top that, you know, warms my heart to see a team that's using R for almost all of their data analysis and processing needs. And they definitely recognize a point where, hey. This project is doing this thing. This other project is doing the similar thing.

Let's be efficient. Let's reuse that code and not duplicate it every time on the same project. So they're already on the right track from a cognitive standpoint, hopefully, beyond the path of minimizing technical debt and also enhancing collaboration. Now, if you're listening to the show and you've been in the art community for even just a little bit, you're probably thinking or even screaming at your, you know, podcast audio right now saying, this sounds like a package, and I would definitely agree with you. I think packages are a wonderful way to distribute shared functionality of data processing or other operational there is a bit of additional, you know, overhead or skill set that there is a bit of additional, you know, overhead or skill set that you need to be effective with packages. And, plus, if you have an update to make, you gotta update that package and then reinstall it or redistribute it amongst your different teams. These are all valid points. I definitely acknowledge it takes some discipline when you're in the the package mindset.

Now their approach, again, this is where things get interesting is they are leveraging the box package. And you may have heard of box more recently because box is actually a fundamental piece behind the Appsilon team's Rhino package for, building, Shiny apps in an enterprise fashion. Now Box and this is where Eric's gonna have some takes here. Mike might have some takes on this too. I think they take some getting used to just like our packages would just in different ways. But I will say, if you are familiar with Python, especially, your box is gonna seem pretty familiar to you because in Python, you're probably used to, at the top of your Python script, importing, say, all the functions from a given, you know, package, whatever you wanna call it, or importing specific functions, possibly even renaming them and being very explicit of what your global environment in that Python session is going to have from these modules. Well, Box basically gives the same thing in the context of our code. You'll have this preamble at the top of your script saying which function, which piece do you wanna use from another script.

That, hence, you know, be very tailored, so to speak, of what's going into your environment. I will admit that's still not, like, comfortable to me yet, but I can see that perspective for sure. And what, what, Peter oh, no. Shit. What Owen has done in his post is also talk about how they're organizing the way they use these modules. He defines 3 different types of these modules within the box framework. 1 is a general bucket. This is the stuff that's gonna be used across different projects. He cites connections to an AWS Redshift data warehouse.

That makes complete sense to me. Having those be able to reuse across different projects. And then within a project, they'll have a specific set of modules, and they call them project modules. Put them in a specific subdirectory that will be, you know, as the name says, specific to that particular project. And then lastly, a section called local modules. This category is meant for, you know, kind of tying it all together in that specific project, maybe more for utility purposes and whatnot. Now with Box, the other interesting piece of this is that much like in the our session, when you, say, call library dplyr, for example, it is searching for where dplyr is installed via what's called the library path.

And that's where you typically can see what these are in your R session when you do the dotlibpaths, function call. Box has a similar mechanism where you can define the box path, which is can be 1 or more paths to where these scripts that are containing these box modules are stored. That can also be set for an environment variable. In that case, they kind of do some custom manipulations of that for a given project via the dotrprofilefile, which again, this is where either at a project level or your main R session level, you can add code that will be run at startup automatically when you launch your r session.

So they're putting these box configurations in a dot r profile file, which is similar to what, say, the RM package does to tell our hey. Don't look at the default library path. Look at this RM specific library path for packages. So I did find it interesting that they're using that technique to customize the box set. And then the other little interesting bit here is that I mentioned they have a category for these general box modules that they'll use across projects. Well, they make a clever use of the get submodule technique that says for a given project, we are going to clone the upstream repository of that general box module, but in a subdirectory of the project git module.

Submodules will take a bit of getting used to. I've used them a couple of times, especially in my blog down, you know, blog down sites where I have the Hugo theme as a submodule for the repository so that I could, like, you know, keep up to date with the developer of that theme from time to time, or I could just keep it as static as is and not touch it ever since. They're using that similar technique with these general purpose box modules. That's quite interesting to me, and I think that technique can be used on a variety of situations.

So it was cool to see how Box can be used in, like, a day to day concept. I will admit I'm personally not not convinced yet to change my workflow, but, nonetheless, I can see for a team that's still not quite comfortable with package development or package basics, but yet are familiar with other languages and how they import dependencies, I can definitely see how this could be a useful fit for them. Curious, Mike. What's your take on this approach here?

[00:18:24] Mike Thomas:

Eric, I think this is super powerful depending on sort of how related, projects are within your organization from project to project. And I I think if you have a lot of projects where there's there's some overlap, right, maybe a little bit of overlap but not not sort of full overlap from project to project to the point where maybe you you wanna build, an R package that you know, contains a bunch of functions that you think are are going to be you're gonna use 80, 90% of them, you know, from project to project, then I think this may be the way to go when you're just trying to borrow sort of bits and pieces, from different places from from project to project where the the Venn diagram still has overlap from project to project but maybe a little bit, a little bit less where you're just trying to take a little bit here and there. And you're exactly right. It's it's sort of a very Python, type of mindset that you would have when leveraging the box package. I think it'll be very familiar to Python folks.

One of the third reasons why, they wanted to or Owen's team wanted to, leverage the box package as opposed to construct in our package and this reason is super relatable. It's that they would have to choose a package name and he admits that that is that is super hard that choosing a package name can be really difficult, which I will totally totally agree with. Although, I think, personally, that's sort of one of the the fun parts for me about developing R packages is I get to try to use that other side of my brain and then come up with an R package name that I think is is clever, and hopefully not one that, you know, a month from then I want to change and, because I, you know, at 3 in the morning I thought of a way better package name. You know, occasionally that may happen, may not happen. But I really do think, the use of Box can be incredibly powerful if it fits your use case. I I really appreciated the way that Owen, spelled out the different sub, the different module types in terms of general modules, project modules, and local modules because I think that makes a ton of sense in how to do that. I'm not personally familiar with git submodules. It sounds like something that I should get up to speed on because I can I can see from his explanation here how it would make a lot of sense to to leverage this git sub module technique, when you're employing, right, functions from these these quote unquote general modules and you have sort of multiple repositories potentially that you're interacting with pulling code from into the context of the current project that you're working on? So I think if you have the ability, capacity, you know, resources, understanding to leverage these 2 together, get sub modules in the box framework. I think it can be a really powerful way to develop within your team and and to share, resources, code, snippets, things like that, functions, that you want to leverage from project to project without introducing, you know, some of the the opinionated overhead that comes with developing in our package. And, Eric, as we always like to say, it's great to have options.

And I think that this is another fantastic option and this is one of, you know, I don't I don't wanna speak out of turn here if I'm not correct, but this is, like, one of the most comprehensive or or best showcase blog posts that I've seen around the box package, at least recently. So a huge thanks to Owen for spelling this all out for us in a a really, really nice way.

[00:22:09] Eric Nantz:

Yeah. I I fully agree with this. I've been looking for additional practical day to day kind of treatments of box or narratives around it. I mean, like, yeah, we've seen all the stuff from Rhino. We we see why they are using box and and the Rhino framework. But in terms of data analysis, data science in general, I still wasn't quite absorbing it. But this this really hits home, you know, why this is important to them and how how they're leveraging it. Definitely, the principles here I see, you know, no matter which route you take, whether it's box or the package route, because you could think of the submodules as their way of, let's imagine I have a Shiny app as a as a project, which I typically do, and I'm using RM for it. And I have as one of my RM package dependencies, an internal package that's doing a lot of the heavy lifting for analysis.

So the sub module is kind of like if I have a separate git repo for that, I'll call back end package. I make an update in it. But then in r m, I have to opt in to upgrading the package version in r m for that overall app library. This, the sub module approach is saying, okay. Maybe the team on the that other team has made an update to that general module. They go into their project modules or they do, like, a get sub module fetch or or pull or whatever, and then they'll get that updated sub module in their project. So it's it's similar to that. And, obviously, people are gonna have preferences, you know, maybe one way or the other or some combination of both for how they keep their dependencies for a project updated.

I do think it's quite elegant, though, because like I said for my Hugo, blogs, for the my original r podcast site and for my shiny dev series site, I did make use to get submodules to help keep upstream with the themes that I was using. And there were times that themes changed quite a bit, but I was like, do I really wanna opt into it yet? So I I got to chose when choose when to opt into it. But then when I did, I just gonna get some module fetcher or whatever, and I was off to the races. Yep. And you you mentioned lots of powerful ways or powerful ideas from that previous post, Mike. We're gonna really harness on the power aspect in this last highlight highlight here because there's been a lot of momentum in the recent weeks about DuckDV and the R community.

And this last post is gonna have somebody who was admittedly a bit skeptical about this and some of their eye opening moments as they kick the tires quite literally on this. This post comes to us from Tim Taylor, and this is normally the part where I talk about their background or what their role is. I actually don't know, Tim, if you're listening, what your what your, day to day is like, but I will say you are 2 for 2 because you have done 2 blog posts. And now this is the second one featured on our weekly highlights. Your last one on indentation and our scripts was on just about a year ago on this very podcast. So kudos to you. You're batting a 1,000 on that one.



[00:25:18] Mike Thomas:

Yes. No. All I can see is that Tim is a proud dad, enthusiastic R user, and when he has time, a climber, with a fondness for for coffee and cake. So, we'll have to to track down some more about him unless sort of it's intentionally that way. But, yeah, it his last blog post on this hidden elephants, blog site that he's developed was exactly one day off, a year apart. So he is batting a 1000 annually here on on our weekly highlights as well for these blogs.

[00:25:52] Eric Nantz:

Yeah. Well, kudos to you again on that one. But, yeah, this is quite a departure from indentation because, like us, he has seen this amazing momentum that DuckDV has had in the community, in particular, the Duckflyer package release that was announced about a week or so ago, and he he is intrigued. He's still skeptical, but he wanted to see just for his own his own, you know, curiosity how the recent duct plier package would stack up with data dot table, which, again, has been renowned for many years on performance on large datasets.

So he decided to take matters in his own hands and download a set of parquet files that corresponding to the New York taxicab trip dataset, which has been used in quite a few examples talking about high performance and databases and the like. And he implemented a few benchmarks to compare the 2 packages. The, again, duck plier and data dot table. And he implemented a series, I believe, 4 benchmarks, and using a series of group summary type operations. Pretty intuitive stuff if you've done any kind of data analysis looking at, like, medium medium pickup times and other, another similar analysis like that.

And then he shows us the results. In this table here, he ran this in in two scenarios for each benchmark. 1 was using a year's worth of data and one using a quarter year's worth or 3 months of data. And you see here, when you look at the median timings, in this case, it was 5 reps, so little caveat there. The ratio of improvement of duct plier performance, the data dot table performance in terms of these run times is between 5 and even up to 70 times improvement. Holy jumping. That gets your attention.

And he even admitted he was shocked at this as well. That that is amazing. Wine to said he needed some wine to take in these results. Yeah. I I probably would need to take a walk away from my desk and be like, what did I just see? Like, that's amazing. And so he dives into a bit more about how he, performed this. And by the way, there is a repo. We have all of his, code to set up these benchmarks. So I'll have a link to that in the show notes as well. But I think, again, trying to read between the lines here, what Doug Plyer and ductedbee are doing is a lot of, quote, unquote, lazy evaluation in terms of not doing the crunching until you absolutely have to, but it has certainly been optimized for performance like I didn't even dreamed of.

And, apparently, it's also being very optimal for joining operations as well. Tim makes, some caveats in the post that there may not be a quite direct apples to apples comparison, so to speak, with the duct flyer and data dot table code because he had to modify the data dot table way of doing things to not be as, in, quote, unquote, lazier of our, approaches. I'll have to dive into that a bit more, but he is very upfront that he wants to keep running these benchmarks as new versions of Datadot Table and Duckplier are released just to kinda see the improvements that we're seeing in performance for each of these frameworks. But he seems to be convinced that this is something he's gonna be paying attention to in his future analysis, and frankly, I am very intrigued as well. And as I was reading through this prepping for this show, I did have one moment.

Imagine something I read about a week ago. I'll put this in the show notes as well. With DuckDB, there is a way that you can read as a read only source, a DuckDb database stored in an http endpoint or an s 3 bucket. You combine that with the forms that we're seeing here. Oh my goodness. I'm thinking shiny live web assembly with Duck DB with a big day. Oh. Oh. The plot thickens. I am very intrigued to explore this, and this really hit home that you could see some massive performance wins in this space. So, yeah, I was skeptical as well. I remember first hearing about DuckDB and Duckflyer, but my goodness, this is a heck of a job to convince me that I need to watch this space very closely.



[00:30:26] Mike Thomas:

I'm in the same boat, Eric. You know, I saw LinkedIn that you were, interacting with some DuckDV content recently and I follow them and I'm always, I'm very excited to see they're developing. It seems like at the speed of light, like, there's new functionality coming out in DuckDV. I've seen people say that DuckDV has sort of native, like, similarity measurement type functions that you could use for, you know, text embeddings and and things like that. And it's it's crazy that something like that, you know, functionality exists in within DuckDV, but it's it's like there's no end to the possibilities when it comes to it. And it's sort of really just exploded here, in the last, I don't know, 12 months something like that or may maybe more.

So it's really interesting and, you know, I think, Tim, you know, is quick to say at the end of this blog post as well that, you know, the current performance of data dot table is still very, very good. And it's it's always been more than just about raw speed. You know, there's people that love the syntax as well, and, you know, it's it's sort of Swiss army like functionality he mentioned. So, you know, even though this this benchmark is against data dot table and I feel like lately we've seen, you know, whether it be Arrow or or DuckDV or something like that, everybody's always benchmarking it against data dot table to show, you know, how much it can outperform data. Table maybe because, you know, that was the the prior sort of fastest game in town. You know, I would have liked to have seen, you know, a benchmark against sort of just raw dplyr code as well or or base r as well to just get that sort of full comparison and maybe not single out a poor data table so so much, because we do try to support that effort as well. But it's like, you know, like you've said, Eric, it's incredible. It seems, you know, as the the size of the data and maybe complexity of the operation increases, you know, the gap between DuckDBS performance and, you know, take your pick data. Table, whatever, it just seems to widen, which is is pretty incredible that we're getting, you know, 60, 70 x improvements, in the timing for some of these queries against, you know, the very famous New York taxi trip dataset, which is is pretty big. You know, Tim notes that it resulted in a data frame at times occupying 15 gigs of memory, in his r session, and and duck duck plier was able to perform all 4 of the queries that he wrote in, like, a little over 2 seconds, which is is it's incredible. And like you said, it definitely opens up the possibility for leveraging DuckDb and and WebAssembly WASM to be able to to run these things completely client side, on a page that doesn't necessarily have, you know, your traditional server behind it, leveraging Shiny Live, you know, connecting to to s three sources, HTTPS sources, and yeah, it's the the possibilities are are just endless here, and I'm so excited to continue to watch this space and see what happens in the next next year, 2 years, things like that. Maybe I've spun up, you know, my last digital ocean droplet here in the next next year or 2, because we'll just be deploying everything on on GitHub pages.

But we'll we'll have to see. We'll have to see. It's really, really exciting time to be in the data

[00:33:50] Eric Nantz:

space. Konas Otterberg and myself, man. I'm really intrigued by where I can where I can take these efforts. And there's gonna be a lot of fast moving pieces in this, so we'll be watching this space quite closely. And where you can watch this and many more developments is, of course, our weekly itself where we have so much more than just these highlights that we share in every issue, such as new packages, updated packages, and awesome showcases of our in the real world and tutorials. Almost there's always something for everybody.

So we'll take a minute for our additional finds here, and I'm gonna gonna plug our our first highlight author, once again, Thomas Sandman. He has another post on this issue about querying JSON files with AWS Athena and this Noctua r package. Now this is pretty sophisticated stuff, but imagine you have data collected as JSON files, which, of course, is quite common in the world of web technology and web development, that you could, with this athena, service and with the Noctua package, treat those as if they were a DBI compliant data source and be able to leverage, say, dplyr code to do manipulations, do summaries, and whatnot.

So, as I think about ways I can store data interoperability wise between R and other frameworks that they all can access it efficiently, I'm definitely gonna keep an eye on this space. So that was,

[00:35:20] Mike Thomas:

a lot of moments on that post Laurent by Thomas. So definitely check that out. No. That's a great find, Eric. Yeah. There's there's a lot of actually additional highlights from the Datawookie blog by Andrew Collier. And, one that I'll I'll point out, which is a nice short and sweet one, it resonates with a lot of what we do is is on model validation, and it looks like, he has a model developed to try to estimate, stock market returns or financial portfolio returns as well and has 3 different methodologies for trying to evaluate whether the model he developed is a quote unquote good model. So it's an interesting one to to check out if you're in the model validation space.



[00:36:00] Eric Nantz:

Nice. Yeah. I'll be checking that out as well. We're dealing with models left and right around here like everyone else. Very cool. And, of course, the rest of the issue is just as cool, so we invite you to check it out. The issue is always linked in the show notes, and John did another wonderful job curating this for us. And, again, our weekly is powered by all of you in the community. So the best way to help our project is to help send us a poll request for that new blog, that new resource, that new package, all marked down all the time. Details are at r wicked dot org. The upper right corner takes you directly to the upcoming issue draft. And, I think, yours truly is on the docket for next week's issue, so I can definitely use all the help I can get. So definitely help out your humble host here if you can.

But we also love hearing from you in the audience as well. You can send us a note on our contact page. This is directly linked in the show notes. Also, you can send us a fun little boost if you're using a modern podcast app like Paverse, Fountain, or Cast O Matic, Curio Castor. There's a whole bunch out there. And so you'll see details about that in the show notes as well. And we are still randomly walking those social media pathways. I'm mostly on Mastodon these days with at our podcast at podcast index.social.

I am also on LinkedIn under my name, Eric Nance, and from time to time on the x thingy at the r cast. Mike, where can the listeners find you?

[00:37:26] Mike Thomas:

Sure. You can find me on mastodon@mike_thomas@phostodon.org, or you can check out what I'm up to on LinkedIn by searching Catch Brook Analytics, ketchbrook. And I wanna shout out Matan Hakim for his message on mastodon after I put out a little post, on Llama 3 saying that they did not have Catchbook Analytics in the training data when we asked them who Catchbook Analytics was, and he recommended, that they the developers, I guess, meta, of Llama 3, should have used the Our Weekly Highlights podcast as training data, which would have enabled them to, definitely spell Catch Brook Analytics because it's something that I apparently do at the end of every single episode. So sorry to the listeners for that, but not sure if it's gonna stop.



[00:38:15] Eric Nantz:

No. No. We we we keep we keep consistency on this show. So, yeah, Nada, get on this, buddy. We got we you gotta update your training data.

[00:38:23] Mike Thomas:

That was awesome. Seriously.

[00:38:25] Eric Nantz:

Yes. Yes. Well, our weekly itself shouldn't be hard to spell. Again, we're at our weekly dotorg for everything, so definitely bookmark it if you haven't. But, of course, we very much appreciate you listening from wherever you are around the world. It's always a blast to do this show, and we'll keep the train going as they say. So that will put a bow on episode 162, and we'll be back with episode 100 63 of our weekly highlights next week."
"57","issue_2024_w_16_highlights",2024-04-16,36M 2S,"Another way to hop on LLM train with the chattr package, a clever use of defensive programming to get to those warnings in your tests faster, and a major milestone for the R-Hub project. Episode Links This week's curator: Tony Elhabr - @tonyelhabr@skrimmage.com (Mastodon) & @TonyElHabr (X/Twitter) Chat with AI in RStudio Test warnings faster R-hub…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode a 161 of the RWKUHollwitz podcast. And depending when you're listening to this, you might get this a day earlier than expected because yours truly has the, day job going on. We have a couple on-site visits, but, nonetheless, we're gonna get an episode to you very quickly here. My name is Eric Nance. And as always, I'm delighted that you join us from wherever you are around the world. And I'm joined at the hip as always by my awesome co host, Mike Thomas. Mike, how are you doing today? Doing well, Eric. Happy tax day for those in the US.



[00:00:33] Mike Thomas:

Having a pretty exciting day today because we're bringing on a new team member at Catch Brook Analytics, and if you're a small team, you know how big of a deal it is to to bring on an additional person. So very, very excited about that. But, yeah. Looking forward to a great week.

[00:00:47] Eric Nantz:

Congratulations, Mike. I know a lot of stuff goes on behind the scenes to make those those acquisitions happen, so to speak. And sounds like you're you're gonna be teaming up quite well. And as always, if you wanna see what Mike's up to, yeah, I saw that in his LinkedIn feed. So definitely check that out. Thank you. Nonetheless yep. Nonetheless, we are gonna get on the business of our weekly here, and we our issue this week has been curated by Tony Elharbor, another one of our longtime contributors to the project. And as always, he had tremendous help from our fellow our our weekly team members and contributors like all of you around the world.

And as you recall, Mike, this is something we talked about last year, I believe, is that there was a pretty big splash across the entire tech sector with large language models and artificial intelligence. And the Pazit group themselves had made a big splash when they announced how they were integrating chatgpt with their RStudio environment. And that's just one piece of the integration here because there is a lot more details, as they say. And that's the subject of our first highlight, which is all about the chatterrpackage, which is coming from the author of this post as well as the author of the package, our posit software engineer, Edgar Ruiz. And he tells us all about how you can chat with AI and large Angular models directly in RStudio itself via the Chatter package.

Then on the 10, the easiest way to interact with it is once you install the package and you link it to the model of your choice, and more on that a little later, you will, by default, be able to bring up a handy little shiny gadget in the panel of your rstudio IDE. And it will look very familiar to those who have used things like chat GPT and other models online. You give it your prompt, hit enter, and then you're going to get some feedback back. And it does do some nice things under the hood that's tailoring it to, say, getting, say, our code for maybe you're developing a package or you're new to a certain package and you want a little snippet. It'll make it super easy for you to copy paste those snippets directly into your source prompt and actually going the other way as well.

Perhaps you're like me, and you like the right markdown for all your expirations of, like, your data analysis and exploring a package. Maybe you write that question in your markdown file itself. And also, the Chatter package also has some nice interactions with the Rstudio add in feature where you could highlight that, say, text. That would basically be a prompt if you, like, manually typed it in. Send that directly to Chatter, and then it'll act as if you just prompted it directly. So nice little bidirectional communication on that front as well. So very nice kind of user experience on that front.

Now the parts that I've been pretty intrigued by, especially as this technology starts to hopefully get more mature, although it still seems like moving at a breakneck pace, is that across multiple industries, across many of the people I listen to, there is a lot of interest in, you know, having the ability to self host your large language models and, hence, potentially being able to leverage the same technology without it leaving your company's firewall. And in my industry, that is a big, big deal because, we could get in big hot water if we if we transferred some confidential stuff in the OpenAI suite just on its own.

Well, that's where Chatter has probably my favorite feature is the ability to integrate with not just the OpenAI type models of Chat, GPT, and the like, but also self hosted models. In particular, they give first class support for the chat LOM models, I believe. And some of these are coming from Meta themselves. And in fact, in particular, it's called Wama GPTJ chat. Again, some of these are being contributed by Meta and their research group. But with that framework, you can plug in different models that you can download that are freely available.

They talk about WAMA itself, GPTJ, Mosaic pre chain transformers. Some of these I'm very not familiar of, but the whole idea is that once you install these on your system, you just set a little configuration in the Chatter package to map to those models. And then lo and behold, you'll be able to communicate directly in your network or even on your your computer itself without it again leaving the confines of your firewall or your own setup to go to another service. I do think that as this technology, yeah, gets more mainstream in industry, having that flexibility is going to be immensely important.

And I'm very happy to see that even this early version of Chatter is bringing support for the local model execution, and they do encourage others to give it a try. And if there are models that they've seen online that are not supported yet, they are inviting, you know, issues to be filed on their issue tracker. And I'm sure Edgar and the team would love to pursue that further. But overall, certainly, as developing your R code, developing your package or data analysis, having a helping hand is never a bad thing. And Chatter gives you the flexibility of going straight to OpenAI or straight to your local system instead. So we're really happy to see it and can't wait to see where it goes.



[00:06:26] Mike Thomas:

Yeah. Eric, this is super exciting and I'm glad that you brought up the the point about the need for leveraging, sort of in house LLMs. You know, I think that in sort of non corporate settings, a lot of the times, right, or if you're just looking for help in your code, potentially, you know, you can leverage something like OpenAI or some of these these models where you are sending information to a third party service. Essentially, anything that you type is is probably gonna be captured and and used to retrain that model, if not for other purposes. And and, you know, that's okay a lot of the time. But if you are asking a question that's sort of specific to, you know, the the IP of your own organization or company, right, it's probably not a very good idea to do that. Your, your your information security team is probably not gonna be too happy if you start doing that.

So the fact that we have the ability to integrate here in our studio with some of these, you know, sort of, in house LLMs, you know, that that are open source and we can install and host ourselves is fantastic. I wasn't necessarily familiar with all of them. I do know that the Mosaic, pre trained Mosaic is a company that I believe was, you know, leading in this space and they were acquired by Databricks not too long ago. Then, I guess, the one other thing that I do wanna point out is if you are, someone who's sort of like me, who's been using Versus Code a little bit more, lately than RStudio, that's okay. You don't have to just be in the RStudio IDE. You know, this article highlights Chatter's integration with with RStudio's IDE.

Specifically, you know, all the screenshots are sort of Rstudio native, but, you know, this will all work outside of Rstudio as well, you know, for example, in the terminal. So it's a great walk through on on getting set up and and getting started. I think it's it's hugely helpful, you know. I have sort of mixed feelings as I'm sure you do about Eric. You do, Eric, about large language models and their utility. You know, I think I was probably a little bit more pessimistic about them when they first came out and I think they're growing on me a little bit to be honest.

1 I listened to a great podcast this weekend, that was with the chief operating officer of GitHub. And long this is just sort of a side story, but I live in this this very rural farm town in Connecticut as you know, Eric. Not much around. We're not we're not close to to any cities or anything like that. And the chief operating officer of GitHub lives 4 minutes away from my house and I never knew. Wow. In a small world. I sent him a little LinkedIn message to see if we're gonna get together sometime but I haven't heard back yet so I I don't blame him. But, no. It was very interesting and one of the points that he was talking about which I hadn't necessarily considered before is you know we talk a lot Eric about you know maintaining open source software and how it can be thankless sometimes. Right? There's not a lot of payoff there. But he was thinking about a world where, you know, somebody opens an issue in your, you know, open source repository that you have and maybe, you know, Copilot or or some sort of LLM is able to immediately show you some code to solve that issue. And obviously, you can go in and edit it, but any time savings that we can provide to open source maintainers, I think could be a huge benefit to the whole community and especially, you know, the many folks that work on open source that really don't get much back for it.

So I I thought that was interesting. They're growing on me. I I love seeing this article come out because I I think that this integration is very timely and looking forward to continuing to watch, you know, how this space evolves.

[00:10:03] Eric Nantz:

Yeah. Certainly, I've been pessimistic just like you have, and I do think depending on the context, there there are ways that if you're expecting too much, it's not you're gonna be you're gonna be disappointed if you think it's gonna solve all your woes of development and also coming up with a magical algorithm to save your company's bottom line. Yeah. Yeah. We're not there yet. But, however, I can definitely see the point of bridging that gap between doing so many things manually, such as looking up the concepts of a different language as you're folding maybe a custom JavaScript pipeline or custom, like, Linux, you know, operation into your typical data workflow.

Having that helping hand to be able to guide you with additional development best practices or snippets that get you started quickly, I think, is still the the the type of low hanging fruit that I think cannot be understated how valuable it is to get that extra helping hand. I think over time as these models get, you know, more updated, more trained, hopefully, with best practices for training those that we're all going to benefit from. But again, to me, the flexibility to be able to pivot to the direction that you choose, I think, is is definitely paramount here. And, yeah, for you Versus Code fans, guess what? The Versus Code, extension for R does indeed have support for RStudio add ins. So you should be able to use Chatter kind of out of the box even in your Versus Code setup. I haven't tried it yet, but I've been able to use add ins in the past with things like Logdown and other packages that had these nice little add in integration. So, yeah, who knows? Give that a try, listener out there, if you're a Versus Code power user because, I would imagine Chatter is gonna fit right in there.



[00:11:54] Mike Thomas:

Yeah. It's a fantastic project and appreciate, you know, all the work that a lot of folks, it seems especially at Guru Ruiz, have put into this.

[00:12:01] Eric Nantz:

Yeah. You can definitely see it. And in fact, he actually gave a a talk at the 2023 r pharma conference about some of his developments. So I'll have a link to that in the show notes. Now we told you how, you know, using things like large diameter miles can make things a bit faster for your development. Well, another thing that you want faster, especially when things go haywire, is if you're in the package development mindset and you're starting to do the right thing. If you've been listening to us and others in the community, you write your function. You know that function is important.

Make a test for that, folks. It'll save you so much time and effort in the long run. Well, you may be in situations where you're writing a function that definitely has a lot going on, so to speak. Maybe it takes a lot of time to execute. Maybe it's impact it's interacting with some, you know, API that needs a little time to digest things. And you wanna still keep in the workflow of being able to test this efficiently, especially when things go haywire. And our next highlight here comes from Mike Mahoney, a PhD candidate who has also had definitely more than a few contributions to our weekly in the past where he talks about some of his recent explorations in helping to decipher the ways to fail fast in your test with respect to custom warnings and errors.

And what we're talking about here is that typically speaking, when you have a function that's depending on some kind of user input, you probably wanna have some guardrails around it to make sure the inputs are meeting the specific needs of that back end processing. And if you detect that something's out of line, you will often wanna send a message to the user or in some cases, even a warning or an error right off the bat to the user that something is not right. They're gonna have to go and try it again.

Well, maybe it is more of a warning concept or maybe they'll still get a result, but it may not be quite what they expect. And what Mike does in his example here, he has a little mock function that is intentionally meant to take a while, but he's making clever use of the rlang package to not only display the message to the user, but also attach a custom class to that message. I glossed over this years ago when I got familiar with Rlang and CLI, but this is a handy little feature because if you attach a class to this type of message or warning or error in your test to determine if that said warning or error actually fires, instead of having to test for the verbatim text of that message that it was showed to the user, you can test for that class of a message, which might be handy if you have more than one of these in a given function.

Now that on its own, he's got an example where he builds a test to do that in his test suite. But there's one bit issue that, again, if you have an expensive function you're going to run it into, is that the function is still going to try to go through the rest of the processing even if your test for that warning is successful. Well, how do you get it so that you wanna, say, escape the hatch, so to speak, when that warning occurs? That's where the the rest of the example that Mike puts together here. He develops a nice little trick that's making use of the try catch function, which comes in base r, and it's a way for you to evaluate a function.

And if you detect that there's an error, you can tell what to do in that post processing. So what Mike does in this example is he takes the detection of that warning condition in his function, which, in this case, is making a huge number with, going above the machine integer max. And then in his try catch block, he's got, okay, he expect an error that that error being actually a warning message instead of a typical error. So he does a little clever thing here, and this is not modifying the function itself. It's modifying the test that declaration of that function.

So the function itself still has that nice, succinct syntax of if the integer is too big, send the warning, carry on. But it's in the test that block that he switches it a bit and says, I'm gonna expect that error for that warning condition. And that way, that the warning does indeed output as I expect. It's just gonna get out, and then I get on with my day, so to speak. Now this may not be a one size fits all for everybody because maybe there are cases where you're still gonna wanna test if that function does perform successfully.

But I could definitely see as you're in, like, the iterative development workflow that you'll want that flexibility to say, okay. I just wanna test for my air conditioning first and then not get bogged down with the execution time in that iterative process. And then when I feel like I'm confident enough, then start building in the rest of the testing suite. Again, testing and development practices can be a highly personal thing, but I just love the use of these built in features of r to kinda make your day to day a bit easier in your testing suite. So a short example here, easy to digest, but I think it's got immense value, especially if you're in your early days of building that really

[00:17:44] Mike Thomas:

extremely clever by Mike and I appreciate him putting this example code together to demonstrate this this use case. It sort of reminds me, about a concept that maybe my Elle, Salmon was the the author behind, but just the concept of thinking about an early return as opposed to return right at the end of your function, you know, if some condition is met then, return early instead of executing all the rest of the code. So, great example here by Mike. I really encourage folks to to check out the example code. It's sort of hard to do it justice in a in a podcast. But this is really easy, base our code for the most part, a little bit of our lang and test that that you could run natively if you wanted to and and see, the time difference here that Mike expresses.



[00:18:29] Eric Nantz:

Yeah. I'm again, great nugget of wisdom here that I'm gonna be taking into my packages that I have to deal with either launching, say, a complicated model or interacting with an API that I have no control over, who maybe those engineers didn't exactly make it efficient on the back end. So if I wanna fail, I wanna fail fast, and hence, it's gonna be a nice little snippet that I can take into my dev toolbox to make it fail fast and still keep that function pretty pure in the process. And speaking of making things easy for you, again, going back into the package development mindset, you've done that awesome work. You've got that package ready to go.

And then you're at that point of, am I really ready for CRAN yet? It can be a daunting task, but there are there's been an existing resource for us in the R community for over 8 years that has been immensely helpful as another sanity check to our package, especially with respect to different environments and different architectures. Well, what we're the project we're talking about has been the Rhub project, which has been led by Gabor Saasardi, a software engineer at Pozit. I believe Ma'al Salmons also contributed heavily to this project as well. Well, they have a very big milestone right now in our last highlight here because they are officially announcing in their blog post is that they are moving to R hub version 2.

And, like I said, the impact that Rhub has had on the community itself cannot be understated with respect to package development and helping authors really get to the nitty gritty of how their package is gonna work across different architecture. And this has a rich history. As I said, this was actually a project funded by the R Consortium, which I've spoken highly about all the way back in 2015. So it has been standing up for years. Now why is this version 2 so exciting? Well, they are making major pivots to their infrastructure where a lot of this has been, in essence, self made or homegrown in terms of architectures for containers and servers.

Now they are piggybacking off of GitHub actions to help automate many of these same checks and more that they've been doing in our hub over the years. And if that doesn't sound familiar, there has been another project we've been speaking highly about called r Universe that is also heavily invested in the GitHub Action route. So now that's 2 major, major influences in the development, community for r that are making a big headway here. Now you as the end user, I. E. The package author wants to leverage this version of rhub. What do you have to do?

The good news is there's always been a package called rhub literally in our ecosystem itself. You just gotta update that package to the latest version that they're announcing here. And then I believe you might have to reregister for interaction with the service. But if you've done it before, it should be a fairly seamless operation. And then the mechanism to you as the end user is exactly the same. You're gonna be able to supply or request that your package be checked in a more custom way. More on that in a minute. But we have a special configuration file that goes to your GitHub repository.

Assuming your package source code is on GitHub, it'll be like a YAML file that in your main branch of the repo and optionally any custom branch that on every push to that said branch, it'll trigger in GitHub actions these other rhub version 2 checks. So then you can check the progress on that in the GitHub actions dialogue much like you would in any other GitHub action. I've been in I've been living in GitHub actions over the years, so it's becoming much more comfortable to me, albeit still not 100% yet. But again, they're taking all the hard work for you. You just got to put this configuration in your repo and the R Hub package itself will let you do that very quickly.

And like I said, there are opportunities if you don't want to host your package itself on GitHub and still want to take advantage of something like GitHub Actions, there is a mechanism, a path forward for you as well where you can leverage the custom runners that the R Hub team has put together that are hosted under the R Consortium itself as these on demand runners, which would be similar to what you saw in the previous version of RHub. But this is useful if you want to keep your package outside of GitHub and still want to leverage these kind of checks.

There are a couple of caveats, though. Once you opt into this kind of check, obviously, the checks that are being run-in this Rhub, GitHub, Org under the R Consortium are still going to be public. So if you're okay with that, great. But, that's just something to keep in mind. With that said, I do think that this is gonna help the team immensely with their back end infrastructure. And, hopefully, you as the end user will be able to seamlessly opt into this, especially if you've already been invested in rhub. It should be a pretty seamless operation.

And if you're new to rhub, it sounds like this is gonna be even faster for you and even more reliable and, hopefully, you know, get you on your way with more efficient package development. So big congratulations

[00:24:09] Mike Thomas:

to Gabor and Myo and others on the rHub team for this, terrific milestone of version 2. Yeah, Eric. This is super exciting. You know, we've talked about this before, but the the need to be able to check your package and ensure that it it works and doesn't throw no any warnings or or notes or anything like that, across multiple different operating systems and setups ups that are that differ from your own, right? Which is really sort of before this, one of the only places that you could check, your package is is fantastic and it's it's super powerful.

It looks like the rhub check function is a really nice interactive function, tries to identify the git and Github repository, that belongs to the package that you are trying to check. And then it has this whole list of it looks like about 20 different platforms that you can check your package on. And you can check it against as many, I think, of those platforms as you want. Just entering into the console, sort of the the index number for each platform, comma separated, that you want. One of the interesting things, Eric, I don't know if you know the answer to this, but if I'm looking at, you know, checks 1 through 4 here on Linux, Mac OS, Windows, It's it's it says that it's going to check that package against our and then it has a little asterisk, any version. So does that mean that, deeper within your your, maybe, YAML configuration file for your GitHub actions check, you're able to specify the exact version of r that you want, or is this sort of choosing the version of r for you?



[00:25:45] Eric Nantz:

I'd imagine it's probably choosing for you, but I wouldn't be surprised if deep in the weeds you are able to customize that in some way. I just haven't tried it myself yet. Got it. No. Because that would be interesting because I have a sort of a use recurring use case where I feel like we need to check

[00:25:58] Mike Thomas:

against maybe 3 versions, you know, the the latest major release of our probably the next beta release as well that's coming and then and then the previous, major release as well because we know some folks in organizations sort of lag behind. So we wanna make sure that everything's working across, past, present, and future, if you will. So I wonder if I'll have to dig a little bit deeper into it. Maybe next week, I can could share my results on, the ability to specify particular r versions here. Anybody on Mastodon that wants to chime in, feel free as well. But, yeah, this is fantastic and, you know, there are as you mentioned, some of those limitations of your package being public.

You know, if you do want to keep your package private and you have it in a private GitHub repository, you just wanna use the rhubsetup and rhubcheck functions instead of the rc submit function, and that should be able to keep all of your your code private for you and take care of that. But this is really exciting stuff, from the R Consortium and or from the R Hub team, and really appreciate all the work that they have done, to allow us to be able to more robustly, develop software, check that our software is working, and build working tools for others to use.



[00:27:16] Eric Nantz:

Yeah. I'm working with a teammate at the day job that we're about to open source a a long time package in our in our, internal pipeline and hopefully get it to CRAN. And they were asking me, yeah. What's this rhub check stuff? I was like, oh, you're asking me at the right time. They just released version 2. So we're gonna have a play with that ourselves probably in the next week or 2 and see how that turns out for us. But, yeah, we're gonna be putting that on CRAN. So between that and our universe, I think we're gonna be in good hands, so to speak, to prepare for that big milestone for us. That's exciting. And I do wanna point out that there is also it looks like a a very new, but a nice place to potentially ask questions as you kick the tire on this is a discussions board in the GitHub repository

[00:27:58] Mike Thomas:

for our hub. So definitely check that out.

[00:28:01] Eric Nantz:

Yeah. This is, again, very welcome to get that that real time kind of feedback, you know, submitted. So definitely have that. Check out the, of course, the post itself, and you'll get a direct link to it. Having this dialogue early is going to be immensely helpful to Gabor and the team to make sure everything's working out correctly and, of course, the help with the future enhancements to our hub itself. And you know what else can help you all out there is, of course, the rest of the rweekly issue. We got a whole smorgasbord of package highlights, new updates, and existing packages, getting major updates, tutorials, great uses of data science across the board. So it'll take a couple of minutes for our additional finds here.

And as I've been leveraging different, you know, data back ends for my data expirations, especially with huge datasets, I've been looking into things like, you know, of course, historically SQLite, DuckDB now, you know, Parquet. It's all it's all coming together as they say. Art Steinmis has a great post on our weekly, this issue, about the truth about tidy wrappers. It's a provocative title, but if you ever want a technical deep dive into kind of the benefits and trade offs of performance and other considerations with some of these wrappers that you heard about that say, let you use dplyr with, say, you know, relational databases, parquet files, DuckDB.

This post is for you. It is a very comprehensive treatment, lots of example metrics so you can make an informed decision as you look at the datasets you're analyzing in your particular project and seeing if it's a right fit to stick with the wrappers or to go full native with the respective database engine. So really, thought provoking read, and great post by Art on that one. And, Mike, what did you find? No. It's a great find, Eric. I found

[00:29:56] Mike Thomas:

a blog post by Hadley Wickham, announcing Tidyverse Developer Day 2024.

[00:30:01] Eric Nantz:

Woo hoo. Yeah. This is exciting. It seems like it's been a little while since they had a Tidyverse Developer Day.

[00:30:12] Mike Thomas:

On August 15th. And if you're curious about what Tidyverse Developer Day is, it's just a really open communal day of folks, sitting and learning together and coding to try to promote contribution to the Tidyverse codebase. They're going to provide everyone with food and all you need to do is is bring your laptop and your, you're yearning to learn, so to speak. So it looks like, anyone can attend if this is regardless of whether you've ever created a pull request before or as Hadley says or if you you've already made your 10th package. So we're welcoming beginners to intermediate to advanced folks. Anybody that's interested in sort of the concepts of maybe con just contributing to open source or contributing to the Tidyverse, specifically.

I'm sure that they will have, many sort of issues already labeled and and ready to go for sort of, low level beginner stuff to probably more advanced stuff if you really wanna get into the weeds of the tidyverse. It's gonna cost $10 and and they say that really that's just because, they don't want people, you know, sort of just a lot of people taking a ticket and and not necessarily showing up. They're trying to encourage, you know, some commitment to the folks that actually say that they're going to, register for this. And and I think they're looking forward to, this day. And I'm going to to try to make it if possible. I'll have to see if the logistics all line up. But this is super exciting. Just another thing that I love about open source. The fact that, you know, they're going to have a day dedicated to this, a very open sort of forum for folks to help contribute to code that's going to get used by, you know, 1,000, if not tens or 100 of 1,000 or millions of people around the world. Pretty exciting.



[00:31:56] Eric Nantz:

Yeah. I've never actually been to a developer day myself, but yet I've interacted with many people that have in the in the earlier years of the rstudio conference, and everybody was so immensely, you know, enjoying the experience. And it helps them get over that hump of package development, you know, contributing to open source. It's a friendly, welcoming environment. Yeah. Like I said, I've heard great stories from many many in the r community of all types of experience levels getting such tangible benefits and, of course, helping open source along the way. So I highly recommend if you have the capacity to join join that effort as well.

And, yeah, as we're recording, Mike, I'll do one little mini plug here. We are only a couple of days away from the 2024 Shiny conference hosted by Absalon starting this Wednesday. Hopefully, by the time you're hearing this, it'll still be a day or so left and you can still register. We'll have a link to the conference site itself if you haven't registered yet. I am thrilled to be chairing the life sciences track, and I'll be leading a panel discussion about some of the major innovations in life sciences these days are shiny.

I'll be joined by Donnie Unardi, Harvey Lieberman, and Becca Krause. They are an all star team, if I dare say so myself, of practitioners in life sciences that are pushing Shiny to another level, and I'll be thrilled to, you know, dive into some of these topics with them. You know, super exciting that Shiny Conf is this week, we are looking forward to it. It's it's finally arrived. We have a little app showcase,

[00:33:27] Mike Thomas:

that's taking place on Thursday that we're excited for, but mostly excited for all the other fantastic content that will be, presented during the conference. So really appreciate Absalon putting that on, I think, with some help from Pazit and others as well. Of course, we hope that you enjoyed this episode and also our weekly

[00:33:46] Eric Nantz:

is meant for you in the community and is powered by you in the community. The best way to help the project is to send your favorite resource or that new resource you found. We have a poll request away. It's all available at rweekly.org. Click that little right hand ribbon in the upper right. You'll get a link to the upcoming issue draft. All marked down all the time. You know marked down. I'm sure you do. If you haven't, it'll take you maybe 5 minutes to learn it. And if you can't learn it in 5 minutes, my good friend, Eway, will give you $5.

Just kidding. He did tell me that one. So, luckily, I didn't need I didn't need 5 minutes to learn it, though. Oh, I love it. I love it. And, of course, we look forward to you, continuing listening to this very show. We love hearing from you as well. We are now about two and a half weeks, I believe, into our new hosting provider, and everything seems to be working smoothly again. But, I'm really excited for the directions I can take this platform in the future. In fact, I'm hoping I can share some really fancy stats of all of you based on some of the new, back end stuff I've been able to integrate with with this new provider. But, nonetheless, we love hearing from you.

You can get in touch with us with the contact page, direct link in this episode show notes. You can also send us a fun little boost along the way with one of these modern podcast apps you may have been hearing about. We have a link to all those in the show notes as well. And, of course, we love hearing from you directly on our various accounts online. I'm mostly on Mastodon these days with at our podcast at podcast index.social. I'm on LinkedIn as well. Just search my name. You'll find me there. And, occasionally, on the weapon x Twitter thingamajig@drcast.

And, Mike, where can our listeners find you?

[00:35:29] Mike Thomas:

Yeah. You can find me on Mastodon as well at mike_thomas@fostodon.org, Or, you can check out what I'm up to on LinkedIn if you search for Catchbrook Analytics, ketchb r o o k.

[00:35:43] Eric Nantz:

Awesome stuff. And, again, Mike's always got some cool stuff cooking in the oven, so to speak. So it's always great to see what you're up to. And with that, we're gonna put this episode out of the oven. We are done for today. We're gonna wrap up this episode of our weekly highlights, and we'll be back with another edition next week."
"58","issue_2024_w_15_highlights",2024-04-10,51M 16S,"The Nix and R train rolls on with automated caching, a collection of big improvements landing in webR, and how hand-crafted visualizations bring fundamental dplyr grouping operations to life. Episode Links This week’s curator: Jon Calder (@jonmcalder) (X/Twitter) Reproducible data science with Nix, part 11 – build and cache binaries with Github…","[00:00:03] Eric Nantz:

Hello, friends. We're back at episode 160 of the our wicked highlights podcast. My name is Eric Nance. And, hopefully, if you're able to listen to this, then the world is still spinning after the, little bit of an eclipse event we had here in this side of the world. But in any event, we are happy that you join us from wherever you are around the world. And as always, this show is our weekly take on the latest highlights that have been featured in this particular week's our weekly issue. And as always, I am never doing this alone, and he also survived the eclipse as well. My co host, Mike Thomas. Mike, how are you doing today? Doing pretty well, Eric. Yeah. We caught,

[00:00:40] Mike Thomas:

it here in Connecticut on the East Coast in the USA. I think, like, 95% of of total coverage. We weren't in the, line of totality as you are, but it was still still a pretty cool experience.

[00:00:54] Eric Nantz:

Yeah. I was here in the, Midwest. Got the lucky straw on this one, so had to take a very long trek. I mean, only a few steps to field in our neighborhood to check it out with a few, friends and and kids around. So, yeah, all in all, a good time. Temperature goes way down. You in about 3 minutes, it looked like we were in twilight zone. But, hey, I won't complain. It was a a ton of fun, and the world has still survived. So didn't have any, like, y two k issues from yesterday or anything like that. Internet is still on. It is still on. We're still recording here. And, thankfully, the Internet's on because that means that y'all can see the latest Our Weekly issue, which we're going to be talking about now, which has been curated by John Calder, another one of our long time contributors and curators on the project.

And as always, he had tremendous help from our fellow Rwicky team members and contributors like all of you around the world. Well, one of the rweekly highlights, especially in the last few months, if we didn't check-in with a good a good friend of the show who is continuing his journey with Knicks and r. And, of course, I am speaking about Bruno Rodriguez and his latest blog post on his blog, which is part 11. Yes. 11 of this series of reproducible data science with Nix. And this is gonna have a lot of aspects that I think you as a user and as a developer will both sympathize with. And I think open your eyes. There are a couple really interesting things that we can do with this ecosystem.

How does this part of the journey begin? Starts innocently enough with a package that Bruno has released years ago called chronicler. And he gets an email from the crane maintainers saying that there's been a failure in the build. What does this mean? So, of course, he checks out the CRAN package results. And this is where things start to really get out here because there is only 2 instances of r on Linux, and in particular, the Fedora distribution of Linux, that have the failure of the checks, yet Debian as well as, of course, the Windows and macOS test results all pass.

Now that is a head scratcher in and of its own because intuitively would think if there was a failure, a, would happen everywhere, and, b, even if it was isolated to Linux, that would hap but not all the Linux distributions that is checking. But that is not the case here. And so Bruno starts a little detective work. Right? And then he realizes, well, what would have happened on that particular day of the failure? Sure enough, there was an update to one of the dependencies. I would say a dependency of a dependency of his Chronicler package because it's not that obvious.

But in particular, the tidy select package, which for those aren't aware, is what powers now many of the tidyverse packages, functions that have to do with helpers in your select calls, like maybe select one of of the starts of a string or things like that. Well, it just so happened that tidy select got a new version on this day or at least the day before that this failure came to be. Now, again, you're wondering why on earth would this only affect one of the Linux variants. Turns out that Fedora is compiling the packages from source on each of these test runs, whereas the Debian distribution, and Debian is actually the base of the very popular Ubuntu distribution, that is using binaries of packages when they're available.

But for Fedora, they're not as available, these binary versions of packages. And, hence, a, you know, recently updated version of tidy select landed on the Fedora version, and there was an update in the tidy select, functions where an error message, in this case for eval select, was giving a more targeted message in terms of where that was actually happening in a select operation versus a subset operation. And Bruno, you know, listening to the advice of many package authors, has a very sophisticated automated testing suite where he is able to pinpoint one of the errors where this was failing because he was on the actual context of the error message or one of these select calls. We'll actually have a link in the show notes to this exact test because I I was just doing a bunch of testing for an internal package myself. So I'm always curious what others are doing in this space. And sure enough, yeah, that test was being triggered.

But because the other versions of Linux were not updated with the new tidy select binary yet, those pass the flying colors because that error message was the same as what he had built it on. Well, okay. Now what? Now what could we do differently here or at least what he could do differently here? Well, now that he's narrowed down where this failure occurred, where could Nix come in here? Well, he has a development environment that's powered by Nix now for building his packages. And, of course, for those who haven't heard the previous episodes, we've been talking about his Rix package, a way to help our users bootstrap these Nix like manifest to bring in dependencies of our packages and be able to constrain those particular versions and whatnot.

Well, he wondered, you know, would this happen on nix? Well, what's interesting enough, and this is another nugget for me, especially some I'll get to in a little bit, is that the packages in NICS that are that are literally corresponding to our packages themselves are actually not updated until a new release of our hits. This is actually kind of marrying a lot of what many of us in the life sciences industry deal with in terms of our internal R installations where there's a central library of packages, but they don't really get updated until a new version of R is released and deployed in production, and then we refresh the library with those. And, apparently, these stable releases on next follow that same paradigm, which that that's kind of food for thought for me as I think about ways of making environments a little more hardened as they say. But that's, again kind of a digression here.

So Bruno, what he wanted to do here is that, well, what what can he do to catch this more in real time with Nicks? And, of course, he has these nice mechanisms in place based on his learning to update what are called expressions in next to cat to bring in more up to date versions of packages. And he was able to do that with tidy select and then be able to then replicate the error on nicks that he was getting on that cran check Infidora. So with that, of course, now he's updated his unit test for chronicler to account for this new message.

That could be it right there. But Bruno, like me, likes to, go go a bit further deep in this and think about what is there a way to have these more up to date versions of packages kind of add availability whenever he needs it instead of, like, custom doing this for one off packages here and there like you did with this tidy select situation. Well, another thing that we've been plugging a lot is using automation when you can. Right? And so Nix can be incorporated quite nicely in GitHub actions, which is, of course, a great way for you to run, like, your CICD testing suite and whatnot for package development.

But could you also combine this with bootstrapping environments of our packages, much like how we often do with r env in a lot of our GitHub actions where we use r env to take in the versions of packages we we use for a given project. And then to get an actual look at that lock file, install everything, and then be just as if we are on our development machine, give or take a few differences, with that particular environment. There is a way to, of course, do this on Nick's, but he had to do a little bit of magic along the way.

Well, him and his colleague, Philip Bowman have started a GitHub organization with their fork of the Nix packages repository, which basically contains all the expressions for every package that's built in the Nix ecosystem. And, yes, this includes r itself and the packages within r that are built with these next expressions. So in theory, you could take this, clone it, update the expression to get, like, a more up to date version of the package. Well, as that version gets up to date, guess what it's gonna have to do? There are no binaries of that new version available at that time, which means you're compiling from source.

And yes, if you ever compiled stuff in the tidy verse from source and you're on Linux, get yourself a coffee or a favorite water beverage, you're gonna be there a while. Ain't nobody got time for that. Right, Mike?

[00:10:38] Mike Thomas:

Nope. No. Especially when it comes to, you know, trying to install the tidy verse or something like that. You know, that's a suite of packages all getting built from source for sure. You're gonna have to, take a walk.

[00:10:51] Eric Nantz:

Take a walk. Get get those steps in, but you don't wanna do that over and over again because you can only do so much walking in the day probably. But there there is a happy medium here. There is a happy ending to this. And that is you can actually, in the Nix ecosystem, have your own binary cache of binary versions of packages. And that is using this service called cachex, which I had never heard about until this post. But, basically, one of the things I've been benoming in my early days of my next journey is the fact that I was having to compile some of our packages from source. And I was living that that very, time this the time sync situation that we were just referencing. But, apparently, with cache x is that you can hook that up to your maybe GitHub action that Bruno is working on here. And in essence, there's some clever manipulation of how this is triggered.

He's got a process now where every day, maybe it's multiple times a day, that then there is a push to this repo that's gonna do the caching. And then that is gonna automatically build these packages if they're new and then push them to this custom cachex binary cache such that if he builds an expression from this point on with next and then he is able to reference this custom cache in his kind of preamble for the expression, it will figure out that, a, is this version of, say, tidy select available on the custom cache that Bruno set up? If yes, it's gonna pull down that binary version right away.

And if it's not in the bind not in that cache x, it's gonna look for most likely where it will be is the default Nix repository for binary cache. If it's a package that hasn't been updated in months, it's probably gonna be there in the next stable repository, and it's gonna pull from that binary cache. So in essence, he's got his he's got a workflow now where he can pull these binary versions either from his custom cache or from the default NICS cache. That to me is massive, folks. That is absolutely massive because this was arguably one of the biggest deterrents I've seen thus far in my limited experience of Knicks that may have just been solved for this quote unquote developer setup.

I'm really intrigued by this because this may be something I want to plug in to my package development because now I actually have 1 or 2 open source packages under my name that are starting to get used by some people that I'm I'm working with. I wanna make sure that I'm up to date on these issues, albeit they're not on CRAN yet. But at the same time, if they ever to get on CRAN, I want to harden my development set up for this. So this is an amazing approach that, again, everything's in the open. They got links, and Bruno's got links in the to the GitHub repository where this cache is being built and this other custom repository that's doing kind of the triggering of all this. I think there are a lot of principles here that we all can take from a developer perspective to give ourselves extra peace of mind for these cases where Kran might have some mysterious errors in these checks. And, of course, they're not gonna tell you how deep the rabbit hole goes of which dependency of a dependency actually caused a failure. It's on you as the as a package maintainer to figure that piece out.

But this may be a step in the right direction to be more proactive in figuring that out. And so I'll be paying attention to this work flow and maybe grabbing some nuggets of this as I continue on my, albeit it's still baby steps, but I've been venturing and running NICS on a virtual machine, trying to start bootstrapping our environments, bootstrapping my other tools of choice from a developer perspective. And this one, I'm definitely gonna keep an eye on as another way to augment my, my potential workflow here. So again, this post had a mix of everything. The detective work to figure out just which dependency caused this error, fixing the annoyance of having to compile all this manually every single time. And lo and behold, we got some really fancy GitHub actions and caching to to thank Bruno for. So kudos to Bruno for this immense rabbit hole journey here in this part of the next journey.



[00:15:19] Mike Thomas:

Yeah. It is a little bit of a rabbit hole, Eric, but I I think there's a lot in here that might be useful to to those using Nix and maybe those even not using Nix or those maybe on their journey as well. You know, one thing that sort of stood out to me is for anyone that has a unit test, in their their R package or their, you know, our repository and that unit test is is maybe running up programmatically or on some schedule or something like that, you should probably be aware that in the latest version of the tidy select package, if you have a dplyr select call and write your that the error message that's going to get returned if one of the columns in that select statement doesn't exist. It used to say can't subset columns that don't exist. Now it says can't select columns that don't exist. So if you have an expect error call in your unit test and you're looking specifically for, that keyword subset, that's not going to to fire anymore. So that test is actually gonna fail because it that that rejects those strings will not match essentially so you may need to go in and update your unit test and take a look at the unit test that you have across your different R packages and repositories to to make sure that you are not too going to get bit by this issue the same way that Bruno did, and it was really interesting to me as well, how, you know, the binaries I guess of the the the release before, I guess, the 1.2.0 release of Tidy Select, which is now, quote unquote, the old version, were the versions of Tidy Select getting installed, on Debian and Windows and and Mac, you know, for these crayon package checks, but it was compiling from source on fedora so that's you know there's there's all these these funky things that take place you know that we I think we see all the time that are huge head scratchers for when your your packages are going through these automated tests across all these different operating systems, when when they're trying to pass their crayon checks.

Right? They'll pass 9 of them and then the 10th one will fail. And it'll probably make you wanna throw your computer out the window half the time. But, you know, it just sometimes takes a little investigative work like like Bruno did or maybe, you know, diving into your your r four d s Slack community or your Mastodon community, and trying to figure out, collectively what's going on here. And hopefully, it didn't take Bruno too long to to figure that out. But, you know, as you you mentioned, I think, Nick's actually is going to allow him to easily, you know, handle these types of situations in the future, where his unit tests need to ensure that they're running against the latest versions of all packages or or maybe, you know, on some OS's, it'll be, you know, building the the latest binaries versus on other OS's, the latest, packages compiled from source.

So, you know, I think one avenue again to to try to do some of this stuff, you know, might be having all sorts of different docker images, to handle all these different edge cases and and use cases and operating systems. But it seems like Nix actually might be a more streamlined, and efficient workflow to be able to to make those switches a little bit quicker and run those tests, within different environments a little quicker to to spin up VMs or something like that or install, you know, all sorts of different versions of our locally and all sorts of different versions of our packages locally and and you know just sort of cross your fingers that, you know, that your package checks would succeed on other operating systems that you didn't really necessarily have any access to.

So you know that's really interesting and then and then maybe the last thing I'll point out here that I thought was interesting and and useful you know one a lot of times when we're you know, running GitHub actions. Right? It's it's running on some piece of compute that we we don't have full access to. So it's we're relying really on a YAML file at the end of the day and maybe this goes back to who was it? Yaron Ooms that authored that blog post a couple weeks ago about like, you know, workflows in our Oh. And Oh. You know, that's not repeat yourself. Miles Macbain. Miles Macbain. Oh. Miles Macbain. That's exactly who it was. Yeah. Oh, my goodness. This sort of brings me back to that where you know eventually you get to try to just do everything with it a YAML file and it would drive you crazy but, we're at the mercy you know with GitHub actions actions, you know, for better or for worse a lot has been abstracted for us which is awesome but we're at the mercy of, a YAML file. Right? To be able to handle, you know, setting up that that piece of compute, that runner and GitHub actions, installing the dependencies on there.

And, you know, one thing I think that can be be tricky sometimes is is maybe authenticating it into different services that you wanna leverage. And that was my first thought here when I was taking a look at the this cache x, this cache utility here that allows you to to cache, different, packages, within sort of your next environment. But one of the interesting things is that you don't need to authenticate to cache x to simply pull binaries. All Bruno had to do was was really just these two lines of YAML specifying the cache x GitHub action version that he wants to use and then he has a particular name I assume you know he's logged into the cache x platform and and developed his own cache. That's that's b dash rodriguez, and that's where his cache lives, and that's all he has to specify in his GitHub action to be able to pull, binaries from that cache. So I think that's incredibly powerful, really nice that how simplified sort of this workflow is to be able to set up, your cache x cache, it within a GitHub Actions workflow. So fantastic blog post from start to finish. It really walks us through sort of the entire problems problem space and and how he was able to solve it.

And another feather in in Nick's cap, I would say.

[00:21:37] Eric Nantz:

Yep. I totally agree. And I'm, again, very much anything that can make your development life easier to be proactive on these issues happening, you've done that hard work of building those unit tests. You're doing what you need to do in terms of giving yourself some sanity, hopefully, as dependencies get updated. But, be able to put this in action first when you need it in an ad hoc way, be able to just pull this environment down within probably a matter of a minute versus, like, an hour like it would be by default.

I mean, they they can't underscore the time savings you get from the caching mechanism, But also there is a lot to glean from these repositories that you mentioned, Mike, that Bruno and Philippe have set up for these GitHub Action workflows. I I was in GitHub action nightmares, a couple weeks ago, debugging things. And I remember with a quartile compilation, I just I just I almost just threw my keyboard out the window. I just could not figure out what the bloody heck was happening. So, yeah, there is a danger sometimes of having the abstraction too high up versus what lower level it's doing. And the other piece of advice I'll give is that, you know, it may seem intimidating to be like, these these kind of actions are doing such magical things with these YAMLs.

You know what's behind these actions most of the time? It's shell scripting. It's just doing stuff on Linux and then be able to troubleshoot that. So, like, I went into the GitHub action source of some of those quartile actions, and I figured out, okay. So these kind of quartile commands with this Boolean check if, like, I have this render flag enabled or not. Then I realized, you know what? Okay. I can take matters in my own hands. I can do my own custom rendering. It's not as intimidating as it sounds. It's just the bugging can be a complete nightmare unless you know where to look. So I'm sure I'd imagine Bruno's had a few of those experiences too like all of us.



[00:23:29] Mike Thomas:

Definitely. And GitHub actions has its own sort of cache as well which That is true. Has bit me a couple times not to speak ill of GitHub actions because I do I do love it. But, yeah, it's it's caches everywhere.

[00:23:43] Eric Nantz:

Yep. Caches, unfortunately, is not the kind of cache I could buy things with. But, you know, nonetheless, it's all everywhere. Right? Speaking of major updates that we're seeing in the ecosystems these days in terms of the tooling we use, another one I've kept my very close eye on with a lot of influential projects I'm trying to bootstrap right now is we are happy to talk about a new release for WebR itself. When this blog post comes from the tidyverse blog and authored by the WebR architect and brilliant, engineer George Stagg, who is in this blog post getting us through, you know, or talking us through a few of these major updates that we see in the WebR ecosystem.

And I'm going to speak to a few of these. And I know some of these I'm going to be able to benefit one very quickly, I would imagine. But the first major of note is that now they are basing WebR on the latest stable release of R itself because in essence, they have to bootstrap R with a couple of the custom shims under the hood to make it seem like you're installing a package from a default repository where instead you're installing it from WebR's binary package repository. So they always have to, in essence, patch R to make all this work. And now they patch the latest stable version of R, which, of course, will be great for consistency with maybe your typical environment that you're building these analyses on. And another great feature that I think the users are going to definitely notice is some improved air reporting too.

Whereas in the past, you might see a custom, like, uncalled exemption if something crazy was happening in the WebR process with respect to maybe an error in the r code itself. Well, now you're gonna be able to see the actual error in the JavaScript console whenever WebR is running, which, of course, is going to help you immensely with the bugging to narrow down, oh, did I get the JavaScript code wrong that I'm using WebR with, or was it in my R code itself? So that, again, for usability, a massive improvement for the quality of life.

And speaking of quality of life enhancements, ones that you'll they'll hopefully be able to see and interact with is big improvements to the graphics engine. Because now the graphics that are drawn with WebRx, they call it the canvas function, they're gonna be captured in that R based session in a similar way so that it's consistent with other output types that you would see in a normal installation of R, which is leading to more streamlined, you know, interaction with these graphics. And, also, along those lines, graphics that you create in Base R have had massive improvements to the rendering engine. And the blog post, when you see this linked in our weekly, you'll see very sharp looking images that they're able to produce when you evaluate that code. Even as I'm talking now, I'm running through these snippets, and they look very nice. Great anti alias text.

The symbols with the r logo look really sharp. Everything looks as if you were in your native r session. Again, just massive amazing improvements under the hood to make this, you know, very seamless and attractive to the end user. And it doesn't just stop with graphic objects. There has been a lot of under the hood enhancements as well as some new user facing enhancements that the R objects that are created in these WebR processes are going to be more easily converted back and forth between their JavaScript counterparts and their R counterparts.

Where does this fit in the big picture? The big thing for me is the data frame and r that is literally the fundamental building block of pricing 99% of my analyses. Now, of course, in r, you have your columns and you got your rows. Right? That's not always what JavaScript treats data as, especially if you're familiar with d 3. It is definitely a different type of layout you often see where it's basically the column is given a name. It's like a named vector of things in a row wise type fashion layout. But there are functions now in WebR to go back and forth between these JavaScript notations and the R native kind of representations.

In fact, there's a handy function called 2d3, where you can simply take that data frame in the R process, put it into a format that you could feed in any of your custom d three visualizations. These things are massive for that interoperability between maybe using r to do the heavy lifting statistically. But if you're familiar with JavaScript and you wanna keep with those JavaScript visuals, have at it. Have your d three magic and be able to get that that construct of the data as you need it. So, again, great way for you to take maybe baby steps if you're new to R, but you're familiar with JavaScript, a way to get the best of both worlds. Really, really a big fan of that.

And then some really fun plumbing type enhancements here that I'm paying a lot of attention to is that with collaboration, you named dropped them earlier, Mike, your own ooms as well as, Utahn. They have new system libraries in the WebR process and as well as a custom WebR Docker container. Hey. Containers again for you. Which they've included some new numeric libraries as well as the ImageMagick image manipulation suite. And wait for it. There is now a Rust compiler configured so that if your R package involves Rust in some way, you're not gonna be able to compile that into web assembly 2.

Absolutely amazing stuff. And to really, you know, you know, intrigue you even further, embedded in the post is a Shiny Live powered web app from your own himself on using that magic package that he's created in the past. But now in the blog post where you can absolutely distort or make fun of this random cat image. And I'm literally doing it right now as I speak. It is so responsive so quick. I can't believe this is happening, Mike. This is just amazing.

[00:30:30] Mike Thomas:

It's as if the shiny apps running locally. It's insane. It's actually, like, faster. I mean, the the fact that you can put effects on this image to like negate it and you know you totally changing how it's visually represented is awesome. You can upload your own image which is what I've done. I uploaded an old hex logo and I'm just messing around with it right now, rotating it, blurring it, imploding it you know it's it's pretty cool it flipping it reversing it yeah It's it's it's awesome. I cannot believe how responsive it is.



[00:31:03] Eric Nantz:

Yeah. This has come such a long way. I mean, even it was maybe, what, 4 or 5 months ago that would take a good minute or 2 for these things at Boostramp, and here we are. We got this as if it was plugged into the blog post directly. Like, this is this is magical stuff. So I'm really, really intrigued by by where this is going. And, again, we're seeing more in the community leverage Rust. So I'm sure there's going to be a lot of grain enhancements with this. And speaking of packages themselves, there is now additional, refresh, if you will, on the packages in in the CRAN ecosystem that are now supported in WebR, that total is now bumped to 12,969 or about 63% of the available CRAN packages that are now supported natively in WebR.

And like I mentioned before, what George and other engineers have to do with this WebR binary cache of packages is they have to compile them in a special way so that they're supported by web assembly. So not everything is there yet, but that's a massive jump compared to where we've been even just like I said a few months ago to what is supported in webr. That is absolutely amazing.

[00:32:18] Mike Thomas:

Yes. I couldn't agree more, and this is just, you know, more exciting improvements. I think, you know, probably the biggest thing for me as a, you know, practitioner, somebody who who's very data, hands on in most of our projects as well as as you talked about, Eric. The ability to to switch back and forth, between JavaScript sort of data frame objects, you know, especially, I guess, particularly in this case, we're talking about raw objects of typed array array buffer and array buffer view those are the types of raw vector objects that may now be used to construct our objects and like you said you can go back from an our data frame to a Javascript object with the either 2 object function or if you want to go more the d3 route that would be the the 2 d three function in Javascript so that's exciting because to me that makes it feel you know much more our framework as well. It kinda feels a little, I've been watching a lot of the observable project and they have some some excellent, utilities, blog post documentation around how to sort of convert back and forth between either pandas data frames, our data frames, and you know the JavaScript data frames that really power the visuals, on that platform. So this sort of reminds me of that in terms of how easy it is to go back and forth between the 2, and I think that that ease of use is really going to go a long way towards helping people get up to speed and build tools, within the WebR framework. So very very excited.

Thanks to to all who work on this. There's a fantastic acknowledgment section at the bottom of this and and a lot of familiar names there of folks who are are working hard to continue to drive this this forward and, you know, especially thanks to to George Stagg for sort of spearheading all of this and putting the blog post together.

[00:34:15] Eric Nantz:

Yeah. And and and any package authors out there that are saying, hey. You know what? I know my package is not crayon. Can I do this a web assembly? Oh, yes. You can. They actually have a r WASM package to help you compile your own r package into WebAssembly and be able to manage a repository for that. And, yes, we were talking about GitHub actions earlier. Another, you know, nugget at the end here is that they have put links to their reusable GitHub Actions that you could get inspiration from or just leverage yourself so that your package is compiled.

And, yes, you can also, check out we talked about this before. The R Universe project is offering WebR binaries of packages on any of the packages that they host too. So there is not just that binary, repository that WebR is hosting themselves. You know, our universe is another great inspiration or a great source of this as well. So lots to choose from in this space.

[00:35:15] Mike Thomas:

Yes. Absolutely. Don't blink.

[00:35:25] Eric Nantz:

Don't blink. You might miss it. Just like how you might miss some very snazzy visuals that we're gonna talk about on our last highlight today. And as you know, Mike, you and I can probably sympathize, especially early on in maybe a data science journey or you're leveraging Are you hearing about things like the tidyverse, you know, suite of packages for the first time, and you're trying to get a handle on a lot of these operations that occur when you want to manipulate your data frames like adding new variables, doing transposing, you know, doing group summaries.

A lot of people, myself included, do like to have a nice visual to accompany maybe the text around just what those particular functions are doing. Well, we're happy to share that Andrew Hice has authored a latest blog post on visualizing many of the important dplyr functions with both, single group as well as multiple group processing. And not just having static pictures, folks, he has handcrafted some absolutely amazing animations to go with this. I can I can imagine he is making heavy use of this in his statistics, lectures and his coursework?

But, yeah, of course, we're we're in an audio podcast talking about visuals here, but boy oh boy, you look at these and how he made them, first off, is that he tried to leverage what Garrigade and Bowie, from has built with his custom visuals in the tidy explain package, but he was running into a couple limitations. I'll be especially around the group processing. So Andrew is showing his, graphical editing shops here, has built custom Adobe Illustrator powered visuals and after effects. But guess what? He's actually shared the the files that if you have the software, you go import this in and try it yourself.

I I'm not an Adobe customer, so I can't exactly run this myself. But, hey, it's there if you have that same software. But throughout the blog posts, he's got not just the narrative of what a function does such as mutate. You can simply hit play in the blog post and then see literally the transformation of that data with this nice transition kind of layout from left to right with new variables or new records being colored the same as the new code snippet that's being shown on the right side. It is very easy to digest.

It does as a mutate with summarize, and then where it really starts to shine is the grouping visuals. Or now, if you're grouping by a categorical variable, he's able to separate these blocks out logically and then really show the impact of that operation and then be able to ungroup that at the end to get that original setback or things like that. And also custom mutations within groups. So he's got the code snippets alongside the visuals. Again, really great to digest visually what is happening here. And, boy, I wish I'd had this when I was learning the manipulation in the old days. But, of course, in my day, not to be that guy saying get off my lawn, we didn't have Dplyr back then. We had Plyr midway through my hour journey, and that did not have nearly the resources that dplyr has both under the hood as well as in educational materials.

But, again, the visuals really do a terrific job of really isolating the key incremental steps in these operations, which I think you can see it in a pipeline operation just with the text. But seeing that visual paired with it, you really start to see what is the impact of each of those statements. So there is a whole lot to digest here. We definitely invite you to check out the post because our ramblings can only do us so much justice, But, kudos kudos to Andrew for handcrafting all this from the ground up. It is truly amazing stuff.



[00:39:28] Mike Thomas:

Yes. This is fantastic. You know, I think I know Andrew is a professor, and this is just go I wish I had him as a professor because this I know. Is just going to go, such a long way towards building his his students understanding of, you know, especially, you know, I think I think mutate, isn't isn't too difficult to wrap your head around. It's a little straightforward, but once you start getting into grouping by aggregation, summarization, doing a mutate on a grouped data frame, joins, things like that, It it, you know, takes it to an additional level of, you know, abstraction that can be difficult to wrap your head around when you're when you're a beginner.

And back in my day, Eric, it was, Excel pivot tables. That's how I first got my my start in, you know, understanding some of these concepts and then I had to bring it over, to the our side and I was like, wow. Hey, you know, this is a, maybe a hot take a little bit but what has a better expressive syntax to tell you exactly what it's doing, than dplyr and the tidyverse. I don't think there's anything else out there. Sure surely a whole lot better than Excel. I I can at least tell you that much in in terms of just being able to to look at the code, you know, I think SQL is probably a close second, but to be able to look at the code and understand sort of exactly what's going on and, you know, these examples applied to data, I think are fantastic. These these visuals go such a long way towards building that understanding as well because you can he's really representing sort of the the action taking place here, whether it be grouping, summarization, or a mutate.

So, yeah. Hats off to him. I would say that that his work doing this, he may not realize just how impactful this might be on his students or, you know, folks learning r who stumble across this blog post. So if you if you know someone, who's who's trying to learn r having a tough time, you know, first point them to the Tidyverse. That would be my recommendation to wrap their head around dang data wrangling, but I think this is going to be a fantastic blog post to to point people to as well, to help them get up to speed, to help them sort of get over the hump, right, in building their understanding of data manipulation in R. So I I really just can't understate, you know, I've said it a few times now, but I can't understate sort of the importance of this and, how useful I think this type of visual instruction can be.



[00:42:03] Eric Nantz:

And another thing I wanna say is critically important, especially in this new age of, I don't know how new it is per se, but really someone I've been gleaming in the community of really openly sharing our our wins and our key learnings to educate ourselves and others is that you may be wondering, oh, wow, Andrew. You did such amazing work with this. I wish I could use this in my materials. Guess what? You can, folks. These are all Creative Commons license, which means he's got links to, like, the finished products, both the video form and animated GIFs and static images of all this. So if you wanna leverage this in your materials, have at it. Right? I mean, that is a huge service that Andrew has done here. He could have easily kept this under his own, like, private coursework, you know, materials, but he's sharing it with all of us. So I think that is a massive win for all of us kind of in this educational space. So even our organization is trying to help teach others that are new to R how some of these operations work.

You better believe I'll be, drawing upon some of this when I get questions about how some of these, you know, group processing and deep wire works and the like. It's just amazing amazing work, Andrew, and even more amazing that you are so willing to share this with everybody. It's a new it's a new age, I would say.

[00:43:29] Mike Thomas:

And has the time to author all of this content out there. I don't know how he does it, but thank you so so much.

[00:43:35] Eric Nantz:

Yeah. I I whatever you're you're, either ingesting or whatever life hack you've done, please pass it our ways because we we would love to have it. I respect the It's gotta be those damn cold tubs. I still gotta do that. Yeah. Yeah. I think we're late on that, aren't we? But, you know, what you're not late on is that if you boot up our week without org, you're never going to be late and seeing the latest and greatest from the R community showcased by the R community. Great new packages, new announcements, and new blog posts, much like what we talked about here. And we'll take a couple of minutes to share some of our additional fines from the issue. And for me, it's a plug, especially for those in the health and medical industries.

The Our Medicine Conference is back for another year, and they have just opened up their their open call for talks. And that conference is taking place on June 10th or 14th this year. And on top of having the call for abstracts and and talks open, they've announced that they have keynotes lined up from Stephanie Hicks and Gunilla Bosch, talking about some really innovative work in medical research and genomic analysis. Always stuff that I resonate with, especially in the early part of my career where r was immensely helpful as I was doing a lot of genetic the to the post from the Arkansas Museum of all the details. And it's always been a really top notch and well run conference. It's been a great kind of companion in arms, so to speak, with our pharma conference that I'm a part of. But, yeah, they all are very complementary of each other and really great to see these resources shared in the community.

Mike, what did you find?

[00:45:27] Mike Thomas:

No. That's awesome, Eric. I found a fantastic blog post, I think, authored by, Hannah Frick, who's a member of the Tidymodels team at Posit, and she's been instrumental in incorporating survival analysis into the tidy models ecosystem. So there is now support for survival analysis for time to event data across tidy models, which is is really really fantastic and exciting for those of us, which, I know you as much in life sciences as, believe it or not, me, in financial services and and credit risk, use these time to event survival analysis types of models because, they handle, you know, leveraging longitudinal data.

They, you know, have a good setup to be able to introduce hierarchical models as well, with mixed effects models or or Bayesian type models that, you know, are and these these longitudinal, types of data sets, as well as these hierarchical types of data sets, we see them all the time. I mean, that that those things are present in a lot of the data sets that we work with in a way that machine learning just can't necessarily handle. Right? So survival analysis is really a great tool to be able to use in those particular use cases.

There's a fantastic amount of links in here. There's also an additional blog post that walks through a very hands on, data use case, a a case study here called how long until building complaints are dispositioned. That's now on the Tidymodels website. It's it's linked at the very bottom of this blog post, and that's, sort of how I I came across this Our Weekly Highlight as well, because I saw that one, I think, come across Mastodon. And it's just a great, great walk through and and use case of all the different functionality that we now have around survival modeling, within the Tidymodels framework. So super excited about this. Thanks to all the folks that worked on this, and thanks to to Hannah for putting this blog post together.



[00:47:31] Eric Nantz:

Yeah. I'm diving in. I'm gonna dive into this case study after this because, I'm starting to get back into the time to event mode now with some custom analysis and clinical operations. And this could be a very nice way to benchmark newer models and take what I've been learning over the years of tiny models and apply it directly to time the event. It has been requested for a long time, and it is gratifying to see the tiny models team has taken this head on, and now we've got a boatload of additional capabilities to choose from. So, yeah, kudos to everybody on the team for that.



[00:48:06] Mike Thomas:

Yeah. No trivial effort, I'm sure, because survival analysis modeling has a lot of different nuances for machine learning, but you can still borrow a lot of practices, you know, that we might traditionally associate with machine learning, like, hyperparameter tuning and things like that, and and leverage those within survival analysis workflow. So, it was no trivial effort, I'm sure, but extremely powerful, when brought together. And I'm very grateful for the fact that they have.

[00:48:35] Eric Nantz:

Yes. Absolutely. And I'm also grateful for hey. Hey. Our weekly even existing. So we can see this on a weekly basis and be able to talk to all of you about it. But it is, like I mentioned, powered by the community, and we always are appreciative of all of your help. And the best way to contribute to our weekly is send us a poll request with that awesome new blog post, that great new package you discovered, that great new resource that you think deserves the attention of the community. Is just a poll request away at the top right corner of r o k dot org. You'll see the frame little Octocad image there. Just click on that, and you got yourself a pull request template right there. It's all marked down all the time. I live in markdown. I live and breathe markdown now. I wish I could write my emails in markdown and have them render in real time, but that thing happened in Microsoft Outlook anytime soon. But I digress. At least, so every week, it does. So you can have that at your at your at your fingertips.

And as well as we love hearing from all of you in the audience as well, you got a few ways to get in touch with us. Got a handy little contact page linked in this episode show notes and as well as you can send us a friendly little boost along the way if you're on those awesome modern podcast apps like Podverse, Fountain, Cast O Matic, CurioCaster. I could go on and on. They're all linked in the show notes if you wanna get a nice selection of all that. And, yeah, this is now gonna be the 2nd episode. Our new podcast host, and everything is yours truly botching his end of the recording. Let's hope that this one goes more smoothly this time around. Did some testing before this, fingers crossed. It'll sound as clear as we've always been, but, we are on a new host. So if you have any trouble finding the show, we have updated all the links at rweekly to our new podcast landing page, so it should be a seamless transition.

But as always, don't hesitate to get in touch with me if you have any difficulties. And the best way to do that, you can find me on Mastodon these days. I am at our podcast at podcast index on social. I'm also on LinkedIn, with show announcements and the like. And on that Twitter x thingy jingy at DR cast. And, Mike, where can the listeners find you?

[00:50:41] Mike Thomas:

Sure. You can find me on mastodon don@mike_thomas@fossedon, dot org, or you can find me on LinkedIn by searching Ketchbrook Analytics, k e t c h b r o o k, and see what I'm up to lately.

[00:50:57] Eric Nantz:

Awesome stuff. And, yep. We we were on the other side of that major eclipse event, but the the community train never stops over the our ecosystem. And as always, we thank you so much for listening to us wherever you are around the world, and we will be back with another edition of our weekly highlights next week."
"59","issue_2024_w_14_highlights",2024-04-03,41M 5S,"Taking the tradition of spring cleaning your R session to a nefarious direction, how a little R and automation crafted together helps with bill payments, and the tried-and-true method of simulation in action to investigate time-to-event inference statistics. Episode Links This week’s curator: Ryo Nakagawara - @R_by_Ryo) (X/Twitter) &…",NA
"60","issue_2024_w_13_highlights",2024-03-27,39M 0S,"How a recent pivot in one of the most popular testing frameworks in R unlocks mocking once again, bringing robust grammar checks to your R development environment with rspell, and flex your Shiny and HTML design muscles with flexbox. Episode Links This week's curator: Batool Almarzouq - @batool664 (https://twitter.com/batool664) (X/Twitter) Update…","[00:00:03] Eric Nantz:

Hello, friends. We're back up to a 158 of the R Weekly Highlights podcast. If you're new to the show, this is the show where every single week, we talk about the latest awesome highlights that are featured in this week's our weekly issue available at ourweekly.org. My name is Eric Nantz, and I'm delighted you joined us from wherever you are around the world. It's been a, you can tell spring is definitely more in the air, and I woke up to, a big old windstorm over here blowing over trash cans and everything around our house. Hopefully, you're safe wherever you are, but not in any event. I never do this alone. I'm joined at the hip here. Hopefully, he doesn't blow away with the winds either by my co host, Mike Thomas. Mike, how are you doing today?



[00:00:44] Mike Thomas:

Eric, I'm doing great because I finally got my devcontainer and Versus Code set up for quarto rendering to PDFs.

[00:00:53] Eric Nantz:

Flawless victory. Oh, listeners, if you only knew the struggles Mike has had on this effort, even I tried to take a look at it and I was scratching my head. So I'm really happy you solved that. But, man, what a game changer that is. Installing Windows fonts and everything. Oh, the pain. You don't wanna know. Yep. Yeah. I I really don't. Yep.

[00:01:15] Mike Thomas:

Blog post, I I guess I owe the the world a blog post on this. I would love to see it. Yeah. We can never have enough content about using

[00:01:23] Eric Nantz:

struggling through the container setup because it is never easy the first time around. Awesome stuff. Well, congrats on that. And, hopefully, you don't have too much debugging as we record this issue. But, nonetheless, we have a fantastic issue today, and it was curated by Batool Almarsakh, now a long time member of the r Wiki curation team. And as always, she had tremendous help from our fellow r Wiki team members and contributors like all of you around the world with your awesome poll requests and suggestions.

And, yeah, we're gonna lead off here with a concept that actually was touched on, you know, a bit earlier episodes of this podcast, but there's been some great, I would say, revisions in kinda how we go about this. And we're talking about the use of the mocking principle for testing your R packages. And in particular, this post is coming from, well, you guessed this. She returns once again to the highlights, Myel Salman, and this time from her post on the rhub blog on some of the latest developments that have happened in this space of automated testing within the R ecosystem and how mocking has just become a bit easier to do, you might say.

And first, a little history lesson. So it was back in 2019 or so that the test that package, which at the time had been experimenting with implementing mocking with a with underscore mock function, decided to pull the plug on that a little bit. They noticed that they weren't too happy with the implementation of it, and they weren't quite sure of the best use cases for it. And as a result, there were some packages that sprung up in the our ecosystem to help fill this now gap, such as mocker and mockery. And the approaches that they took apparently caught the eye of Hadley Wickham himself because in the latest release of Testat 3.2 that we covered in a previous episode, the mocking system has returned with now new functions that replicate that existing functionality.

But in Hadley's words, in a much better implementation that they can scale up and make sure that it's robust in the future. So Ma'al now has written this post as a revision to an example that they did back in 2019 that is now taking advantage of TestDAT's new functionality. And in my crack research, if you will, before the show, I did dig up the very poll request that ended up bringing the mocking support back in the test net. We'll have that linked in the show notes if you want to hear the dialogue or see the dialogue between Hadley and other developers on this great development.

So how do you end up doing this in your test suite? So in the post that or an example that they draw up here, Mahal first leverages to use this package to actually create a very simple prototype package that illustrates this point. And then the, the base example of this is a little callback to what you just said earlier, Mike, about your adventures with Windows fonts. They have a little fun function here called is encoding a pane. Guess where it's a pane, folks. It's a pane on the Windows system. I hey. I didn't write it. The Mal wrote this one, but it's a simple function that runs the base sys.info function to determine what operating system the r session is running on. And, of course, if it comes back as Windows, yeah, encoding is a pain. We all have been there. It's a very simple function.

But here's the thing, though. Let's say this package is going to CRAN or another, you know, other system and you want to run your automated test, but you're probably going to run this either on a GitHub action, you know, container or whatnot. And most of those are not running Windows with the exception of, say, the CRAN wind builder service. So what do you do when you want to test this function but don't want it to actually do the check itself? And that's where this new enhancement to test that comes in.

In particular, they, leverages the function local mocked bindings inside the test that she crafted here where she basically makes a fake version of the sys.info function but gives it a simple return value, you know, hard coded, if you will, so that when these tests are run, it's not going to call the actual base r installation sys.info. It's gonna call this what you might call the mocked version of sys.info. So this technique, yes, this is a bit of a contrived example, I'll be it. You know, there can be panes of windows on various things. We won't get into that here. But it does illustrate the point that if you want to temporarily, for your test purposes, inject your own return value or circumvent what the function actually would do and put in your own return value instead for the sake of the test.

The mocking principle with local mock bindings is going to be a very helpful way to make this happen without a bunch of refactoring of your basic code in your package. And she does have link to the manual page for local mock bindings if you want to see where this fits in terms of test stats, you know, you know, baseline functionality. But that's not all you can do with with the mock bindings functions. And, Mike, why don't you take us through another revision that Mau does here with what she calls revisiting their escape hatch example.



[00:07:17] Mike Thomas:

I remember this example fondly, I think, from in our weekly post, from from Elle on this very same topic. You know, it's probably a year or so ago. Now, at this point and and I thought it was really fascinating. And sort of the example is that there's a function in the r directory of your your package, so it's gonna be one of your package functions. And, you know, it it has this check sort of at the top of the function to see if, the user has an Internet connection. There's a function from the curl package called has_internet, which, I guess, detects whether or not, you're connected to the Internet. So, you know, imagine that you want to write a test to ensure that, that function is is firing correctly. You know, for example, say that, in your r function, you're returning a message letting the user know that there's there's no Internet, so they should make sure that they connect to the Internet in order to run this function. Right? And you wanna make sure that that message is is firing correctly.

When you go to run these automated tests in something like GitHub actions, you're probably not gonna be able to shut the Internet off, right, during that action. So this is where this local mock bindings can really come in handy, where you are essentially forcing the result of of this function, that that is called is InternetDown, this example function that they have, to be true. So you're you're essentially setting that and then your function is going to return that message that you would expect, and you can match that up in in an expect underscore message call from TestThat, and then life is good, essentially. But, you know, sort of before we had this local mocked bindings, ability in the TestThat package, it was very difficult to do this, sort of very hacky. Maybe you don't even maybe you don't even write the test because it's it's difficult to actually test.

Maybe you're using the with our package too to to try to control some of this stuff, but I think, these these new local mock functions actually make it make our lives even a little bit more streamlined than having to use with our to to handle our environment to do something like that. So I'm really excited. That was a great example. It sort of brings me back to when I was, writing unit tests for a recent package that was unzipping files in a different order than I was expecting them to be unzipped in in my GitHub actions call. And I it was my first attempt at trying to to be a good, software developer using the with r package in my tests and, shot me in the foot. But that's the story, for for a previous time and maybe another time if anybody wants to hear it again. But at the end of this blog post, Mel, provides some some great additional examples where and this is another one that resonates with me.

You know, mocking might allow you to simulate having an older version of a package installed. So, you know, what if somebody has, you know, a version of r that's like 3.5? Or or I guess in the example of a package, what if they have a version of dplyr, as my an example with here? That's that's version 1.0.0. Maybe you have some code that is dependent on, a a particular version, a particular package being installed, so you can pretend that, in another example here, provides, you know, pretend that Arlang isn't installed. You can use local mock bindings to do exactly that.

You know, you can create OAuth tokens or simulated OAuth tokens. You can create a fake GitHub URL for a project. The the sort of the possibilities here are endless. I really appreciate all the different real life examples and links that Mael has provided here. And it's it's a fantastic blog post showcasing this new, you know, albeit fairly straightforward,

[00:10:56] Eric Nantz:

function, local mock bindings, I think really powerful. So I'm excited to, leverage this new functionality and test that. Me as well. I was telling you in the preshow, I'm working on an internal package that's wrapping an API for scoring some health outcome measures. And this will be, you know, a great example of leverage a lot of the principles here with other, you know, best practices of API, package wrapping development. And, you know, this is yeah. These principles are really cool. And, yeah, that real life example about the package version, literally as you were talking about that, I just thought about a situation that I get confronted with for teams that are wary of building internal frameworks that are depending on what we call our central R library of packages, the ones that our IT group freezes until R is upgraded itself.

Well, of course, let's say we upgrade R. We upgrade to this new version, say, 3.2 or whatever. And then those same tools that we're working on, say, you know, 3.0 or whatever have you, they wanna make sure that with that upgraded base library that's coming in a new version, will their internal tools still work as expected? Boy, oh, boy. If they wanna put some automated tests here, it seems like the mock bind the mock bindings with package versions would be a huge way to go. So thank you, Myelle, for putting that in front of me and seeing this great example from the pool package because that comes up time and time again when people are wary of, quote, unquote, the fast pace of our package versioning.

You know, it's not really that fast paced, but I digress. But either way, there are lots of there's lots of, great principles here that I think you can take wherever you're building wrappers to APIs or wrappers to system processes or other the the possibilities are endless as you see here.

[00:12:45] Mike Thomas:

Absolutely. I agree. And, yeah. I mean, a lot of, a lot of throwbacks for me here not only to old blog post but the old situations that I've I've dealt with sort of this this overarching idea of how to really set, you know, minimum versions of R and minimum versions of packages as well for the R package that you're developing is that I think a really sort of complicated process as well. So maybe maybe some of these functions can help us, help us in that

[00:13:18] Eric Nantz:

journey. Yeah. You know, Mike, it's always good to have a helping hand now and again. And another area where I definitely need some help is if I'm drafting that very comprehensive report and I get it ready to send out, but, you know, I need that double check. Right? Misspelled check because, yeah, the fat fingers here often do misspellings in my documents. And also, it's not just a spell check. Right? Sometimes we need a little help with our grammar because as much as we learn about ours in our school education, we sometimes forget some of those rules. But that's where the magic of technology can help us. Right? And our next highlight here, we we highlight, if you will, this great capability where maybe what we see built in to an IDE isn't quite enough, but we want to tap in some more power or recite the grammar checking. And in particular, we're gonna be talking about this, I believe, a new package in the R ecosystem called Rspell.

Rspell has been authored by Rafael Saldanha who is a postdoc at INRIA, a French National Research Institute. And what is rspell for here? Well, rspell, the main objective here is to help plug in potential gaps that might happen in IDEs such as Rstudio for being able to do not just spell checking but grammar checking as well. Because some in the community and, like, the tech world at large will rely on services such as Grammarly or others that help with their document writing or even email proofreading and whatnot.

Well, some of those are not really, you know, at this point integrated into, say, deposit or Rstudio IDE. So where Rspell comes in is that it is, wait for it, a wrapper package to an API called the language tool API, which apparently does have a free tier. I did some digging on this. So it looks like this is leveraging the free tier of that API where then the user can simply select some text in the document they're authoring. It could be a markdown file, a markdown file, or a quartile markdown file, what have you, even just the documentation in their R script or whatnot.

And then it will have a command or a function called check selection. And then the console will display any potential grammar errors after sending that, to the API for checking. And, of course, who wants to do that always in the click and drag approach? There are some nice keyboard shortcuts that you can add in your Rstudio preferences as an add in as well that you could key bind all that. And this is another major selling point. It will have support for multiple languages, and it will be smart enough to detect if your default language in POSIT or Rstudio has been set, it's going to automatically leverage that particular language when it sends these grammar checks to the language tools API.

It'll also respect any definitions that have been made for, you might say, exceptions in the user's dictionary set in Rstudio as well. And then also it may be able to just say, I want to see the errors. Don't worry. Modify them. Yeah. Let me be the judge of that. You can do that with an argument ask modify equal false in the check selection function. Where I see this is honestly not just with the RStudio ID. I think this can be a great help for any development environment that you're leveraging with R. Why not put this in something like Versus Code or or Emax or N VIM, whatever have you? Like, this is giving you the power to do those grammar checks in a in a custom fit for purpose package that can be independent of, say, the ID.

So really great niche that's being filled here by by Rafael's Rspell package. And certainly, I think it's going to be a huge win for those of us that are not exactly using, you know, the Office type document, you know, software for our our document writing. It's always like a pain in the rear end for me to write something in Microsoft Word anymore. I love to do quartal and R Markdown all the way, and now I can have my nice grammar checks in there too just like anything else.

[00:17:43] Mike Thomas:

Yes, Erica. I couldn't agree with you more. I have a very hard time, like, authoring anything or putting any sort of text on screen in Microsoft Word as opposed to if I'm in in quarto, in in IDE, like Rstudio or Versus Code, I can I can write away? I don't know what's wrong with my brain that that's the case, but that's just, I guess where I'm at these days. And and this language tool API is is really really interesting and really useful. We author, you know, some pretty long, quarto documents that are either, you know, model validation reports or, you know, RFP responses, things like that. So so we do, you know, have a need for grammar checking, you know, on these these large pieces of text. And, I think that this package is actually going to be be useful sort of immediately for us. You know, one thing that I think is fascinating is the ability to select some text with your cursor, it sounds like, and run this check selection command at the console. That that to me is fascinating.

You know, I was taking a look at some of the, limitations that that you have. And and even on the the free plan, it's, I don't know, it seems, you know, pretty friendly, in in terms of how many requests you're able to make. 20 per minute. Number of characters you're able to, make in a request per minute, 75,000 free. You know, maximum number of characters per request, 20,000 free. It's pretty incredible. And even if you go to the the premium version, which which scales that up, you know, about fourfold across all those different metrics, I think it's like, you know, $39 a month for a 100 API calls per day. It seems seems pretty reasonable. It seems like a really fascinating tool. I know Grammarly has been, you know, one of the the larger name, larger brand resources out there for grammar checking. But, you know, something that has a beautiful API that we can really easily bring into our software development workflows, like this language tool.

I don't know. It seems pretty powerful to me. I don't know if there's I'd be curious to see if in Versus Code there's any extensions that, you know, are sort of higher level, more powerful grammar checkers than just sort of, you know, the spell check that you get out of the box, you know, that look something more like this language tool API or like Grammarly. Not a 100% sure there, but but it might be interesting to check out. But regardless, I think this is a a phenomenal solution. In the meantime, it looks like there's about 80 different languages that are supported in this Rspell package, and, I I really don't see how this can't be a benefit for everyone out there who's authoring any sort of scientific, report documentation.



[00:20:24] Eric Nantz:

Yeah. I I wholeheartedly agree. And I was looking under the hood a little bit at this, at this package, and, this is another one of the newer ones that is leveraging the h t t r 2 under the hood that recently had its 1 dot 0 production release. So if you're wanting to see another example in the community that's leveraging, you know, Hadley's new HTTR 2 package, this will be a great one to take a look at for inspiration. And I think, again, my my biggest win here is the democratization, if you will, of being able to leverage this wherever your development is happening. I think that's gonna be a huge help for everybody.

And, yeah, I'd imagine in the world of Versus Code, there's basically an Accenture or just about anything these days. But this means that you get this power of powerful, grammar checks wherever you are. And I'd imagine that as over time this package is more widely used, perhaps there will even be more features under the hood. So congrats again, Rafael, for this excellent release. You know, whenever you get Mike and I together on any show or podcast, event of where Shiny will enter the picture here in respect to our application development, our interfaces that we're trying to make a little more robust with some nice aesthetic stylings in HTML and CSS and whatnot.

And one of the areas that in my day to day life is shiny in the early days that I definitely have one of those kind of love hate relationship with was the grid system. How many times did I try to get the column width right between these rows and columns that are just scattered across my dashboard? Now things are looking better in the Shiny space, of course, with the advent of Versus Lib and whatnot. And sometimes you might need to still get a little lower level with some of your CSS styling. But our last highlight today is coming from the esteemed Albert Rapp on how you can take advantage of now a very popular technique in the world of web development and styling, in particular, the Flexbox, capability within CSS.

So, Mike, is Flexbox gonna gonna take me away from my column with woes, you think?

[00:22:47] Mike Thomas:

I think it will, Eric. And I think it's already probably being leveraged pretty heavily in packages like bslib that are are trying to alleviate some of the alignment issues or hurdles that we had to overcome in the past. You know, I I remember fondly with that that grid system, especially in Shiny, you know, doing things with columns and a fluid row to try to align things that probably should have been aligned, like in a div with some CSS manually because it wasn't perfect. And I think I think Albert is walking us through sort of best practices around alignment, you know, using this this new sort of Flexbox framework. And he has this weather app, which looks like a, you know, phenomenal mobile application. It looks like something straight out of my iPhone, that he has been developing. I think just mostly sort of a UI for, and and showcasing, you know, different aspects of of UI development and HTML and CSS to to bring sort of best practices to the design of this weather app. So, you know, we're walking through a use case here where he's trying to accomplish a few things. The first one is at the top of the app, UI, there are sort of 4 icon elements. One is a time stamp for for what the current time is that you you might can imagine see on your on your iPhone.

Then he has a, icon that represents sort of your your service, your your vertical bars, you know, can you hear me now? And then there is a WiFi logo, and then finally followed by a battery logo. And when the way that he puts these together is actually in a nested div, where at the top level of the div is the time, and then there's a a a div, you know, the nested portion that contains those 3 other icons, your your signal, your WiFi, and your battery status. And when he initially tries to do this this nested div without the flexbox, what happens is the time stamp is above the other 3 icons below it. And he really wants to align those, really nicely horizontally.

So the way that he's able to do that is in that the style argument of this main div, he has two arguments to the CSS function. The first is is a margin function, which, you know, doesn't have any impact, essentially, here. But the the second argument here is display equals flex. And that is how you implement Flexbox, you know, instead of grid, and that is all that was necessary here to align these necessary here to align these four elements, horizontally instead of, you know, part of the div being stacked on top of the rest of the div. So that was a really clever, sort of simple implementation and and really powerful to see how that plays out.

The second thing that he wants to do is he actually wants to, have the timestamp be on the left side of the screen, and then the other three icons be on the right side of the screen, be pulled to the right side of the screen, instead of them all aligned, you know, one right after the other. He wants this large space in the middle of the screen, between these icons. And the way that he's able to do that is is, again, you know, just essentially adding, this argument to the CSS function that that ends up, you know, being supplied to the style argument of the main div.

And this additional argument is called justify_content and that equals, in quotes, space hyphen between. So that's how he creates space between the top level of that div, which is just the time space, and then the the nested portion, the sub div, which are those 3 different icons. And it's it's beautiful the way that it plays out. It it creates the perfect amount of space, such that the timestamp is pulled to the left side of the screen and the other three icons are pulled to the right side of the screen, but they are all, again, on the same horizontal plane, which is fantastic. And the last, thing that that Albert does here, Eric, is centering this big icon that's that's below, these these top of your screen icons. This big cloud icon that's right in the middle of the screen, which I I believe, you know, sorta tells you what the weather is today. So I don't know if you wanna take a stab, Eric, at at, Albert's approach to centering that icon with some margin elements.



[00:27:01] Eric Nantz:

Yeah. And, first, it it gets wonky in the beginning here. Right? Because when you start just doing your your grouping with the div, which again is a I cannot undersell the technique of if you just think of these divs as, like, these virtual boxes of stuff and then be able to do whatever the heck you want with that and then have it only affect that particular, you know, grouping versus the rest of your app, that's just such a a game changing technique if you're new to this world of web styling. It took me a long time to grasp, but now in my apps, I'm dipping all sorts of things so that I get that flexibility.

But, yes, going back to this example when he first implements this grouping, and then you see that everything is all jumbled on this canvas. And it's not even fitting in this very nice styled cell phone like, you know, rec you know, vertical rectangle background. Been there. How the heck how the heck do we get this so that instead of spreading it out, like, on the column like fashion, it's spread into a row like fashion? And that's where in the Flexbox nomenclature, there is yet another argument or parameter called flex direction where now you can say, you know what? I want this text, all of or all these elements representing both the text and the image to be stacked on each other in this column wise fashion and not splat across this entire row by default.

So wave forty decides to use the flex direction, set it to the value of column. And now look at that. It's looking much better. Now you see the text and the symbol embedded into the cell phone background. And now, yeah, maybe it's still a little not quite there yet because you want to do some additional moving around. And that's where the margin arguments come in where you can define, you know, if it's intelligent enough to space it on, like, the margin left and margin right being auto. So it kinda, based on your screen resolution, will fit it appropriately into that container. And sure enough, once you do that, now you're really seeing a very fantastic looking layout already.

The city name at the and the quote unquote the top row. You got the degrees and, and the weather status on the next row. And then the actual icon indicating the weather. Front and center, literally center with a good size. And then there's placeholders from additional metadata, but they're nicely justified in the lower left. So really, really top notch styling here. Again, the key nuggets here are just taking advantage of Flexbox and the various parameters that you have at your play here. And one quick tip, it's been mentioned in his previous blog post, and other people have mentioned this as well.

It's one thing to run this, you know, like recompile your app, if you will, see the layout on your, you know, browser window and kind of rinse and repeat, right, like we typically do with programming. Remember, with web development and that fancy schmancy browser you're running in, you've got those developer tools. Right? And then you can do the inspect capability and drill down to that very CSS that's outlining that but to your element that you select with your cursor. And then in real time, you could change like the margin was. You could change the flex directions, and you'll see that happen in real time. And that can be a nice time saver if you know you're gonna need to experiment a lot with this and you just wanna quickly see the impact of about rerunning your Shiny app, it's another technique that took me years to master between what Albert mentioned in his previous post and what, our good friend, David Grandgen, has taught me with his various web development. In fact, Mike, you and I were TA ing his posit workshop last year, learning about the ins and outs of Shiny UI design.

Those nuggets, they add up to really minimizing your iteration time or really getting you to the point you need to be at. So looks like there's more to come in this space for his next, installment when he wants to, you know, make things even more polished here. It looks like that's coming soon. But in any event, if he had told me this was a professional weather based app that was in its early stages, I would say, yep. It sure is. How much is it? Nope. It's all free of charge, folks. It's all part of learning here. So congrats, Albert. A fantastic post as always, and we look forward to the next stage of this.



[00:31:41] Mike Thomas:

Absolutely. No. I remember my first, foray into looking, popping open that developer tools pain and wondering what the heck have I gotten myself into. Why would I ever want to even, you know, attempt to mess with what's going on in here because it all looked so foreign to me but now I am grateful for whoever created that developer tools, you know, concept and technology because it allows us to iterate so much faster across the styling of our apps.

[00:32:10] Eric Nantz:

Yeah. And it's easy to get overwhelmed when you look at that at the first glance because there are so many different tabs of functionality or respect to even just seeing what are the actual requests that the page is generating and that almost like a like a Systrace log on Linux. It looks very, you know, overwhelming. But then when you get to the meat of what you want, which typically for me is either, like I said, the CSS tweaking or the JavaScript console, and I wanna look at inspecting some variable values and whatnot. Those are the 2 I'm in right now, but I know there's so much more in that in that council space.

I would be remiss to not offer up my little conflict and my inner geek, if you will. You know, I I ragged on the grid system as kind of being a bit painful over the years. But when I think of grid, I think of my favorite movie Tron. They were in the grid, man. They were in the grid. Flexbox just doesn't quite have that same ring to it. So I I'm gonna have to reconcile that in my head sometime.

[00:33:07] Mike Thomas:

Favorite movie?

[00:33:08] Eric Nantz:

Tron. It's my favorite. Memorized it. Good to know. Yeah. Yeah. That's a that's a way to get Greek cred with me. Anybody that likes Tron as much as me is as a friend already. So but, maybe Flexbox will grow on me in that sense, but we're not quite there yet. But in you know what's there, Mike. We got a fantastic rest of this issue of our week that Patul has curated for us, and we'll take a couple of minutes to talk about our additional finds here. And, yeah, I kind of alluded to my, you know, my, tech and gaming roots here. And so this post really caught my eye from Rasmus Bath. He has a post on modeling his pinball scores.

Those of you that rep the arcades back in the old days, remember those awesome pinball machines that you would sink your quarters into. Highly addictive and the bling you would get by hitting all those sensors and seeing the score rack up. There's nothing like that that rush, you would say. And, apparently, what Rad Rasmuth has done here is he had downloaded a virtual version of a pinball game called fishtails, I believe. And he tabulated all of his scores that he got from that virtual game and has made a nice little jiggy pot out of it. And, yeah, it definitely looks like a somewhat random distribution, but the fact that you could do a data driven analysis on that, yeah, huge win in my book. Took me back to the days when, myself and a few of my winter skis friends were doing a virtual racing league, and we had to do screenshots of the race times to collect our data. But 2 of us did it, and we had tons of fun doing it. So awesome post from Rasmus, and, yeah, now I got the use to play pinball again.



[00:34:50] Mike Thomas:

That's really cool. I found a, incredible set of slides, authored by Michael Friendly for his PSYCH 6135 course on visualizing uncertainty. And just fantastic resource for doing exactly that in your statistical, analysis and journey. You know, I think it's really important to to not just communicate point estimates, but to communicate, the uncertainty that you do have around those point estimates, so people can make, you know, better risk based decisions. It's a great walk through of the functionality of packages like ggdist and tidybayes.

And, it's just a phenomenal resource, one that I am definitely going to to save, in my notes for the future because it's spelled out really nicely for us. So definitely wanna highlight that as well.

[00:35:42] Eric Nantz:

Yeah. I love seeing these these awesome visualizations in this space because in my line of work, Uncertainty is not just a buzzword. We really have to account for and then be very clear to our stakeholders, our leadership, when we're looking at these different, say, study designs, just the uncertainty we might have with that particular outcome, that particular time point. You know, we've got to make sure we're transparent about it. So anything we do to communicate that uncertainty in a more concise way is a huge win in our book. So really great great post by Michael Friendly there, set of slides. I'll be definitely checking that out. And we want you to check out the rest of the rweekly issue. Like I said, a ton of amazing content here.

So much to choose from, from the world of package development, you know, data science and the industry, academic research, calls to action. It's all right there. And everything is available at rweekly.org. Everything is there. All the back issues are there. And as well as hearing from you because one of the easiest way to help with our weekly is to send us a poll request of your favorite resource, new package, new blog post. We love to see it. We'll have it featured in the next issue. So it's just a poll request away.

If you can write markdown, you can contribute to our weekly. It's that simple. Very easy to get set up with. And, also, we love hearing from you in the audience as well. Got a little contact page directly linked in this episode show notes on your favorite podcast player of choice. Speaking of those podcast players, if you have one of those modern ones out there like Podverse, Fountain, Cast O Matic, CurioCaster, I could go on and on. There's a Honeywell boost functionality you could have to send your hosts, yours yours truly, and Mike here. A fun little boost to read along in the show if you wanna send us a message. So all right there. And, also, we're on the social media sporadically.

I am on Mastodon mostly these days. So that our podcast at podcast index dot social. I'm also on the weapon x thing, that the r cast, and I'm on LinkedIn from time to time as well. And, ironically, you this will have happened since I've recorded this. But literally in a couple hours after finishing this recording, I'll be taking a part in the, latest Epsilon Shiny gatherings to help do little,

[00:37:56] Mike Thomas:

fun little, teasers for the upcoming shiny comps. So I'm really looking forward to that. And, Mike, where can they find you? I will be there too as well, Eric. I'm excited for, that gathering. Yeah. And folks can find me on LinkedIn. If you search Catchbrook Analytics, you can see what I'm up to. Ketchb r o o k. You can also find me on mastodon@mike_thomas@phostodon.org. And one last, callback to Michael Friendly. His GitHub handle is just Friendly. Github.com/ friendly. That's awesome.

[00:38:29] Eric Nantz:

Sure is. You got it at the right time. That's easy. Yeah. I wish I'd been more, well, at least I got our podcast eventually. I just had a fight for it. So one of those things. One of those things. First of all, the problem is the social handles. But, you know, it's never a problem to listen to our weekly. We hope you enjoyed it. Again, we love hearing from you, so don't hesitate to give us your feedback. And also we hope to see you for episode 159 of this very podcast next week."
"61","issue_2024_w_12_highlights",2024-03-20,46M 44S,"An honest take on common patterns and anti-patterns for re-use of data analyses that hit a bit too close to home for your hosts, a cautionary tale of garbage online references pretending to be authentic material, and a new (human-created) cheat sheet with terrific best practices taking front and center. Episode Links This week's curator: Sam Parmar…","[00:00:03] Eric Nantz:

Hello, friends. We're back of episode 157 of the R Weekly Highlights podcast. My name is Eric Nantz, and I'm so delighted you joined us from wherever you are around the world for our weekly show where we talk about the latest highlights that you can see on this week's Our Weekly Issue. And as always, I am joined at the hip here. My line mate in Our Weekly Fund is my co host, Mike Thomas. Mike, how are you doing today?

[00:00:27] Mike Thomas:

I'm doing well, Eric. It was starting to warm up here on the East Coast. Now we were getting a a cold week. So it's it's a little frustrating, but I think that maybe the theme of this week's highlights. We are venting this week in some of these highlights and I am here for it.

[00:00:42] Eric Nantz:

Oh, as am I. And and even in the preshow, that all you can listen to, you heard about Mike here heard about my events on some recent rabbit holes that I went under. But, yeah, we're gonna we're gonna have a lot to share today because we do, feel very relatable to a lot of the concepts we're about to talk about here. And how is this issue possible? Well, our curator this week was Sam Parmer, another good friend of mine from the life sciences industry. He has put together a terrific issue we're gonna talk about here. And as always, he had tremendous help from our fellow Rwicky team members and contributors like all of you around the world with your awesome pro requests and other suggestions.

So, yeah, let's get the, quote, unquote, venting session going, and we're gonna go fast with this because Miles McBain here, Mike, has some very insightful, tidbits to share, which I can tell have been gleaned from a lot of experience in data science called the patterns and antipatterns of data analysis reuse. So what do we have here? Why don't you set this up for us? This is too relatable. It's a phenomenal blog post

[00:01:52] Mike Thomas:

talking about, you know, the data analyst curse and, you know, understanding that, you know, every data analysis and data scientist role that Miles has been in and I agree with this as well. At some point in time, you're redoing variations of the same analysis. And there there's 2 assumptions that he's making, for those who would be able to to relate to this blog post. The the first one is that your work is is written in code like R, Python, Julia, or Rust. If you're using if you're using, in his words, Power BI or God forbid Excel, you probably won't relate to this. And then second, you're using a technology like Quarto, Rmarkdown, or Shiny, such that sort of your end deliverable is generated from that that code. So if if this sounds like you, I am assuming that maybe you'll be able to relate to this blog post as I did.

And when you're redoing, you know, that same analysis, in sort of different ways, one of the first things maybe that you may start out doing as a as a beginning developer is copying and pasting. Right? From from one, version of your report to the next version of your report that you need to create. And, you know, this can be a quick solution that you have, but it may not be very extensible or maintainable because when it comes time to update some sort of global, you know, version of this analysis, you would have to copy and paste to each different version that you have out there.

And when you're trying to fix a bug or or create an enhancement, that same concept would apply that instead of just doing that update in one specific place, you would need to copy and paste that to all of the different places because you don't have something like a template. And this is where you can start to move on towards oh, I'm going to create a sort of a single template that is going to have, parameters in it that I can set that will, be able to allow me to run different versions of this analysis just based upon, the different parameters that I'm passing to this.

And in theory, this is this is great. And I think this is exactly what we all strive for. But if you have done this long enough, if you have sort of been in this world long enough, you'll start to see that as you create this parameterized global version, with each new variation of your analysis that someone's asking you to run, or each new dataset that's coming in, there's going to be a new edge case that you're going to have to handle. And what that means is probably an additional parameter and if you're like me, this can get into now you're starting to write like conditional if statements, to test and see, you know, if this particular variation matches this one very, very specific edge case. And now your your global template is just getting super bloated because, it's trying to handle all of these different particular cases. And then at some point, someone says, hey. Why don't we, why don't we manage those parameters with with YAML or JSON? Because, for sure. Right? Now we're talking about configuration.

And, you know, then you have a a YAML or or a JSON file that is supposed to to manage and handle these parameters. And maybe it starts out small because there's only a few global parameters, there. But, again, as you introduce these different versions of this analysis or somebody asks for this new thing or this new dataset comes in, you know, it wants to look at your analysis at a different angle, you're just adding additional parameters. And now all of a sudden your YAML file starts to get pretty pretty long.

And then maybe at some point, you are having a second YAML file to manage the configuration of of the first YAML file, and it's it's YAML all the way down. And, it things just start to be unwieldy, and you start to think, hey, maybe I just need to go back to cut and paste. Copy and paste. And it can drive you a little insane as I would say, Miles may have gotten to in the, the the paragraph here titled Power Overwhelming, which I think is is where we start to, sort of go off off the rails and it I mean that in the nicest way because this is incredibly incredibly relatable. So then, Eric, if you wanna take it away to talk about maybe we start to move towards a a package framework.



[00:06:31] Eric Nantz:

Right. And first, yes, we all can very much relate to this because the entire spectrum of that that build up up to this point, I have seen with my own eyes. I have committed some of this in my own hands, if you will. And, yes, sometimes the only way we learn is through painful experiences. I have had some extremely sophisticated templates in our markdown before that had a boatload of params in the end. And sometimes I would be shared with different teams, and then they realize, oh, yep. That particular dataset for that study has this type of efficacy variable, and I didn't cover it. So it just keeps adding on, adding on, adding on until it's a point where no one knows where the central place is for that thing. And everybody copy pasted to a different study. Some of this is ironically still happening. We are trying to put the reins on it. But yes.

And when you get to this point, you think about what are ways that I can make it easier for us to maintain some kind of structure to this, still make it easy for the end user to implement in their analysis pipelines, but still be able to tap into some of the modern practices to help maintain this reusable code. And, yes, spoiler alert, that does mean creating an internal package, and that may be intimidating to many people. Well but the thing is, I would say, once you've been through these hardships, you're at a point to appreciate the upfront work to build a package, maybe more so than if we just told you this if you're brand new to data science in your particular industry or particular group.

Then you're gonna be ready to absorb some great resources out there already, especially in the our ecosystem to get a package off the ground. Why should you do this, though? Is that that way instead of having these template variables and these massive templates, you can have functions with function parameters that cover much of this much of this, operation and functionality. And you don't have to have it perfect the first time. Maybe you just automate certain parts of it and just kinda build on it over time. But having the package is gonna let you opt into additional best practices to get ready for cases where maybe your package analysis functions are being used in cases you didn't anticipate.

But you can build in things like automated testing. You can build in documentation on these parameters so that you can use the wonderful tools like use this, test that, dev tools to help make this package more robust in the R ecosystem. And, of course, Python fans, you have similar frameworks on that side as well. But just getting to that package step is a huge first step to start in writing some of the wrongs that you may have experienced in your respective effort. Now, like anything, there's some there's some gotchas to worry about.

And another issue that I've seen firsthand and I've seen very talented people do this firsthand is that you started this great analysis package for your group. Maybe you called it the name of your group. Who knows? And then over time, either you or others say, hey. You know what? What if this package did this new thing? What if this package did do this new thing? Suddenly, your your catalog of functions going into this internal package starts to balloon up. And maybe it gets to the point where you have so many functions, and some of them just don't really relate to each other. But because it felt like such hard work to get that package off the ground, everybody just wants to put it in one place. So they only have to load 1 package, and then it's all there.

But you're running into the risk, as Miles points out, of complexity overload and a lot of bloat. And especially if you need to make a change or deprecate something in that package, suddenly, the whole package is being updated in ways that maybe you didn't anticipate. And so that's where going through this exercise, yes, getting to the package is a great first step. But there are a lot of diminishing returns if you decide to put everything but the kitchen sink, so to speak, in this one internal package. I've also seen this in a capability tool that we used to author for helping design clinical trials.

We had a monolithic, and I do mean monolithic application that was meant to do everything for our clinical program, and we just could not maintain it anymore. There was just so much in frameworks that honestly half of us didn't even understand, but that it was all in one monolithic code base. It just at some point, the technical debt became too much. What did we realize and what Miles transitioned to in this piece is that instead of having this single package, try making your own internal, like, group of packages.

They call he calls it the personal Versa packages. Of course, we're familiar within the R ecosystem, the tidyverse and other groups of related packages that may make some decisions, may have common, you might say, data structures that they operate on, but they've separated their purposes. They separated their concerns into fit for purpose packages. This way, instead of having to update this monolithic piece with maybe that one little change, now you have a set of packages. They all contribute to a greater whole as they say, but now you can write updates to these in fit for purpose fashion.

So he mentions in his, examples here in his current job, he's got a fun a package called check yourself, definitely before you wreck yourself. I'm just saying, to help you look some quality checks, you know, for your dataset. Great. First step in a data analytical pipeline that makes a lot of sense. And then because Miles is a huge fan of reproducible analytical pipelines, they have a package on top of targets called t d TDC targets. And that's helping them build these pipelines in a unified way. It's still leveraging targets on the back end, but they're helping bootstrap that a bit easier. But, see, he separated out the data checking and the analytical pipeline building into this two sets of packages.

There may be others that deal with internal APIs. I'm living that world right now. Do I wanna put all API calls in 1, like, company package? Oh, heck no. We wanna separate that out into its own fit for purpose thing because, spoiler alert for me, testing APIs is a much more wieldy effort than testing normal r functions. So why do you wanna boatload why do you wanna put a monolithic package to do all of that? You wanna separate that out as best you can. You're getting flexibility, but it is gonna take discipline to get there.

So I do think that it's not gonna be easy to do this all right away. You've gotta start somewhere. But, honestly, the first step is recognizing when you have a problem. Because sometimes you may send these great, like, end products of, like, these templates or these monolithic templates or monolithic packages. And everybody in leadership thinks, oh, you're doing great. Yeah. This is helping the company so much. But what are you standing on? Is that foundation solid? You really gotta pay attention to that because it's one thing to get the short term win by doing the copy paste, you know, method, but it's gonna fall down on you at some point.

And, honestly, nobody like I said, nobody gets this perfect the first time, and we are all continuing learning on this. And that's where the post concludes where you're gonna have these humble beginnings. Right? But then you're really honing your craft as you go along. You all don't wanna see my first internal package I did at the company. It took a lot of shortcuts that I'm not proud of. But getting there was a huge first step for me. And then as I learned from the communities, I learned from my, my teammates, learning from you know, I'm I'm very privileged to learn from Will Landau, basically, every time I talk to him about something new. So all these things just build upon it. You're you're going to get even more comfortable with this.

And, obviously, time is another factor. Time is not infinite. We have to prioritize this. But, honestly, I'm of the belief if you take the time up front to set this up the right way, even if you don't quite know how you're gonna get to that end goal yet, but you know that there are some best practices you wanna start with, that is gonna be a huge help to minimize the technical debt that Miles is definitely outlining here if you go with that quote, unquote easy approach to do this at first. So all in all, I think the the biggest piece of advice is when you see you're doing your copy pasting a bit too much, stop, pause, try to think about what are ways that we can make this reusable and more importantly, maintainable in your team.

I really resonate with many things in this and credit to Miles for putting this in such a, you know, comprehensive yet, you know, very much an evolutionary type story of what data analysis pipelines are all about. And definitely start, like I said, start small. But once you start small and do fit for purpose, you know, I think I think you're gonna be on the right foot. So

[00:16:20] Mike Thomas:

definitely spoken to for experience. I can tell with his insights here. So, yeah. Really excellent post and I think, Mike, you and I have been through been through this quite a few times in our internal adventures. Right? No. It's a little too relatable and there's, you know, there's a lot of things to to try to balance here. Right? As you move from a script to to function to a package, and then and then back and forth sort of depending on your use case. If I may just read a very small excerpt that I think is is worth reading. You know, he he talks about, you know, creating a massive function that gets written with with maybe a dozen arguments, you know, that has hundreds of lines of code that's it's not really much different than just a wrapper around some sort of data analysis script that you would have. And it's great that you're using functions, but you're actually, like, attempting to to template your entire solution using the functions arguments.

And he has a little little footnote in here that says, if a function starts taking YAML configuration files as arguments, you are on borrowed time. And and the last paragraph I wanna read, this is if Shakespeare was a data scientist, he would have written this. Such a function is it's pretty much untestable due to the combinatoric explosion of possible sets of inputs, and you can bet that the internals are not written in high level domain specific code. When I look at that function signature, I hear the screams of matrices being ground to a fine powder of bits through the conical burrs of nested for loops and labored index arithmetic. I mean, it's it's it's incredible. It's poetic.



[00:17:52] Eric Nantz:

And it's it's most definitely real. Right? Yep. I have seen this. The approach I've seen is, hey. You know what? This package, we're just gonna have to use our modem, custom CSV with all the params inside. Like, oh my gosh. No. Please stop the pain. Yes. But it it it is the the the the crutch that people will fall back on is, you know what? It it's a lot of work to put all those as function parameters, and I don't wanna test that. Just prune the config. It's all about the user configs. No. It's not about the user configs. What it's about is building an actual package that has actual documentation and actual testing.

Yes. I am firm on that because when you don't do that, you may not pay for it right away, but somebody's gonna pay for it. And it'll most likely be your end users, and that's about the worst result of all.

[00:18:49] Mike Thomas:

I agree. No. We have a client who leverages a third party API that in order to send your params to that API, you send your data and then you send this this wild like ASCII file, a text file that just you know has has no the limit, you know, elimination or or whatever you wanna call it. You just have to like add an n or or a y depending on which, which things you want to receive back from the API and then you have to zip it all up and send a zip zip file. It's it's pretty wild but let's let's vent about something else, Eric.



[00:19:29] Eric Nantz:

Yeah. I think we need to now think about you know what? We've we've thrown a lot of knowledge your way, and we acknowledge that it's not always easy for you to know everything at once. Right? We're all continuing learning. And, of course, if we don't have the answer to a question, we're probably gonna ask for help in certain ways, especially online. Right? I mean, how many times have I googled for how to do something esoteric with, like, an r to call this, you know, API parameter or whatnot or do this new statistical function that I'm just not as familiar with? So like anybody else, Mike, I know you and I have searched, you know, the Internet's and webs quite a bit for a certain help here and there.

And, you know, with the advent of r over the years, you're starting to see a lot more results there, Taylor, with r itself, certainly post on Stack Overflow and things like that. And then you'll start to see in these search results, you know, some things that look interesting but don't quite look right. And what is this trend we're talking about here? Well, our next highlight is coming from Matt Dre, another previous contributor to R Weekly. He, he's noticed this trend too, and we'll just kinda take this bit for a bit because I think those of you that have used R for a bit and have been searching for your various help or tutorials on there have probably seen this because you will often see these websites that are now coming up higher in the rankings that look almost too good to be true, but they kind of are too good to be true because these are sites that are coming up that have clearly you can kind of tell whether it's said explicitly or kind of implicitly, they're being written by some kind of bot, maybe some kind of AI framework.

We don't want to send traffic there. So, like, he, Matt, or I are not going to tell you the names of these sites, but they are very easy to kinda hook you in to to acting like these are authenticated, you know, very authoritative sources, I should say. And sometimes, you'll see that the same overall site is producing, like, thousands of these guides for each package individually. But the guides are not coming from the package authors. They're not coming from people in the community that we've, you know, seen or or, like, an authentic blog post.

They're these, like, AI generated, you know, narratives that somehow have some great search engine optimization built in so that they're showing near at the top of your search results. But, clearly, they're not they're not playing the right way here, and I think it kinda stinks. So the I think that's pretty clear to me why this is a bad thing for to see happen, but it's also reality that we all need to deal with. It's not like a quite one to one example here, but, of course, most of us have cell phones and we get these robocalls left and right on our cell phones. And we know they're bogus even though they try to act like they're coming from our area code. Right? But these these are, like, some real bogus results that are trying to show, quote, unquote, a guy to use a certain package.

It's not. It's not. But, yeah, Matt continues in his post about just why, in his opinion, this is a bad thing that this keeps happening. Mike, why don't you take us through why? What are some of the downsides of what we're seeing here? Yeah. You know, I think what's gonna happen here for for the most part is that,

[00:23:05] Mike Thomas:

some of these summarizations that that take place, when it's a bot or, you know, AI, whatever you want, whatever that means, is is sort of summarizing and and trying to scrape the web and put together, these these are, you know, help sites. You know, I think what's gonna happen here is just a lot of the specificity is is going to get washed out. And, not only is the content terrible, because it's not written by a human for humans, but the ethics are pretty bad as well. They're really just trying to, either make money off of you somehow, you know, redirect you to some affiliate site, you know. And you also have to remember that, in a way, they're sort of stealing their content, you know.

A lot of the the content on the web that people are are putting out there in terms of, you know, Stack Overflow help and, you know, stuff that's actually authored by by someone. And, we went down the rabbit hole a little bit, when we started talking about Copilot, is a lot of this content does not necessarily, you know, have, like, a Creative Commons license behind it that says, hey, you know, go for it and and scrape this and use it however you want. You know, it's it's probably scraping stuff that the author may have not given them consent to to scrape. So that's that's crappy.

You know, the fact that it's moving up, in terms of SEO, because, you know, I guess that's probably something that AI is is fairly good at as well. Right? To try to make this site look like a site that gets a lot of clicks. You know, that's, I guess, the name of the game these days, unfortunately. And I'm seeing it myself, you know, I'm having a harder time getting to Stack Overflow links, which are have traditionally been really the thing that has helped me the most, has helped get me to my answer the quickest. And usually, maybe I have to sort through, 2 or 3 or 4 different Stack Overflow links to to find my exact solution. But in the past, that would come up those links would come up very very high, you know, if not the first result, you know, the second result.

And and now, they're much sparser, unfortunately. And I'm I am having to sort, like Matt, through a lot of this this crap, unfortunately, to so it stinks, you know. It's, and that is taking sort of a pessimistic view here, which which maybe I share. I don't know. I haven't put I hadn't put too too much thought into this until Matt's blog post, but, don't you know, we don't see really an avenue where this gets better as opposed to to gets worse. I don't know, Eric.

[00:25:51] Eric Nantz:

Yeah. I I think it's a reality, like I said, even my robocall example. It's like, no matter how many you block, there's always others that are gonna keep coming. And I think there's gonna be these sites that crop up with now AI and automation becoming so much easier for the masses or in the case of some of these, you know, non unethical corporations or whoever's behind some of these to just launch all these automated processes on some server somewhere and do the SEO gaming up of of of search results.

I think the biggest thing we can recommend is to never to to have, like, a a careful eye as you're searching these results. And I think over time, you'll see these patterns such as Matt has been talking about here in this post. But I'm gonna say the best places to draw upon for, you know, you know, help for, say, a package itself is hopefully the package documentation itself. Most of them now have package down sites. They usually have a GitHub repository or GitHub like repository and, you know, seeing what issues have been talked about for that particular package on their issue board. Like, that's a great way to learn even just by scraping through that.

And also leveraging community based built resources that you know are being built by humans. And guess what? Another spoiler. We're our weekly is built by humans. Right? We are linking to content that has been created by package authors, by data scientists, by others in the field that you that are authentic. And that's why we have a curator to always sift through. We get noise too just as much as anyone else, but we wipe the heck out of those. We make sure those don't get into our issue. Unfortunately, for search engines, we don't have that control. Right? They're just always gonna kinda pop up from time to time.

I think if, if it sounds too good to be true, it probably is, so to speak. So definitely have a careful eye to that, especially those, as Matt points out, will have some random affiliate links somewhere in maybe the footer of the site or it's a sidebar or whatnot. No. I don't see that for authentic R based content or data science content in my day. So I think it's more about with experience. You're gonna be able to see this more quickly. But we I think what Matt does here is at least bringing awareness to the issue that this is real. It's probably not slowing down.

And so just making sure that you are, you know, looking at the authentic community based or, in some cases, the developer authored resources to really get you in the right direction for your particular issue. But, Matt, I see at the end of the post, yeah, you also grew up in the times I grew up with. Good old floppy disk. Right? We didn't have fancy AI bots generating these queries. We had to make sure that 5.25 floppy disk somehow worked in our IBM, you know, PCs or Apple 2 GS's. Shout out to all those that use vintage Apple computers.

So it's a different time now. And that I think with that just comes some some new skills that we have to learn about finding the the best from the noise as they say.

[00:29:01] Mike Thomas:

Yep. No. I remember, you wanted to know something. You looked it up in an encyclopedia. So I very much yeah. At the library no less. Yeah. At the library no less. I think we had we had an encyclopedia on a CD ROM at home or something like that when I was growing up. That's right. Yep. No. Times have certainly changed, and I think, you know, unfortunately, that means navigating navigating the Internet and search results, you know, requires new skills to be able to do so. But it's unfortunate that some of these sites out there, exist because it sort of feels like cheating.



[00:29:49] Eric Nantz:

Now with that said, of course, what are ways that you can kinda get your your journey of data science started off right, especially if you're new and you wanna turn to maybe a human generated resource to help you. Well, one thing that has helped me over the years, and I think many others would agree, is the concept of having that handy cheat sheet next to you. So if you're looking up stuff all the time but you just want a quick reminder of how something works, you know, cheat sheets are a great way to have at your desk or at your virtual wallpaper or whatever have you to kinda get those concepts reinforced, from time to time. And our last highlight is, actually a new cheat sheet in the R community authored by Jacob Scott who is a data scientist based in the UK.

And Jacob has put together this best practice for our cheat sheet, and he is very much upfront in his repository that this is highly opinionated in some of his preferred workflows. But I think there are some concepts here that we can very much relate to, especially in the context of if you are following the advice of what Miles authored and our first highlight, some ways you can get started pretty quickly. One thing that whether you're running our studio proper or not, but having some kind of project structure is it's one of those things that you take for granted. But boy, oh, boy, I have seen countless times people, like, throw all their r scripts in one directory that have no real relation to each other. Just throw it all in there. Right?

Have you ever done that, Mike?

[00:31:27] Mike Thomas:

I've seen it. Maybe I did it when I was starting out potentially, when I didn't necessarily know best practices around, you know, what an R project even was. You know, like I said, when I was taught R in undergrad, we were only taught R markdown. I didn't even know what an R script was. I I only knew the existence of of dotrmd files. So there's there's a possibility that at some point in my journey, which I don't wanna, maybe dive back into that that I would have been guilty of that, but I am very happy to have, found it and understood our projects.



[00:32:04] Eric Nantz:

Yes. And and, yeah, certainly the examples he's talking about here, they are specific to RStudio, but you can do this in any of our typical ID as well. I mean, Versus Code has workspaces you can utilize. And, of course, you know, there are loads of extensions in the classical frameworks that people turn into IDEs like Vim or Emax that do similar things too. The idea is just logical grouping of your code. And he's got a nice little snippet here in the cheat sheet about what his project structure looks like. It's got, you know, subfolders for the scripts itself. It's got, you know, potential database query, SQL scripts. You know, all and, of course, he's using RM too. That's an even another best practice that I think goes it needs to be reinforced is that these projects can have wildly different dependency requirements. And in the r side of things, having RM is a real bulletproof way, give or take a few gotchas here and there, making sure that you can reproduce that R base execution environment within reason, from project to project and be able to have that finer tuned, that finer tuned control for your dependencies.

And then also, there's some great sections in here about how to create a repreqs, which, again, we were talking about getting help. Right? Finding ways to effectively communicate and effectively search. Well, if you know that you're having an issue with a said package or or another, you know, utility, the best way to to get help from the community, whether it's in Stack Overflow or, say, PASA community or whatnot, is having a reprec so that it shows in a very concise manner what exact error you're getting and let others reproduce that error. Reprec is not this is not the first time we mentioned reprec on this highlights podcast over the years.

So I think having that skill set is a great way to put yourself in best position to not only ask for help, but then to receive it as well. And then there are also some great sections here about how do you connect the databases. And I've used the DBI package quite a bit. He's got a little snippet about connecting to, a a database with that as well. And then others such as styling. Again, there can be different takes on how you style your code. What, Jacob recommends here, I believe, is a variation of the tidyverse style guide with certain pieces. But, again, I think the key, as we mentioned maybe a few weeks ago, is just consistency.

Once you have consistent styling, no matter if you use the tidyverse framework or, let's say, Google's framework or any your own company's framework, having consistency is gonna help you as well as your future self and collaborators on those projects. And then it concludes with, some links to learning more about the R community and building R for, you know, our projects such as r for data science, building packages of r packages, and really getting into the nuts and bolts of r, where the Vance r. And, yes, for the shiny fans out there, a link to mastering shiny as well. Highly recommended.

But and I think it's a it's a great way to get started. I think for those new to the R framework, this is one of those great examples to get you started off the right way, and it's gonna wet your appetite, so to speak, to dive into some of these concepts in more detail. So really nice nice job here, Jacob, and I think it's a it belongs in many, collections of cheat sheets out there.

[00:35:36] Mike Thomas:

Yeah. This is absolutely beautiful, design wise, the way that he he drew this up. It says that, he originally created a similar version of this this cheat sheet specifically for use in the UK Department of Education, but he's created this more generalized version, I think for for everybody else, which is fantastic. And it it sort of makes me think about perhaps, you know, you may wanna create a cheat sheet similar to this within your own organization that, you know, mentions and outlines some of the specific best practices that you wanna follow and employ within your own organization. So if you're looking for inspiration to do something like that, this would be a great place to start.



[00:36:16] Eric Nantz:

Yeah. And he does have link in the GitHub repo to additional, additional cheat sheets that are available on Posit's site. You know, we see many contribute to that both from Posit and outside of Posit too. That style is is very reminiscent of that. So I think it's it's interesting, yeah, interesting way to get started the right way, and I'm always all for it. Again, human generated our resources out there. No bot made that resource. I can I could pretty much tell that one? That's right. Yep. And like I said, what else do bots not create? Well, it's the RWQ issue itself. We've got a curator helping with that every single week. And, again, Sam did a tremendous job with this issue, and we'll take a couple minutes to talk about our additional fines for that we found in this issue.

For me, I'm still very much in my learning journey of, you know, APIs with R, but also developing web based resources with R and pushing, like, Shiny to new directions and pushing even the portal sites I create the new directions. And friend of the show, Albert Rapp, has another terrific blog post here in his web dev for R section. One area that I simply have to always keep looking up every time, it's not muscle memory yet, but getting a handle on selecting certain elements in your CSS style sheets. And he's got both a video and an accompanying blog post that talk about how you can select particular tags of a certain type with both the source code and solution right there in in the in the post itself, selecting elements by class, lots of things that unless you really practice a bit, especially if you're new to web dev, it's gonna seem pretty foreign to you. But he brings it home with how he used these techniques to modify some of the styling behind a GT table that he was, creating. So you can give a little extra personality, a little extra style along the way. So if you're in the world of CSS style and you're just not sure where to go to get that particular nagging element that you wanna make like a bold font or make a red color background, this is a great post to let you dive into just what kind of detective skills you might need to get to that set element and make it look the way you want it to.



[00:38:41] Mike Thomas:

And, Mike, what did you find? Oh, that's a great find, Eric. And the the shiny developer in me and the web scraper in me, is very interested in in checking that one out to to dive down and figure out very specific CSS and style elements on a on a web page. That's awesome. I found, very interestingly, a phenomenal article in Nature by Jeffrey M Pirkle called No Installation Required, How WebAssembly is Changing Scientific Computing. I think we teased this a little bit last week, but it's a fantastic walk through about WebAssembly. It starts off with some quotes from, George Stagg, at Posit, who has done so much work on the WebR package to allow us to, write our code that is compatible with the WebAssembly framework and essentially have have the the work run-in the user's browser and no server required, which obviously is is pretty game changing, something we've talked about many times here. And it's really the theme in this blog post. And, it's a really interesting article because there's little anecdotes from from many people with many different perspectives on this topic.

One of those people, being, my co host, statistician Eric Nance. Oh, hello. And, you you have an awesome quote here that you're you believe WebAssembly will minimize, from the reviewer's perspective, many of the steps that they had to take to get an application running on their own machine in the context of clinical trials. I totally agree. I I I really enjoyed reading this article, and it's it's very exciting again, for me to see this topic being picked up in something like nature.

[00:40:20] Eric Nantz:

It's it's super exciting because it it means that it's it's gaining steam out there. Yeah. I'm as excited as anybody right now in this space and the fact that we're we're being recognized in places I never even dreamed of as we're we're kinda pushing the envelope here. I think it's just another piece where this has a chance to transform so many things and not only my industry, but many other places as well. And, we're just at the tip of the iceberg. There's still a lot of growth here, but I I I sense that, you know, we're gonna be talking about this for for years to come as one of these next big evolutions in technology at the intersection of data science. We're we're on the way, Mike. We're on the way.

Yep. And one other thing I wanna leave off with, and a good friend of the show as well, John Harmon, he's been, you know, sending out some posts on his Mastodon account and LinkedIn about a recent unfortunate event with the r for data science community and that their particular provider that they've been using for assembly funds to keep the project going has unfortunately changed direction. And now they're kind of, looking at other ways to receive, you know, robust funding through a robust infrastructure.

So I will just mention if you're in this space, maybe be able to help John out with some advice on where to go for additional funding opportunities for our for data science and platforms that they can leverage. Certainly get get in touch with John personally. I'm sure he would love to hear, you know, some other advice that people have along the way. So, again, really hoping to see the r for data science group keep going. But I know it's gonna be it's always tough when you rely on a platform to help centralize some of this, then they pull the rug from under you. So let's hope for the best, John. And, certainly,

[00:42:16] Mike Thomas:

if we find any, you know, resources, we'll pass them along your way. Yes. No. Please help out, John. If you have the the means to be able to do so because this R4DS community, is is fantastic. I think that it's it's helping a lot of people get up to speed with R and get introduced with R. It's helping people like me, with very niche questions and, just an incredible community of folks who are willing to to help one another and to listen and to try to encourage each other in our R programming journeys.



[00:42:47] Eric Nantz:

Absolutely. Absolutely. So, I was reading some of his latest posts here as of 4 days ago. They did get some additional funding before that host kinda pulled the rug from under them, but that's not gonna last forever. So, again, he's I'll put a link in the show notes to where you can you can contact John and contact the project. So, again, we we really hope for the best here. And talking about finding great resources for help, I mean, we've said this many times. The Hartford Data Science community has so many helpful participants at all skill levels. It is an it is a wonderful resource out there. They have book clubs. They have groups dedicated to different packages or different frameworks. It is all there for the taking.

And, really, some of the best support you can do is even just helping out with that community on top of financial donations. So I'm sure he would welcome that as well. And speaking of welcoming, we welcome your feedback too with this humble little, you know, endeavor we call a podcast here. And what are ways that you can help us out? Well, first, the Rwicky project itself. We'd love to get your new package idea or new package resource. If you have a blog post, a tutorial, or announcement you wanna share, we're just a poll request away. It's all marked down all the time. Where you can do that is linked right in the top right corner of rweekly.org, the link of this current issue's draft or upcoming issue draft, I should say. And then you can just send your poll request there, and our curator of the week will be glad to merge that in for you. And as well, we love to hear from you in the community.

There are many ways to do that. We have a little contact page in this episode show notes that you can send us direct feedback with. You can also have a new modern podcast app like Paverse, Fountain, Cast O Matic, CurioCaster. I could go on and on. They have a little boost functionality. You can send a fun little message along the way. And, quick congrats to my friends at Jupiter Broadcasting because they use this modern infrastructure from the Fountain app to do live podcast episodes on the ground at the recent SCALE and Knicks conferences in California. It was a good time to be had, so you might wanna search them out. That was some amazing content there. And, yeah, the Knicks stuff was, quite entertaining. So I'm thinking of Bruno right away when I when I listen to this. I may have to dust off the Knicks stuff now. And, again, I'm even more inspired than I was last week. I'm sure his ears are ringing.

Yes. Yeah. I'll have to link that in the show notes. Bruno, I think you'll find it very interesting. But, as well as what's awful interesting is is hearing from you, as I said. You can also get in touch with us on social media. I am on Mastodon mostly these days with at our podcast at podcast index on social. I am also sporadically on the weapon x thing at the r cast and as well on LinkedIn as well. You can just search for my name, and you'll see all my show announcements and other fun announcements there. And, Mike, where can the listeners find you? Yeah. Probably best on LinkedIn. If you search Catchbrook Analytics, k e t c h b r o o k, you can find out what I'm up to,

[00:45:54] Mike Thomas:

or on mastodon@mike_thomas@fostodon.org.

[00:45:59] Eric Nantz:

Awesome stuff. And certainly, yeah, we love hearing from you, as I said. And our weekly training keeps on going. And hopefully, we keep going again for the foreseeable future. But I can guarantee you there will be no robotic voices on this podcast. You can be sure we are the authentic Eric and Mike, whether you like it or not.

[00:46:18] Mike Thomas:

That's right.

[00:46:19] Eric Nantz:

Yep. So we will close-up shop here. And thanks again for joining us from wherever you are. And definitely helps if you wanna spread the word for others in your in your organizations learning data science. You know? Spread your word about the podcast is probably some of the best support we can get, so we greatly appreciate that. So we will close-up shop here and we will be back with another episode of our weekly highlights next week"
"62","issue_2024_w_11_highlights",2024-03-13,48M 33S,"A collection of tips for spreading the good word about your awesome R package, how spring cleaning a package codebase doesn't have to be a dreadful experience thanks to usethis, and the culmination of a learning journey to bootstrap node JS projects powered by webR. Episode Links This week's curator: Colin Fay -…","[00:00:03] Eric Nantz:

Hello, friends. We are back at episode 156 of the R Weekly Highlights podcast. If you're new to the show, this is the weekly podcast where we talk about the latest highlights that have been featured on this week's our weekly issue. My name is Eric Nantz, and I'm delighted you joined us from wherever you are around the world. And, you know, I never do this alone. I have my awesome cohost join right here, my line mate, partner in crime here, Mike Thomas. Mike, how are you doing today?

[00:00:30] Mike Thomas:

I'm doing well, Eric. A little better than the the Red Wings, though. It seems like they've been skidding in the last 3. Come on, Red Wings. Let's let's pick it up here.

[00:00:39] Eric Nantz:

There's been a a bit of anger, and there's speculation that I think you're familiar with this and those that follow sports in general in the US are familiar with this. There's more advertising now on players' jerseys. They literally just put a patch for, of all things, a trash company on the Red Wings jersey. Oh, that's bad. They are winless since then. Now I'm not one of those people who's gonna say this is exactly a correlated event, but I'm just saying, couldn't they have waited till next year? I'm just saying. So

[00:01:14] Mike Thomas:

Yeah. We'll see, Mike. Yeah. Well, it's certainly correlated, but we'll hope it's not causal.

[00:01:19] Eric Nantz:

Exactly. Yes. Thank you for cleaning that up. Ironically, talking about trash coming and cleaning it up. Well, luckily, you know it's not trash here. What we're talking about here today, we don't have to worry about losing streaks here. We're on a a hot streak of awesome highlights this year for sure. And our curator this week is the esteemed Colin Faye, of course, the architect of all things GOLM and many of our shiny and web in general technology tools, which we'll be talking about later in this episode. But as always, he had tremendous help from our ROK team members and contributors like all of you around the world with your awesome pull requests, suggestions and general feedback.

So let's get right to this. Right? And one of the kind of rites of passage, you might say, as you develop your R skills and your journey into data science, do you have that great idea for maybe that new analysis technique, maybe that new data source that you want to make as easy as possible for yourself and potentially others to bring into R and do some cool analysis with? Well, that, of course, is running a package. Right? This used to sound so intimidating, but with the frameworks that we've been featuring heavily on our weekly highlights since the life of this show and the life of our weekly in general, there are lots of amazing tools in place that get you started right on that journey to create your package.

And let's say you've used those tools. You've got an awesome package ready to go. But you might ask yourself, now what? How do we exactly get this in the hands of our users? And our first highlight is coming from, once again, the very awesome rOpenSci blog authored by Ioannini Balenis Salbin and Ma'al Salmon returning once again with their series that is inspired by their recent workshops with our open side champions about how you can promote and release your r package to the world. And so like I said, Mike, first step is just getting it out there at all. What kind of advice do you have for us here?



[00:03:20] Mike Thomas:

Yeah. Well, I I mean, I'll even set the tone before that. Creating an r package to me is just such a great idea to try at some point because it sort of forces you to use a lot of great best practices around writing good software around the the science that underlies what you're trying to do. So I would highly recommend taking a stab at creating a package if you have never tried to do so before, I think you'll find it a pretty rewarding experience and I think it'll make you a better better programmer.

But once you have created that package, you're exactly right, Eric. How can we how can we market it? And and one of the first steps, which I wholeheartedly agree with that Yanina and Mael recommend is to create a great read me. I can't stress this enough, you know, your read me will help others, understand exactly what it is your package does, how to install it, and maybe a few different examples of how to get started using it. It may even, as you create that Readme, help you sort of refine your idea around what you actually wanted this package to do and may cause some changes to your actual functionality not speaking from experience or anything like that but it's it's one of those, exercises kinda like a rubber duck I think that forces you to explain exactly, you know, in in layman's terms, non code terms, what your package is trying to accomplish. And and it's a a great idea.

Spend all the time creating, you know, the coolest hex logo that you possibly can as well. Not that I've wasted hours doing that at the end of a project, but that's a super fun part of it as well and I think that from a marketing perspective any sort of visual fun aids can help market your your package as well if you use GitHub you Nina and Mel recommend that you pin that package repository to your profile so that as soon as somebody visits your GitHub that'll be sort of the first thing that they see that stands out and that's a great idea as well.

The next, the next recommendation that they have around publishing is one that I need to take to heart because I have not done this yet and it is create a universe on our universe, which as we've talked about on the podcast before, is this absolutely incredible resource created by your own. And, it is a phenomenal place to to host your packages that automatically, I think, displays the documentation and metadata around your package in just a beautiful really accessible way. And also, I think allows others to install it it very quickly because I think in in some cases it'll build binaries if I have that correct. Yes. It is building binaries. Yep. Okay. Which can make the installation experience a lot better for your users and then you know after that sort of I guess the the holy grail right would be potentially public publishing it to crayon which is sort of the final way to make installing your package probably the easiest to the largest array of users, across different experience levels out there.

And one thing that that I also hadn't thought of it as well, and if you're familiar with the rOpenSci project, is that they have a peer review process which is a phenomenal thing that is in place to sort of ensure robustness and rigor around your R package and ensure, again, that you're using, some of the best practices around software development and creating an R package. It's not something that I've done before. I I think that it's a fantastic fantastic resource and it's something that I wanna take advantage of, to me, and I think maybe this blog is debunking that that myth a little bit. I was never sure that my packages were sciency enough, for our OpenSci to necessarily, consider, you know, peer reviewing the work that I've done. But, as I know that they have office hours and and things like that that are publicly available, I would definitely recommend that folks take advantage of these resources that they offer. And in that peer review process, I think, as they mentioned, they may catch a lot of things, that you might get flagged on when submitting to CRAN. So they may help you expedite that process of getting your package onto CRAN.

So those are their recommendations around publishing. And then maybe, Eric, you can talk a little bit about their recommendations around promoting your package.

[00:07:46] Eric Nantz:

Yes. And this is a a skill set that is admittedly sometimes not intuitive to many of us, especially as we're new to this this, situation of getting the package out there but trying to get it into the hands of the user base that we intended to have, especially as we wanna garner initial feedback and, frankly, make make a positive difference. Right? And so there are lots of interesting ways. And, again, maybe not a one size fits all for everybody, but I think the advice here in general is quite sound.

One of those is taking advantage of the lowest friction to get this out there on various either social media or other publishing platforms. One of them, of course, I'm not gonna be ashamed to say I'm biased with this, is, hey. Send it to us at our weekly. Right? We have a section every single week on new packages that have been released to the R ecosystem, whether they're on CRAN or on GitHub only or or a GitHub like repository. And we always also link to the R Universe project in every issue as well. So that is, again, we are, as I say at the end of the show all the time, a poll request away from making that announcement on our weekly. We definitely recommend you take advantage of that.

And, also, if you wanna spread the word on kind of your intention of the package and maybe some more up to date notes from yourself to your audience, another great way is to start your own blog. Right? There have been plenty of frameworks now in the R community that will make creating a blog with markdown super easy. I, of course, speak with great success with the blog down package. Now Quarto, of course, that we can talk about routinely on the highlights has its own mechanism for creating a website with a blog component.

So those would be another terrific way that you could spread the word about your package and then posting that on various social media channels such as Mastodon, LinkedIn, some of the others out there. And those, again, are great ways to get the word out. Again, not natural for yours truly to do all this, but over time, you really start to see some really great nuggets come out in the community as you follow these feeds, and you could definitely put your package as one of those items in those feeds. And then as you as you said, Mike, going through the rOpenSci process from the creation and peer review process of your package is a terrific way to enhance the quality.

I personally have not done it, but I've lived vicariously through my esteemed friend Will Landau and his peer review process for targets, which is you know, why we recognize in the R community for innovations, to say the very least. And, yeah, rOpenSci has done a tremendous job with targets. And one of the other bits of advice that are in this post here is that if it is on rOpenSci, they have additional channels to market your package, such as featured tech notes that are going on their our open side blog, which is what we're reading through right here as we speak. And also, as you mentioned, they also have community calls and even can set up dedicated working sessions where maybe you, as a package author, wanna give a chance for, you know, a prospective user to hop on a video working session with you. And you can talk about the package and maybe debunk some issues, But it's another great way to get the word out because those are very, you know, relaxed atmosphere, just, you know, practical discussions, and another excellent way for your users to learn about, the way your package works if you're in the rOpenSci ecosystem here. And, of course, social media and blog posts are just one way to get the word out. Another terrific way, especially in the community of the R community, the worldwide presence of these user groups and also the R Ladies user groups.

Another terrific way to maybe have a short presentation or a short working session, maybe an online workshop of 1 of these working groups. Another terrific way to get the word out about your package. I've seen some really great great showcases of that throughout the years on these various online forums now, especially since the pandemic. Many of these are remote. They're sharing recordings on YouTube or other video channels. Another terrific way to get the word out. And I dare say another fun way, if you're really adventurous, to do a little livestream once in a while like I used to do in the past, which I hope they get through someday. But, again, there are many different avenues for you to get your get your package out there. And certainly, if it is a very scientifically focused package, there are some very well renowned manuscripts out there such as getting it into the R Journal itself or the Journal of Statistical Software, many others that are domain specific as well, which, of course, in my field, we do a lot of literature review. We're seeing a lot of new algorithms being published, and they often have an accompanying R package to go with that publication.

Another very traditional yet very powerful mechanism for getting the word out there. And one little bit, Mike, as you said, you were really sure if a package you're creating is, quote unquote, scientific enough for an rOpenSci, you know, scope. But what we'll what we'll link to in the show notes is, a section on their online peer review book where they do talk about the intended scope that they look for with respect to bringing a package on board to rOpenSci. And the great news is that doesn't have to be a very focused domain specific scientific algorithm or method. They have many packages that are involved in making data more accessible, making APIs more accessible. There are lots of interesting domains here that, you know, could be a good fit. Again, may not be for everybody, but, again, if it does fit in that scope, you might benefit greatly by going on our open side.

And certainly, I'll also speak on the perspective of those in an industry where maybe you don't get a chance to publicize this to the worldwide, our community, until you get the, quote, unquote, blessing of getting an open source. Maybe you have to deal with this internally at your organization. If you have a large organization, how do you make sure that your user base within the company get their eyes on it? I'll go back to what I said maybe a few minutes ago. Having an internal blog is a cool thing to do too. I'm actually trying this out now as I speak, doing a little portal blog for our internal group to share package announcements that our group is creating and having those broadcasts on either some newsletter or some other, you know, distribution service within the company. So it's not just us making these cool packages out there and then, you know, others in statistics or data sciences not not getting the word of it. We're gonna find ways to get that message across. So I think a lot of these principles can apply to those of you that I'm gonna steal a phrase from my friend Michael Dominic with the Coda Radio podcast.

You dark matter developers out there, I see you. I know you're out there. There are great ways you can take some of these techniques internally at your respective organizations, too. So really, really great blog post. Gives you a lot of great ideas to follow-up on. And again, I think just getting the package out there, you've done immense work to do that. It's a journey. I know how it goes but getting the word out there so that your user base can get their hands on this, is, you know, a really critical component to making sure that you can make the package even better as your users get their hands on it. So, yeah, really great advice here, and I highly recommend what we're talking about here. Yeah. I know. And that's a great point, Eric, too, because not all of us can share

[00:15:41] Mike Thomas:

our our packages with, you know, the the general public in the outside world, but I I think you absolutely can take these principles and leverage them within the communities inside your own organization, through whatever means, you know, necessary that you have available to to do that, but but still leveraging these principles I think is a great idea.

[00:16:10] Eric Nantz:

Now what if, Mike, you're in the situation of you've you built that package, but maybe it was, I don't know, 5, 6, maybe even 10 years ago. You look at the code base and you realize, oh, past me did that. If past me knew what present me knows now, I probably wouldn't have done it that way. You might be in the situation where your package may deserve a bit of what we'll call spring cleaning, as they say. And so our next highlight, comes to us from the Jumpy Rivers blog. We featured them quite heavily on highlights in the past, authored by Rhian Davies, and is appropriately entitled Spring Cleaning Your our packages. And they start off with relating to that Jumpy Rivers themselves have put many packages on, say, GitHub or CRAN and whatnot, and some of them develop it more than, say, 5 years ago.

And then as we learn I mean, I'm a continuous learner, as they say. Lots of new, you know, best practices, maybe modifications to existing best practices. And then you realize, yeah, you know what? I should try some of that in my legacy package that needs a refresh. How do we go about that efficiently? There are some practical things you can start with, one of which is if your package is on a GitHub or GitHub like repository. There was about 3 or 4 years ago, Mike, there was a movement to change the nomenclature of default branches.

We won't get into all the details here, but the connotation of master didn't exactly sit well with many people in today's, you know, communities. So there's been a movement to change that to a more friendly term such as Maine or something like that. And so there is an easy way to rename your branch right away. And we're going to talk about this heavily on this segment. They use this package author by Posit is the superstar here, so to speak. We're getting these tips operated on efficiently with as less friction as possible. So there is a handy function called get default branch rename.

It's a long function name, but it literally says what's on the tin, what it does. So once you do that, your branch becomes main. You can push that up to GitHub, and you are all set. But, of course, it doesn't end there. We've got we might have some additional things that we wanna tidy up with respect to the package metadata. This was new to me, Mike. I'm curious if you knew about this before, but there were times I wrote my description file for a package, frankly, by hand back in the old days. Yep. You remember.

And so I'm updating a package that, again, I've lost there, like, 8, 9 years ago. And there is this little gem that's in this blog post Now if you wanna tidy that up so that things like you have fields in more alphabetical order, maybe spacing correct, making sure that consistent names with maintainers and and author fields are correct, There is a use tidy description function that will basically put everything in the standard order, alphabetize the dependencies, and make sure everything just looks really tidy.

So if you happen to do all that manually, that is awesome. I love seeing that. And then another part is we don't always want to do things ourselves all the time, right? We're in 2024. There is a new technology in certain platforms such as GitHub to automate many of these checks in action. And, Mike, there is even more use this magic for getting that set up for you, isn't there?

[00:20:00] Mike Thomas:

Yes. There are. And as we talk about things that we used to do that we no longer do. Right? Travis CI, I think used to be that the most popular tool for continuous integration, which is, you know, running code essentially, on maybe some separate server. And a lot of times this was around running tests, ensuring that all your unit tests pass, when you create a pull request before that pull request actually gets merged into the main branch. Nowadays, we're using continuous integration quite a bit as well for, like, creating package down sites, and updating, you know, what's shown in that that, package down branch of your repository that spins up the the whole package down site that folks can go to and see the beautiful version of your your packages documentation. So, Travis c, I sort of used to be the only game in town, but it's GitHub actions now.

You know, I think I think that's probably the primary way that folks are going these days. And fortunately, again, use this package, allows you to easily create, that continuous integration GitHub action with, the function. Stop me, if you're not expecting this, but the function name is use GitHub action. And you can, supply sort of what type of check you want to create and that essentially creates a whole entire YAML file that will execute, the unit tests in your package, run those tests, when a PR takes place, I think, for the most part. And you you can alter that YAML file if you want to, you know, change when, those checks get fired, or or other certain specifications of those unit tests, getting run-in this continuous integration, situation.

So there's a lot of different options here, within, you know, being able to use GitHub actions depending on sort of how strictly you want to run the tests on your package, and then, you may also want to take a look at it and see how much test coverage quote unquote, your package has, which is sort of the ratio, I believe, of the number of lines of unit testing code versus the number of lines of of sort of our code that your functions themselves have, and and you wanna sort of ideally be as as close to a 100%, I think, as possible. I think there's a lot of a lot of opinions out there on that that we don't necessarily need to get into. But it's probably a good idea to show your users sort of in general at a high level that you are are writing a lot of tests around the functionality for your package to ensure that, you know, your logic is doing what you expect it to do, and that it continues to satisfy those expectations as you you make changes and and refactor over time.

And the last two things that, they recommend are 1, creating a hex sticker, which is a callback to our our first blog as well, which I will highly recommend. I use a site called Canva, which I think I pay a couple bucks a month for, but, it is it's pretty incredible just the the stuff that you can do. And nowadays, it seems like everybody's leveraging, you know, these generative a ai models to, sort of write a prompt of what you want shown on your HEX logo and then, you know, that'll spit out a wild image for you that that you can crop to, a HEX background. In my case, I I do that pretty easily with Canva. And, we've been I've spent way too much time on that recently but it's it's super fun and can be a fun way for folks to, it can be the first thing that they see when they navigate to your package down site or, you know, browse the vignettes within in your package, in the RStudio IDE.

And I think it can can sort of create some excitement and engagement around your package. And then the last thing, that they will recommend is, contributing and code of conduct, adding that to your repository in your package as well to let users know the best way, and sort of the guidelines and principles that you expect people, that want to contribute to your package, to contribute to your package in in a friendly way, in a safe environment that, works sort of for everybody that that's working on that that repository.

So some excellent excellent spring cleaning, if you will, ideas and and examples from Jumping River. Spring is here, and I think it's it's time for all of us to start diving into these.

[00:24:27] Eric Nantz:

Yeah. I've literally been living this life for, like, 3 weeks now with this legacy package and seen so many areas that need a little attention, a little package and seen so many areas that need a little attention, a little cleanup here and there. And so all these principles either have or will take action quite a bit. And back to HEC stickers. Yes. Yours truly did revise a HEC sticker for this legacy package. I'm gonna give a quick plug. If you want, in the in the blog post here, they're referencing the hex sticker or, I believe, the sticker package or hex sticker package, easy for me to say, as in our way of doing it. And then, also, if you wanna stick with VAR about bringing all shiny in it, many years ago, Colin Faye, as part of a shiny contest submission, released the Hex Make Shiny app where you can literally create a hex sticker, superimpose an image on top all within a Shiny app and download it. So I actually use that literally to make a new hex sticker for my internal package.

That was that was a lot of fun. So I'm never shy to plug that that fun shiny app for my bookmarks as well. And, honestly, yeah, back to the contributing guidelines, when I would build these legacy packages, you know, back then, maybe I was naive. It always seemed like it would be just me, so I didn't put a lot of thought into it. But you know what? The these packages, again, I think can really thrive when you have somebody at least that wants to be active with you, if not maybe developing day to day, but at least, you know, helping you test things out. Maybe they're a liaison to other users, and then they have feedback, and then they can, you know, find the best way to, you know, help you with that feedback.

But you wanna give them the easiest way to get started with that. So these contributing guides, whether your package is open source or within the confines of your industry firewall, I think those are critically important to make sure that you give these others that maybe are willing to step in this. They'll know where to start. Things like a contributing guide, also making good use of issue labeling in your whatever your system is for issue ticketing, things like good first issue or help wanted or, you know, you know, things like that. You know? And, obviously, it's project specific, of course, but making it as easy as possible for people to really drill down to see which areas they can contribute to the most. So, again, great things to think about as you're already in the midst of making your pack as a little more tidy along the way. Yeah. Really good advice here.

Well, I teased this earlier, Mike, but, our curator here has been hard at work not just curating this issue, but, Colin has been knee deep in this learning journey and this saga of supercharging his workflows with WebAssembly. And in fact, we are gonna be talking about in the last segment here, it is, I believe, the 6 posts in his series of exploring WebR and WebAssembly with respect to, you know, interactive web applications. And what we're going to talk about here, what seems to be kind of a culmination of everything he's been learning here, is the idea of having new tools available that within, say, the native JavaScript world of bringing WebR, WebAssembly powered by R into these, these applications via 2 new utilities that work in tandem. So let's dive right into this where earlier in his explorations, he's been prototyping some interesting use cases of, say, converting an existing Shiny app in the WebR, preloading packages in an Express JS API, you know, bringing your own functions in WebR and then building them into the the the Node. Js app and whatnot.

Well, he realizes that a lot of that was, you know, kind of, you know, piecemeal learning a bit here and there ad hoc. What if we wanna take those best those practices that he's been outlined into a very easy way to make it happen that might have some parallels to what we get in the R community when we build packages with, say, dev tools, use this. And in the Shiny situation, of course, what Collins authored with the golem package. What's a way to bring that all together in this native kind of WebAssembly and JavaScript world?

So in this blog post, he announces 2 new utilities to make this happen, one of which is called WebRCLI, which again is going to be very similar to kind of a dev tools use this paradigm along with other functionality we'll get to later that's going to help you create a no JavaScript project, but with the bindings to WebR already baked inside, things that he was building manually in the earlier stages of this journey. This package or this utility is going to bootstrap that for you, not too unlike what you would do with use this and say use this use package or create package. I forgot the exact name of it, but it's where it gives you the scaffolding of an R package right away. And then it's up to you to fill in the blanks, if you will. This is doing a very similar framework with, again, the WebAssembly piece of all this.

And so that in tandem with the other utility, which is called Spider, which is looking like a way to build extensional functionality on top of WebR itself, such as what we get in typical R installations where we want a package from CRAN or or maybe even from GitHub with the remote package. We have functions to literally install that package. Right? Install that packages or remote install GitHub or whatnot. Spyder is giving you a utility to use a native JavaScript function that will look very similar to those installation commands, and it's giving you those built on top of WebR to bring those packages down to your local project.

This, this is kind of amazing to me. It's not just taking their installation. It is putting them in a project specific directory that, if you're familiar with r m, will look very similar. I went through the GitHub example that we'll have linked to in the show notes. He ignores this, but I and his GitHub ignore. But what I did is I cloned this locally to give it a try. And sure enough, there is a directory that when you go inside it, it will it's called webrpackages. You go in there, It will look very similar to your r mv library where you download packages.

This is fascinating to me. Colin has figured out how to load these packages from a file store into these WebAssembly powered applications. This is massive to me because where I'm going with this, ongoing pilot submission with WebAssembly, we want to explore ways of not just grabbing packages from the WebR binary repository on the fly, so to speak. But should we want to distribute packages as part of a bundle? How do we bring those into the application locally? So I will be looking into this quite closely to see if I can take some nuggets from this, whether it's for this particular pilot or for future explorations the CI can mirror this with things like Shiny Live, that we're using right now in our pilot submission.

So the wheels are turning after I read Collins post here. But this is again, this is all fascinating to me. So one thing you notice that we didn't mention here is that we're not talking about Shiny here. Right? He is speaking on behalf of those that maybe are familiar with JavaScript native ways of building a web application, but you have a function in R or a package in R you wanna leverage as the back end to that Node. Js or other JavaScript like app, this set of utilities is your way to make that happen.

And I definitely invite you, if you do have, you know, Node. Js and NPM installed on your machine, give this a shot. I literally ran through the blog post this morning, and everything worked to a tee. Everything worked as advertised. So this, I think, is opening a lot of possibilities here. But as we often say in these explorations, it's early days. He has not tested this over than a few examples, and he is very eager to get community feedback on how this goes for those also that are willing to explore this kind of blazing trail, if you will, of this of this new journey here.

So there's notes at the end about how he kinda pulled this off from, like, a back end perspective. But again, he's looking for feedback on this, and I definitely am intrigued by what I'm seeing here. And I can't wait to to learn more about how this works under the hood. But this is a great time to talk about WebAssembly right now because I'm thrilled to say as of yesterday when we record this episode, there is a fascinating new article released by Nature authored by Jeffrey Pirkel. It's entitled no installation required, how web assembly is changing scientific computing.

And I'm humbled to say to yours truly has a small little quote in here based on our current explorations, but I will say this is the kind of stuff I am super excited about. We are trying to push the envelope here. We think there is a massive potential in many industries for this. Of course, I'm coming from life sciences, but there are many, many others that I think can make heavy use of WebAssembly. This article has terrific narrative around kind of the genesis of this from George Stagg himself as he started prototyping WebR along with other members of the scientific community and how they're showcasing the use of this technology. So great time for me to see this. And, again, super excited to dive into what Colin's exploring here and see how we can supercharge this in the future.



[00:34:47] Mike Thomas:

Yes. Me as well. And, Eric, I'm glad that you you shouted out that article because if you didn't, I was going to you know, when I first started doing this podcast, I was starstruck that I got to record with the, you know, the the host of the r podcast and the Shiny Dev series. Then I think I've gotten a little comfortable, with you, but but now you are featured in Nature Magazine and I'm right back where I started. So hats off hats off to you that that that is an awesome awesome accolade and it's it's super exciting as well to see that, you know, the scientific community is is talking about this stuff as well. And it's not just us software nerds, you know, that that are the only ones caring about this, that it's it's really something that other folks seem to be seeing as well as a pretty revolutionary thing that's starting to come into the ecosystem. And and fortunately, we do have folks like Colin who are at the the cutting edge of, the this WebAssembly stuff. You know, Colin curated this week, and and when I saw the blog post, I thought this

[00:35:55] Eric Nantz:

was a little bit of insider trading, but I'm very glad that, I'm very glad that this one made the highlights. It's a great example.

[00:35:57] Mike Thomas:

You know, one of the the the toy examples here is is, called this WebR SpongeBob, example that he has, which I think just sort of allows you to, you know, essentially change some text, a string that you write to what's called sponge case. A quick story, during a particularly slow period a couple years ago, for me, I highly considered creating this exact r package. I didn't end up doing it, and and glad I didn't because it looks like maybe Colin was the one who created the spongebob package. I'm not not sure if it was him or somebody else, but, somebody somebody took care of it for all of us. Obviously, that's a very important package in the art community. So it's nice nice to have that one out there.

But this is a phenomenal guy. To use your own. Right? To use your own. Yeah. And like you said, you know, it's incredible. I haven't actually tried it myself, but it sounds like you maybe forked the repository as well and ran through this and found that there were no issues. Obviously, I think Colin in both the the read me's in these repositories and this blog post as well makes a lot of disclaimers that, this is very early on, very experimental. You know, expect a lot of bugs. He, I think may already be seeing some bugs and edge cases that he's hoping to to solve, but regardless, I think the the fundamental concepts here of what's being done are are really driving sort of this this idea in this space forward about, you know, that this this web assembly topic and not needing to manage dependencies, in ways that traditionally were were a little difficult and sort of making that much easier and much more accessible to a wider variety of people, which is is incredible. So I'm excited to see, how this this continues and patiently waiting on on blog post 7.



[00:37:45] Eric Nantz:

Yeah. Me as well. And it's, Collins in this realm of, I'm gonna say, you know, key amazing thought leaders in this space that are being very adventurous in what's happening here, in the same category as I would consider Bob Rudis and his explorations with WebR tying into things like observable framework and whatnot. WebR is this engine that is powering so many things. Yes. I've been coming to them mostly from the shiny perspective with Shiny Live, but it is so much more than just that. We even feature, what was it, 2 or 3 weeks on this very podcast, a blog post that had, you know, our counsel basically embedded into the post itself to try out the code that was being being showcased there. Right? Education side, web application side. And now as as that nature article is showing even, you know, high throughput, high HPC like computation in the browser, It's all it's all coming together. It is. I mean, I don't know. I haven't been this geeked out in years that of ways that we can tie our entire data science with a novel technology. And I'm I know I can't stop talking about it, but at the same time, this is the start of something. I still remember sitting at the positconf presentation by Joe Chang at the end there, and all of us are looking at each other across the room is like, yep, we're going with this. We're going to try stuff out and see what happens. Challenge accepted, Joe, if you're here listening to this. So, yeah, really cool stuff to see what Colin's exploring here. And it does show that I still have a lot to learn, but at the same time, I'm gonna enjoy learning about this.



[00:39:28] Mike Thomas:

Likewise. Likewise. And then on the you know, I I take it from a shiny perspective as well. And then with the sort of duct deep, you have to think about the data side of it right and maybe you have an external connection to a database which makes things easier maybe not and you know the fact that there's, now these integrations between DuckDV and and WebAssembly that I think are going to solve sort of that final last piece for us in a lot of ways in terms of connecting the data to the application or or whatever you're showing on screen in an easy way, it's it's incredible.



[00:40:00] Eric Nantz:

Yeah. Absolutely is. And we're gonna be hearing a lot more about this throughout the year. I'll also give a a plug once again that we'll we're thrilled to have George Stagg give a keynote at the upcoming shiny conference coming up in April. So if you're not registered for that, I highly recommend coming to that event as well. And, yeah, my cohost here is gonna have a shiny app on there as well, so we're really excited for that. Yes. That's exciting. Coming up quick. It sure is. It sure is. But, you know, it's quick. You know, it's always a a quick yet very educational read whenever you see our weekly every single week. We don't try to bog you down too much where we give you, you know, the the awesome resources, blog posts, tutorials, as we mentioned at the top, new packages, hitting the ecosystem, updated packages, and much, much more. So we're gonna take a couple of minutes to share some additional highlights here. And going back to the Shiny train for a little bit, I had a thought provoking insight here that it was led by this blog post from Jacob Soboliewski over at Absalon entitled Using Test to Develop Shiny Modules.

Now this is something that usually you don't really think about until you get to the stage where your module is almost done and you're thinking about, okay, how do I make sure that it's robust enough? But, Jacob here does a great outline here about how the concepts of test driven development really come into play. Whereas if you're really iterating on a specific module, there are ways to test it efficiently without having to run the entire app every single time, making clever use of test that functionality and custom functions and whatnot.

And this looks like some I'm gonna start looking at as I start revamping some of my major shiny apps or building new ones in terms of making that development cycle of developing modules just a wee bit faster to my to get to get things done quicker as they say. So, yeah, really thought provoking for a post from Jacob here. Yes. I

[00:42:01] Mike Thomas:

found a blog post, an additional highlight here from Alexandros Kuretsis, from Absalon, entitled Our Plumber How to Craft Error Responses That Speak Fluent HTTP. You know, we talk about Shiny a lot, Eric. I've said that a few times today. But, you know, one thing that we talk about a lot is creating the best user experience as possible around our Shiny apps. And a lot of times that includes error handling in a graceful way for the user to understand sort of what went wrong instead of just getting disconnected from the server. Right?

That that's what we try to avoid. And the same principles I believe apply to APIs. You know like plumber APIs. Right? That where if something does go wrong there's going to be an error code that gets sent back, to the other application that's making the request and typically that error code is going to be either of a 400 type error or a 500 type error. And and 500 type errors typically mean that something went wrong, I think, on the server side. Whereas, 400 type errors are are typically, you know, something bad happened, in terms of the the the inputs that went into that request of the API didn't satisfy sort of what the API was expecting, as opposed to, you know, the the server being down or something like that. So understanding sort of the difference between those and being able to to return something more informative, back to the applications that maybe they can create some sort of a UX based upon what type of error code comes back, so the user can understand exactly what went wrong. You know, should they should they fix this particular field that they just filled out incorrectly before clicking a button that sent that API request, you know, or or give them some information about how to potentially rectify the problem, or do they need to contact IT because the the server itself is down. Right? And understanding that difference is is really important. So this is a great blog post that I think walks through, a discussion around that, how to make things safer there. And then I will also shout out a project by, Peter Salamos and his team at Analytium, and the package is called tri r, t r y r, that tries to do the exact same thing. I think it's client server error handling for HTTP APIs, and he has a lot of examples with Plumber there and the same exact idea, you know, that you're trying to provide sort sort of a more informative error code, response back to the application that, sent that request initially. So some great resources here, to to shout out. And I have been knee deep in Plumber lately and and really enjoying it. So this is very timely for me.



[00:44:43] Eric Nantz:

Yeah. Really, really awesome insights there. And I'm also in the train of EVRA helping build new APIs or consuming existing APIs and having our layers on top of that. So, yeah, having any way to give that UX, you know, a much more pleasant experience for not only me as a developer, but my end user who is not gonna give 2 wits about what's actually behind the scenes. They just wanna know what happened and how to fix it. So anything like this to translate to crypto 403s or to 502s or, 69, whatever you wanna call it. They're all cryptic at the end to most statisticians and data scientists. So being able to translate that, and having a robust kind of paradigm for air handling is very welcome in this space. But I think it speaks to this new trend that we're seeing is that we're interfacing with other systems of some sort. I've traditionally been HPC systems, and now I'm really augmenting that with these web services that may or may not be high performing, but at the same time, they're doing one thing. They're doing it well, and they want to be agnostic to what front end we have of it. So, of course, I'm biased to r. Why wouldn't I be? So having to package interfaces with that and making that UX seamless, that's a win for me.

You know what else helps you win? Unlike what's happening to my poor red wings, is that reading our weekly every single week will help you win the game of leveling up your data science knowledge. I tried. I tried. I'm trying to give them good luck for tonight. But, anyway, yeah, every single week we have a new issue online and it's released basically every Monday morning. And then, you know, the train keeps going and we are powered by the community. Right. We, as I mentioned earlier, you know, every single week, we look at your awesome pull requests. And you may wonder, how do I get that on there? It's all linked at rweekly.org.

We have a link to the upcoming issue draft right at the top right corner. We're just a pull request away from that new blog post, maybe that new package that you just created following the advice we just mentioned in the first highlight. Our week is a great way to showcase that. It's all marked down all the time. You know, I've lived marked down lifestyle with my package documentation, my internal blog posts, some of this external stuff I'm doing. You know? Without markdown, if I had to do, like, LaTeX for all this, I would cry. I would just cry, Mike. Thank goodness for markdown.

Yes. You are exactly right. Thank goodness for the shiny include markdown function as well. Shout out. Very, very nice. Yes. I've used that heavily and with no regrets at all. Yes. So, yep, all markdown all the time of our weekly, And also, we'd love to hear from you directly as well. There are many ways to do that. We have a contact page linked in the episode show notes right at the bottom of our show notes that you can click to. We also if you're listening to a modern podcast app, WebPoverse, Fountain, Cast O Matic, CurioCaster, there's a whole boatload out there. You can send us a fun little boost along the way right in your podcast app itself. All details are linked in the show notes as well. And lastly, we are sporadically on various social media outlets. I'm mostly on Mastodon with atrpodcast, at podcastindex.social.

Also, on the Weapon X thing from time to time with atrcast as well as LinkedIn. You can find me on there with show announcements and, you know, blog posts and the like. And, Mike, where can the listeners find you? Sure. LinkedIn is probably the best place to see what I'm up to. You can just search Catchbrook Analytics,

[00:48:09] Mike Thomas:

k e t c h b r o o k. And if you wanna find me on Mastodon, you can find me at mike_thomas@fostodon.org.

[00:48:20] Eric Nantz:

Yep. I think we've, put a nice little bow on this episode. But, again, it's been a great recording session once again, Mike, and, we hope to see you all for our next edition of our weekly highlights next week."
"63","issue_2024_w_10_highlights",2024-03-06,46M 41S,"How an attempt to solve a clever programming exercise led to a new patch to the R language itself, a review of the enlightening results for the recent data.table community survey, and creating a Doom map in R, because why not? Episode Links This week's curator: Eric Nantz - @theRcast (https://twitter.com/theRcast) (Twitter) &…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 155 of the R weekly highlights podcast. This is the weekly show where we showcase the awesome resources that are available every single week on this week's our weekly issue. My name is Eric Nantz. And as always, I'm delighted that you joined us from wherever we are around the world. And, yes, spring is in the air around here, and I'm feeling happy as always to be joined at the hip by my awesome cohost, Mike Thomas. Mike, how are you doing this morning?

[00:00:30] Mike Thomas:

Doing well, Eric. Yep. Spring is in the air here in the the East Coast as well. Trying to start planning some travel and some conferences and looking forward to, getting back and and seeing some folks that I will not have seen in a year here coming up, in the next couple of months. Summer feels like it's not that far away.

[00:00:48] Eric Nantz:

That's right. And this is a little bit closer to my, sports exploits. We're getting closer to hockey playoff season, and the nerves are starting to happen for my beloved bread wings to try and squeeze in a wild card slot. And it won't be easy, but we're we're getting the vibes. We're getting the positive vibes here. We'll find out.

[00:01:06] Mike Thomas:

Yes.

[00:01:07] Eric Nantz:

But as always, I just want you all about to talk about hockey stuff, but we're gonna talk about our weekly here and the awesome resources that we mentioned in this week's current issue. And, let's check the notes here. Oh, oh, oh, yep. That was me curating this week. In between random visits to, like, school libraries and swim meets. I somehow curated this issue, but I think we got a good one to talk about here. And I never am able to do this alone whenever it's my turn because I have tremendous help, as always, from our fellow r Wiki team members and contributors with your, I believe, 8 or so poll requests to this issue, which is very welcome. Awesome addition, indeed.

And I thank all of you that have been contributing to rweekly. So without further ado, we're gonna dive right into it, Mike, and I think you're gonna lead us off with a really fun exploration that has a lot of twists and turns that eventually involve patching the r language itself.

[00:02:04] Mike Thomas:

Yes. This is a blog post from Jonathan Carroll titled, I patched r to solve an exorcism problem. I didn't know where exorcism was going. I wasn't sure if we were getting into to religion or what sort of road we were going down here, but there is a website called exorcism.org that has, a lot of different challenges across many different programming languages that allow you to to try out a different programming language each languages each month and try to solve some sort of non trivial problems, you know, not just printing hello world in that language, but actually actually trying to solve a fun little toy, exercise. Sort of reminds me of Advent of Code, you know, on the on this particular website and, you know, Jonathan shows how he has been doing these exercises across a multitude of languages including Haskell, Go, Julia, Python, JavaScript, Scholar, Rust, Fortran, Lua.

Pretty pretty incredible work that he's done. Here he's gotten quite a few badges on exorcism based upon some of these challenges. And one of the the recent challenges was to write an algorithm that converts, integers into roman numerals. And probably in a lot of languages, this is something that's tricky. But for those who know or for those who may not know, there is a function in base r called as.roman in the standard base r library and it allows you to just provide that function with an integer and it will return what I thought was a string. I guess it's of class Roman, which is quite interesting.

But it'll provide you with the the Roman numeral equivalent. And that's pretty incredible. So I think Jonathan thought, you know, at this point, he's done. It was almost like, you know, a little cheat that he has in the R language to be able to very easily solve this problem and, it's a pretty short algorithm when it's just a single function that's already been implemented. And one of the wild things about, R's ability to work with roman numerals as well is you could assign, you know, the output of this as Roman function to a variable, and then you could do that again with a different integer that you're converting to a Roman, a Roman numeral.

And you can do math with those 2 different objects that are both these these roman, numeral objects. You can add them together. You can multiply them. It's it's pretty it's pretty incredible. I'm not sure how useful this is on a on a day to day basis. It's something I've never, I guess, had a a use case for, but I'm sure there's folks out there, you know, that that had a particular use case where it made sense to not only, you know, provide roman numerals to whatever that end output deliverable is, or maybe to even do some math on multiple roman numeral objects. So I I guess a pretty cool the more you know type thing with with base r.

So, you know, Jonathan realized, unfortunately, as he began to run some some tests to try to convert, numbers, I believe, 1 through 3,999 to Roman numerals, that one of the that one of the tests was failing. And, there was a mismatch between what he he was expecting and what the as Roman function returned because, the last a 100 integers from 3,899 to 3,999 returned NA values. And this was a little confusing. I guess, in a lot of other languages, they sort of state that, and this might be in R as well, I believe, that, any of their Roman numeral conversion algorithms really go up to 3,999.

That's sort of the the final integer value, that we have Roman numerals for. So Jonathan was sort of expecting, the the limit here to be 3,999, not 3,899. So we had to dive into the source code and and this is sort of where it goes from, you know, oh, I have this, problem on exorcism.org that are already has a nice little base function for As Roman. I got a one liner. I'm gonna get this this new badge and it it quickly cascades into, you know, in the spirit of yak shaving, quickly cascades into oh my goodness.

Now I have to dive into this and it looks like I'm gonna need to submit a patch to r itself and becomes something much bigger, than maybe he initially set out for. So, Eric, do you wanna take it away with, the the patching to r?

[00:06:52] Eric Nantz:

Absolutely. And, boy, do I feel seen with the yak shaving analogy here because I literally have been going through this on an internal package at the day job where I just wanted to beef up the test suite a little bit. And, boy, did I know now I'm solely in the internals of Unix batch processes along the way. So Network tests. Yeah. Unit tests. Exactly. So, luckily, I knew how to patch it. I knew who it was responsible for it, but this is a little different here because John has indeed discovered that within the source code of r itself that's responsible for these roman numeral conversions, he did indeed see traces of the number not being 3,000999 9, but 3,899 littered throughout the code base.

Now you may ask, how on earth do you actually search the source code for R itself? Well, we are very thankful as a community that there is, on GitHub, a mirror of the r source code. I believe it's actually under Winston Chang's account still. It's called r dash source. You've been here before, Mike. I sure have. This is gonna be the bookmarks for a very, very long time. And in fact, it'll often turn up on Google if you're searching for a package of source code on a GitHub repository. Oftentimes, if the package is already on CRAN, this will be in, like, the top five results, this CRAN mirror of that said package.

But, regardless, we're talking about the r source here. So taking advantage of that platform, John did indeed, like I said, search for where this number is actually showing up. And, yeah, it is showing up quite a bit, albeit some of these are what you might call false positives. They're not really having to do with that function itself. But in the typical GREP call, you did indeed find a lot of files in the source r library, both in in r files, c files, and documentation files. So he did have to do a little more intelligent filtering to figure out just where all this is really taking place, and sure enough, he does eventually find it. And within format call and the like, but then he discovers there is a utility type function with the name roman.r.

So very straightforward. And there sure enough, there is a comparison of the range being from 0 or greater than 3,900. There it is. He has found it. And sure enough, now what's the next step? Right? Well, r itself, we mentioned there is a mirror of the source code on GitHub. That's not actually where the upstream code lives for development. It's actually using the subversion repository. Shout out to all those who use subversion in the past. It's been a while for me, but that is where if you are wanting to learn about contributing to the art project itself, you're gonna have to pull down that SVN mirror to your local machine and then run a patch through SVN.

The and there's an SVN diff, patch there, I believe. The command I'm rusty with my subversion coding here. But, when John reached out to the maintainers, on the mailing list, for r, they did recommend, hey. You know what? It looks like you're on to something. Please submit a patch and file a Bugzilla report. Just like we talk about for contrary to open source in general, finding the best way to reach a project and making sure that issue is tracked and then there's actionable feedback on that, that's the way to go. Right? So John is following the protocols that have been established by the our project team to submit this report.

And then now comes a part well, he submitted it. Now you wait. Is it gonna get merged in? Sure enough. It does get merged in. This is exciting stuff here. Right? John has literally patched the R language itself for this issue. Now as you think about, well, will this really work? What's a great way to test if your patch is gonna work? Well, guess what? Comes containers again. John discovered, you know, what I've been using for years now and, you know, that our community has been using for years is being able to bootstrap particular R versions with Docker and particular the Rocker project to be able to check if this patch is indeed going to work on the upstream version of R that's coming from the bleeding edge of the subversion repository.

He was able to pull that down into a container and then verify that his patch actually indeed works. So what's next? Well, obviously, when the next version of point release of r is released, this patch will be included in it. When that happens, it'll probably be later this year. But this process this blog post illustrates such a unique story here in terms of the nature of open source and the fact that one little learning exercise turned into patching the language itself. But John concludes the post with some really great advice if you find yourself in a similar situation in the future, whether it's in our package or another language entirely.

First, don't always assume that the language itself is broken. Of course, you want to check that you haven't misspecified some. So read the documentation, run some additional tests. That's always helpful. And then when you do think you've pinpointed something, guess what? Nature of open source, go into the source code itself. And, yes, we have learned that, yes, even with the R source code, the base R source code, there are ways to grep that both on the GitHub repo and also, through Linux utilities like grep and the like. So having a good knowledge of that is extremely helpful for some of these niche bug bugs like this.

And then don't wait to communicate. Again, John reached out to the mailing list, put out what he was finding in his explorations, got a response from the maintainer, able to get direction on how to proceed next without, you know, going too far without that buy in. And that can happen sometimes. Some people can submit patches without checking with a maintainer first, and then there might be a little disagreement or maybe other work that wasn't merged in earlier. Always communicate early. Nothing bad can happen. My opinion from communicating early on this.

And then, yeah, if you find this issue, of course, if you have the capacity, it's excellent. If you can ease the burden of the maintainers to fix the patch yourself, sometimes you might need a little help. And, again, don't hesitate to ask. Maybe a code review, maybe another test case that you like someone to assist them with. So I think this post is a terrific story of how to go about this process. And, yeah, don't be afraid of communicating with the R team on these issues because guess what? Like anything open source, it's not like they're gonna be able to catch everything themselves. And sure enough, this this hard coded limit went through r for years years years for roman numeral conversion without somebody really discovering it. So better late than never. Right? But with open source, you can, you know, do your part as a user and as a contributor to make that fixed and benefit everyone else in the process. So, again, if nothing else, also count John's blog post because there is some gratuitous, very fun Simpsons imagery too that always warms my, retro viewing hearts.



[00:14:47] Mike Thomas:

Yes. No. I I really appreciated sort of those last points that Jonathan made to to talk about, you know, how he was able to succeed and and maybe those those 4 different things that he recommends. You may consider if you find yourself in the same situation and it's a pretty empowering thing, right, to be able to because we live in open source world to be able to, you know, contribute and submit a patch to the the our, language itself that you know, you know, thousands, millions of people are going to to use and be affected by. That's that's pretty incredible. And I think Jonathan's put together a pretty nice road map here to help you do that if you find yourself in a similar situation. I think you you may need to to turn back time to, you know, about 2,005 to use SVN in a mailing list to do so. But, we we gotta use the tools tools that we have, and, that that's just a that's just teasing.



[00:15:42] Eric Nantz:

I I would say sometimes it can be intimidating to figure out, okay, just how deep does this rabbit hole go. But sometimes with a little perseverance, it does indeed pay off. This was really, really interesting interesting exercise. And and you know what? I'm gonna bookmark that exorcism site. That that is some really top notch ways to hone your programming craft. So nice find there as well. I agree.

[00:16:20] Mike Thomas:

Eric, you know what else is interesting? The results of the 2023 data dot table survey.

[00:16:26] Eric Nantz:

Oh, yes. And this is a good callback to just a few weeks ago. We were mentioning how the data dot table project was indeed, you know, revamping some of its governance and making easier and more transparent for ongoing road map ideas and how users can contribute. So, of course, what's the best way to hear how users are receiving your package and wanting either suggestions for improvement or what? And that is to release a survey earlier in the year. And this blog post is coming from the data. Table blog and, in particular, the author, Alja Sluga.

And he starts off with, first of all, thanking everybody that has filled out this survey, and they got almost 400 responses, which is really nice for a survey like this. And we'll walk through a couple of the key findings here and where it might relate to the data dot table project in the future. There is the post leading off a little bit of demographic style information showing that, you know, the majority of users did have an a very much an experience set using r for 7 plus years and data dot table, you know, quite a bit in that time frame as well.

And then, you know, many are using it every day that responded to the survey, so there might be a little bit of selection bias here going on. But, hey, it's always good to quantify that information. And then he gets into some of the, you know, the the tangible feedback itself. And there were very specific questions, but there was very a very obvious kind of trend that came in terms of what users appreciate the most about data dot table, and it's something that actually brought me to some use of data dot table in my early days of our programming.

That is performance. It is very memory efficient. If you've been down the road of having that massive CSV or other text file and having the base r, read dot CSV, crash your r session because of memory limits, well, data dot table has always been very efficient in this space. And when people need speed, they turn to data dot table more often than not. And then another positive feature, which ironically has another side to the coin to bear on your perspective, is the syntax of data dot table itself. I think, Mike, you and I agree that it is very unique in the syntax as compared to other frameworks in the R language.

But when you invest in that DSL, if you will, you can accomplish a lot in a pretty concise way. As for me, I'm just not a regular Data. Table user. So I do identify with some of the feedback that we're seeing in this post of those in the community having to look it up most of the time to figure out how to do certain operations. Again, there is some great documentation out there. It's just for me, not muscle memory yet of how to implement the syntax. So, again, it's good to see kind of tangible data showing these different trends across a different spectrum of user bases here.

And, overall, it looks like people are pretty satisfied with the with the package itself. Again, not everything is perfect. Again, performance is becoming one of the most favorable areas. But then you might see some, you know, some not so great issues as well. In terms of desired functionality, there were some feature requests out there. And, Mike, why don't you take us through some of what the users are kinda hoping for in the future in data dot table?

[00:20:03] Mike Thomas:

Yeah. Absolutely. You know, I think one of the the most insightful charts for me in this blog post is this importance versus satisfaction, plot, which is really interesting. And I think just to to highlight and to sort of summarize, the the feedback from the community, you know, the the sort of data point here or feature of data dot table that had the highest importance rated with the highest importance and had the highest satisfaction, was performance. And then, you know, lower on the important side, but high in satisfaction was the the minimal dependencies, which is absolutely a strength of data dot table.

And then higher on the importance, but lower in satisfaction, so I think these are things that, hope, you know, respondents are hoping that data dot table may improve would be, you know, the docs and the legibility of the syntax itself. So in terms of that desired functionality that they're talking about, one would be support for out of memory processing. So I think this is something that, you know, has has come to light, especially with, I believe the arrow package. Does that do out of memory processing?

I believe so. Yes. Okay. So that sort of allows you to operate on disk, operate on the file on disk without bringing it all into memory first. You know, folks are looking for richer import and export functionality with parquet sort of being the the most commonly mentioned, item followed by good old xlsx format. And then, the last We can't escape the spreadsheets, can we? Oh my goodness. And then the last, piece of desired functionality that they have listed here is integration with the pipe operator. You know, which also lined up with, you know, how some of the questions around how, much folks are using the pipe, and I imagine they're talking about mostly the the native pipe here.

Most respondents here or or the majority of respondents are responding that, the pipe is is very useful to them and they would find some sort of a convenience function for using Data. Table with the pipe, to be very very helpful. And then there is this notion of the I don't know how these things get these names, but this is the name that's been around for forever. But the the alias for the walrus operator, which is just a colon followed by an equal sign. And I guess the that sort of lines up with data dot table's mascot. Right? It's a walrus?

That's right. Yeah. Let's go synergy there. Yes. So I I think folks who were looking for maybe a, you know, a more plain English, alias for that operator, with some of the options being either set, let, or set j. And set seem to be, you know, the function name that would provide an alias for that Walrus operator to be the most, popular response there. And then sort of the final chart in the way that, this blog post starts to wrap up is on the topic of actually contributing to Data. Table and to gauge folks' interest to actually contributing to the project or their contributions in the past.

I guess not surprisingly, you know, spreading the word about data dot table and and just reporting issues were sort of the the top two responses in terms of what folks, would be interested in and then what maybe they have the capacity to do, you know, followed by actually contributing to the code base itself. So, you know, some users, I guess, in conclusion, are are are a little worried that the package may be abandoned or stagnating. One thing that I would wanna say that, you know, I've seen on social media before is, like, you know, this is the next iteration of Language Wars. It's now, like, oh, dplyr or Data. Table and, you know, you have to be in one camp or the other. And if you're in one camp, you have to not like the people in the other and vice versa. And I think that's that's absolutely ridiculous. I hope that that doesn't really exist.

And I would say that, you know, like anything else, it's amazing to have options and use the tool that that fits your use case and fits your comfort the best. You know, data dot table is fantastic if you wanna use it. If you wanna use dplyr and and Arrow or, you know, DuckDV, you know, you can use that too. So, I I think as long as the community continues to to rally around the package, and I think initiatives like this one here to try to get feedback and to understand how it can be improved will go a long way towards the longevity of data. Table as well.

And I know that they have done a lot of work on this package around documentation and community, just in the last maybe 6 to 12 months. So excited to see these results, and, I I think the community is strong.

[00:24:58] Eric Nantz:

Yeah. And lots of positive momentum, like you said, this year with some of the steps they're taking. And not that it was very whacking in any way, but this is you know, as as open source projects evolve, you do often have, you know, newer contributors or newer users come on board and looking at what are the available options for, say, data processing, data manipulation. And it's always great to have choice in this space. I know sometimes in my industry, there are some people, they get a little, you know, maybe confused about having so many choices in domains. But you know what? For your specific project, if data. Table fits your needs and, boy, I remember many days of importing some huge textual, you know, biomarker data files and data. Table was as fast as could be in that space. And, yeah, we have lots of great code bases that leverage that package heavily. So I'm always of the mindset if it ain't broke, don't fix it. And then, also, with respect to data dot table maintainership, yeah, it is alive and well. They are really spreading the message out for various channels, and this survey should serve as a reassurance to everybody that they are really thinking of the users in mind, both those that have been using data dot table for years upon years and those that are coming new to the project because they are both equally important in the lifespan of this space.

And, certainly, I'm really appreciative of the transparency, and I'd see nothing but great things happening for the project going forward. And the fact that they're sharing this more actively, I think, is a is a huge step, to bringing this, not that it wasn't a first class citizen before, but really putting this into the mind share of most of the R community. I think data dot table, the project itself is doing great things to make that happen.

[00:26:47] Mike Thomas:

No. I I agree as well. Lots of positive momentum, lots to look forward to, and in no way is this project doomed.

[00:27:05] Eric Nantz:

Well, luckily, Mike, we're not doomed in terms of the rest of this episode because we do have some fun things to talk about here, especially on the visualization side of it. But, of course, you listening, maybe you're wondering why the heck are we talking about doom and gloom here? Well, we're not referencing that kind of doom. We're kind of referencing some that did, be a part of my retro gaming heart mech in many, many years ago in my college days, getting together with some friends and playing the heck out of the doom, game by id Software that was often a trendsetter for all these first person perspective games on here. Now just where does this have to do with r itself? Well, our last highlight has kind of done this very interesting geometric type exercise for just how do maps could be created in the context of R itself in the aspect of 3 d style visualizations.

Now Mike and I had to do a bit of detective work on this, but we're pretty certain that this blog post has been authored by, Ivan Krylov. But we admit we could not find any trace of that on the blog post itself. We did some spoofing on their GitHub repo. So, hopefully, we're correct. One way or another, we're gonna go with that for now unless we hear otherwise. But Ivan leads off this post about talking about when would you want to visualize in a 3 d type landscape a function surface. So you may be thinking, if you had experience in this space, kinda like a contour map where you see, like, the elevation in a in a map setting. In fact, it reminded me of a lot of the packages that have been developed such as ray shader and ray render and the like have been doing a lot of those 3 d visualizations in R itself.

And guess what? Base R itself comes with this built in. Especially if you're using the extension packages like Lattice. There is a way to do contour plots in that. The RGL package in the R community helps you do 3 d plots in R. But, you know, we could he he thought we could just do that, but let's let's make this fun. Let's make make a do map out of it. Now I've only seen the end product of a do map, but just what does that really entail? Well, Mike, we're going to geometry school for a little bit on this one, so buckle up here.

But, apparently, in the first and second iterations of doom, there was no concept of a floor that could go up a hill or down a hill. So, apparently, you would have, like, the sky for height, you know, but then you'd have your tiles at a certain level, maybe done at another level in a stepwise fashion. And, of course, R itself in terms of how you would visualize this is not gonna be coming with everything out of the box. So there are some open source utilities, called Zdoom and Zanodrome, which are apparently gonna help with the overall visualization of this before we feed into it in the r itself. But here comes the geometry, school at play here that a do map is gonna have a series of points or vertices, lines, sides, and sectors.

And, yes, there are obviously point coordinates for the vertices of an x and y, and you got lines connecting them. And then you've got the sides that are available to the user when they look left or right. And then, also, there will be textures, but that's not really the point of this post. And then where the actual how the height information is presented, and those are called sectors. So Ivan's original idea was to start with the contour lines package or contour lines function and then tried to kind of makeshift some, you know, artificial slope involved to get to the heights of this. But, apparently, it didn't quite cut it where the editor was trying to fix some things that were missed in the translation.

So he kinda had to go back to the drawing board and start to go with something more universal with respect to doom maps, and that is literally called the universal doom map format, also known as text map, where then it can store the additional information of the heights of these points and not just the x and y coordinates on kinda like the lower plane, if you will. And then it gets to be really math heavy or geometry heavier because, apparently, you need to be able to split these maps of the height into triangular shapes.

We're literally and the blog post has this, a great illustration of splitting a rectangle into 2 triangles of equal area with the vertices, you know, interpolation along the way and then become some clever use within base r of the array function, capturing data frames of these x, y, and now z coordinates that capture the height of the contours of these planes. And then a lot of more manipulation to start to figure out how do we connect all this together. Lots of custom data frames being created here. Lots of other temp files being created here for that mapping utility.

And then once he's able to feed in these variables into the mapping software, yes, at the end, you have yourself a doom literal doom screenshot of he fed it into this open source utility that I mentioned earlier. I think it was more more manual processing of another utility called SLADE. And sure enough, there is a a reproducible rscrubber. If you have that same map emulation software, you do get a shot. The player looking at a contoured hill with looks like from the game itself, like, I wouldn't be able to tell the difference. Like, you're some overworld type area. So I admit I have never thought to try anything like this, but guess what? If you wanna try this out with the right software installed on your system, the our script is downloadable.

You can check it out yourself and give it a shot. And, yeah, maybe it's a great way to boost your geometry and mapping skill set at the same time and having some fun along the way. So, hopefully, Ivan, we're getting your name right here, but, thanks for opening our eyes to use of r that I never thought I'd see happen in my lifetime. But guess what? There's nothing that r can't do. Right, Mike? Absolutely. And it's incredible how much of

[00:33:50] Mike Thomas:

what's generated here is from base r's plotting functions as well, and just, you know, sort of vectors and and things like that. If you download this r script, that's linked at the end of this blog post, it's it's fairly concise, I think, you know, what's necessary. He has these 3 different functions, triangulate as text map, and then the final one, image to doom, that spits out a file that I believe you can pass to this software slate or something like that that'll help generate, this exact image that we're seeing on screen.

Fairly concise. It's a really cool, you know, I just I'm really enjoying reading the code here. I learn something new every day. Today, I learned that there's a function in r called is dot unsorted to test if, the vector that you pass to it is sorted in ascending order or not. I'm not sure if I have any use cases for it, but I'm certain that probably sometime in the future, I will. The code comments are are incredible. He has an a beautiful actual diagram in the code comments here, plotting this coordinate map.

Just just literally using comments and characters on your keyboard. That's absolutely fantastic, and it lines up with, the diagram that's in the blog post under the triangular sectors, section. So, you know, really interesting use case. I'd be interested to see sort of how maybe you could take this to the next level with RayShader, and then, you know, maybe make doom look like it's in the, you know, 2023 sort of graphic state. You know, Eric, to be honest, I don't wanna date you, but I'm not familiar with doom. Halo was probably my my first, you know, the foray into first person shooters, if you will, on the old Xbox 1. And, even all the way back then, I think the the graphics were a little bit of a step up than, than than what we have in Doom. But, you know, I'm sure I I'm sure I would have enjoyed Doom if if I had been there.



[00:35:53] Eric Nantz:

Yeah. I think I dare say I would have. And, that if if if that was dating me too much, and I better not mention Wolfenstein because that even predated Doom and that, Id's first entry into the FPS space that kinda changed the world. But, yeah, if you're you talk about going step back in retro graphics. Yeah. That one's a bit hard on the eyes. But, but, yeah, we've actually seen very interesting use cases of games like this where maybe it's not so much the actual end product that you can get, but they are extendable via mods and things like that. And that's where having code like I believe the doom code's in the public domain now. So, like, you could literally browse this yourself, and, hence, you see the modding community go to town on things like this. But but, yeah, I I definitely got the same same vibes as you did, Mike, about how you could combine this with some of the awesome work of, like, Ray Shader and the like to really beef up a a fun demonstration that's built entirely with R itself.

But, yeah, I did take a look at the script. Like you said, that's available for download. Very well commented. And, yeah, easily reproducible with the right software. So I think this is, again, if you thought R wasn't able to do certain things in terms of visualization that combines with retro gaming, well, this post has definitely solved that for you. Yes. I'll have to check out Ivan's previous post because he's definitely got a a great selection of additional topics with respect to, you know, integrations with c, looks like, others on on contributing to r itself. Yeah. Lots of great nuggets here, and, I'll definitely keep this bookmark.



[00:37:33] Mike Thomas:

You know what? You know what I always say? R is the 2nd best language for doing just about anything. Conan said it better myself.

[00:37:40] Eric Nantz:

And maybe And what may be the 2nd best resource for everything in r might be rweekly itself because we have a mix of everything as well from highlights of what we talked about today, our awesome interesting, use cases via blog posts, tutorials, new packages and updated packages, and the like. So we'll take a couple minutes to talk about our additional finds here. And for me, this isn't so much our specific, but we alluded to it earlier, Mike, that it is conference season. It's starting to get underway with various conferences out there.

And maybe you, are like me, especially in my earlier days where I would go to these meetups for the first time, I'm a bit of a shy dude, I must say. So, you know, what's the best way to kinda feel comfortable and, you know, ways of connecting with others? Well, my additional find here is from the jumping rivers blog authored by Rhian Davies and Keith Newman called an introvert's guide to networking at a conference. So this is very nice very nice way to kinda ease that, maybe, that little fear or apprehension you might have at the beginning of these events and how you might navigate certain situations, how to keep contact with people that you do end up networking with, you know, what are some ideas for icebreakers and whatnot, And not to feel too much pressure if you're being sent on behalf of, say, your organization that you're a part of, but really is trying to soak in that experience in an optimal way. So, yeah, I I definitely resonate with a lot of these points here. And, also, I'll mention a a a heads up that we often hear at the various posit conferences is the idea when you're in a group setting, having the Pac Man rule, having, like, an open slot so that people can join your group to to join in on the discussion.

Things like this with practice really do add up and help make you feel a lot more comfortable. So really great post by the jumping rivers blog, and, yeah, it'll be hopefully a Pazitconf would be my next, in person event, and I'll be taking this to heart like always.

[00:39:44] Mike Thomas:

I like that one a lot. Another one that I found was from El Saman on the key advantages of using the key ring package. And the Keyring package allows you to essentially store secrets, that are retrievable, I think, through environment variables would be the most common way to do that. And, you know, one of the differences between using Keyring and and maybe using a dot r environment file that would be, like, project specific is with Keyring, you can store that particular secret once and for all per computer that it's on, which which is nice. You know, you don't necessarily have to do that on a project to project basis.

You also do not have to worry about somebody accidentally, forgetting to git ignore that dotrEnviron file and it making its way up to, GitHub or GitLab or whatever sort of hosting service that you use for your git repository. So that's a nice feature as well, that you may be interested in in leveraging as opposed to sort of doing the, the old hard coded way with, you know, sys dot, get env and, setting environment variables that way. So it might be interesting for some folks who are are looking to, brush up on their best practices around security and environment variables and passwords and secrets and all that stuff.



[00:41:04] Eric Nantz:

Yeah. This is terrific when you're using R and, like, a traditional client kind of setting where you may have a a team using RStudio IDE or whatnot on your local machine. The key ring package is gonna be instrumental to helping, like you said, keep some of those credentials secure and not nag them all the time for it and minimize the potential for leakage. Unfortunately, I don't think this would be a way this would be compatible with, like, a Shiny app that's deployed on a server somewhere, but I will have to look into this a bit more because I know the Keyring itself is used every single day. I go on to my Linux system here at home. I often have to prompt once for my administrative password to do a certain task, but that's being stored in the Keyring credential store and not anywhere else. So lots of lots of ways that I'm sure this could be used, that I'm probably not even aware of. So great find as always.



[00:41:58] Mike Thomas:

And then we have one more that I think we would be remiss not to mention, at least to give a quick shout out to Bruno Rodriguez. We are at part 10 of reproducible data science with Nix, and the discussion here is on contributing to Nix packages. So if you have been following along with Bruno's saga, and crusade on getting folks to check out next next for for doing reproducible data science and having that that fully sort of reproducible environment, that you can come back to, you know, years from now and run your code, and it'll still output that same thing.

Check out part 10. It's the latest in the series, and it will not disappoint.

[00:42:38] Eric Nantz:

Yeah. It goes so nice. So we talked about earlier with the idea of patching such an influential project as the R language itself. But guess what? Yeah. NIX, the momentum keeps coming. And, yeah, I was even doing a little poking unrelated to NIX itself when I'm then continuing my efforts with this shiny application as a web assembly bundle for my R Consortium work. And I'm poking around the WebR repo, that George Stagapos has been working on. And I see a commit saying they've made it or I should say it was Shiny Live, Shiny Live for R. I see a commit that they are making things compatible with Nick's packaging.

So plot thickens. It seems like more attraction's happening with respect to the the big players in the art community itself with Nick. So, yep, Bruno, I'm really excited to to not sure if your series is ever gonna end, but I'll be bookmarking in one way or another.

[00:43:34] Mike Thomas:

I hope it doesn't. That's awesome.

[00:43:36] Eric Nantz:

Yeah. There's much more than just that in this week's issue. Again, tremendous fun curating this for all of you. And, thanks to John Carroll again for his awesome utility. We call it the Curinator to help boost drive some of these feeds for us in a more systematic way with GitHub Action. So thanks, John, for making that for our curator team here. But, of course, our weekly does not move, does not live without all of you in the community. For your contributions, we invite you. If you see a great blog post, a great new package, or a great new tutorial, and you want the our weekly audience to see it, well, we're a poll request away talking about contributing. Right? You won't have to dive into any internals of R itself to do this. You just have to go to rw.org.

There's a little handy link to the draft right at the upper right corner. You can just submit a poll request with your markdown link all formatted for you and all set to go. That's a great way to contribute to the project. And as always, we are looking for curators as well. If you wanna sign up for that or get to know the process around that, we also have links directly linked at the top of each issue. Probably you can get involved with our weekly. And then, also, we love hearing from you and the community. We got the handy contact page and the episode show notes of this episode as well as with a modern podcast app like Pawverse or Fountain. You can send us a little boost along the way directly in your app to give us a little fun along the way, with with all of you. And then, also, we are sporadically on these social medias.

I'm more often on Mastodon these days. We're vet our podcast at podcast index.social. Sporadically on the weapon x thing, we've got the r cast. And on LinkedIn from time to time, cross posting the episodes and chiming in from time to time with some fun art projects.

[00:45:30] Mike Thomas:

Mike, where can the listeners get a hold of you? Sure. You can find me on LinkedIn if you search, Catchbrook Analytics, k e t c h b r o o k. You can find out what I'm up to. Or occasionally on mastodon as well at mike_thomas atphostodon.org. And, I guess a little episode cleanup, quickly, around 2 things that I had mentioned. I I think I did shout out at some point in the podcast, don't write tests. Please write unit tests. That was, course, satire and a joke. And secondly, I think I may have said that R is the 2nd best language for for doing just about anything. Obviously, it's the first best language for doing just about anything. So little clean up there.



[00:46:11] Eric Nantz:

I think it's implied, but, you know, it never hurts. Right? And, yeah, I expect the transparency on this show. Yeah. So we fully appreciate that, Mike, as always. And, yeah, I'm about to probably go through some more react shaving, if you will, of an internal project. But just as I think I'm at the finish line, I'm probably gonna find something else to to buy my time with. But, yep. Thank you as always for all of you around the world for listening, and we will be back with another edition of ROWG highlights next week.

"
"64","issue_2024_w_09_highlights",2024-02-28,46M 8S,"Flipping a Hello World function on its head, assorted improvements landing in ggplot2 3.5.0, and why authoring beautiful code is so worth it. Episode Links This week's curator: Jon Carroll - @carroll_jono (https://twitter.com/carroll_jono) (Twitter) & @jonocarroll@fosstodon.org (https://fosstodon.org/@jonocarroll) (Mastodon) HelloWorld(“print”)…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 154 of the R Weekly Highlights podcast. This is the weekly podcast where we talk about the latest and awesome resources that you can find every single week on the latest our weekly issue. My name is Eric Nantz, and I'm so delighted you joined us today from wherever you are around the world.

[00:00:21] Mike Thomas:

And I never do this alone. He is my line mate and tag team partner here, Mike Thomas. Mike, how are you doing today? Good. I like that hockey reference, Eric. I have been living in the terminal for the last couple days, so I'm going to crawl out of the terminal here for a few minutes and, excited to get a little higher level with the highlights today.

[00:00:41] Eric Nantz:

Yeah. I've been in the terminal myself. Fun little, I don't wanna call it a hack because it's a legit tool. But, I was getting jealous of some of these really fancy Git GUI interfaces I often use locally. Shout out to the GitKraken project. That's one of these. Can't really install that on my, my company's HPC infrastructure. So I may put this in the show notes just for kicks. Found a terminal based Git tool called lazy Git. It's not lazy. It's really powerful. And it's written in Go, actually. But that's my end cursor's Git interface, which has been super smooth for me. So if you if any of you out there are a need for a great kinda terminal git experience that gives you that great overview of, like, branches, your staging area, commit history, It's all right there. So, shout out to Lazy Git. Fun project.



[00:01:39] Mike Thomas:

No. That's a great shout out. I I love the the Git GUI clients or or sort of anything that tries to help make it a little bit more manageable than it is. Understand that there is need, right, to go straight to the the git bash shell, once in a while for doing in particular things, but I I think sort of in general for 99% of my use cases, it helps to use something that's a little more gooey to help you avoid making Git mistakes, which can be hard to undo.

[00:02:08] Eric Nantz:

Yeah. No hope. Don't get me started. I had to do undo a lot of nonsense the past couple weeks in one of my repos, but I digress. Only we can undo our recordings of this podcast. We gotta get our act together, shall we? You might you might dare say it's showtime, folks. But, yes, this, issue this week was curated by John Carroll who is, another longtime contributor and curator for our weekly. And as always, he had tremendous help from our fellow Rwicky team members and contributors like all of you around the world.

Now we're gonna lead off here with a post that's gonna flip a lot of your assumptions perhaps on it on their head, so to speak. Because our first post here comes from June Cho who is a PhD candidate in linguistics at the University of Pennsylvania and has often been at the cutting edge of going not just a little bit into r, but really deep into the fundamentals of r itself. And, boy, this one is if you wanna go deep in how functions are composed, this is for you. So he leads off with a typical premise that when you're learning any language, typically, you're gonna do the infamous hello world type example just to make sure things are quote unquote working.

Well, apparently, there's been some, over the years, albeit I'm not seeing this until this post, there have been some pretty, adventurous developers out there for various languages that might play a little trick on your mind by not just having a function that prints the 10 stacks hello world as in a typical print call in things like Java or or other languages. But instead of having a function literally called hello world, putting the the string of print in it, and it still somehow prints hello world. Like, what is going on there? Well, apparently, there's a lot of ways and multiple languages to kind of flip the concept of arguments and functions.

So June explores in this post, what can we do with the R language in this in this case? Well, in order to get there, you gotta learn about some of the, self described quirks in the R syntax that you may not see until you really dive further into it. Case in point, anytime we define a function, he has some examples here of, like, summing or adding numbers together, It first needs to see that that is represented as a expression or not. And if it does, it needs to determine if that value is a function or not.

And how does it know that it's a function or not? Apparently, there are very intricate orderings here with respect to evaluating the scope of this in terms of these language objects. In a language object, if it is a function, it's always gonna be first in line. And he has an example where he literally goes to this expression, finds the first item of it, and, indeed, it is the function that's being wrapped into that. And, of course, to review, even the operators you see in r, like the plus, multiplication, etcetera, those are all functions under the hood. Right? So they would be first in this stack of the language object.

Once you know that, you can now start to do some crazy stuff with actually flipping the order of this and superimposing different different ways of architecting this. And this is where you need to dive into some concepts that scared the heck out of me in my early days of R, and that is deparsing. And also a new function, not, I mean, new to me, I should say, the sys.call function which can actually find where the which returns the expression of a function of where that call was taking place. And, again, we're gonna try to explain this at a high level, but, obviously, look at the post for the detailed examples here. But then he shows how to actually get these functions from these syscalls and what is actually returned inside of them, which, again, first align is the function itself, and then the second would be the arguments being supplied to it.

So once you have that, you can now start to do a little bit of flipping of that order. And instead of having the typical print hello, world, you can have the hello, world with the syntax of print, and it's still gonna give you what that output of that function would be in ordinary language. Now that gets it gets even more bizarre here, bizarre to me anyway, because, again, I haven't dived this much into functions ever. But you can also write wrappers around this to do this with any function, not just a manually specified like hello world printing.

He has an example register function that he defines where it's going to dynamically grab the name of that function in this language object calling stack, register a new function with that name, and then basically in that cut function environment, now give you that alias to, again, flip the argument and function on its head. And then lastly, in terms of where I see this, you can go back to the other way and make it the typical print hello world, which is called unflipping. Again, some clever use of the substitute function to make that happen. He has an example here called unflippery. He shows how to reverse this kind of bizarre sequence so you can get back to what you wanted to do with a call statement to get there.

But, yeah, if you ever wanted to know just how far you can take this reversing of function arguments and function calls themselves to mimic what you often see in the other programming languages in terms of these thought experiments of just how far you can take it. June's example, again, fully reproducible. You can run all this in your console and inspect these language objects, these calling stacks, and just how these functions built into r, like matching function calls, finding the system call itself, and then clever use of the eval and substitute functions to kind of change the ordering of things.

This can this can be pretty powerful, albeit This probably could be a great fodder for maybe an April fools joke someday for somebody not knowing what to expect out of your package functions. I don't know. I'm just saying we're still out of April yet, but may y'all keep this in mind for some good time, pranks in the future with my art friends.

[00:09:08] Mike Thomas:

I would have to agree, Eric. If you do prank me with that, I I can't say. I'd be be laughing too hard because some of this stuff is fairly convoluted. I think some of this can can trip up beginners as well, and there's probably some fair critiques of the R language, you know, for for newcomers who who might get tripped up in some of, you know, that this meta programming and and real quirks about the ability to sort of program on the language itself. And like you said, I think it is important though for for anyone, using R, probably experienced developers, maybe more so to to read a blog post like this and understand these different things. You know, I think it's very important to to understand that your operators are functions in and of themselves.

It was a refresher for me. I I think maybe something I knew at one time but happened to forget, that you could wrap the function name in your console in quotations and run that and it would still return sort of what you would expect. So I ran, you know, some 4 and then I I wrapped some in quotes, double quotes, and and ran that again and it returned 4 again. And, it was just a little little bit of a shock to the system to to recall that, you know, that is is possible. You know, I think we see a lot of code sometimes with with folks using, like, the the get function or assign or manipulating, the environments that you're using, you know, within sort of beginner r code. And I think that stuff can be pretty powerful, but it can can trip you up if you are trying to to build, you know, our software that is going to go into production somewhere or is going to, as you may, lament with Eric, you know, run through a GitHub action that that may treat environments, you know, a little bit differently than what you have going on on your local machine. Sorry to sorry to dig that that stuff. Bad memories. My bad memories.

But, this this also reminds me of the the advanced r book, which I think would be a nice complement to a lot of the content in here. There is a chapter in there called metaprogramming that runs through, you know, the big pictures there, the important concepts, expressions, you know, quasi quotation when we think about nonstandard evaluation and our and our sort of ability to do that, which is is unique to R in a way that I believe is not really possible in in Python. And, you know, a lot of these different quirks that you have to think about when understanding and working with, this type of functionality.

So really, really interesting blog post, you know, I I think really creative examples here by June to to just show us how some of these internals work under the hood.

[00:12:01] Eric Nantz:

Yeah. You'll definitely want your r terminal side by side as you're as you're reading this and kinda try this out interactively. Boy, I wonder if this could be augmented with Quartle and having that fancy evaluator inside, but I digress. But in any event, evaluator inside, but I digress. But in any event, one way or another, you'll wanna practice this if you ever wanna see this in action. Because someone like me, I definitely like to be hands on when I'm learning these concepts. So I would I would definitely have my fancy terminal side by side as as reading June's post. But, yeah, he's got a whole boat of of awesome posts in his, in his blog, especially around other areas of the tidy verse. He's been front and center. So definitely check out his his site, with his back catalog of really awesome explorations with the language, in more ways than one.

And in our next highlight today, we've got a a great, showcase of the recent advancements that landed in the latest version of ggplot 2. Again, one of the more fundamental pillars of visualization in the art language itself. Ggplot2 just recently had version 3.5.0 land on CRAN. And in this highlight, we got a terrific blog post from the tidyverse blog by one of the, I believe, newer ggpod 2 maintainers to an brand. I haven't seen his name before this. But, yeah, great, great to see this post here, and we'll walk through some of the the key features and and key improvements here. Leading off here is a very important infrastructure improvement to help bring the mechanism behind guides in ggplot2 to a little more consistency with other systems in ggplot2.

Mainly speaking that up to this version, the object oriented paradigm that the guide system was following was still using s 3. Well, now with this rewrite that they've had in ggplot2, 3.5.0, now guides are now being the system behind guides is now rebranded to use gg proto, bringing it in line with the other parts of ggpod 2 that have been used in heavy customizations, such as, you know, layers, facets, scales, and whatnot. Meaning that now the door is open to treat new extensions on the guide system just like any other extension that we could do in this space. So I think this is gonna be, hopefully, a launching point for others to make even more customized versions of the guide system as they see fit in the ggplot2 landscape. So really nice to see that consistency being brought in from a back end level.

And speaking of visuals, with ggplot2, a lot of the graphs these days are making heavy use of gradients and patterns in their visualizations. Well, now they are first class citizens in the ggpa 2 ecosystem with respect to new functions that are that can be used such as within the fill argument using the patterns argument. And you get being able to tap into the grid system where it has built in functions in the grid package called linear gradient, radial gradient, and others, within the pattern function. So you can now have those really nice looking gradient bar charts, gradient backgrounds for scatter plots. This looks really sharp and not just gradients too.

If you have a pattern you wanna use to really distinguish that particular facet or that particular bar from the others, you can also leverage patterns using within the scale fill manual directive. That is really powerful stuff. There's some great examples in there of a bar chart that has multiple patterns inside to really, you know, really catch your eye, so to speak. But looks like what they've done is some really important improvements to how the alpha aesthetic was being applied to this situation. And that was a hurdle they had to overcome to make all this happen. So lots of great improvements there for even more custom visualization for how you display colors in your ggplot graph. But, of course, there is much more to this.



[00:16:46] Mike Thomas:

Yes. There is, Eric. You know, in terms of the scales, you know, ggplot has has changed how plots interact with these variables created with the I function, which I believe is is from base r, and it creates this this class, if I'm not mistaken, or or a pen prepends the class as is to the object's class. So, what this allows you to do from from my interpretation of the blog post is to be able to prevent, you know, some of the clashes that happen, when you are introducing, you know, for example, here, like, an additional scale on your plot. So one of the, examples that they give is if you have, you know, sort of 2 calls to to Geonpoint 2 to Geonpoint layers here on your ggplot, you know, just using the empty cars package and, you know, for one of those layers, within your aesthetic, you're you're setting the color as a variable within the MT cars, dataset, the drv drive variable. And then you wanna layer on a second point on top of that that that's going to serve as as, like, a circle, around the dots from the first layer, and you want those colors to be, you know, some predefined string of colors like red, blue, green that you had set. You know, previously, this would you would actually run into an error here and it would not be able to find, those colors for your 2nd layer, but now, if you leverage that I function around the the variable that holds the string containing your colors, you'll be able to add the circles around that or or add this additional layer aesthetic, without that clashing with the guide, with the legend that was developed in the first layer at all. So we're on a podcast trying to describe DataViz again, Eric.

The best way to best way to check this out is certainly through reading the blog post. And another sort of improvement here around ignoring scales is the ability within the, ggplot annotate function to add some text in specific locations, that that will not clash against multiple annotate layers, again, leveraging this this as is function, this this capital I function, that allows you to have sort of greater control over where you wanna annotate different, text overlaid on different parts or layers of your ggplot chart.

So lots of great code examples here. I I think sort of the best way to dive into this content and these improvements, which I think are are mostly subtle, and may not affect most of your day to day work within ggplot, but the the best way to do that is definitely to take a look at this this blog post. Take a look at the code snippets and and see, how these may relate to your Dataviz work on a day to day basis right now with ggplot.

[00:19:43] Eric Nantz:

Yeah. I'm I'm definitely seeing, especially towards the end, these examples, I think, have taken inspiration by the community itself, large with ggplot too. Some of these features that I think have been exposed in additional packages are now coming into ggplot2 proper. You'll see kinda towards the end of the post some new ways to angle the orientation of labels on on your various, point annotations. I have an example with the empty car set and flipping the annotations sometimes with, like, 45 degrees or or or less.

And then others, being able to do some padding around the labels too. Again, that's really neat. I think I've seen that in additional packages. And, yes, certainly, those that have been creating those fancy violin plots or box plots, in general, and have been really frustrated of how to deal with outliers efficiently. Well, guess what? Now geomboxplot has an option to remove outliers entirely or just the outlier is a false directive. Very nice. Very nice. But you can still just hide them with sending that outlier shape of na. So you've got you got you got multiple ways to handle outliers. But, again, it's great to see if you just wanna wipe them out, you wipe them out. So really nice improvements to the, ggplot to, box plot directives. But, again, lots of looks like very nice improvements, and it sounds like they wanted to do this more incrementally. But it just so happened that a bunch of these improvements landed in 3.5.0.

But again, we're all to benefit from it. And I always like to see at the end of these posts on the Tidyverse blog and others from Pawsit, they always make a point to recognize all of those that have contributed to this release. So you get all the GitHub handles, of the numerous contributors to this particular release. But, again, congrats to the team, and look forward to putting this, in the production for my workflows very soon.

[00:21:43] Mike Thomas:

Yes. And you could be included in that list of acknowledgments and famous if you, find, you know, even that for the smallest use case, a grammatical issue in in a vignette or some documentation as well so always feel free that that you can contribute to open source and there there is no pull request too small in my opinion

[00:22:17] Eric Nantz:

And rounding out our highlights today, we've got a really fun post here because Mike and I have dealt with this in various ways in our respective workloads, making sure that the code that we personally are writing and the code that we have with our collaborators into a more, you know, central project. Then we're kind of on the same page as the cliche goes. But there are ways that you can make sure that that is easy to opt into. And in this case, we have a terrific set of resources and narrative from our last highlight today.

A blog post from the esteemed rOpenSci blog. Not one, but 2 authors here. We got Mao Salmon who again returns back to the highlights yet again. Her her streak continues. And also coauthor with Ioannina Bellini Salbin, who is a community manager at rOpenSci now and very, frequent contributor in the open source community space and data science space. And they have this awesome blog blog post titled Beautiful Code. Because you're worth it. No. Don't don't get worried, folks. We're not we're not getting sponsor from a certain fashion company. But I digress.

Let's dive into what makes beautiful code in the minds of my own. I mean, Yanina here. Well, let's start off with spacing. And, you know, this is something I have to I have to have a little confession here as I read through this the first time is that it's it's one thing when you see in the example here, you've got inconsistent use of spacing between operators, maybe between arguments or, you know, separated arguments and whatnot. And, yeah, that that can that can just be a little bad UX, so to speak, as you're reviewing that. But having the unified system for how you're treating both space between function parameter names or operators after the function call and this indentation on the new lines, that's hugely important for readability.

But I I admit they also have great advice too of not necessarily putting, like, a new line between all of your declaratives. And I've been kinda guilty of maybe putting too many new lines between my various function calls, but instead to try to group them in kind of related chunks, so to speak. Whereas maybe you have a tidyverse pipeline ish, you know, syntax, and you wanna keep, like, a lot of that data manipulation in one concise area, then you maybe break it up with another part of your function operation and you're doing a new operation.

A lot of times in my shiny ass, I would kinda break things up maybe a bit too much. But again, I think it's not so much what's right or wrong. It's be consistent. Be consistent with yourself. Be consistent with your main your collaborators. And I think then you're gonna have what they envision as well. Proportion code will be easier for reviewing, easier for debugging. And also, another trick that they recommend as well is maybe you realize you have a lot of lines in that particular pipeline. Well, there's nothing stopping you from having more fit for purpose functions inside that overall pipeline to help break out some of that potential long scrolling syndrome that you might have with these more of a reversed pipeline. So being able to leverage that mechanism is really important too.

But, it's not just about this, obviously, the spacing and the use of maybe fit for purpose functions. There are obviously other ways that you can have concise and beautiful code too without being in the code itself. And that we have to talk about comments now, Mike. What what do what do they say about comments here?

[00:26:14] Mike Thomas:

Yeah. So the the section title is not too wordy, just the right amount of comments, and they even link to a blog post on our hub called why comment your code as little and as well as possible. This is one that I am probably super guilty of as well just like creating sort of too much vertical space probably between, you know, different pieces of logic, within a lot of a lot of our code. And, you know, I I guess I have sort of mixed mixed feelings on on this. And I think, you know, the idea is to use, you know, very self explanatory functions, function names, or variable names where by just by looking at the code, you know, it's very easy to understand exactly what's going on.

I I think in a perfect world, you know, that that we wouldn't have to write any any comments at all because, you know, function names and and our variables would be so self explanatory. But I think we all know that that's just not necessarily the case. And I think, you know, this is something that I see a lot in a lot of the open source packages that that posit, you know, formally, RStudio has put out for years years years. And something that I probably need to adopt a little bit better, but it's, I I think the the concept of really only introducing comments when you think it's not necessarily self explanatory.

What's going on. You know, when there's an additional anecdote, additional piece of information that you need to provide on top of, you know, what the logic is doing itself. Because if if somebody wanted to understand exactly what was going on and didn't, you know, they could always dive into the the help documentation for each of those functions, to to understand exactly what's going on in it. As long as you're writing good descriptions in your roxigen comments above those functions, you know, defining what the parameters, represent and defining sort of the overall goal of the function and and what it returns, then I think there's a lot of good arguments there.

But again, you know, I would agree with you, Eric, that it's consistency would be key here. It's probably, you know, we're starting to get even more and more into sort of gray area when to comment and when not to comment. But if you can set, you know, some, you know, basic high level rules and decisions within, you know, your team about, you know, when to comment and when not to comment, and try your best to to follow those. I think that that consistency will will help your code base be more maintainable over time.



[00:28:51] Eric Nantz:

Yeah. And and I do admit being in industry versus releasing a package open source. There's, it's almost I got to be 2 personas in one of for a lot of my projects. The hear me out here. This may sound bizarre, but hear me out. Is that there is the purest in me that wants to make things as concise as possible from a development standpoint. I know the project very well. Right? I mean, I I built the package. I built this shiny app for five years. I I know the intricacies, but I need to think about, do I really wanna be the only one on this project to help maintain and help develop a new feature?

No. I want people from my team or maybe others in the organization to help me out from time to time. Well, sometimes the comments that I put in my package source, you know, functions and documentation alike, they're kind of serving another purpose. It's not just to highlight a particular idiosyncrasy or a particular area we need to be aware of. It's kind of doubling as a teaching mechanism too. Like, often in my comments, I'll maybe describe what it's solving, and then I'll put, like, a reference. And guess what? It's gonna probably be a Stack Overflow reference or or a blog post. You know? Just to get that get that in there right into the eyes of that collaborator is gonna help me. Yes. Ideally, that would all go in a GitHub issue or or a dev notes journal or whatnot.

But sometimes you gotta get strike while the iron's hot, so to speak. When you have a collaborator looking at your code base, maybe whipping up posit workbench or whatnot and looking at this code, you wanna put that front and center of, like, not just to be aware of the issue, but how did I or anyone get an insight into how to solve that? And a lot of times, I don't solve these myself. I've leveraged a vast R community that have treaded those waters before, maybe an API call or maybe other operations like that in the Shiny space. And I am not shy about putting those links to external references in the code base itself.

Again, I'm an industry. 99% of what I do doesn't see the light of day outside the firewall. So I wanna make sure that for future me and future collaborators, they have a better understanding of why that solution's in place. So that's my mini soapbox for today.

[00:31:17] Mike Thomas:

Yeah. And I think it even I couldn't agree more, Eric. And I think it even in a higher level, you know, just having that, you know sort of code style guide within your organization can go a really really long way towards getting everybody on the same page here. And you know, I think we can all agree on a couple of these last tips from Yael, Amael, and Yanini, on early return and and the switch function. So if you have a particular function and and I think Jenny Bryan refers to this as like, the happy path. If there is an if statement, it within that function, and if else, if you will, and you sort of expect most of the time for it to go down this first path, You can actually early return, have a return call, within that first chunk of your if statement, sort of assuming that it will never get to that that second portion most of the time. And that can save you a little bit of time, make your code a little bit more lightweight.

And you know, Erica, as you and I know, these these little things, you know, might save like a a millisecond, right, to to make a change like this. And it may seem like like not much. But if you consistently do this throughout your projects, you know, those little milliseconds can add up and and turn into an improved user experience and and that's regardless of whether you're developing a shiny app or if you're developing a, just an R package in general that others are going to be using. You know, I I think, you know, these little things, especially these early returns, can can add up over time. And then the the switch function is one that I don't see used enough. It is the definition of an oldie but a goodie, Eric. If you have nested if statements, if you have like an an if else, or an if, and then an else if, and then another else if, and then another else if, to handle all of these different cases of what this, variable could potentially take on for a value.

Please leverage the switch function. It makes it so much easier to define all of those different all that different case logic, for the different values that that particular variable can take on and it it looks much cleaner it's just much easier to handle so and that's a phenomenal recommendation as well because that is something that I do see time and time again way too often are these long lengthy nested, if else statements.

[00:33:48] Eric Nantz:

Yeah. I I, there's a lot of legacy projects where I fell into the if else else else if trap. And, yeah, I definitely need to refactor that the switch sometime. You know? It it it you know, I I was thinking as I'm I'm reading through all these tips. You know? There are some analogies you can make, especially as you're doing maybe some of these things you're just not as comfortable with because you didn't know about them in the first place. Like I like, what you're talking about, the early returns, the switch, and and a different naming convention.

Honestly, I think it's gonna it's gonna be a little hard at first, especially if you're, you know, you have old habits like I do. And I I see my old code bases from 4 or 5 years ago. I'm like, oh my goodness. What was I thinking? Well, this is similar to, frankly, keeping healthy from, like, a fitness standpoint. You may it may seem uncomfortable at first, but you build up. You build up. You build up. And then suddenly, the the next time you make that new shiny app, that new R package, even just that new set of functions you're gonna pass off to that colleague, these will be front and center. It won't be the old habits anymore.

Of course, easier said than done. Right? You gotta start somewhere. This is fresh in my mind because I'm refactoring a 7 year old package as I speak. And boy, oh, boy, were there some issues there, which is a great segue into kinda how this post concludes where, you know, occasionally, if you do have the time, and I realize time is hard to come by with a lot of our jobs these days, but taking a little bit of time to do what they call spring cleaning of your code, seeing what are some gaps that you can solve with the knowledge you've gained from hopefully reading our weekly and listening to this podcast or other ways of of learning about codevelopment.

And there's a link also in the blog post to about the how the tidyverse team is doing spring cleaning. So that might be some inspiration as well. And then, of course, take advantage of automation when you can. There is the lint r package. It's just gonna help you with things like the spacing issues and syntax issues that, again, can automatically point where these are so you don't have to manually scan it. This is fresh in my mind too because I was helping, do a little, new feature to one of the internal companies packages that my esteemed teammate, Will Landau, maintains.

And he built in winter checks in the GitHub action, and I forgot to run that locally. And I was like, oh goodness. I messed up stuff up. But it it pointed it to me. Then I ran it locally, got it fixed. Now they get that fancy green check mark in the in the PR check. So lots, lots of great great tips here for sure.

[00:36:28] Mike Thomas:

Yes. Absolutely. And I think if you're a manager, especially of data analysts or data scientists, try to build in. I know it's hard, but but try to build in time, at least once during the year to to take a day or or a couple days or a week even to to go through your repositories and take a look at that code and and see what you can do. I think they're calling it referring to it as spring cleaning here to to maybe improve that code styling or develop some refactoring to keep that code up to date, and keep it as maintainable as possible. As a as a a quick story, I have a former employer who, won't work with me on a on a project because of some code that I wrote 6 years ago that broke, I guess, internally recently. So they they they think that I'm a pretty terrible R programmer, because the code that I wrote 6 years ago is no no longer working there even though I've I've offered to help. So this is I'm not gonna name name any names or anything like that but, I I would just say don't don't be that person. Understand that that software needs to be maintained and managed and improved over time and, you know, don't judge somebody on the code that they wrote, you know, even a couple years ago because we're all consistently learning, improving, and and I don't even look like looking at the code that I, you know, wrote a couple years ago. So we're, but let's lift each other up here.



[00:37:56] Eric Nantz:

Exactly. All positive vibes. Yeah. We don't we don't need that. We could do a whole another hour podcast on that kinda issue. Trust me on that. But, you spotted someone else in here to post in this post, Mike, because I there's there's an opportunity for a a real, nice quote to live by here. Right? Oh, there is a quote that I absolutely love. I we need to get t shirts made up of this, Eric. I might get this tattooed on myself.

[00:38:19] Mike Thomas:

But, the the the line in here is the code you don't write has no bug. Unbelievable.

[00:38:26] Eric Nantz:

That oh, my goodness. Yes. We we need a shirt, whoever's listening out there. Yeah. Please please make this. We will take our money after you print it. We will buy it. That is, I love that line and it just speaks to so many aspects of my development life. Yeah. So there there's there's a boatload of additional resources that they link to at the end of this post. And, also, I have a link to the, one of the inspirations that it's supposed to begin with is that rOpenSci recently had their their second cohort of champions, onboarded and they ran some virtual workshops and some of those materials are online with respect to package development. So I'll have a link to that in the show notes too. And that particular external resource, wowed me for another reason, is that they also use the same Hugo theme that I did for an internal documentation site at the company about our HPC system. System. I was like, hey, I know this theme. That was awesome. So it's great when I feel like I'm I'm thinking similar to all these people I look up to in the community. That was just that was awesome stuff. Oh, that is awesome, Eric. You know what I think we should put on the back of that t shirt? I should, I think, you know, the front could say the code you don't write has no bug and the back could say, the code that an LLM writes for you probably does have a bug.

Bingo. We need we need a patent soon or or well, somebody's gonna take that run with it. Oh, goodness. Just kidding. Yeah. You know. You know. You know how it goes. But, we also know how it goes is that, yeah, the rest of the issue has a set of fantastic blog posts, new packages, updated packages, calls to action, you know, call to events, and everything else that you can find every single week at Our Weekly. So we're gonna take a couple minutes, tell us some additional finds that came our way that we we wanted to highlight here.

And, of course, me being an audio video kind of, you know, junkie, so to speak, with doing this podcast and other media ventures. This post here really hit home. There was a recent post, that I saw a Mastodon from Matt Crump about how he was exploring importing MIDI audio data into R for his cognition experiments. Well, he ended up using a mix of command line calls, the FFmpeg, which is kinda like the Swiss army knife, so to speak, of media, conversions and then another utility called fluid synth and some Python code, but, using a lot of shell commands. Well, your own ooms who, of course, is heavily involved with the infrastructure behind our OpenSci and the our universe project, decided to take matters in his own hands and decided to create a package called fluid synth to help wrap some of these system utilities for bringing in and parsing MIDI data. So if you ever find yourself having to analyze these and maybe use them in a data driven way and then also rendering that to an audio file, yeah, your Roam's package got you covered. So I have to add that to my toolbox amongst many other great utilities in the audio visual space in the art community.



[00:41:41] Mike Thomas:

That's a that's a super niche little, package there. I like it. I wanted to highlight a webinar series that's actually been going on through the our consortium, our insurance series. I believe it's hosted by, 2 folks at Swiss Re, which is an insurance company. Georgios Bacalukas and Benedicte Chamberge. And they it this video series looks fantastic. Eric, I'm just gonna walk you through the titles of the first few videos here. The first one is from Excel to programming in R, great content applicable everywhere.

From programming in R to putting R into production. Now, I know I'm getting you more excited. Oh, yeah. Our performance culture, and lastly, high performance programming in our so these are the 4 webinars that are now available through the our consortium's website. I'm not sure if they're going to continue to have more webinars or not. But if you are in the insurance space or if you're, into actuarial science, I would highly recommend checking out these webinars.

[00:42:45] Eric Nantz:

Yeah. What a what a excellent, you know, set of resources here. And I love the fact that they're being shared with others because I know that, you know, r is making big headways in the world of insurance and the world of finance and everything else in between. And, of course, I'm in life sciences, but it's great to see these tailored to that audience but with concepts that are most definitely universal to anybody in our respective industries because you gotta start somewhere. Right? More often than not, Excel is that window to data analysis that people use routinely and then be able to take that programming based approach with our boat tailored to that kind of audience going all the way to writing highly performing code.

Yeah. That's something that I am doing, trying every single day, and I can't pretend that I know everything about. So I'll definitely have to check these out. Looks like even they got wind of a certain project called Parquet. So that's really speaking to our eyes on this.

[00:43:44] Mike Thomas:

No. That that was my journey from Excel Excel guru,

[00:43:49] Eric Nantz:

into to R and changed my life. There's a lot more in this issue. Of course, we're gonna we're gonna have to start to wrap things up here. But, if you wanna get in touch with us, if you wanna help with the Rweekly project itself, that's always something we welcome. Whether it's your poll request, contributions, or suggestions, we're all just a poll request away to the upcoming issue draft, all linked at rweekly.org. That's where you'll find everything. I I have, inkling that the next curator could definitely use a bit of help if you get my drift. So yeah. Please send those requests to the project way.

And also, you can get in touch with us directly. A few ways to do that. We have in this episode's show notes a handy link to the contact page if you want to send us feedback there. You can also if you're on the, podcast 2.0 train with your modern podcast app, there's a boatload to choose from out there at podcastapps.com. You could send us a fun little boost along the way to give us a little message directly from within your app itself. Details on setting that up are also in the show notes. But, also, we are on the various social media spheres from time to time. I'm mostly on Mastodon these days with at our podcast, at podcast index dot social.

I will admit I'm a little late, replying back to Bruno's been checking in with me on my next journey. I I have some follow-up with you. It's coming soon. Trust me. But, also, I am sporadically on the weapon x thing with at the r cast. And lastly, on LinkedIn from time to time popping in with some announcements and episode posts. But, Mike, where can the listeners get a hold of you?

[00:45:27] Mike Thomas:

Sure. You can find me on mastodon@mike_thomas@fostodon.org, or you can check out what I'm up to on Catchbrook Analytics, k e t c h b r o o k.

[00:45:41] Eric Nantz:

Awesome stuff, my friend. And, yeah. We we had a we had a heck of a a kind of a therapeutic preshow session. You all didn't get to hear it. But, Mike, listened to my GitHub action rant that may be becoming a rep for x in the very near future so that I can talk about it here later. But in any event, I'm gonna get back to the old day job here. So we're gonna close out this, episode of our weekly highlights, and we'll be back with another episode next week."
"65","issue_2024_w_08_highlights",2024-02-21,47M 17S,"Putting those bike pedals to work with a comprehensive exploratory data analysis, navigating through a near-inferno of namespace and dependency issues in package development, and how you can ensure bragging rights during your next play of Guess My Name using decision trees. Episode Links This week's curator: Tony Elhabr - @TonyElHabr…","[00:00:03] Eric Nantz:

Hello, friends. We are back with episode 153 of the R Weekly Highlights podcast. This is the weekly show where we talk about the latest happenings and the tremendous resources that you can find every single week at the rweekly.org website. My name is Eric Nantz, and I'm delighted that you joined us from wherever you are around the world. We're about in the past the halfway point in February, so spring is coming soon, I hope. But I can't do the show alone. Of course, I am joined by my awesome cohost, Mike Thomas. Mike, how are you doing this morning? I'm doing well, Eric. Yeah. I think probably the audience can can hear in our voices that spring isn't quite here yet, but, we're getting there.

Yeah, combo. The cold weather, kids bringing you know what home from their respective day cares or schools. It just it it never ends. It never ends. But, nonetheless, we're gonna power on through here. We got a lot of exciting content to share with you all today, and this content for this particular issue was curated by Tony Elhaubar, who had tremendous help, as always, from our fellow rweekly team members and members like you, contributors like all of you around the world with your awesome poll requests and suggestions for more excellent resources.

As I said, Mike, we do sense spring is coming, and that's where I know my kids like to start getting their bikes out to take their bikes rides around the neighborhood and whatnot. Well, it's appropriate that as the weather's warming up, our first highlight today is taking a very data driven approach to just how far you can take your bikes and analyze that for some real fun, exploratory data analysis and and and the like. And this post comes to us from Greg Dubrow, who is a data analyst who is now based in Denmark, and he has a, you know, very passionate, hobby, so to speak, of riding his bike basically everywhere he can go. He's been doing this wherever he's been in the world. His first part of the blog post gives a nice background on the various bikes he's had growing up, even a little mishap he had, last year during that, which any bike enthusiast can probably relate to.

But, nonetheless, he talks about, you know, taking advantage of recording his bike riding data using an app called Strava. I've not heard of this before, but, apparently, it gives you a boatload of metrics having to do with your bike riding. And the first question comes, okay. Well, you got this data on the app. Right? How do you get it out of that? Well, you could download a bundle from your profile on the Strava site as one way to get a CSV text dump of that, which he does end up doing. But like anything else, there is an API for that. Right? And not only that, there is an R package to help you grab this data from R itself called RStrava, which he utilizes as well as, like I said, the aforementioned CSV data dump, if you will, and merges that together to give them a nice tidy dataset after some, you know, very usual cleaning and reshaping and and manipulation of dates and whatnot so that it's actually ready for analysis. So, again, an awesome data driven approach, to take advantage of modern tech to put this data into R itself.

You've got the data. Now what? Like a lot of the posts we cover in our weekly, we start with some fun exploratory data analysis. And to get things going quickly, he makes use of a very cool package that I've actually seen utilized with some of my colleagues at the day job as well as others in the community called data explorer. This is a really nice package that gives you a very quick way to explore, say, the missingness in your data as well as doing some very nice correlation heat maps right off the bat with your numeric variables.

And he senses that, yeah, most of these variables have a positive correlation to the key metrics, such as distance and the actual moving time of the bike, where this app is apparently smart enough to detect when the bike is actually moving versus stationary. So really novel use of tech here, but the post has both the variables and their percent of missing values as well as this aforementioned, heat map with the correlations to help begin informing just what kind of relationships he will explore later on in the post. And you start to sense some of the things you might think intuitively, such as the average speed of his bike being positively correlated with distance, albeit not as a huge relationship right off the bat, and also some correlations with the power output of the ride and measured an average wattage usage may have some negative correlations with other metrics and the like. And then augmenting these visuals from the correlation perspective is the tried and true scatterplot, which uses a very novel functional approach that he got inspiration from one of Cedric Shurer's posts that he does all things ggplot2 and with a little bit of permac magic with patchwork, able to get these nice correlation plots for the key response variables of distance, moving time, and average speed, and you, again, start to see positive correlations amongst many of the key metrics, such as calories, average watts, moving time, and etcetera, to give him a better idea of what he might expect out of a more rigorous analysis.

And just where does this regular analysis take place? Well, we all like some tables. Right? So he starts off with creating some fun GT tables of just the metrics in terms of the total time, elevation, total calories output throughout the year. And you can see out of 446 rides, yeah, he's burned a lot of calories and generated a lot of energy. Lots of cool, cool summaries there. And then we got some more visuals, Mike, where he's looks at the seasonal pattern of his ride shares per month. So why don't you take us through some of the visuals you're seeing here? Yeah. It's a really nice, visual blog post. I I think Greg notes that he leveraged

[00:06:31] Mike Thomas:

some of the tutorials that Cedric Shearer has put together. And if you are in the DataViz space, especially in the our DataViz space, that is a name that you are certainly familiar with. One of the visuals that I thought was was pretty cool, that that he used was, most rides during, you know, it's showing, the number of rides that he has and and the type, sort of in this polar area coordinate diagram. And the reason that he did that is to correlate them to the hour of the day that it took place. So it's a this this chart is sort of representing a clock which is a really cool, I think, use case of these polar area coordinate, diagram type charts. I struggle to find a lot of use cases for those those charts in a lot of my EDA analysis and DataViz work, but I think this is a perfect use case for it.

So I really appreciated that. You know, going back to talking about some of the dependent variables that he used. One of the the dependent variables, that Greg used was called kilojoules, I believe. Hopefully, I'm pronouncing that somewhat correctly. And, he used that variable instead of calories because, according to Garmin, frequently asked questions, calories expended are the total energy, in the time that it took to do the workout that you expended, while kilojoules is the energy burned, actually burned by the workout and that formula is watts times seconds times a1000. So that was that was very interesting to me, and, you know, I know a lot of workout apps there sort of focus on calories, and and maybe they should be focusing on kilojoules instead. So I I thought that that was pretty interesting, you know, some of these GT tables, and the way that he was able to format them in the blog post to have a lot of these GT tables actually side by side, instead of one on top of the other using some HTML was a pretty cool, nifty trick to make this, this blog post sort of nice and neatly put together as well.

So a lot of really interesting, work here, and then he actually fit some models at the end of this here. There's a time model, a kilojoules model, and then a Watts model as well. And the the model fits all look look pretty good. I'm pretty impressed and it's sort of a function, I think, of just the amount of data that, this Strava app allows you to to have, control over and to take a look at. And you can do some pretty cool, as as Greg shows us here, some pretty cool analysis with your your workout and exercise data, particularly with your your cycling data in the Strava app. So a really cool use case, I think walking through a lot of different types of data visualization, some predictive modeling, sort of an an end to end data science project here using a pretty nifty data set. So, hats off to to Greg for a great start to our weekly highlights this week.



[00:09:25] Eric Nantz:

Yeah. Really awesome approaches here. And looking at even the source of this quartile document that this blog post is based on, you're gonna see, like you Mikey said, those nifty tricks of putting the the tables and the some of the plot side by side. Really, really nice, easily viewable post here. We got the table of contents on the right margin. Yeah. Lots of ways to hop back and forth amongst us. So, again, Quartle gives you a lot of these niceties out of the box. And really, hats off to owning your data as best you can, albeit, yeah, a third party app is collecting it, but fair play to Strava for giving an API for users to expose this because there are some others out there in the fitness tracking space that aren't quite as friendly about you getting your exercise and workout metrics out of it. So really encouraging to see For all you cyclists listening out there and I'm a part time cyclist when I can.

Yeah, that's really cool to see you be able to take advantage of this amount of data. And, you know, he's got to be, Greg's got to be a pretty fit person to be able to have this amount of rides in in 2023 even with the injury that he underwent earlier in the year. Like, that is impressive stuff, impressive dedication, and, yeah, may maybe I need to not be so lazy this year. We'll see.

[00:10:48] Mike Thomas:

And hats off to Greg as well and Quarto for the nice, collapsed code chunks that allow you to see exactly how he did what he did in this blog post.

[00:10:58] Eric Nantz:

Yeah. Isn't that a great, you know, UX, so to speak, of digesting the parts you like? And then maybe you're more interested in, say, the modeling part or more interested in the visualization part or, of course, interested in everything, but you can opt in to looking at all those details and still get the full cohesive story here. So, yeah, really enjoyed the post. Looks like he spent a lot of time drafting this together. But, again, it's all all available for us to see in the open. And and, yeah, I'm gonna have to maybe get a new bike this year so I can start tracking some metrics.



[00:11:28] Mike Thomas:

Me too.

[00:11:38] Eric Nantz:

Now maybe, Mike, you you're on a long bike ride. It may seem, Mike, every time you think you've got to your destination, there's some little hurdle along the way. Right? And maybe it's a traffic light. Maybe it's, you know, who knows what else is happening out there. Sometimes package development can feel like you're so close. You get that glimmer of hope, and then something really crazy happens. That's where our next highlight comes in, coming us from our our fine threads at Think R. This has been authored by Swan Fuller Clay. I probably didn't pronounce that at all correct, but apologies in advance.

But they have an excellent blog post here about how you can tame the namespace of your R package and making use of the suggest call of it. So like any good story, this starts with what seems to be a smooth road. There, the example package in this post is a simple wrapper on top of ggplot2 to help export the plot with a simple function they call saveplot. It looks innocent enough. Right? We are simply letting the user specify, after they specify the ggplot object, the extension, which can be 1 or more of, like, PNG, JPEG, or PDF, where to put it, what's the file name.

Pretty straightforward stuff is just wrapping a call to gg save with a little bit of per on top of that. And the usage looks very straightforward. The example looks very logical. You're gonna plot your dataset, use the save underscore plot, clean up after yourself. Well constructed example. So like anything in package development, you're gonna start checking this on your local system using dev tools colon colon check most of the time. It comes through flying colors. No issues at all. No errors. No warnings. No notes. You are feeling good about it.

And like in good practice, this package code is on version control with GitHub. So why not just rely on your local system to do the checking? We like to use GitHub Actions now to do a lot of this automated checking as well. And that's where things start to go a little off the rails because we are now embodying the infamous slogan in CS and development. It works fine on my machine. But on the CI check, we see a cryptic error. And where does this rely? Well, you go down the rabbit hole of checking the logs.

In the details, there's an error saying that there was an error running the example code, which, again, the example looks straightforward. Works fine locally. Right? Go down that back trace a bit further a bit further, and then you see this error about load namespace. There is no package called SVG lite. Oh, boy. What on earth happened here? Now we gotta put our detective hat on. Where in the world is SVG light being used, Mike? What gives?

[00:14:52] Mike Thomas:

Well, it's going to be in, and you'd only see this in the source code potentially, or the, the the package description file, but it's it's going to be in the, it's a dependency of ggplot specific to the ggsave function. When you are saving a plot using the ggsave function from ggplot2, as an SVG, it's going to employ the SVG lite package. And one of the reasons why you don't have this, in your CICD check is because SVG Lite is not a hard dependency of ggplot2. Svglight is in the suggest portion of, the dependencies of gg plot 2. So it is not going to get installed automatically when you specify that ggplot 2 is a dependency of your package.

So, you know, this is a very familiar, probably, territory for those of us who have done a lot of our package development and ran into similar situations like this before either, you know, wrestling with, whether dependencies dependency should be, a hard import or should be in the suggests, section of your description file. How to manage dependencies that are only specific to, maybe, vignettes, or things like that. So this is very familiar territory to me. I have I have, maybe, a story that that I might tell if we have a little bit of time here, but I think it it sort of just goes to the the overall narrative here that sometimes, you know, things may work well on your machine, but when you start to employ CICD that, you know, is going to leverage a different machine to build and test that package, you may see a failure there. And and honestly, that's that's a good thing. Because what that means is that when someone else on a completely different machine than yours wants to use your package, and that's that's the whole idea, to build software that is useful for other folks as well, they might run into the same issue. Even though you saw no issues, warnings, errors, or notes in your your own DevTools check run, you know, it's a good thing, in my opinion, that when you you send it off to GitHub actions in this case, that, this error presented itself and pops up and, you know, let you know sort of exactly what the issue was. I think the error message is is pretty descriptive, and, obviously, you do have to do a little bit of detective work to to figure out, hey, where the heck is is f SVG light, even impacting us in this particular case. But, if you if you've been around the block a little bit with ggplot2 and you're you sort of understand what you're trying to do here in terms of saving that plot, hopefully, it won't take you too long to figure that out, as it it did in the case of of Swan.

So, you know, I think this blog post, also nicely calls out the use of the the Fusen package, not Fusen, it's the Fusen package, which, for those who are unfamiliar, is a package that streamlines, the development of our packages. And I believe it sort of uses an R Markdown approach, a chunks approach to sort of execute different commands, through a a nice documentation framework, and sort of build out all of the different components that you need, for that R package. So, you know, I don't know. Eric, if you wanna take over sort of, more of the the final solution here to ensure that this,

[00:18:23] Eric Nantz:

that this package passed all of its CICD checks when it went to GitHub actions. Oh, yeah. I'm chomping at the bit for this because, boy, do I feel seen on some of the ways this can be implemented. So now that we realize that SVG light is definitely required for this safe plot function, there are a couple options or how this could be tackled here. One is that in the r oxygen preamble for this function, we declare an import from SVG light, the SVG light function. That is certainly a valid approach. Right?

Well, when you run a check again, even locally, you're gonna see something appear that may seem really scary, and frankly, it can be, where you will be warned about, hey, you know what? Now, the imports of your package may seem small, may only seem like it needs YuJaPaw 2 and SVG lite. But that has now ballooned to 21 non default packages that are now gonna be required at install time to get your package installed. Now the CRAN maintainers put this check-in our command. Check this note, I'm about to say, is that importing from so many packages make the package vulnerable to any of them becoming unavailable.

Yes. We have covered in years of this podcast when one dependency suddenly got, quote, unquote, archived on CRAN. In fact, they even infected ggplot2, I believe, and amongst others. So how how we appropriate here. So but you but you still need this package, or do you need it? Now there this is where suggest comes in in terms of now taking this preamble out of the r oxygen, putting SVG in the suggest field, and then in your function having a check via the require namespace function to check if that user has installed it or not on their local system and to prompt them to install it in order to get the full support of all those file types that are being exported, but it would still import, in the case of the solution here, it will still import the file types or export, I should say, the file types that don't need SVG light. So there's a happy medium in here from the UX experience to what they politely say avoid the backlash of what can happen from both you as a developer, but also as the end user to know what to do next.

So it may seem like a little more upfront work, but the good news is is that now the dependency footprint of your package going to CRAN has become much less, such that now the user or in your CICD for running the examples can opt into installing this package without having the hard dependency on it and, hence, minimizing the potential for the dreaded archival status on CRAN if any of these dependencies end up going away. So there is another nugget here, though, is that when you install a package from, say, GitHub, like, they have a nice snippet on here using the remotes colon colon install GitHub, name of the package repository, by default, it will not install dependencies that are marked as suggests.

That's where you have to supply the dependencies flag of true in order for your local system as an end user to grab, in this case, that SVG like dependency. That's a nuance that has tripped me up so much in my day to day work when I thought I'd install a package from GitHub. I'm ready to throw it in my shiny app or throw it in my over pipeline, and then I realized, oh, what's that error? Oh, nope. Didn't get that suggest package when I installed it. So that's at the end of the post, but it's bitten me up many times. And that's contrary to the base r install dot packages function, right, which will install by default your the packages that are listed in the suggest portion of the description file.

That is correct. There's a dichotomy there that unless you really stumble into it, you don't really know exists. So that was that was some shared learning for me. So, well, I'm curious, Mike, to hear your your your tail, if you will, of this of this issue. For me, I have two minds on this. If I'm developing a package that's just for internal use at my day job, admittedly, I'm probably just gonna put it in imports anyway because at that point, I know I'm not going to CRAN. It's more about does this pipeline I'm making is the benefit of throwing this on to, say, a Shiny app hosted on Pawsit Connect or other areas.

What's the easiest way for me to control what's happening there? And that's typically if I have a description file, just throw it all in imports. But if in the situation of either a, you know, CICD or CRAN itself, yeah, I definitely would take this advice to heart because it will make your life as a maintainer easier not to worry about s v in this case, SVG lite's tangled web of dependencies when all you needed is for one additional type of file to export in this case. It is not gonna impact the baseline functionality of said package.

It's just an enhancement on top of it. I admit sometimes it's hard to find that good, like, threshold of when you go to import only or when you go to suggest only. I think that comes through experience. But for me, it also depends on what context you're gonna be releasing this package in.

[00:24:04] Mike Thomas:

Yeah. I would agree. You know, I think I'll default here and punt a little bit and say that the our packages book, which is authored by Hadley Wickham and and maybe Jenny Brian and and a few others, don't quote me on that. I think has some really good discussion about when to list a package as a hard dependency versus a soft dependency, and and also how to handle, you know, packages that are just being used. This is something that we run into a lot, packages that are just being used in a vignette, you know. And ggplot2 is one for me that that happens quite a bit with because, you know, maybe I I don't have any functions within my package, itself that, you know, leverage ggplot2. We're just returning data. But in my vignette, I wanna show how this package can be useful to users. So I I wanna build a beautiful chart, you know, and have that be on our package down site and folks come and see that and look at that and be like, wow, this is what I can do with this package. But not necessarily, you know, auto plot anything for them because I have plenty of opinions on why I don't necessarily like like functions that do that. We just try to return the data and let you, you know, use ggplot2, use Echarts for r, do do whatever you want, GT, to make it beautiful.

But, I guess one other thing that I will add as an anecdote here that that Swan mentioned, which I think is really good advice is when you are managing the dependencies, you know, within your particular package, highly recommend using the the use this, use package function, which will handle, not only, you know, where things should be listed within that description file, making sure you don't have anything duplicated in there, but also it will handle the relationship between that description file and your namespace file, and any updates that need to be, taken on that name space file, which should not be done by hand, which is highly recommended. So I I would recommend leveraging that package. Just another example where use this can be awesome.

I had had a I'll try to keep this brief, but we developed an open source package recently that actually downloads some publicly available data that's stored as a zip file on a website. And when it downloads, you know, onto my machine or or onto any of our our, company resources, local machines, any of my teammates machines, the the data comes in, and it's it's strange data set in the zip file of these text files, where, say, there's say, there's 4 datasets, for example. There would be 8 text files total. The first four would be the column headers, and then the next 4, would be the actual data itself without column headers. So you just have to stitch them together and and, you know, file 1 is the headers for the data in file 5. You know, file 2 goes with file 6. File 3 with 7 and 4 with 8. You can match them up. They're they're ordered that way when you download them online.

So we never had any issues locally with essentially, you know, returning a data frame that matches the headers to the data itself, until we try to use the the with our package, in our unit testing to do this programmatically deployed to GitHub Action CICD. And when, we, you know, ran these tests locally, everything works totally fine because we just said, you know, you match file 1 with with file 5 and and 2 with 6, and so on, and so forth. When, the with our package ran, you know, on this this Linux box, probably, on GitHub actions, the files that got unzipped from the zip folder got unzipped in strange orders. Not the order that they were stored in. So just got totally totally totally reordered. And it was very difficult to figure out, you know, what the issue was that was was going on here. And, so we just had to write in a little extra logic that uses the naming conventions of the filenames to match them together, which wasn't a big deal at all. But we couldn't rely on the order that those files came in because for whatever reason, when this this ran on GitHub actions, and unzipped these files, they unzipped in a totally different order than what took place locally for us. So, you know, just an example of how you can pass all of your checks locally, but not necessarily, when you, you know, are running it in a separate environment, which is a good thing. I was glad that we ran into that issue, so that if for some reason, you know, somebody, leveraging our package experienced the same issue where the the files unzipped in a different order, you know, our our functions would still work.



[00:28:41] Eric Nantz:

Yeah. Boy, is that is that just par for the course. When you see the expect the unexpected, so to speak, when you put in things in CICD, I I had a similar thing, albeit it was more of me shooting myself a bit in the foot on this. But I was doing a pipeline in GitHub Action. It's not an RPAC. It's about a set of functions that grabs a SQLite database from online, does some transposing, does some massaging, does some fuzzy duplicate finding, and then sends it back out as S3 objects. Well, in my development, I was starting to do, like, date cleaning with the luberday package.

I had installed it locally. I forgot to add it to my manifest for the GitHub action to install. And I actually was leveraging a friend of the show, Peter Solomis' DEPS package to make a JSON file of the dependencies. I just forgot to rerun that darn thing and then commit it. So I'm like, wait. That worked fine on my machine. I was like, well, dependencies again. So it happens even in non package context. So you just gotta gotta keep keep the vote of keeping that stuff up to date, whether it's an rmblock file, a dep.json, or whatever else. Just, yeah, keep that stuff up to date, man.



[00:29:51] Mike Thomas:

It's hard. It's hard. There's a lot to it. But in my opinion, it's it's super important because we want the user experience of others, you know, using the software that we create to be as as high quality as possible.

[00:30:04] Eric Nantz:

That's, it's all the name of the game, isn't it? Yep. But, really entertaining read here and very informative too. So credit to thank Arver sharing her knowledge of the developments in the trenches, so to speak. And, Mike, yeah, those are the 22 pretty heavy content highlights here. And we're going to have some fun with this next one because I like me a little game now and then. So our next highlight is going to do a little fun classification magic to hopefully help you win this game even even faster than you might expect.

And this is coming to us from Michael Hoe, professor in statistics and data science at the University of Gresfel, Germany. Again, pronunciation is not my strong suit here. But he starts introducing this blog post about a game that he likes to play called guess my name where each player will have a card with 16 kinda avatars on them. They each have different, like, you know, hair color, maybe slightly shirt, you know, different genders, all that. And the object of the game is the each player will pick who they wanna represent on this game, but then the opponent has to ask questions to help narrow down who that opponent would actually be on on the board itself.

And they can only ask questions about the picture of the person, and they must and they the response to that must be either yes or no. So you can kinda start crossing off who is not and then figuring out eventually who that person actually is. So, of course, naturally, the winner of the game is gonna be one that finds the answer in the least amount of questions. And so Michael, what he does in the first part of his post is actually compiles a spreadsheet of all the about 12 questions that we will logically ask in this game amongst then the players that this these questions would represent.

So you can download that spreadsheet right off the the blog post if you want to look at that for reference. He's got a snippet of it in the post itself such as, like, do they have headgear, do they have glasses, blonde hair, etcetera, etcetera. Now that doesn't that's a great starting point. Right? But what where do you help determine what should you ask first in order to maximize your chance of winning? It's decisions, decisions. Right? Well, literally here because we're gonna look at decision trees as a way to help take a data driven approach to find the solution.

This is not something I would have expected, but it's a pretty clever use of the classical classification tree method where he feeds in the data. In this case, like I said, that spreadsheet of questions and then the membership of each person responding yes or no to those questions, throws it in in our part, call after it does some massaging of the data. And then the rest of the blog post now gives you a decision tree that you can use as a strategy going forward for how you might identify these players. So if before seeing this post, Mike, would you have guessed that the first question that someone should ask is whether the player has blonde hair?



[00:33:31] Mike Thomas:

I wouldn't have guessed that, but it has been a little bit, of time since I last played, a game like this. I think there's a game very, very similar. Maybe it's it's more US based. And I think it's called called Guess Who. Played it a lot when I was growing up and it you know, you you flip the flip the people up and down after you ask questions, and it's kinda like a 20 questions game to see who who can figure it out first. So this sounds like exactly the same thing. And if I remember correctly, I think that was one of my one of my top questions. Does the player have blonde hair? You know?

And trying to narrow down, who the person is that you're you're trying to get at. But I guess it all depends on, it all depends on your on your strategy and the the data or or the people that you have on your on your card.

[00:34:17] Eric Nantz:

That's right. And so when you look at this decision tree further, once you get through that initial question, then we start branching off into questions like, is the is the picture have something green in it or something red? So you're looking at color next across the entire picture. Now the type of gear they're wearing, is there some headgear? Is there a short sleeve or a long sleeve shirt visible? Alright. Can we see the eyebrows or the glasses? Like, it is a very interesting dichotomy here of where you should go next. But, yeah, it just shows you another clever use of classification methods, which, again, are typically the backbone of almost all types of machine learning, especially in the dichotomous approach where you're trying to optimize that response or predicting a response.

These are the fundamental building blocks. Obviously, if you're in a more rigorous analysis, you might look at other methods and classifications as a random forest, GBM, and the like. But if you ever want a gentle introduction to what classification trees are actually doing, with a little, perhaps, ways you can use this in your in your activity time later on on the weekends, this, you could do a lot worse than seeing what Michael's post has here. No. This is a super super cool, super fun application. I guess not something that I've I've seen too often in the highlights before. We've seen some exercise data,

[00:35:42] Mike Thomas:

you know, before, but in terms of actually, like, playing a game, I think this is is super cool, and a great use case, and I think a great learning case for leveraging decision trees and the the r part package itself. You know, sometimes I think algorithms can be explained best on, like, small data sets where you can see sort of exactly the decisions that are being made by the algorithm or or how the math really plays out. Just as a another final story for for the week, I do have somebody in my family. I'm not gonna name names. But when we play Clue, they essentially need a laptop with Excel open next to them to be able to sort of track everybody's responses in a spreadsheet, which is way over the top, analytics approach.

And, to be honest, they they actually are the person that that usually wins, which makes me feel like I need to to do something during that game. But I'm off the clock, you know, so to speak, when I'm playing board games. So I try not try not to go try not to pull out all the stops. But if this person does keep winning, I may have to spin up r in something like this and come back to Michael's Michael's post because I think a decision tree type of approach is is not only applicable to this particular game that we're looking at here, but, Clue is another one that that comes up for me that would be a fantastic use case and application for leveraging something like a decision tree, because it's a question based game as well, where you're trying to sort of narrow things down, into the the smallest, you know, number of pieces of information that sort of give you the the the best idea of of what the true answer is at the end of the day. So,

[00:37:24] Eric Nantz:

that that's the end of story time for me for this week. It does make me think, albeit it probably have to use even more rigorous methods for this. But as a kid, I loved playing that game Battleship and trying to figure out which space should I target first. And then based on that, knowing that it didn't hit, how far away should I go from my next target? I could sense there'd be a lot of fun data driven approaches to that. But for those of you listening, if you're interested in more the technical math behind this idea, well, the blog post has a terrific appendix here where you can really, really get get to school, so to speak, on how classification methodology works here. Michael does a terrific job with kind of the mathematical optimization formulas that are under the hood. There's also some nice visuals along the way. So this is if you're in in a situation and trying to learn about classification in general, like I said, not only do we get the the fun intro of this post, but also the the appendix is really giving you a lot of great details for how this all really works under the hood.



[00:38:29] Mike Thomas:

Absolutely. No. That break that battleship. Oh, that's that's another good one. Where to where to guess next? That kinda, like, brings me to the Monty Hall problem a little bit, like, which door should you pick based on the first door that you you selected? I once I don't know. Spent spent too much time with an actual deck of cards trying to prove out whether the Monty Hall problem and solution was actually the right way to go and and that was probably my first introduction into into Bays without really knowing it.



[00:38:55] Eric Nantz:

Isn't it interesting? It's almost everywhere in your life, but you just may not realize it until it's all in Chelsea. Yeah. Well, you make me wanna play games the rest of the day, but, unfortunately, I won't be able to do that. But what we can tell you, though, is that the rest of the hour week, we issue as a terrific section of additional resources, tutorials, blog posts, new packages, updated packages, tons more to choose from. So we'll take a couple minutes here to talk about our additional highlights here. And sticking with the earlier part of the show, looking at great uses of ggplot2 for EDA and whatnot, there is a terrific package called g g magnify.

This has been authored by David Hugh Jones, who I believe we have featured on the highlights before, where, in essence, you can take a ggplot object, and then within that same plot, draw a boundary of, in essence, a panel that you can then use as zooming in on those particular data points and put that in a section on the same plot. This is pretty interesting to me because now, obviously, I'm a big fan of interactive graphics and interactive HTML where if you had this in, say, PlotViewer or not, you could just do the, you know, the zooming of a particular plot in real time, get those coordinates, and then zoom back out. But if you're in the realm where maybe you're confined to static representations, then gg magnify might be a really interesting way for you to call out a particular section of that scatterplot or that distribution plot and then be able to really emphasize just kinda what's happening in that subregion of the plot while maintaining, you know, a good clever use of space and whatnot. So gg magnify, I never heard of this one before, so I'm gonna put that in my visualization bookmarks to follow-up with later on.



[00:40:48] Mike Thomas:

No. That's a great one, Eric. I'm gonna shout out, Romain Francois for his blog post on the request perform stream, function from the HTTR 2 package, and how he found a bug and submitted a pull request as well. So he actually asked it. And I don't know for folks who are connected to him on social media, you may have seen that he has authored a package recently, that will write a Valentine's or not necessarily Valentine's Day poem, but I think will write a poem for you, I believe, leveraging chat GPT about anything that you want. I think it started out around Valentine's Day, which is is where I saw it first. And he actually ran into an issue asking the, Chatter package, which is from, I believe, the the ML verse, which leverages chat GPT, which I know a lot of tangential packages have been built on top of at this point. He was asking it to write a poem about Gollum, and he asked it to use many many emojis, in in that poem.

And, unfortunately, he received an error. And one of the reasons for the error, or I think the big reason in particular, is that, the this package, Chatter, leverages the h t t r two package, and and obviously streams back the response from the ChatGPT API. And it streams it back in in chunks of bytes. And, unfortunately, it was cutting off an emoji in the middle of the number of bytes within that emoji because it's encoded as as a few bytes. And it couldn't essentially stitch together the emoji from 2 separate chunks of bytes, if you will. So, this was, I guess, you know, an unexpected bug, and, Romaine went as far as submitting a pull request to the h t t r two package about a way to go about this.

Potentially, I think using the the read lines function instead of the read bin function. And there's, actually, I mean, you can go into the pull request. It's it's linked in the blog post, and you can see, you know, the fantastic conversation, the way that he frames the problem to the Rlib, team that manages the h t t r two package. You can see his back and forth conversation with with Hadley Wickham on how they eventually went about and resolved, and closed this this pull request, and resolved this bug, and merged it back into main. So now, if you are installing the htrt2 package from from GitHub, and hopefully soon from CRAN, you won't run into the same issue that Romaine ran into when trying to, ask ChatCPT for a response that includes emojis.



[00:43:39] Eric Nantz:

My goodness. Yeah. You saw my rabbit holes. Right? I mean, that credit to Romaine and Hadley for finding a a now we can fix this because this is this is an area where I take for granted that all these symbols are just gonna work no matter if we're passing data in or taking data out. Oh, there's a lot behind those emojis, folks. It ain't just the fun graphics. There are a lot of these raw bytes and bits that if you're not you're not taking it correctly because we're involving, like you said, the APIs of chat GPT and what this chatter package, knowing what's being handed off, that can be quite important. So, well, credit to credit to Romaine and Hadley for putting this together. And, yes, at the end of the post is a very lovely poem about one of our paper packages called Gollum that will never cease to amaze us.



[00:44:26] Mike Thomas:

Absolutely. And I think, you know, it's also if you if you do sort of follow that pull request, it's just a great example of how to contribute to a package and to make it easy on the on the maintainer or as easy as possible on the maintainers to get that bug fixed.

[00:44:42] Eric Nantz:

Excellent. Excellent. And you know what else makes it easier for you to learn about what's happening in the data science and the art communities? Well, that's where you just bookmark our weekly .org. Have you have checked that out every Monday morning. We have a new issue released like this one we talked about here. But, of course, the entire back catalog is also available. You can see right on the home page. And, of course, this is a project by the community, for the community. The lifeblood is you and the community. So we love your poll request. We love your suggestions.

You can get in touch with sharing that resource via poll request. We just thought about poll requests. Right? Our weekly is very embedded into that workflow. The link is directly on each each issue's front page where you get a link to the upcoming issue draft for next week. You'll be able to quickly send your poll request there. It's all marked down all the time. Very easy to get up and running quickly. And, also, yeah, we're always happy if you wanna join the team as a curation role. We definitely have spots, and we have links so you can get information on that at r wicked.org.

And, of course, we love hearing from you and the audience as well. You have a few ways to get in touch with your humble host here. One is the contact page. We have the link link directly in the episode show notes. We also have a fun little if you have a modern podcast app like Paverse, Fountaincast O Matic, and whatnot, you can send those fun little boosts along the way, which is directly from you to us in your favorite podcast app. And, of course, we have, some presence on the social medias. I am mostly on Mastodon these days with at our podcast, at podcast index.social.

Sporaglia on the weapon x thing with at the r cast and then also on LinkedIn sharing some posts and other fun announcements from time to time. And, Mike, where can the listeners get ahold of you? Sure. You can find me on Mastodon as well at mike _thomas@phostodon.org,

[00:46:39] Mike Thomas:

or the other place that I am present on social media a lot is on LinkedIn. If you search Ketchbrook Analytics, k e t c h b r o o k, you can probably find out what I'm up to.

[00:46:50] Eric Nantz:

Awesome stuff. Like I like we heard about before. Congrats on that recent package you open sourced. I'm sure there are lots of fun stories behind that as well that you'll see on Mike's, LinkedIn post from time to time. So Yes. Thank you. Absolutely. So we're gonna close-up shop here for episode 153, and we hope to see you back for episode 154 of the Our Weekly Highlights podcast next week."
"66","issue_2024_w_07_highlights",2024-02-14,34M 50S,"A few great tips for ensuring your R package doesn't ""talk too much"" (within reason), shrinking down the size of your images with a new API directly available in a new package, and the first opportunity in 2024 for submitting your proposals for R Consortium projects is on the horizon. Episode Links This week's curator: Jon Calder (@jonmcalder…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 152 of the R Weekly Highlights podcast. We're happy to join us wherever you are around the world. And this is the weekly show where we highlight, no pun intended, the awesome, highlights section of the our weekly website, particular issue atrog.org. My name is Eric Nansen. Yeah. We're almost midway through February. This has always been the time of year where I kinda wanna get to spring now, you know, just to cheer things up a bit. But, yes, we are back, and I'm not alone. I'm always joined at the virtual hip here by my awesome cohost, Mike Thomas.

Mike, how are you doing today?

[00:00:40] Mike Thomas:

Doing well. Waiting on spring. Also, we had about 60 degree weather here in New England this past weekend. And today, we got a foot of snow. So

[00:00:50] Eric Nantz:

Them's the brakes. Them's the brakes, man. It just never ends, does it? Yep. But we'll get through February soon enough. But, one way to speed things up is listening to those banter about this week's our weekly issue. And if you're not familiar with the project, every week, we have a new curator to to take the reins. And this week, it is John Calder, another longtime member of our curation team. And as always, he had tremendous help from our ROCE team members and contributors like you all around the world.

So I wouldn't be shocked that if some of you listen to this podcast, there might be a section or 2 when you might hear yours truly kind of ramble, but you'd be like, yeah. Yeah. We get it, Eric. Maybe tone it down a notch. Well, guess what? That can also happen with our packages as well, especially those that like to, you know, through no fault of their own, give you a heads up through messages or warnings or other diagnostics as, you know, operations are commencing analytics. But there may be some cases where you want the user to kind of be able to opt in or opt out of this kind of behavior.

So our first is gonna talk about, as a package author, how you can take advantage advantage of some nice utilities both within r itself and within the r ecosystem to help give you a give your package users a little more control on the verbosity, so to speak, within their package experience. And this comes from the awesome rOpenSci project once again. In particular, the blog post has been authored by Mark Padgham, who is a open source software developer at rOpenSci as well as, returning once again, Myles Salmon. The street continues with highlights that she's involved with. And this blog post starts off with, basically, if you've written any package or function, you've probably have done this once or twice or, frankly, in my case, a lot more where you have a function. You know it's going to do some complicated stuff. And especially for you as a developer, you want to see what's happening in your console as things are being processed.

And you might have an argument that says something like verbose or quiet as a simple true or false just indicating, you know, a little switch to turn that diagnostic on and off. It's spread throughout the R ecosystem. You're going to find many, many functions and packages that have this approach. But as they say in the blog, it could introduce a bit of clutter and making the user have to customize this every time for each function. Well, maybe instead, the approach you might wanna take is having this configured at the package level.

And that's where using an option statement could come in quite handy as you, the package author, where you might have an option talking about, you know, is my package message gonna be quiet or not? And then the user could set that option themselves, run the function without having to introduce another parameter in that set function. And then maybe they want the situation switched, they can just run that option again, rerun the function without any changes to the function code or the function call, and it will give them the behavior they want.

So that is one approach, and we're starting to see more of that. But, you know, guilty as charged here on this very podcast. I have not done this much enough, but I'm definitely thinking about, this approach. Now there as usual, there are community, you know, you know, comes to the rescue again, so to speak. If you want to tap into somebody kind of doing this boilerplate for you, so to speak, guess what? There is, packages that we talk about on this show quite a bit, the CLI package and by proxy, the rlang package, which is actually, you know, building a lot of this functionality with their own equivalent of, like, the typical message, warning, and stop functions that you find in base r.

And so you can opt into using rlang and CLI in your package if you want to take a dependency on that to kind of mimic this kind of behavior at a package level. They have functions called, like, inform, COINFORM. And then there are options that you can tweak at the COI or rlang level that could your package could tap into. And so that is another great approach if you just want to take advantage of other great work in your package to control verbosity. Now there are some things that you wanna consider with that, especially if you think about, you know, how deep you wanna go with this.

One of those is just simply how do you wanna control the level verbosity.

[00:05:42] Mike Thomas:

Yeah. So one interesting thing that I've seen done in a lot of packages that I've used, but never really knew exactly how to implement it in some of the packages that we've authored is the idea of displaying a warning or a message only once per session. I think that's that's really interesting. I think it's really powerful because it sort of lets lets you know, okay, this is something that you should be aware of but we're not gonna continue to to throw it in your face over and over and over again. And, I think that's really useful, probably a nice feature of the the R ecosystem. I don't know if that really that concept exists in other ecosystems as well.

But, you're able to do that by setting, the frequency parameter of the rlib message verbosity options to the string once, which I hadn't seen before. So there's a great little section here that talks about exactly how to do that and I am looking forward to trying to implement that in some of the packages that we've developed because I think that could be a much better user experience, to just be able to display some of these these messages or warnings, only one time per session. Then Mel also talks about and, excuse me, not just Mel, but Mark as well, also talk about, regaining package level control from your global options. And and this is sort of the issue that takes place when you have dependencies, right, upon other packages that are using our lang or CLI or with our to display messages, and you don't necessarily have as much control to turn that verbosity on and off because it's it's not your package. It's not your code. It's a dependency, of your package. So there's a a really nice couple of code snippets in here that employ the local options function from R link, which is really interesting. And that would allow you to essentially locally, within the function that you're authoring, turn on verbosity or or off. So you could set the r rlibmessage verbosity, as as verbose or wants or or essentially whatever you want and control that sort of in this to me, it feels with R ish, but it but it's a function called local options from the rlang package that allows, you know, within that function to to handle the verbosity specifically.

And I think that a lot of these concepts and code snippets can be especially helpful when you are developing your package or debugging your package because maybe you you know that this warning message is going to pop up, and you're you're trying to do a lot of little tweaks and you don't necessarily wanna see that over and over and over again as you develop. But, obviously, in in the production version of your package that you're going to release out to the world, you you do want those warnings to pop up to users who aren't going to be running the same exact function over and over and over again like you might be doing during development. So I thought that that was a really powerful concept too. There's some fantastic, R code snippets as well. And it's a really great overview and blog post around this topic of verbosity, which which, again, Eric, I think is pretty unique to the highlights and not one that we see too often. And it gets back to really usability, user experience, and, you know, how easy it is for other folks to be able to to use and be comfortable with your r package. And to me, the little things go a long way. I know you feel the same way. That's certainly a a shiny concept as well, but I think it it extends very much, into our package development to try to make the tools that we create as useful as possible to others.



[00:09:18] Eric Nantz:

Yeah. It was really excellent, summary here. And, also, I I kind of laugh about this, but I stumbled into this by happy accident almost as I'm updating an internal package at the day job where I'm deprecating a couple of function arguments in favor of a more simple third one. And I tapped into, you know, looking at the r packages online book, talking about their deprecation section, and I saw a package called life cycle, which is what the tidyverse often uses to give these messages. And, apparently, they default to once per session, as you said, with the Arlang options of, hey. This function parameter for, like, say, tidy r, gather, or spread is deprecated. Please use this instead. And lo and behold, I was able to tap into that with my internal package as well where, you know, for it's been over 5 years of existence, if not longer. And it's only now that I'm introducing this, in essence, soft deprecation right now.

And then a version later, it's gonna be like it's gone after that. But I'm being nice right now and saying, hey. Guess what? Use this new path argument, not these, like, 2 arguments instead. And this Boeing display once per session, so they don't get annoyed by it, but enough to get the hint. So it's nice that life cycle is just another one in these packages. We wanna see how others wrap the use of Arling or COI. That's a great demonstration of it. And, yeah, Mike, it's something where I just haven't done this a lot in practice, but seeing what options are available, whether it's just a simple Boolean to turn it on and off or the different levels of it. Another great complement to this as well, if you wanna do more systematic messages and maybe parse by other systems, a lot of these concepts also apply with the logger package, which I've been using quite a bit in my more back endy kind of package development in Shiny apps where I need to send that session kind of operation pipeline diagnostic to not just the r console, so to speak, but also to, say, a database where I'm keeping track of all the activity so I know where maybe some of the gotchas are that or where time is being spent on my app or my package. So there's lots of the principles here can apply in many different ways. So it's really, really good to see here.



[00:11:34] Mike Thomas:

I couldn't agree more, Eric. And those are 2 great shout outs to the life cycle and logger package. I think that complement these concepts, similarly.

[00:11:52] Eric Nantz:

Speaking of compliments, I I don't have a great segue for this, but I'll go with it. Bear with me, Mike, here. But, you remember the early days of the Internet when you would load these pages that were, you know, configured by frameworks like GeoCities, things like this? And there might be a page that you found that's kind of entertaining. But do you notice that it's taken a while to load? Has this big image, and it just scrolls slowly, slowly, slowly, slowly into focus until it's finally rendered. That's one of those cases where an image was probably uploaded in its most raw form possible, you know, straight from whatever software they use to produce it or or camera that they took a picture of. Who knows what else?

Well, this can also happen if you're developing or writing a blog, maybe with R Markdown or another framework. And you notice that, yeah, that image you put on there, that's that's a bit hefty. Well, there are ways that you can, optimize that for web viewing or other documents that you want to minimize the footprint of. That's where our second highlight comes in. This is a blog post called optimize your images with r and the resmush API, fun name. This is authored by Diego h. I couldn't quite track down, what he does, but, apparently, he is part of the rOpen Spain project, which is quite intriguing.

But he talks about a recent need that he had in in his, in his work with one of his package vignettes, such as his package called TidyTerra, which has a lot of images. And he likes to include precom precomputed images or precomputed results in his vignettes. And he noticed that that was producing higher size file size images that maybe the CRAN maintainers wouldn't necessarily like in big nets and such in PDF form or whatnot. So he decides, write this r package that ties into this, service called resmush where it's an API.

And this is interesting. It's freely available. Don't need your API keys for this, at least yet, where it'll give you for an image of, say, 5 megabytes or less, it'll give you a way to compress that with various optimization algorithms. Throw your PNGs or JPEGs or bitmaps or whatever have you, and it will give you that compressed image. And with the demonstration in this blog post, it really looks like no discernible difference. Really top notch, easy to use. And it seems like this is a great use case for not just those static files you might produce as part of a standalone document, but it looks pretty online as or friendly for online files as well.

So I yeah. I'm intrigued by this. I never heard of this service before, but curious, Mike, if you had to deal with image sizes and looking for a way to optimize those further like this? I have but,

[00:14:56] Mike Thomas:

you know, I haven't found a great tool for optimizing image sizes. So this blog post is is very, very timely. It's it's pretty incredible to me that he was able to get this, you know, geospatial image down from from 1.7 meg to, like, what is it? 762 kilobytes? I mean, he almost took an entire meg off of this image size and I can't tell the difference at all. And obviously, when we are submitting packages to Kran, I think 10 meg is the the top, the largest package size that you can submit to to Crayon, if I'm not mistaken, Eric. Does that sound right? That sounds about right. Yeah. That sounds right. So I mean if if you have vignettes, detailed vignettes, especially that are containing images and things like that, that can you can hit that fairly quickly with some of these additional assets like images and it makes it very difficult.

I will say this blog post is going to be super helpful for some folks who are trying to create detailed vignettes, with images that are looking to to ensure they stay under that CRAN threshold. It looks like resmush, allows a lot of different formats that all of some of the existing packages like XFun, TinyR, OptOut, maybe handled a couple of these cases like PNG and JPEGs, but but not necessarily GIFs or bump files or or TIFFs or PDFs. And ReSmush handles all of these different formats, I guess, except PDF. It looks like it's the only format that it doesn't handle. But, obviously, I think most of the time when you're you're working on image compression, you're you're typically working with probably a PNG or a JPEG or a TIFF file or something like that, on the case of some of these geospatial images. So this is super handy. One of the other things this just reminded me of that I I will call out as well is if you are trying to create very thorough documentation around your R package, particularly with a package down site, you'll often see a section on the package down site that has articles on it. And initially, I thought that that was just sort of an, representation of the vignettes that you have in your R package.

And it is. But you can also do something called create an article, which is not a vignette, but it lives on your package down site within, you know, within that same article's section, but doesn't go to CRAN if you were to essentially submit that package. It's it's it's ignored from from the r build, and you can create one of these articles, if you will, using the use this package, use article functions. That's really handy, I think, when you are trying to, you know, add additional documentation around your package that isn't necessarily a vignette that that folks using your R package in an interactive setting within an r an IDE necessarily need to be able to have access to. You know, as long as they have an internet connection, they can go take a look at, this this article's section on your package down site. And it's not something that's going to be built as part of your r package. So it'll help save that size if you are looking to keep your package size, smaller. So I think these are all fantastic resources. I'm excited to check out the re smash package. It looks like it takes advantage. Eric, you you may have mentioned this, of this re smush dot it, free online API, that does a lot of the horse work here, which is is pretty cool. So shout out to that service as well, and and shout out to Diego for this awesome r package.



[00:18:33] Eric Nantz:

Absolutely. And, apparently, this ReSmush, service has been used heavily in content management frameworks like WordPress, Drupal. I mean, those things may sound familiar. So, apparently, it's got its battle tested as they say. So I'm curious to give it a shot. And and not to sound too meta here, but in our weekly itself, whenever you see images in each issue, we actually do have some routines that go through the images that the user has linked to. And we we used to have an automated way of doing this. Now it's a script way of doing it where we compress those images using some other utilities. So I'll have to see if Resmush can give us an even better take on that. But it is a nice segue to say that there are more than one way to do this in the our ecosystem. So, Diego's blog post below talks about some functions packages from EWAY, who, of course, we've mentioned many times on this very podcast.

His x fund package has two functions called Tinnify and OptiPNG that if you have those utilities on your system, well, one uses a service called tiny PNG, an API service. Another uses an OptiPNG system library. So, yeah, you may you may find that there are other routines in your ecosystem that can help you just as much as the resmush one. But it's a good roundup at the end of the blog post if you wanna see just what is available in this space because, as usual, there's always more than one one way to handle it.

These will have their advantages and disadvantages. But, hey, an API service that's free to use, it almost sounds too good to be true, but, apparently, it's still there. So we'll we'll keep leveraging it, I guess, after seeing this blog post.

[00:20:16] Mike Thomas:

Free to use and no API key. It's pretty incredible.

[00:20:33] Eric Nantz:

Yes. And wouldn't it be nice, Mike, if everything we could do just didn't really have a big cost to it or much time involved? It just magically worked. Well, sometimes you have this idea for a major project that can benefit perhaps the entire our ecosystem, whether it's a package or a suite of packages or to help with community efforts as well. Well, this is a time of year. If you wanted some backing for it, this is the time to think about it because our last highlight is more of a, my call call to action or public service announcement here where the our consortium for another year is about to open their proposal, opportunity for their infrastructure steering committee to accept, you know, proposals for grants backed by the r consortium project itself. So if you're not familiar with the r consortium, as we talked about quite a bit on the podcast, but just to state a brief background of it, it is a joint collective effort of multiple industry multiple companies within industry as well as the R Foundation to help bring support for infrastructure of the R project itself and community efforts that support the mission of R itself.

And so starting March 1st this year, they will have the open, open call for proposals. And you may wonder what will these proposals look like. Well, guess what? On the our consortium site, they do have a section on all of the already funded projects. If you wanna get a familiar or you wanna get familiar of what's been funded before, both on a technical level and community effort. And this call for proposal is a bit on the technical side, but it can be used to help, you know, things with community efforts.

Actually, a a listener of this very show, a regular listener, John Harmon, actually has been funded by the r consortium with this API to r package development, which is we've been keeping eyes on that in the suite of packages that John's been developing. That's been really fun to watch. We've seen efforts like DBI go through the R Consortium, which is, of course, the translation of database APIs to R. That's been huge in my daily work and seeing DBI really take their infrastructure up, you know, a few notches.

That was that was great to see as well. And like I said, technical efforts to bootstrap the community as well. So there's lots of opportunities here. So if you or a team are thinking of a way to help the R ecosystem, help the R community in general, and you want support from the R Consortium, this is the time to get those proposals ready to go. But I can speak from experience working in the ARC consortium and my, submissions working group. It's been a pleasure to work with them, and they've been instrumental in a lot of our life science efforts. So certainly, highly recommend to check that out.



[00:23:34] Mike Thomas:

No. And, again, this is one of those things that just makes me love the our ecosystem. You know, the fact that we have initiatives like the ARC consortium who are are trying to essentially, you know, further the science and improve the world using our software. And it's it's incredible. It warms my heart. So, you know, this is also another reason if you you have, you know, at year end, you start thinking about donations and causes that you or your company may want to support, you know, take a look at our open site, take a look at our consortium.

I guess this is part of the Linux Foundation projects Correct. Which is is pretty cool. And consider, I would say, contributing, to one of these causes because because they're phenomenal, that they help everybody. And I do like the fact that this is both a technical and social sort of call. So submit your proposals, check out the blog post. All the information that you could ever need is in here, including a link to learning more about how exactly you can submit that proposal. And, those important dates around the grant cycle are are March 1st, closes April 1st. So you have that that one month window. And then, I guess, there's a second grant cycle that'll be later in the fall of 2024.

So keep your eye out. I'm sure we will, rehash this as some of these dates get closer. And looking forward to seeing what comes out of this next round of proposals.

[00:25:00] Eric Nantz:

Yeah. And, I I've got my eye on this in a few ways. So, that'll be the, teaser in this business to say stay tuned. But in any event, as you said, Mike, we're we're gonna continually keep an eye on this. And you may be wondering, well, how do you keep an eye on efforts like this in general? Well, guess what? It's all in our weekly itself. Right? Each issue does have a section on upcoming events. Maybe it's webinars or presentations where you might hear about these efforts, and there is a specific section on grants and funding proposals. So this is right in that very section. So that's why you if you haven't bookmarked our weekly, you definitely should. Am I biased? Heck, yes. I am, but in a good way, of course. And speaking of the issue, there's always more than just the highlights that we touch on here. So we'll take a few minutes to talk about the additional highlights that we found here. And for me, I found a, quite entertaining post from Angela who has the r critique blog about some of the discoveries she had about, quote, unquote, hidden objects.

Definitely had her, books like scratch her head a little bit. At a high level, what she discovered is that in r, much like you have in your Linux or Unix system, you might have, in the case of r, objects that have a period in front of their name. That's kind of in Linux and Unix, and I'm a clencher, to denote what we call a hidden object, which by default viewing, you're not gonna see right away, only if you do, like, a special option to show hidden files. Well, guess what? When she tried to wipe out objects in her Rstudio session using, like, the erase button in the environment pane or whatever it is, she saw a dialogue appear that says, okay, so wipe out all your objects. And then there's a checkbox that's saying include hidden objects as well.

That was kind of bewildering, and that would be for somebody new to this. But apparently, you can take advantage of this in r. Maybe you want to make an object that may be more, like, behind the scenes and you don't want it to, like, be up front and center in any environment viewer. Putting the period prefix on on that could help in that case. In any event, she does have some interesting commentary on how that might not be the best approach depending on your situation. But nonetheless, one of those things where I can sympathize seeing some as odd as that and kind of going deep into just why that is.

And, apparently, it's still a feature request to be able to view those, like, more easily in in the viewer. But, yeah, to each their own, I guess. But entertaining read nonetheless.

[00:27:38] Mike Thomas:

No. That's that's a great call out. And sometimes, when I am removing some of these objects in my environment, and I think I'm always including that that checkbox, including the hidden objects as well, sometimes my memory doesn't really decrease as much as I was sort of expecting it to. So I gotta look in into that, another hidden, hidden feature of of our studio that I have to dive into, maybe for another blog post for another day. But an awesome blog post that I am super super excited about because I've been watching this project.

It's been, I think, probably over a year in the making with some folks working really, really hard on it from ideation to now, production, is the censored package has arrived. I believe censored 0.3.0 is now on CRAN, and this is a package that allows you to work hand in hand within the Tidymodels ecosystem, for censored regression and survival analysis. So if you are in the life sciences, wink wink, to to somebody I know, I I think you may be very excited about this package as well. You may be surprised that even for some of us who are not in the Life Sciences, for credit risk problems, which are problems that that we work on Fairly often, we may want to know the time until a loan defaults or Absolutely. Until some bad credit event takes place or some good credit event takes place. You can you can have the opposite. Right? If somebody's gonna gonna prepay their mortgage early, I I kinda wanna know, when that could potentially happen. But now I'm now I'm getting into the weeds, because survival analysis makes me makes me very happy and and gets me very excited. And I'm nerding out, but this censored package is all I've ever been looking for to be able to work doing survival, regression analysis within the Tidymodels ecosystem.

There's all different, type arguments that allow you to predict, survival probabilities, the the quantiles of the event time itself, the hazard, just based upon tweaking this one argument, called time within the particular engine, that you are setting. And if you use tidy models, you'll be familiar with the syntax of how you construct these models. It's gonna feel very natural. So I'm I'm very excited about this. Big thanks to Hannah Frick and Emil Fettfield for, I think, doing a lot of the legwork to get this package, onto CRAN and in a place where we can all use it.



[00:30:03] Eric Nantz:

Oh, this is awesome. Yeah. I remember when Tidy Miles was kinda first person on the scene. The survival analysis was requested quite heavily in the beginning, and it's great to see, yeah, to see this factored in. Hey, man. I have a soft spot for survival techniques as well. My dissertation used this quite heavily, and, boy, we didn't have tiny miles around back then. So don't don't look at my r code from back then. My goodness. Shout out to those that dealt with the s sweep and and latex compile issues. I see you. I hear you. I've been there. Done that.

But, yeah, sensor looks absolutely awesome. I in fact, we have some projects going on that are, you know, not quite the typical, like, did a patient survive or not? It's, certain events in, say, a trial, like, life cycle of, like, enrollment prediction and things like that, and and we could definitely use some time to event analysis on that. So I will definitely be keeping an eye on this.

[00:31:00] Mike Thomas:

Me too. And, just to to let folks know, it looks like there are probably 11 different engines or algorithms that you can use within, this censored package all the way from tree based models to proportional hazard models to, random forests and and more of your traditional survival regression model. So lots of options here to leverage sort of that that tidy models feel and approach to survival analysis problems.

[00:31:28] Eric Nantz:

Yeah. It it fits really nicely with the whole ecosystem in general, and I definitely am expecting that as the community gets our hands on this, we're gonna see more engine supported. In fact, if I had if in a time, I would go back to said dissertation research and see if I could augment what's called a competing risk engine to this because that's exactly the the type of survival technique I used. Hey. Maybe someday. Who knows? But, nonetheless, ER communities, you know, and the tidy models team does a terrific effort once again.

And there's a whole boatload of terrific efforts you see in each our weekly issue. If you haven't known by now, please please do this. Bookmark arweque.org. Just do it now. You'll never be disappointed. There's so much great content on here. Awesome packages that we didn't scratch the surface of, especially as I see this issue a lot in the space of spatial visualization and other visualizations. I can definitely help an EDA, like an exploratory data analysis, and to visualize these things quite quickly.

Some awesome enhancements to data. Table, which we covered about last week, getting more details on that. And, of course, upcoming events that we're seeing in the r community. So definitely bookmark rweekly.org. And, of course, this is for the community, by the community, so to speak. So we value your contributions from wherever you are. If you found a great blog post, new package, or other resource that you want the world to know about, we're just a pull request away, folks. It's all marked down all the time. If you can write a sentence in plain text, you know how to do this. And it's simply linked directly in each each issue. We have a link directly to the upcoming draft. Very easy to send us a poll request on that front. And, also, we'd love to hear from all of you, not just for your poll request, but how we're doing on this very podcast. We have a little contact page linked directly in the episode show notes. We also have a fun little opportunity if you have a new modern podcast app like Paverse, Fountain, Cast O Matic, many others.

I just responded to somebody's post on Mastodon asking for a better podcast app, and I sent them to a nice resource called podcastapps.com, which has a whole boatload of these to choose from. They can send us a little boost along the way if you wanna get in touch with us directly. And then, also, we are available on the social medias, mostly on Mastodon these days for me personally, where I'm at our podcast at podcast index.social, sporadically on the weapon x thing with at the r cast, and mostly also on LinkedIn from time to time with, a little little post about these very episodes. And I love hearing from you and the community. It's always great to to hear from all of you. But, Mike, where can the listeners find you? Likewise. Probably the best place is LinkedIn. You can

[00:34:14] Mike Thomas:

see what I'm up to if you search for Catchbrook Analytics, k e t c h b r o o k. Or you can get in touch with me on mastodon@mike_thomas@phostodon.org.

[00:34:26] Eric Nantz:

Awesome stuff. And, yeah, you keep up with Mike. He's he's never he's never slowing down. He's had some great posts on LinkedIn recently, so you definitely wanna check those out. And, yeah. So nice and tidy episode this week. We're gonna close-up shop here. And, again, thanks for all of you for listening. And, we hope to see you back for another episode of our weekly highlights next week."
"67","issue_2024_w_06_highlights",2024-02-07,44M 34S,"Key learnings from learners in recent R workshops, advice on navigating thorny package installation issues within renv, and a showdown of how the parquet and RDS formats perform with large data sets. Episode Links This week's curator: Ryo Nakagawara - @RbyRyo (https://twitter.com/R_by_Ryo)) (Twitter) & @RbyRyo@mstdn.social…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode a 151 of the R Weekly Holidays podcast. If you are new to the show, this is where we talk about the latest issue of Our Weekly that you can find at rweekly.org, and in particular, the highlights that have been selected by our curation team along with our usual banter and rambles along the way. My name is Eric Nantz, and I'm delighted that you joined us wherever you are around the world. And as always, joining me right at the virtual hip here is my cohost, Mike Thomas. Mike, how are you doing this morning?



[00:00:31] Mike Thomas:

Doing well, Eric. It's pretty crazy that we've surpassed a 150, recordings now of the our weekly highlights. And, I guess, what's the next milestone? 200 to look forward to?

[00:00:43] Eric Nantz:

That's right. The Vague 200. And, yeah. I know a lot of the podcasts I've listened to, they'll either do a fun little retrospectively thing, or they just might act like everything's business as usual. So we'll see what happens when we get there, but it should be fun one way or another. And this week's issue, speaking of fun, is from our longtime curator on the team, Ryo Nakagawara. So I have very fond memories of meeting him in IRL at one of the pa or r studio conferences long ago. That was a fun time. I hope I get to meet up with him again someday. But as always, he had a tremendous help from our fellow, our Wiki team members, and contributors like all of you around the world with your poll requests and other awesome recommendations.

Well, Mike, you and I are both at one point, one way or another in our various projects or consultations. We do have to do a little guidance or teaching along the way on various concepts. For me, I've definitely been doing a bit of that with, you know, helping with a little bit of inside our training in my organization, getting some analysts lined up with the latest and greatest resources that we have in your ecosystem. Well, our first highlight is doing just that. But it's a great perspective because anytime that I've done, whether it's a tutorial, a forum meetup, or a mini workshop, it's not just I'm trying to help the the learners, if you will, you know, learn a new concept.

I often learn just as much as they do, especially from that perspective or persona of somebody maybe new to those frameworks or new to the the language itself. And our first highlight today comes from Athanasia Mowenkel, who is a cognitive neuroscientist and now a great R developer who recently gave a fantastic talk, by the way, at Posikoff this past year on using R Universe for her package development. So really recommend that talk if you haven't seen it. Well, on her latest blog post, she talks about some of the learnings she's had while she was helping others learn at a recent digital scholarship days at her University of Oslo.

And in particular, she talked about her findings from 2 workshops that she gave. One was called Kortaki. Cool name. And that's an introduction to Quarto. We've actually been talking about Quarto quite a bit in our off recording here, so that's timely. As well as our project management, another area that we're gonna be touching on a little bit later in the show, actually. So in particular, in these two workshops, she had a few interesting findings that probably have you have encountered before in one way or another, but one of them is definitely pretty esoteric that has happened to me, many times before. And we'll start with quarto here Because if you're familiar with creating slides in quarto or, frankly, even Rmarkdown before that, a lot of the organization of slides is governed by the use of headings, like the the markdown syntax headings.

In particular, having a single heading is going to give you one of those kind of title like slides with a big text in the middle. And then when you do a two level heading, I. E. With the 2 hashtags, that's when you typically denote a new slide with content underneath. Well, what was interesting in the workshop prep that she did for this quarter workshop is that she noticed that there was a slide that had some nice content, and then suddenly the huge text in the middle superimposed on top, which I have had mishaps of in the past with sharing in back when before I adopted Qartle.

And I remember I was getting bewildered by just what exactly is going on here. Well, apparently, one of the learners picked up about this is that if you do a single heading but then do another heading that's 3 or more hashes in front. It's not going to do a new slide after that first level heading. It's just simply going to superimpose that big text on top of the context that was under that 3 or, say, 4 level heading, like in her example. So the switch was to force a new slide that doesn't have, say, a typical two level markdown heading.

There is the 3 dashes syntax, and this works for both Quarto, and I believe for sharing it as well, where then you force it to create a new slide. Now that's something that you kind of learn from experience. It's typical we don't have to use that, but I have used that sparingly in the past. And it's a good mental note to me and future self that I can use that same syntax of, okay, I can force a new slide with that nomenclature or that, 3 dashes instead of having the mishap that we saw in her presentation slides. But the good news is once you make the fix, it's all in version control. Right? Just fix that, commit it, push it, and recompile your slides. So all is well that ends well, as I say.

But speaking of quartile, another interesting nuance is the idea of how things are actually named between HTML and some of the interfaces we use to build the slides. In particular, if you're familiar with web development, you've probably heard about something called a horizontal rule, which is literally the HR tag in HTML, which gives you that nice horizontal straight line that can use a separate or partition content, if you will. Well, she asked her learners to add that in her portal slides. Right. And then some are using the RStudio IDE with the insert field and the markdown editor, the visual editor.

Others are using the nice command palette, which has a shortcut to start adding in commands. And you notice there's a discrepancy between the 2 is that in the visual editor, the insert calls it horizontal rule. But then the command palette version, it's called horizontal line. And, yeah, this may seem trivial, but it can trip up people sometimes. Like, well, that what did what did she ask me to put in? Why is it called different? And sure enough, you know, she put an issue on the Cornell issue tracker with this difference, and sure enough, the Pasa team has fixed this. So now it's consistently called horizontal line in the IDE now. So good catch.

But, again, one of those things that you don't really notice until you put this in front of people. So that was that was an interesting find on the portal side of things. Yeah. I think it's interesting that they called it that they I guess our studio or or posit decided to go with

[00:07:27] Mike Thomas:

horizontal line instead of horizontal rule which is sort of from a development perspective and especially web development, what it's what it's always been called.

[00:07:37] Eric Nantz:

Now the next sections of her blog post definitely hit home with certain things I've dealt with, and that's dealing with the constraints that sometimes IT will bring upon you. And that is in particular one of my biggest bugaboos is spaces and file names or directory names. I have never had good luck with that. And apparently, some of the learners got tripped up with some of this too in respect to some of the materials that she had put together for the project management piece. And again, sometimes you can't control what you can't control. Right? But she has been in contact with IT about how can we make sure that these directory names at least have a little more structure around them or these file names have a little more structure about them to try and have the best of both worlds.

Those that are doing more programming, dealing with the file systems that are being created, and those that are simply just trying to get their work done. So it can be a thorny issue. I do admit every time I tell people how to interact, you know, nicely with our POSIT workbench internally or over our HPC systems internally, I'm always that annoying person that says no spaces, only underscores and dashes in your file and their directory names. You'll thank me later. And usually they do actually. But that can trip people up as well. So I felt seen on that one.

And this next one is really hitting home is that as installations, let's say, r itself accumulate over time, you got multiple r versions, which might mean that you've messed around with settings on some of the versions, maybe not. Maybe you've interacted with those site configuration files, which are basically the installation level, our profile, or our environment files. Maybe your IT group or whoever admins have done some tricky things with library pass. Well, guess what? That what she discovered is that there are some of their shared, you know, project areas.

They had 5 library paths at various levels in the stack, and some of them were just completely empty. So sure enough, a lot of crap can happen. She has a fun little ggplot of the different r versions that they have available and how many libraries are installed inside of them. So that, again, these things happen over time. I'm privileged because our Linux team, which is top notch at my org, has a system in place where we can load a specific R version as needed with what's called the module command in Linux. So we can just quickly say if we want to use r 422 or go back to r 363 for, like, an esoteric reason, we have all that segregated away. But not everybody's that lucky. So I can definitely sympathize with the Wild West of where packages are installed.

So that was an interesting finding that she had with respect to her organization. So all in all, really entertaining blog posts. And again, just shows you that it's not just the learners benefiting from the workshop. Us on the other side of it, on the other side of the fence, so to speak. We learn just as much. So terrific blog post by Athanasia, and we really, again, I highly recommend her previous talks as well. Really entertaining stuff.

[00:10:56] Mike Thomas:

Yeah. It it seems like all the content that Athanasia is is putting together lately is is really awesome. I really enjoy, following her work and I couldn't agree more with sort of the overall sentiment of this blog post that sometimes you learn more when you teach than, what you might expect, you know, because it's one of the best debugging exercises as she puts it, that you can possibly do is to actually go out and and teach something and and say it out loud. It's it's somewhat like talking to the little rubber ducky, right, that, since developers say that you should have, on your desk. So it's it's great feedback. It's a great exercise in order to, sort of, troubleshoot, your understanding and and really solidify, sort of, what you what you thought you knew and then figure out, where the gaps exist when you actually go to teach that to other folks. So that that's a really interesting reflection that resonated a lot with me, as well as the the Quarto stuff and project management. You know, I think we've absolutely all been there been there, with multiple versions of R on shared file systems and and trying to collaborate sort of before the world of RN, then posit package manager, and, now Docker, which is very relevant to me because we have a a new open source package, and I am trying to figure out what is the earliest version of R that our package will successfully install with.

We went back to to 3.5 and realized that it breaks there because some dependencies, I think Tidyr being one of them, rely on our 3.6 or greater. So, instead of having to install all of those different versions of R on my computer, I'm able to just change the change the base image and and spin up a Docker container and and run the installation and see if it succeeds or if it fails of our package which is is pretty cool I think just overall we have a lot more tooling now around our version management and dependency management than we used to have but but I certainly remember the heydays of logging on to sort of a shared network and seeing a 1000000 different versions of R, a 1000000 different library paths as she notes that they had 5 different R library paths at various levels, across many different versions of R. So that was that was, sort of nostalgic to see for me, but a great write up. Lots to learn from here, and thanks to Athanasya for this blog post.



[00:13:22] Eric Nantz:

Yeah. And we're gonna have direct links to each of the workshops that she gave in the show notes because of material freely available. Terrific slides. Terrific, you know, hands on work in those in those workshops. So, again, if you're in the space of kind of beginning your educational journey, if you will, with your respective organization on these concepts, yeah, this is some great material to draw upon. So as we were just talking about that, Mike, one of the, features in, in the previous highlight workshop about project management was leveraging the r env package for managing your r dependencies in a given project.

And I have used r env quite a bit as the foundation for my reproducibly wares at the day job, even for my open source projects and, yes, some very important collaborations with a certain government agency and submission pilots. But like anything in life, nothing is perfect. Right? And there are going to be some snags you encounter along the way, some of which may not be inflicted by RM directly, but things that a new user might encounter in their various setups. And our next blog post for our second highlight today comes from Hugo Gruensohn, who is a data engineer with the data.org organization and part of the Epiverse project, which has been featured in highlights in the past. And his blog post is talking about some of the things that can go a little wrong when you use r m. So I'm gonna run through some of the issues that, that Hugo highlights, and then, Mike, I'll turn it over to you for some of the solutions that he talks about. But the first issue, and as a Linux user, oh, boy, do I know about this, is the issue of binary package installations versus compiling from source.

So, yes, if you are on those operating systems and you want to use a current version of a package, there's usually no problem installing the binary versions. What happens, though, is that you might need an older version of a package. And in that case, CRAN is not going to have binary versions of older package sources available. You are now into the compiling from source world that actually is the default role for most of the Linux usage of R itself. And that's where you want to pay attention to maybe the package's description file and see if they call out any system requirements.

Because there are certain packages, there are going to be system requirements to be able to compile from source. Typically, these are like, c level libraries, other utilities that you might get in your package manager, for instance. But that's going to trip up our end, especially because it might be trying to install an older version of the package from CRAN. And then you're into some issues trying to figure out, okay, what what other system library do I need? Now, there are some tools in the ecosystem to help with finding this.

I know there are some packages out, I believe, by Pazit's team on help to, you know, identify package dependency from a system level. A lot of times you'll end up Googling it anyway, and then you'll figure out the package name that you might need if you're on an Ubuntu system or a Red Hat system or a Mac OS, what kind of library you might need for that. And apparently, there are some gotchas in addition for those on Apple Silicon, I. E. The M one chips with binary, I should say, with source package installation. So there's a a good note, especially around G4tran for that. So my sympathies or anybody that's encountering that issue because that must be thorny to troubleshoot if you don't know what you're looking for. So I know this from experience because when, you know, we mentioned Michael Mike just now that with Docker environments, guess what? That's Linux. So you're gonna have to put in those system dependencies before you start installing those packages. And that if you don't know what you're looking for, that can be tricky. Must use r two u. There you go. Plug right 1. So they're trying to simplify this if if you're in the container world. But if you don't know, you don't know. And now you do.

Well, there are other issues to deal with as well in this space is that maybe you've done the homework. Maybe you've got that system dependency already installed. But how long ago did you install that? And did a recent package that ended up having to be compiled from source utilize a newer version of that same library? That's the example that's in Hugo's post here is that, for example, the matrix stats package had a compilation error from those trying to install 1 from a version from 2021 about a double max double X max undeclared variable.

And sure enough, there was an explanation for this in the release notes of matrix stats that mentioned that they were moving to a different construct for this constant DBL max instead of a legacy one called double x max. And there you go. That could trip you up too because you think, hey, I've done what I needed to do. I got that system library in there. But sometimes you have to keep those up to date as well. So lots of little gotchas. And you may be wondering, oh, my goodness, I'm gonna be in this? How the heck do I navigate this? And that's where the next part of the post talks about some potential solutions in this space.



[00:18:52] Mike Thomas:

Yeah. So as you noted, you know, CRAN will only provide binaries, I think, for the most recent versions of the R Packages that are available on CRAN. However, POSIT package manager, provides a larger collection of binaries, for different package versions historically across different platforms as well via, you know, the public Posit package manager, which is awesome. So for for those using our end, by default, it may try to install the packages in your random dot lock file from CRAN. My recommendation is to switch that over to to posit package manager as quickly as possible. I think you'll find the installation experience, not only less prone to running into errors with some of those system dependencies, but also faster, if you're installing binaries as opposed to from source. So that would be recommendation number 1, and that's that's the first sort of solution, that's that's positive here. No pun intended.

And then they talk about extending the scope of reproducibility and introducing the Rig Package, which is honestly a package that I have not used enough, but absolutely should. And RIG is a package, an R package within the Rlib, ecosystem, and it allows you to sort of go back and forth between different versions of R. I believe you can run code against multiple versions of R at the same time. There's some pretty pretty wild things that you can do with RIG that I think help, solve some of these issues that you may run into, working with different versions of packages across different versions of R. So I think Rig can be a really helpful tool for for troubleshooting, or doing some of that exploratory work to make sure that your environment is set up correctly and appropriately in a way that's that's not going to fail. And then, obviously, you know, these we've talked about at length, even discussed already in the highlights, but there's there's Docker, there's Nix. Shout out, Bruno Rodriguez.

He has a series of blog posts on Nick's which are linked within this this blog. So hopefully, he's he's super stoked to to see, Nick's being represented here again with all of the fantastic resources that he's put together there. There's a link to using our end with Docker, the the RN vignette that's that's within the RN, I believe, package down site, as well as a link to a paper, that's an introduction to Rocker, which is one of the most popular images out there for working with R, and that paper is authored, by none other than the the creator of R2U himself, Dirk Edelbuttel, who I was talking about, as well as Carl, Boettiger.

So that might be a paper that you may be interested in in checking out that was published in 2017 in the R Journal, but some just fantastic resources here, that allow you to explore some of the different potential solutions for for handling these issues that you might be coming coming into when you are trying to to work on a new project with potentially an older version of R or older version of R packages. And sort of the the final note here, and to summarize this blog post in its entirety and, Eric, I think you can you can share this sentiment.

I don't think that package management is a solved problem quite yet at this point, our environment management. So I think this there are a lot of similar sentiments in the Python ecosystem as well. There's there's Pynth, there's there's Vnth, there's Pynth, there's Pynth. A lot of different ways to go about trying to do it and I don't know if any of them are perfect. And in our end is obviously, you know, in my opinion, at least, you know, the the the most, recent and and sort of, you know, best attempt at package management thus far in the R ecosystem.

I think it it, improves upon some of the things that Packrat, the previous package management, package, tried to handle. I know that there is the pack, p a k, package as well, which does allow you to create sort of a a lock file as well and and manage, some of these things. But, again, I I don't think it's a a perfectly solved problem yet. Maybe it never will be. You know, it's a very tricky thing to manage. But I think, in terms of some of those issues that you may run into when using RM, this blog post is a great resource on some of the ways that you can try to troubleshoot those issues.



[00:23:17] Eric Nantz:

Yeah. I echo a lot of those same thoughts, Mike. And it does take me to even just at a broader level, the issue of distributing software even just on Linux in general, because there are a lot of issues that are very common here with what's happening in the R ecosystem of packages and the Python ecosystem of package dependencies. There have been some new standards in place to help give developers kind of a single, quote, unquote, single target so that those on any Linux distribution, no matter what, can install these software utilities.

I'm thinking of flat pack is one that's gotten the most attention with Snaps probably close behind that. And R, you're right. There's a lot of different ways to tackle this, and I don't think there is a perfect one in place. I do think what needs to happen, though, and I think this blog post is a great kind of precursor to it, is that these different paths of reproducibility that you want to take, whether it's the full system reproducibility, talking to the full stack, if you will, or if it's just the package dependencies, just that perspective. Those personas can mean different things into how far you go with these solutions.

So, certainly, what I'm keeping an eye on is, yes, I do often integrate r m with containers, but not r m valve of the box. I am gonna configure a little bit to my liking to make sure that it plays nicely, like you said, with deposit package manager, a huge win for container development and package environments. But also, again, you shouted them out. Bruno is on such a role here with spreading the message of of Nicks, in particular the Rick's package that he is codeveloping. Nix is taking a lot of, you know, I would say, mindshare in the general software development communities.

Certainly, it's a huge topic of the podcast I listen to. And I think with time, we're gonna start seeing some enhancements to what Bruno is working on with Rick's, but also maybe others sharing their thoughts on it. I know quite a few people in the community are starting to dip their toes in it, myself included, still got a ways to go. But anything that can simplify that full stack with or without containers, I think is going to come up kind of above the surface, if you will, as teams and organizations figure out the best way to tackle this. But there was a nugget in the in the conclusion here I wanna emphasize here is that when you have multiple team or multiple members involved on a team for a reproducibility kind of project, there needs to be a real team effort to keep up to date with everything.

And I still recommend that if there's, like, a even a 2 person or more team, that one person is kind of in charge of kind of handling the RM side of things if you're doing RM for your package management because, trust me, there'd be dragons when you have multiple people clobbering that RM block file in a GitHub repo and not knowing which one is which. Which change should I pull into? So you're smiling. I know you know what I'm talking about, Mike. We've been there and it is rough when you don't have that delineation

[00:26:32] Mike Thomas:

set up front. I think there are a ton of organizations out there that struggle with this. We do a lot of work around this to try to set up, Data Science teams and Data Science collaborative workflows within some of our clients organizations that we work with and you know like you said it's an it's an evolving, you know, not perfectly solved problem but you have to you have to implement a framework and you have to framework and you have to set some sort of controls around how you're going to at least try to employ some of these best practices for collaboration, between team members across projects.

Otherwise, you'll just be in a world of pain.

[00:27:07] Eric Nantz:

Yeah. And, you know, data science is hard enough, folks. We don't need more pain alongside our data science adventures. So, yeah, certainly, if you've had your share of ill successes or, frankly, maybe even not so great moments with package and environment reproduce. We'd love to hear about it. We'll tell you how to get in touch with us later in the show. And rounding out our highlights today, some of that's right up both of our wheelhouses lately in different ways. But, you know, we've been pretty vocal on this podcast and some of our other ventures about it's a it's a new era in terms of data storage formats. We're talking about databases traditionally or some of these newer methods.

And in particular, a format that we are very excited about is the parquet format, part of the, you know, Apache Arrow project. There are lots of interesting ways that you can leverage this technology to streamline your data storage needs. And, yeah, my cohost here, Mike, yeah, you know a thing or 2 about this. But, this this last highlight is coming from Colin Gillespie, who is, the CTO of Jumping Rivers, who have been big proponents of advancing computing and their data science consulting projects and blogging for all of us to to learn about. And this is part of a series of posts that are diving deep into Apache Arrow in respect to the R ecosystem.

And in this blog post here, Colin talks about some of the benefits that you can see in parquet versus what is the traditional format that we've been using in the our ecosystem since frankly the beginning of the language, and that's called the RDS format.

[00:28:53] Mike Thomas:

Absolutely. And you know that I am a huge fan, of par the parquet format and sort of all the advances that have come within, data storage in the last I don't even know how long it's been. 12, 18 months between parquet, DuckDV, all those things. It's it's happened very very quickly. And Colin leverages, one of the most popular, I think, built in datasets, I believe within the Arrow R package, which allows you to easily work with, the parquet format files and query them using dplyr syntax, which we we know and love. And that that dataset is called the NYC, data and I believe that's that's on New York City taxi data, which is a pretty pretty large dataset, so it makes for a good example when wrangling and querying this large parquet file. And so one of the, you know, big comparisons here between parquet is RDS files as you talked about, Eric, which is a file format that us as our users have been using, I think, for as long as ours been around or as long as I can remember at least for essentially saving any type of object right it could be a data frame could be a list could be a model often so it's a very flexible file storage format and you know to date when we typically compared you know RDS storage to like a CSV especially if you are storing a data frame and most of the time that RDS file was going to be smaller and snappier to load than a c really having to read a CSV file. But now that we have this new file storage format called Parquet, which is columnar storage, we've sort of gone through that comparison again and this time comparing RDS to to parquet file format.

And that's what Colin's blog post is doing here. And I think you'll be you'll be fairly surprised at the results in taking a look at, at least, this example, New York City taxi dataset appears to outperform, the parquet version appears to outperform the RDS version of this file across a few different metrics. So I don't know if you wanna dive into, I could do the drum roll and you can dive into the results here for us, Eric.

[00:31:09] Eric Nantz:

Alright. Here we go, folks. Yes. And, the results are in. And one thing to note with the parquet format and how the arrow package writes to parquet is it's taking advantage of a compression utility called Snappy, which is a fun little name right there. But that alone is a huge gain in terms of writing this taxi dataset to disk. And in particular, in the average of the metrics that the columns put together here, it takes on average about 4 seconds to write to parquet format of this taxi dataset. Whereas for using the gzip compression library in RDS, takes 27 seconds on average to write that to disk. Now that is some massive savings right there.

Some nuances here about parquet versus, like, the traditional things like CSV and whatnot is that parquet is column based partitioning of how it writes the data set, which means they can take advantage of repeating, say, you know, values of, like, a numeric index, advantages of, like, common character strings, advantages of POSIX times, lots of interesting optimizations. We don't have time to get into it all on this podcast, but there are also some references in Colin's post if you wanna really dive into that. So, yes, we already see writing is significant here. How about reading itself?

Now the results aren't quite as drastic, But as you might guess, because of the different way data is organized behind the scenes of these formats, it actually takes on average about 0.3 seconds or 0.4 seconds to read that into memory from parquet. Whereas for RDS, it takes about 5 ish 6 seconds on average. Now that, if you're doing interactive analysis, may not be a huge deal to you if you're just kinda doing your data reporting and expirations. But what's the space that you and I play with, Mike? Is that it's Shiny apps. Yep. It can mean everything.

Yeah. It can mean absolutely everything. And I'm literally dealing with this right now as I speak with an open source project where I don't want to load the entire contents of, in this case, a 4,000,000 row dataset. I wanna just grab what I need at the app load and then as needed, add in more. I am using Parquet for that. It is a very optimal solution. And, yeah, Mike, you know a thing or two about loading Parquet in the Shiny app. So you wrote a darn article about it, didn't you? Yep. You could find it on the deposit blog post. It's a couple years old now. It may need some updating,

[00:33:48] Mike Thomas:

but, yes, there's a blog post called Shiny and Arrow, a match made in high performance, computing heaven or something like that. So feel free to to check that out if you are interested in leveraging Parquet files to make your Shiny app so it's snappy.

[00:34:04] Eric Nantz:

Absolutely. So you can see that, you know, we don't wanna get the cliche. It depends on your use case. But how it concludes or, you know, the obvious question is, okay, you as new to this world, which one should you use, parquet or RDS for your your next project? Well, as you saw from the metrics, writing, there are just massive gains for writing volumeless data like this taxi data to disc with parquet. I think that if that's a concern to you and you're doing this on a regular basis and for efficiency, it does seem like parquet is a clear winner on that. For reading, importing into your r session, again, I think it depends on the context you're dealing with here. But I do think that, yeah, if you're in a pipeline that needs as much, you know, fast response time, whether that's a Shiny app or other situations, I think parquet is very attractive for those for those features alone.

Now, one thing to keep in mind, though, is that if you are trying to keep as lean of a stack as possible, we were talking about dependencies earlier. Right? Well, guess what? RDS is built into R. It's been built into R since the very beginning. So if you don't want to depend on the arrow package for importing this into your R session, that's another, you know, win for the RDS camp, if you will. And again, for smaller data sets, RDS has had no issues in my shiny app or my other, you know, data science needs. So, again, it's there. It's always there. You can depend on it no matter where you're running or which version of our, no headaches on that front alone.

But I did have an interesting use case for parkade. I'm gonna, you know, give a little insider baseball here on this very podcast on my exploration on this at the day job where our clinical sets are organized and, you've guessed it, SAS data sets organized across many, many, many different directory, subdirectory patterns based on the treatment, based on the study name and whatnot. Many subdirectories inside. Right? Well, we get questions from leadership about kind of how many sets do we actually have or, like, how many are SAS? How many programs do we have in this whole space that are SAS based? How many are R based? You know, can we get some metrics around it?

So no one's going to do this manually. Right? Nate, nobody got time for that. So let's see if we can read all this metadata into some form of a data structure so we can interrogate it just to go to any database. Right? I used to use a bloated and I do mean bloated SQLite database to house all this. It worked fine ish until recently because I had a silly thing with modification times. I kind of had to re pivot. So in this re pivoting, I thought, well, wait a minute here. Since there's a logical grouping and how these are organized where it's like the, I'll call it treatment ID of the treatment.

And then within that, there's an umbrella of different studies or experiments under this. Well, this is right for grouping in a logical way by those two variables. And instead of having everything written to one massive file, why not distribute these as parquet files for the metadata? So that if I know I only need one particular treatment ID and one particular study I wanna get the data from, I can get this just as easily of arrow parquet files as I could with anything else. Plus, if I need to update only a specific treatment ID and study combination, I don't have to touch the rest of the study and data combination or study and treatment combinations. I can just update that one set and it will still magically bind all together if I need to further on. The magic of Arrow and the the dplyr, dplyr packages. It's all right there.

So that is saving me immense time. And the parquet files are fast. They're they're in efficient size. And I just feel a lot more organized on how I'm keeping track of all this. So that was my recent success with parquet. So, yes, your voice was in my head, Mike, as I was in this rearchitecting adventure. Like, I gotta get away from this monolithic set. What can I do here? And Parquet was the answer.

[00:38:14] Mike Thomas:

I don't wanna sound too cliche, but I am very proud of you, Eric. Great job.

[00:38:19] Eric Nantz:

Well, as you know, we just scratched the surface here. Every R weekly issue has a lot more terrific content for you to to learn about the R ecosystem, data science, integrations of R, and many other ways to inspire your daily journeys with data science. And, of course, we have the link to the issue in the show notes, but we're going to take a couple of minutes for some additional finds here. And I want to give a great shout out to a project that just keeps rolling along and had a massive update recently, and that is data dot table just had a major new release combining with a new governance structure for how they manage the project's life going forward.

This is a really fascinating post authored by Toby Dylan Hocking, and, again, we'll link to it in the in the show notes, but a really great kind of road map of what they've done to help put a little more governance around the data dot table project. There's been newer members joining the team. There's been new maintainership and lots of transparency on what they're looking at as new features going forward. So on top of that new release, it's really a great time if you're been using data. Table and you wanna get involved with the project. They're making that even more transparent on what the road map is and their contribution guidelines and kind of where things are are at going forward. So a big shout out again to the data. Table team. They're doing immense work in this space.

Always have tremendous respect for that project, and congrats

[00:39:48] Mike Thomas:

on the release of 1 dot 15 dot o. Yes. Congrats to that team. That's that's fantastic news. I'm gonna reach across the aisle to Bruno and, shout out his new blog post called reproducible data science with Nix part 9. Rix is looking for testers. So this is a call to action blog post in the r the rix package, spelled r I x, is in our package that leverages Nix. Essentially, allows you, I believe, to work with, Nix in that configuration from R. So if you are interested in Nix for environment and package management and, want to kick the tires on his Rick package and give some feedback, I think that would be really greatly appreciated. So check out this blog post, for info on how to get started.



[00:40:36] Eric Nantz:

Yeah. Huge congrats to to Bruno and Philip, the the comaintainer of REx. They have been doing immense work on this over 5 months and and and counting according to the blog post. So, yeah, getting real world usage of REx is hugely important as they get to this stable state, if you will. And and, yeah, count me in, Bruno. I'm gonna be testing the heck out of Ricks. I've already done initial explorations near the end of last year, but I am firmly on board with seeing just how far we can take it. And my initial experiences have been quite positive to say the least. But, yeah, I'll definitely put it in some more rigor and call again, shout out to all of you in the community that have been even just remotely curious about this. Give it a shot. Let them know what do you think? Because I do think in the reproducibility story that this is going to get a lot more traction as we get more users involved. So, again, huge congrats to Bruno and Philip on getting close to this major milestone.

But, of course, he doesn't just want to hear from all of you. We want to hear from all of you too. Right? And the best way to get in touch with us, you got a few ways to do it, actually. First of which is there's a contact page directly linked in the episode show notes if you want to give us a quick shout out on that. Also, if you're on the modern podcast app train like a few of us are using, say, Podverse, Fountain, Cast O Matic, many others, Pod Home, whatever have you, there are lots of great ways to get in touch with us on that via the boost functionality again or from the podcast index directly where this podcast has probably hosted it. You can find details on that in the show notes of the episode as well. And I'm doing some fun projects analyzing the massive amount of literally podcast metadata as we speak. And it's got a lot of geekery behind the scenes with, quartile, point blank, Docker, GitHub actions. There's going to be a lot to talk about with this.

I just conquered GitHub Actions successfully on an automated run time of these pipelines, which I felt pretty stoked about because I'm still a bit of a newer user of GitHub

[00:42:46] Mike Thomas:

but I'm I'm getting there. I'm getting there, Mike. The the old dog here learns a new trick once in a while. No. That's awesome. I have no doubt that you'll nail that. GitHub Actions has been definitely a game changer for us at Catchbrook.

[00:42:56] Eric Nantz:

Yeah. It's one of those things where you just can't imagine how you live without it all these years, but it is there, and it's a great service. So and, also, if you wanna get in touch with us on the social medias, I'm mostly on Mastodon these days. My handle is atrpodcast@podcastindex.social. I am sporadically on the Weapon X thingy with atdrcast and also on LinkedIn from time to time on there. And also a quick reminder, we plugged this a couple of weeks ago, but the call for talks is still open for the upcoming Epsilon Shiny conference. So if you have a talk you'd like to share with the rest of those shiny enthusiasts out there, Mike and I are obviously big fans of this conference. Yeah. We'll have a link again to the conference registration and talk submissions in the show notes. And, Mike, where can a list who's gonna get a hold of you? Sure. You can find me on LinkedIn by searching Catchbrook Analytics,

[00:43:50] Mike Thomas:

k e t c h b r o o k. Or you can find me on mastodon@mike_thomas@fostodon.org.

[00:43:59] Eric Nantz:

Very nice, Mike. And, yeah. You know, Mike deserves some extra praise here. You're not gonna know this from the polished version you hear of this episode, but he had to put up a lot of shenanigans during our recording today. So my thanks to you for putting up all that. As if you haven't had to put up with me on other episodes. So we're even. Yeah. We'll see who who causes the chaos next time around. But in any event, that's gonna wrap episode a 151 of our movie highlights, and, we're so happy to listen to us, and we hope you join us for another episode of RWBY highlights next week.

"
"68","issue_2024_w_05_highlights",2024-01-31,23M 40S,"The R-Weekly Highlights podcast has crossed another milestone with episode 150! In this episode we cover a terrific collection of development nuggets of wisdom revealed in a recent package review livestream, and how a feature flying under the radar from Git can facilitate investigations of multiple package versions. Episode Links This week's…","[00:00:03] Eric Nantz:

Hello, friends. We are back at episode 150 of the R weekly highlights podcast. I knew we're gonna get to an awesome number, and we finally did. We're happy to have you join us from wherever you are around the world where we talk about the latest and greatest highlights that we have seen in this current week's our weekly issue. My name is Eric Nantz, and I'm delighted that you joined us today. And as always, I have my awesome cohost who never stops the hustle, Mike Thomas. Mike, how are you doing today?



[00:00:27] Mike Thomas:

I'm doing well, Eric. I am going on-site to a client for the first time in a long time. So, I'm showered, dressed, you know, all before 9 o'clock which is

[00:00:38] Eric Nantz:

occasionally unusual. I'm not gonna fully admit to that but, yeah. Looking forward to that today and looking forward to a a quick, highlights here. That's right. Yep. You got yourself ready for the the old business professional look. I'm doing that tomorrow for an on-site thing. So we got our our week of on-site stuff. But guess what? The power of virtual means we can do this from our comfortable homes for this episode. And this episode is not possible, of course, our weekly itself. And this week's issue was curated by Batool Almazak who, of course, had great help from our fellow Rwicky team members and contributors like you all around the world.

Now, Mike, you know that in our little post show last week when we were just getting our files sorted out, I had kind of lamented the fact that I was a little jealous of a certain individual that just did a really fun screen cast of a package review. Well, guess what? That is our first highlight here. I'm referring to a live package review that was conducted by Nick Tierney, very well established member of the R community and has cooperated our open side quite a bit in his tenure. Yes. And he did a terrific package review of JD Ryan's soils package, which is one of a very, ambitious, yet very powerful package trying to help surface up some very innovative data and innovative workflows for her team. To Nick's credit, he was very practical and very upfront with some of his process of evaluating a package, which, again, draws a lot from his rOpenSci roots.

And at a high level, a few of the things that he illustrates here that I definitely need to take note of is using from the good practice package a function called GB to literally automate the more standard types of checks that they would do in rOpenSci whenever a package is on board, which, again, everybody can benefit from because it's not like rOpenSci has some esoteric requirements. These are all great practices for software development and especially in the space of our package development. Also, extensive use of the COI package. We've been singing the praises of COI quite a bit, and Nick had some nice, you know, targeted comments on making that even more seamless to give a more friendly looking message for various notes, bullet points, or even error messages that can occur in this workflow.

And then, also, the use this package comes into play yet again. Right? We use this quite a bit in package development and getting kind of the basics of package documentation lined up with the usepackagedoc function is a terrific way to get that package level documentation up and running quickly. Throughout it, very much, in JD's blog post that we're linking to in the highlights here, she says she's rewatched this a couple of times, and her blog post is literally going through the recommendations that Nick had and the ways that she is now improving the soils package, even things like logic and the functions for directory and file paths checking. These are all things that we sometimes take for granted.

But what was also interesting is that Nick had in the live chat other really well established members of the community, such as Miles McBain, also giving his 2¢ on some of the operations that the package was doing and high level looks at how things are completely organized. So, yeah, some of the things I took away, Mike, is, yeah, I need to invest in this good practices package a lot more because that's gonna help me up my game with documentation as well. And, also, some of the nice pointers that they have here in the package documentation itself with the markdown files and the snapshotting of tests and various practices for committing these with descriptive messages. So, again, worth a watch for sure to see Nick in action because to me, I love seeing the journey just as much as the destination to rip off some friends in the Linux podcast and ecosystem.

And Nick really showed the the actual process of package review, which I think is extremely helpful from no matter where you are in your organization or academic institution. There there are a bunch of nuggets for you to learn from here.

[00:05:09] Mike Thomas:

Yeah. I'm super impressed with the soils package. I'm super impressed with the work that, JD and her team at Washington Department of Agriculture have done. It's really exciting for me to see, you know, government organizations, large organizations, you know, not only using R, but like creating beautiful R packages, and package down sites, and utilities for their team, and may maybe for for others to use as well and doing some of this work out in the open for us to be able to take a look at it, learn from it and potentially even contribute to it as well. You know, one of the the really interesting things, there are many really interesting things here, in my opinion. So this package is not on Quran. I don't know if they have the, the desire to to put this package on Quran, but it is on our universe.

And we recently, at Catchbook authored an open source package that, obviously, folks can install from from GitHub. We haven't pushed it to Kran or submitted it to Kran yet either. But I would be interested in seeing, I guess, the the process. And I should know this by now because we've covered our universe enough on this podcast. But the process of getting a package onto our universe that isn't on crayon. I believe that there's some workflow that Yaron has for our universe to actually take a look at what is on crayon and sort of copy that over Right.

Onto our universe. But I didn't realize, I guess, what the workflow was for submitting a package to to be on our universe, but not necessarily on crayon. And that's not not speaking ill of crayon, but I think there's just some particular packages, you know, in our case that, maybe, aren't necessarily worth going through, the entire crayon workflow for. This is a really really really cool idea and I learned a ton from this, you know. Take a look at the YouTube video. Take a look at how Nick walks through this package, and Miles and Adam, walked through this package and the different things that they call out in terms of things that, she did well, things that she didn't do well. 1, I don't know if you have the GitHub repository open, Eric, at all.

One thing that's like blowing my mind a little bit that I can't figure out is so in the read me, which, you know, extends to the package down site, it has a bunch of videos in it that are video demos on how to create a soils project, render Right. A word or an HTML report. And if you look in the read me on how these videos are sort of embedded, there's a link to the same GitHub repo and a folder called assets. Mhmm. And the folder called assets, I can't find on the repository anywhere. Wow. And it's also not get ignored. That's interesting. So I'm I'm curious as to how, like, maybe at build time when you're you're building the read me, she was able to embed these these files with a link to this this assets subfolder. But unless I'm going crazy, I can't find it. So that's really cool because one of the things that's that's called out in the blog post is the package size is very large, which would be an issue if you're submitting to CRAN, but not necessarily an issue otherwise. And and, you know, when I'm I'm thinking about packages that are large, I'm I'm obviously thinking, you know, what sort of types of files could be within that package to to make it large. And then, I I looked at the read me right away and I saw, oh, we have a bunch of video demos that must that could potentially be it. But I can't find them in the read me anywhere. So I'm very, perplexed to say the least.

And then, maybe, the the last thing that I'll I'll call out here is, you know, her team went all the way down the path of being able to use the Rstudio IDE to create a new Soils Rstudio project. The same way that we would create a new Gollum, Gollum package. Right? Or create a new new R package through the RStudio IDE. And the the little hex sticker from soils is is on the RStudio IDE right there for for creating a quarto soil health report. It's incredible. Obviously, this this package, that they've created is going to make other folks in our organization's lives a ton easier to just get their projects up and running sort of immediately instead of having to start from scratch. So if you are someone working in an organization where you find yourself do doing the same types of projects over and over, and our package could be a huge benefit to you and a great place to start for a template would be this soils package. It's phenomenal.



[00:09:53] Eric Nantz:

Yeah. There's a lot to unpack here on what JD's done. And as you're talking about the the videos and the Remi, in this blog post, I'll put the direct link to this in the show notes. She does say that there is a video tutorial from GitHub directly on how to pull this off. And it does have to do with GitHub flavored markdown. So he must have done some magic with GitHub itself. So we'll link to that, directly because my goodness, if my package is on GitHub, I definitely want to take advantage of this and make it easier for people to see some of the workflow in action for some of the packages I have in mind in the Shiny space in the future. So lots of great points, Mike. I think knowing that many of the analysts that she's working with and, frankly, the ones I work with are are using POSIT or Rstudio as their front end to this, using that new project feature and getting things ready right away is just so helpful for them. I mentioned I'm on a kind of a crusade at the day job to help make some of these initial clinical projects easier for people, and this project feature is going to be something we look at quite closely here. Definitely.

And, yeah, you and I, Mike, we've been living the life of creating the internal or company packages for our various clients or stakeholders. And a lot of times, things move. Sometimes things move pretty fast. And sometimes, we might need to check just what happened, maybe a version behind or 2 versions behind. But then you're kind of wondering, how do I handle that? Do I have to do a whole separate r installation on a virtual machine that has, like, an old version installed? Well, we have some good news for you, folks. If you are leveraging Git for your version control, whether you're putting on GitHub or not, but just using Git for version control of some sort.

Mylesalmon is back on the highlights once again. Definitely a repeatable pattern here in a great way because she has discovered in her continued, I would say, journey of leveling up her git knowledge that there is a way to kind of have your cake and eat it too of loading different r package versions kind of at once without a lot of fuss involved using a very, I would say, niche feature in Git called Git Worktree. I admit I have not seen this at all, and I've been using Git for over, what, 10 years. I did not know about this feature at all. So let's break it down for you real quick. Yeah. Yeah. We're both learning something here, Mike. So I think what you and I are familiar is a concept of branching. Where in branching, you could say, I'm on my main branch, but I know I'm gonna work on this new feature or new bug fix. But I don't wanna commit that to main yet until I get through this fix. And I'll do a new branch to work through that, iterate, and then push that up and do a code review or whatnot to push that into main.

I knew about that, but Git work tree is a little different. And in fact, it's more comparable, not so much the branching, but to the idea of Git stash where you just wanna put things aside in your working area of Git for a bit and then maybe fix something else real quick and then bring that back forward when you're ready. Well, apparently, with Git work tree, you can and as Mao's post illustrates, create a new folder somewhere on your computer and then have that folder be linked to that same Git repository of that package but to a different state of that package, maybe based on a commit, maybe based on another branch, maybe based on a previous release, which means that you could use that additional area that's separated from your main working area to look at, say, a previous package version.

And she gives an example of, say, another package called riGraph and then putting the tag after that and then making a directory for that and then using git work tree to check that tag out into that other folder. And then you can remove that. Clean clean up after yourself, so to speak, when you're done doing that investigation or that previous version of using git work tree remove and then that folder name. And then then it's as if nothing happened. I'm still wrapping my head a little bit around this because it it it I've never used Git Worktree before.

But there are plenty of times at the day job where maybe I've already gone, like, 1 version, 2 versions ahead on what I need to finish. But then I'll get a request from, like, an analyst or a client or or a customer in my in my various departments. And they have a question that admittedly, some reason they're using an older version of the package. So now I can use get work tree to investigate that really quickly without having to, you know, do some clever library magic along the way. So I'm still wrapping my head around this, but definitely as we do always, we'll have a link to Miles' blog post here. But if you need to quickly check what you did a version or 2 behind, Git Worktree seems like the way to go.



[00:15:11] Mike Thomas:

This is somewhat mind blowing to me. I think it's a great utility function that I now know about, with Git, and it's it's fantastic. I think it's it's very simply explained by Maelle to be able to create this just additional directories. This additional subdirectory essentially is that I think the way that she set it up, which would contain the specific version of the package that you want to work in temporarily. So, you know, I I think you you covered it excellently. This is a very nice short and sweet blog post, but I again appreciate, Mel, pointing out these nifty little tricks and tools that that we have. I didn't I had a similar use case but but not quite the same use case. I actually wanted to try out, a pack this package that we've developed on a different version of R, which I I guess I could have done and opened up a separate sort of IDE and had a local installation, of an older version of R, because I wanted to make sure that the package worked on an older installation of R. But then, we, we're taking a look at at of the utilities like R hub and things like that that allow you to test your package, against multiple versions of R on multiple different platforms and things like that.

And that also sort of is where where Docker, I think, can come into play and be your friends to allow you to be able to spin up a container that contains a particular version of R without having to necessarily install it on your local machine, and then worry about uninstalling it and things like that. But that can be a little more tricky. But I can absolutely see plenty of use cases where, you know, instead of changing the version of R, I would wanna actually change a version of of a particular R package and and take a look at, you know, how that package was functioning in that version, compared to a previous version. The scales package is one that was giving me some headaches lately. There's some new there's some new arguments in your, your label, number or label percent, that deal with positive and negative values that that were newly introduced and and giving me some headaches recently. So this is a use case that I think I might have to to spin up by this Get Work Tree function today and dive back into that. But, excellent, excellent blog post again by Ma'el Asserta. Just again, pulling out a bag of tricks that that I didn't really know existed that are absolutely gonna be helpful for me in the future.



[00:17:35] Eric Nantz:

Yes. And, I believe even though he's halfway across the world potentially, I might hear Bruno's voice in my ears saying, you could probably combine your use case, Mike, of checking different package versions with my Al's use case of different or of different package versions with your use case of different our versions with Nick's. I I bet there's way the titles do together. So, Bruno, I heard you even if you weren't saying that. I can I can hear you telepathically? So this would this would fit really nicely in this, and I'm excited to maybe try out some of these newer ideas as I'm getting more in the weeds, especially this past month, on some internal package development and trying to make it easier for both future me and future, collaborators as well. But if you ever thought you knew everything about Git no. I I I'm I'm one of those people that seems like I've learned something new every week about Git. So it is just amazing what we're learning in this space. And speaking of amazing, the rest of the art week of issue is just as amazing. You're gonna learn so much along the way if you read through the entire list of new blog posts, new packages, updated packages, and offer tremendous resources.

So it'll take a couple of minutes for our additional finds here. And, I I admit sometimes and I'll read an old I hate to say old, but maybe a somewhat newish or, you know, senior statistical research book. You wonder how would what would happen if I just updated my the code examples in that book to use a newer package framework, a newer paradigm? How does that compare and contrast? Well, Imo Vitfeld from posit has done just that with the introduction to statistical learning labs converted to using tidy models.

This is massive. If you ever wanna see just relating a newer framework for machine learning and and and everything like that. But with a very critically renowned, well established literature of getting into the nuts and bolts of predictive modeling and machine learning. This quartile book online of Tidy Models Labs has you covered. I've been watching this over a little bit. It looks like it's had a ton of updates since I last looked at this, but it is a very direct one to one relationship with the labs that are mentioned in the in the second edition of ISLR with using tidy models. So if you ever wondered how, like, classification, you know, linear model selection, support vector machines would look in the ISLR context. But with tidy models, this is the place to go. Highly recommended.



[00:20:12] Mike Thomas:

I was looking at that, that book and I cannot wait to fully check that out. The ISLR book is is absolutely phenomenal and tidy models is absolutely phenomenal as well. That's that's our package, suite of packages of of choice here at Catch Brook for when we're doing predictive modeling projects, and then, ISLR is is sitting on my desk essentially at all times. So it's it's going to make my life even even easier to be able to have this resource that sort of, is the serves as the translation between those two things immediately instead of having to to do it ourselves.

And I just want to point out quarto 1.4 has been released. Big improvement here, I think, is around dashboards. You know, we've talked about it a lot, but quarto dashboards are here. The new iteration of of flex dashboard. So try it out yourself. The other thing that I'm super excited about, but I I think is in the early stages, I'll have to check out sort of how stable it is, but it's this new new manuscript project type called typest, t y p s t, if I'm pronouncing that correctly, I'm not sure. Sounds like a much lighter weight version, of maybe Pandoc or or for rendering PDFs, really lightning fast, or or Latex essentially. I think it's it's replacing Pandoc and LaTeX.

I haven't dug into it yet but if there is something out there that can render PDF reports for us, much faster and much more lightweight than what the current options are, I am super interested in. So we'll we'll see how that goes.

[00:21:47] Eric Nantz:

I'm very interested as well, and I believe there was a talk at Positconf about the type support coming for Cortl. So if I'm able to find that, I'll put that in the show notes as well. But I do have a use case at the day job or maybe you wanna make a PDF even not just of the statistical results of, like, a model fit, but also we can might even use this for, like, an internal newsletter or a new internal update and send that out because in any corporation, sometimes email is the only way to get ahold of people, and this will be a great way to have attractive kind of, branding, if you will, on some of the things we do. But types can make that a lot easier.

And, certainly, we hope that our weekly itself makes your journeys in r and data science much easier. And, of course, we love hearing from you. The best ways to get a hold of us are on the contact page linked in this episode show notes. You can also have a modern podcast app like Pod Versa Foundation. Us a fun boost along the way, and details about that are in the show notes as well. And, also, we are variously sporadically on social medias. I am at, our podcast at podcastindex.social on the Mastodon servers,

[00:22:57] Mike Thomas:

sporadically on the Weapon X thing with at the Rcast and LinkedIn from time to time. And, Mike, where can listeners find you? Sure. On LinkedIn, you could search Catchbrook Analytics, k e t c h b r o o k, and see what I'm up to there. Or you can find me on mastodon@mike_thomas@fostodon.org.

[00:23:16] Eric Nantz:

Awesome stuff as always. And, yeah, it's a nice tidy episode this week. And but as always, every single week, we're trying to be back here with awesome art content for all of you. So that'll do it for us for episode 150. Only 50 more to go than the big 200. We'll see if we get there. And in any event, we hope you enjoy listening, and we'll see you back for another edition of our weekly highlights next week."
"69","issue_2024_w_04_highlights",2024-01-24,43M 57S,"How the babeldown package enables low-friction updates to living documents, uncovering innovative functions all within the base R installation, and supercharging a static Quarto dashboard with interactive tables and visualizations. Episode Links This week's curator: Sam Parmar - @parmsam_ (https://twitter.com/parmsam_) (Twitter) &…","[00:00:03] Eric Nantz:

Hello, friends. We're back with episode 149 of the R Weekly Highlights podcast. Oh, we're getting close to another fun little milestone, I guess. The episode numbers keep going up and so does each issue of Our Weekly. We're here to talk about the current issues, latest resources, tutorials, and specifically the highlights from the particular issue. My name is Eric Nantz, and I'm delighted that you joined us wherever you are around the world. And, hopefully, you're staying warm, especially if you're in the winter season and hopefully avoiding some ice apocalypses out there. But, nonetheless, we hope you enjoy this episode.



[00:00:36] Mike Thomas:

And staying warm in his humble abode is my awesome cohost, Mike Thomas. Mike, how are you doing today? Doing well, Eric. Yep. It's, it's pretty chilly out here in Connecticut. We live fairly close to a lake that's that's frozen for the first time in a few years, which is it's kind of nice, going out in the ice and and skating around. So trying to enjoy, as much as we can and excited to have some consistency now in 2024. I think we're we're back to back weeks for a couple weeks now on our weekly and hoping to keep it up. Yeah. The momentum is, in our favor, so to speak. So we'll keep that keep that rolling along here. And

[00:01:11] Eric Nantz:

and as always, the Rwicky project rolls along because we have an awesome team of curators that are helping every single week. And this week, our curator is Sam Parmer. And as always, he had tremendous help from our fellow Rwicky team members and contributors like all of you around the world with your awesome poll requests and other, heads up to us about the latest resources that you found. So let's dive right into this. We know, Mike, we have a lot of advances in technology right at our disposal via the magical world of APIs to help automate a lot of the stuff that would take a long time to do.

Well, there is a very interesting area that this first highlight exposes in terms of where these APIs can really help in a much needed domain of translation of different languages for our documentation. So our first highlight is a blog post from the esteemed rOpenSci blog by Mel Salmon, who, of course, is a former curator here at rweekly and now is a research software engineer supporting rOpenSci as well as other endeavors. And this blog post, in particular, talks about the use of the babble down R package to update an existing translation after its changes.

And this is, apparently, part of a more broader initiative from rOpenSci for publishing in multiple languages, their various pieces of documentation. And as part of that effort, this babble down package has been developed to help translate markdown based content with leveraging what's called the DeepL API, which before this, I actually didn't know this exists. But apparently, this is a full fledged API built specifically for translation across many of the common languages in the world. So in this blog post, my old blogs are a pretty simple example but yet very relatable.

Having an existing markdown document in English language, and then as it's being kicked off, how would you go ahead and translate that to French in this example? So we have a very simple markdown syntax, which has got a typical headings, subtitles, and narrative inside. The babble down package has a function called deepltranslate, Give it the path to the markdown file, the source language, the target language for translation, and there you go. It's going to call the API under the hood, and you'll get that text right back, in this case, in the French language.

Looks good to me, although I'm not a French speaker, so I'll I'll defer to my own others for the authenticity of it. But that's not all. That's great for, like, your initial document. But what happens if, like anything else, you're gonna update that document, you know, through, you know, maybe pull requests from your collaborators. Maybe you got a new feature you wanna document in that package or tool or whatever this is meant for. And so assuming that this document is in version control because, well, if you're not using version control, you should, especially for larger efforts. Mike and I can attest to that.

The babble down package is doing some pretty clever things under the hood to detect the changes that are happening in this document so that when you feed in this updated document to the the BabbleDown package, you can there's a function called deeplupdate where it's going to take this newly changed file. And again, with very much a similar function parameters as the kind of the initial launch of the of the translation, you will get that new updated language of the document in French with your changes reflected.

Now this is using, apparently, a hybrid of the get kind of diffs under the hood, but not just that. It's actually translating the representation of that markdown syntax of that file into XML representation. Because, again, in the web, even though markdown looks like we're just writing this in plain text, when you render it to HTML, you're putting it into another markup language, and XML and HTML are very much related in that space. So, apparently, this XML representation is a bigger help to pinpoint exactly what is changed in that document so that she the the user of the BabbleDown package doesn't have to send the entire document back for retranslation.

It's only gonna send the bits that change. And just like anything in the API world, there's no such thing as a free lunch sometimes. So if you were sending a volumous, you know, lengthy document over and over again, that could perhaps incur some costs if you're leveraging this API more regularly. So being able to only take what you need and translate it back, I think, is a really neat feature and pretty welcome, I'm sure, for those that are using this regularly. So this is very much scratching the itch of a big need in the community as a whole in data science and other domains of making sure that we make our documentation for our tools, packages, or other, like, analytical pipelines as accessible as possible to those around the world. So I'm really excited to see just all the nifty things going on under the hope of this babble down package. Something I'm gonna keep in mind for my open source projects in the future. Yeah. I couldn't agree more, Eric. And I think this is the topic that we've talked about

[00:07:01] Mike Thomas:

on previous episodes, you know, trying to make R and the packages that we develop, and the documentation that we write as accessible as possible to as many folks as possible. Right? Around the world because R is an international programming language, and that means that we should try to do as much as we can to try to accommodate those folks. And the fact that the people working on on BabbleDown, including Ma'el, have provided us with this tool to just make it much easier for us to do so is is awesome. I think it's what open source is all about. And I really like sort of this walk through with the the API.

This deep l translate function is is really cool. As you mentioned, it allows you to specify your input file, where you want your output file to be written to, your source and your target language. And one interesting argument that I saw is an argument called formality. And in the blog post, Mal has specified this argument, as the string less. But I imagine that you could, need to have your translation be be more formal or less formal or or maybe somewhere in the middle, I'd be interested to learn a little bit more about that. And I imagine that that's sort of a parameter that the API, itself handles, which, you know, is really interesting. I think that there's probably a whole field of study here in terms of language and translation, and, you know, how different cultures represent formality versus informality.

But I I just thought that that was pretty nifty that that argument exists. And would be interested to see, sort of, how changing that argument would change the output. And I imagine that it it fully depends on on your input text and and how specific that is. This this DeepL update function is is really impressive too, and also really impressive that it doesn't use the git diff at all. As you said, it's this XML representation, and it looks like there's a package called Tinkar that sort of helps, with that translation between, your original document, it's XML representation, and finding the differences between those two XML structures. So that's that's really fascinating to me. Sort of reminds me of of the Waldo package, maybe, in some of its functionality, and being able to compare 2 files against each other. Recently, worked on a project using the Waldo package, where we're just reading, you know, these these dot TXT files, which have this this really sort of strange structure, but we're able to really easily show our end users sort of the differences between, 2 different dot TXT files, which is is super important to them. And it's it's just incredible sort of what the the community has created in terms of these packages that allow us to identify these differences, and then take action based upon the differences that we find. So this is a a really nice short and sweet introduction into,

[00:09:56] Eric Nantz:

how these DeepL functions within the BabbleDown package work and maybe able to help you out in your own work. Yeah. I'm even thinking if I do, like, open source Shiny work in the future, having, like, a toggle for, like, changing the language of, like, the interface elements. But then, yeah, it does bring up a lot of possibilities how this could be tied with something like BabbleDown. I'm the the wheels are turning or this could make my apps way more accessible. So this is really neat. Moving on with our second highlight today, we got, you know, as we talk about a lot on the show as well, yes, a community of our packages can supercharge so many of your workflows in data science, tool development, computing in general.

But you know what? Base R itself comes with a lot under the hood, and, honestly, sometimes it doesn't get enough of the spotlight. So that's where this blog post is gonna shed a little bit of much needed spotlight on some additional functions that may come from Bayesar, but they're definitely not so basic and they're quite powerful. And this is authored by Isabella Velasquez, who is a senior product marketing manager at Posit. And she does really great work of her blog as usual. And, this blog post in particular, we'll get to the meat of this shortly, but she's got some bells and whistles here that I think you're gonna really like as we we talk about this. But she opens with the list of 6 functions, actually, and an operator on top of that, that she's been using quite a bit and things that deserve a little more love. And we'll hit each of these quickly 1 by 1. But, we probably won't do each of them enough justice. But you may have seen as you've, like, perused maybe someone else's R package, you know, source code, that sometimes at the end, instead of an explicit return state or return of, like, a function parameter with a return function, you might see a function called invisible put in at the end instead.

What this really means, and a cool name, by the way, is that you are, in essence, returning a temporarily invisible copy of the object. So it's going to still execute normally if you run this like in the R console. But then if you want to save this to a variable, it's not going to print the result when you run that function after saving it to the variable. So you can kind of run it interactively with just calling the function itself and then also when it's not. But here's the cool part about this blog post. You're going to see the snippets of code, but you notice that little run code button at the top there?

Guess what, folks? You hit that button, it's going to run it in your web browser. Two guesses what that's powered by. I smell a WebR implementation here. This looks really nifty. So this is this is just as an aside here. This is the potential we're starting to see here, folks, is that on top of sharing code to do something, WebR, WebAssembly in general, is gonna let us try it out in the browser about you installing a single thing. So if you're a new user to R, man alive. This is a great time to get into the language of these kind of resources inside. But you can quickly see the examples that, that Isabella puts in here and run them yourself and see what's happening. So it's really, really neat to see. So invisible definitely is something I'm starting to use more in my function authoring, in the future.

Another one that I did not know existed, so, you know, mission accomplished for her blog post is the no quote function which basically means if you want to show the syntax of, like, a character string but don't want the quotes around it, you can simply feed it into the no quote function, and now it's going to print as if you don't have the quotes around it. I think this can be very helpful, especially as you're dealing with HTML language, like links or other things that you want to maybe copy into another program or a browser toolbar, then having that no quote function for, like, a URL type function that she highlights here would be very helpful to let you copy and paste without too much friction there.

So I could see other uses for that as well. Here's one that brings back memories for me in my very early days of my R usage of visualization is the co plot function. This is a very handy function when you have, you know, a situation of analyzing multiple variables at once where you could look at different pairs of variables, perhaps even conditioning on another one as well, and you can quickly, kind of, get a read for how these variables are going to interact with each other visually. Great for correlation analysis or other association analysis at a very high level. So that's all built right in. Very nice straight to the point. You can even customize how the rows are constructed and everything like that. So really nice examples throughout.

This one, I have theories on why it's named this way, but we'll see what you think, Mike. Nz char or nz car, depending on how you want to pronounce it. When I look at that name alone, I honestly have no idea what that does at first glance. But what this function really does is that it is a way to simply return true or false on whether the character vector that you supply to it is empty or not. Now n z, at first, I thought, does that mean like non zero? I don't know about that. But then I thought, well, here's maybe an egg. This is speculation on my part.

We know, if you're a historian about the R language itself, that it was founded by Ross Ahaka and Robert Gentlemen while they were teaching at the University of Auckland

[00:16:12] Mike Thomas:

in New Zealand. For it. New Zealand.

[00:16:16] Eric Nantz:

I I cannot take I I don't know. I've never seen this in writing, but I wouldn't be shocked if there was a little Easter egg in there somewhere because why is it called nzchar otherwise? I don't know. But in any event, I don't use this function enough. Have you used this function before, Mike? Nzchar?

[00:16:31] Mike Thomas:

No, I haven't. I haven't. I've seen it, obviously just come across like my, you know, sort of automated, whatever it is, let, you know, within our our studio that sort of pops up, different functions for you as you start start typing. But I don't think I've used it before. Let me see if I can take a look at what the documentation says about nzchar.

[00:16:57] Eric Nantz:

I did look at this before the show. I didn't see any references to my You know. You know. That's

[00:17:02] Mike Thomas:

like thinking there. Easter eggs there, but I would have thought non zero. Well, the n char obviously means number of characters.

[00:17:13] Eric Nantz:

Yes. That one I get. Yeah. Yeah. Number of

[00:17:18] Mike Thomas:

I don't know. I don't know. Because it's not number of 0 length character vectors. It's the opposite.

[00:17:26] Eric Nantz:

Right. Right. So if you're listening, I'm gonna let you know how to get feedback to us. We love to hear theories from all of you in the audience on this particular one because I've I've wondered about this for years, but, admittedly, I have not used the function much in daily practice. But, hey, now if I have a need to check if they're empty or not, I will

[00:17:45] Mike Thomas:

leverage this for sure. Well, I wonder if, you know, sometimes I feel like functions like this, especially within bass art sort of inherit their names maybe from the the c functions that underlay them. So I wonder That could be. A relationship there.

[00:17:59] Eric Nantz:

That could be because r is standing on the shoulders of c, Fortran, and the like, and of course, the s language before r. So there's a lot of legacy under the hood that, you know, you could go down lots of rabbit holes for the history of r on this. So alright. We'll move along here. Another function that I meant I have a checkered pass with, curious your pass on this, Mike, is the with package. The when I first used this, this was, in essence, a shortcut function for me where if I wanted to feed into another function, in this case, the example, Isabelle puts in here is the plot function, and you're feeding into it a data frame, but there are specifically variables of a data frame. If you're lazy and don't want to type like, in this case, mtcars$HP or mtcars $mpg, you can use the width function, supply the data frame, and then the plot function and just reference the HP and MPG without the dollar sign syntax, not too dissimilar to what you might see in a tidyverse pipeline as you're doing the piping operations.

Yes. It is helpful in this case, but I have tripped myself up more than I care to admit when I use this in the past. So, admittedly, I moved away from it. But hey. You know what? It is a way to to take that shortcut as long as you use it responsibly, I would guess. Yeah. It's another it's another option. Not one that I admittedly use very often, but it is an option. Yes. It is. And now this next one, I knew about what I call the single version of this. But I know there was a plural version of it, and that is the length function with the s at the end because I use length all the time to check, you know, the it says the length of a number or string or whatnot.

But if you want to quickly check for each element in a vector, length is basically a shortcut to, like, the more verbose, like, s apply or l apply syntax for doing this. So this is great. This may be another shortcut that you can put in your toolbox instead of having to do a per map on using length under the hood or an s supply under the hood. So that was that was a new one to me for sure. And then here comes the operator that I admittedly should have used way long ago. And that is called the no coacine operator. I'm probably not saying that right. But if you've seen in various conditional logic, the percent sign to vertical pipes and the other percent sign closing it, that is this null coalescing operator.

And this is a shortcut. If you've ever done a check for if an object being null or not and you've got like an if statement. If is dot null something, then return something else, return something else. This is a shortcut to that. And, also, the esteemed Jenny Bryan herself highlighted this in one of her talks at the USAR itself in 2018, which is linked to in the blog post. So part of her underlying kind of theme of code smells and things that you can shore up in your day to day coding development and R. This operator, she shows some great examples of how it streamlines a whole lot on that if else syntax.

So that's that's a whirlwind tour of all this, but there's a lot to choose from. And, of course, that just scratches the surface of what Baysar has. So Isabella concludes the post with some additional blog posts from others in the community and what they've seen in Bayesar that's been useful to them. So really like to see it. And, again, love, love the idea of being able to run the code directly in this blog post to try things out. So

[00:21:53] Mike Thomas:

awesome stuff all around. I think it is thanks to Isabella's implementation or integration here with WebR that I have finally wrapped my brain around what the invisible function does because I have seen it in so much code on GitHub. I have never used it. I still don't know where I would really have much of a use case for it, maybe in some of our more object oriented programming, but, you know, really the idea again there is that, your your your function is primarily called for its side effect. So I guess, one of the examples, that I read up on, I can't remember whether it's within this blog post or outside this blog post, but it's like the the right CSV function from the reader package. So that's that's obviously going to write a CSV somewhere to to some destination path that you supplied and won't return anything within your console when that happens. Right? Most of the time you're doing that and you're not you're not assigning that right CSV function to a variable.

So you wouldn't know, right, that that would potentially ever return anything. But if you did assign that to a variable, it would actually return the data frame, I believe, to that variable. Which is, which is pretty interesting. You know, it makes me curious about how many different functions out there do have this invisible call at the bottom of them, such that if you did assign that function to an object, you know, that objects would be populated with with something. So I think it's interesting. I think it's really good to know. Maybe something nice to have in your back pocket, and I've finally wrapped my brain around that. And I'm not gonna walk through all the other functions that you walk through, Eric, but I will just note that the this null coalescing operator, I I think is going to save me a lot of code writing once it's finally integrated in BaseR here shortly, as opposed to, you know, in just about every project I'm working on, I I do have some test for if if this is null, right? Then take this action within some particular if statement.

The coalesce operator the the coalesce function within SQL SQL has been huge, for for Catchbook and a lot of the SQL work that we do for our clients on. Some particular, address standardization projects where we're we're trying to test if, you know, the the user has a valid address too. Right? As a second line in their address like a PO box or something like that or not. So that's that's a function in sequel that we use really really often. I believe that within d player, there is a similar coalesce function as well that can return sort of the first non null, value within a list of values that you you pass it. So we I find that pretty handy, and maybe some other folks will as well. But I couldn't agree more that the WebR implementation here that allows us to actually touch and feel these functions as we're reading about them is a game changer. So thanks to Isabella for taking the time to do that, and I am going to dive deep into the GitHub behind her pipe dreams blog here to see how she did that.



[00:25:11] Eric Nantz:

Yeah. And the best part is, in fact, I remember when I first saw these kind of posts, I would see the run code, and I would I would hit it. I mean, this reminds me of the learn r package a little bit too. But then I realized, oh, wait. I can actually click in that code box and change it myself. Like, it's not just the pre canned example, no less. You can experiment with all this, which makes it even more fun. My goodness. So you can kinda see why this is gonna become a huge in the realm of teaching, the realm of illustrating these concepts.

I mean, think about this, Mike. I know we're not far off, I think, from a packages documentation site. We're gonna let you run the package code itself as a way to try it before you, quote unquote buy so to speak. I can't wait until we can integrate that into our package down sites. It's gonna be incredible, right? Within your examples or

[00:26:02] Mike Thomas:

or wherever, your your reference. That's gonna be huge. And, you know, from the first blog post that we ever saw with WebR chunks in it to where we're at now, in particular, this installation of the the tidy tab package and Isabella's blog that's coming from our universe. The installation is so fast, so much faster than it used to be. I think we're waiting, like, you know, 2 or 3 minutes previously. And now, you know, we might be waiting 10, 15 seconds.

[00:26:30] Eric Nantz:

Yeah. George Stagg, the the engineer behind this at Pazit, he is, he's doing some divine work here, I must say, to make all this happen. And we are all super, super appreciative for it. So this is an area that I mentioned on this podcast I'm exploring very actively right now, and the possibilities are practically endless. But, yeah. And credit to Isabella for, again, putting a much needed spotlight on these gems inside the R language itself that you get every time you install the language for free, just like everything in R, it is open source all for you to leverage at your leisure.

And speaking of interactivity, Mike, as we saw in Isabelle's awesome blog post from the last highlight, making it interactive with the WebR functionality Well, in the Quartle ecosystem, now we've got tremendous ways to make interactive dashboards as well. And dashboards, if you're familiar with the flexdashboard package that many use in the R Markdown ecosystem, The quartile syntax or dashboards is very similar. So you can get up and running pretty quickly, but just what is the easiest ways for you to make that into a more interactive display and not just a stack display?

Well, friend of the show, frequent contributor, Albert Rapp, is back once again returning to the highlights with his, latest 3 minutes Wednesday style post of making a portal dashboard interactive. And you might say, well, just how do we go about this? Well, this is a continuation of a previous post where he put in the syntax needed to make, in essence, a placeholder dashboard. We've got, you know, a column here, a column there, and a row below it, and then a sidebar, but nothing in it yet. So how do we replace all that with things that can be both static or interactive?

Well, just like anything in quartile, give yourself an R code chunk or even a Python code chunk, and you'd be able to add in things like a really nice markdown syntax for your sidebar. You can leverage in HTML tools hidden gem. I use this a lot in my Shiny apps. The include markdown function, where instead of writing the markdown literally inside the source code of your app or your UI function, you could have that as an external markdown file. Just reference that markdown file, and it's going to compile it in the web markup and put it anywhere you want in your HTML report or Shiny app. So he does that in the sidebar with a little bit of narrative around the dashboard itself.

And then let's spruce it up with some nice tables, shall we? And that's where the first example of an element to put in this table or put in this dashboard is a table of the palmer penguin set using the very incredible GT package by Riccione over at Posit. Love this package. And it gives you a very attractive looking, static looking table, which, again, for many purposes would be very, very good for the majority of reports. Now, of course, no dashboard would be completely about some form of, you know, more traditional visualization.

And that's where, of course, ggplot2 will feed in very nicely into a portal dashboard or any type of report for that matter. But on the old net, it's static. Right? How do we make that interactive? Well, Albert highlights another package that didn't get a lot of love initially, but, boy, it sure took off, especially last year, the ggiraffe package or ggiraffe package, maybe how you wanna pronounce it. I still don't know which one it is, but I'm go with either one, authored by David Goel on being able to turn a static ggplot produced visualization into interactive with tooltips and other great, little interactive features as well. So you can quickly give you that kind of hover functionality or filtering functionality by clicking on different points. There are lots of lots of cool things you can do with an interactive visualization on that.

Now how about going back to that table? The GT table looks great, but it is a bit static in its presentation. Well, making a short little pivot to the reactable package, which, again, is one of my favorites from my Shiny apps and reports, you can have that sorting and filtering functionality inside. And then lo and behold, you could even bake in at the end of the post a way to filter the table with some controls that are embedded in the sidebar of the portal dashboard itself. And you don't necessarily need Shiny for that. There are ways you can build that with the observable JS code chunks or other ways with crosstalk as well to make that HTML element linked to that input that you put in, even the sidebar or maybe above the table or visual.

And you can have that interactivity so that the user can customize how they want to display. In this case, the penguin's weight distribution in that table. Apparently, there's gonna be another video that he releases coming soon about making those reactable tables even more interactive. So we're gonna be staying tuned for that. That's a space I'm looking at quite closely in my exploits at the day job. So, again, portal dashboards are becoming very popular now, and you can make them very interactive very quickly.

And we didn't get into the the bits that you could do with a Shiny back end as well, but, you could do a lot with portal dashboards. And I'm actively pursuing this as we speak of an open source project right now. So credit to Albert. Once again, terrific post, easily digestible with links to more detailed tutorials that he's done on these various topics, including his ebook that he's written as well on creating GT tables. So he's I don't know if he ever sleeps, man, but, boy, he is busy.

[00:32:51] Mike Thomas:

Reminds me of somebody else I know who I feel like never sleeps. Who would give you that idea? But, this is awesome blog post. I am already deep into Albert's code which is, looks like we got a little bit of JavaScript going on to connect these these filters, these check boxes to the reactable table. I did not know that that was possible, but that is really really cool that he's done that. Obviously, I've seen that, sort of interactivity that you could deploy to a static site if you are using Observable JS, with the ability to to have filters that drive your charts and things like that, but did not know, we could do that with reactables. So that is super cool. That is some code that I'm going to be taking a hard look at, and really excited and grateful that Albert has put this out in the open. You know, one thing that is consistent with Albert is his data visualization projects and products are always really aesthetically nice.

So I think that that's that's awesome. And you can see him leveraging sort of the new quarto dashboard framework, where you have a card with a plot in it, and that card has a little icon in the bottom right hand corner to expand that full page, which is another one of my favorite features. Our clients absolutely love that with our quarto dashboards that we're creating these days and and with, leveraging bslib, essentially within our Shiny apps, we have the same functionality within that card function. And it looks like maybe the g g I raf package also allows you to download these interactive plots, maybe as a a static image or an HTML file as well with a little icon, a little save icon in the top right hand corner, which is just a nice utility on top of that. Albert, phenomenal blog post, with videos, content, visuals, GitHub links, everything, you name it. So this is actually a pretty big repository, it looks like, behind, this behind this blog post. It's a blog it's a repository, under albertwrap, slash quarto dashboard is where you're going to be able to find that on GitHub if you're interested. And it looks like there's just a ton of examples for interactive plots, tables, interactive selection, OJS, reactable plots, the whole 9 yards. So this is a wealth of knowledge baked into a fairly concise blog post, which seems to be a theme with Albert, and I hope nothing changes in the future. So thank you for not sleeping, Albert. And, thank you for putting this together.



[00:35:26] Eric Nantz:

Yeah. There's a lot to look at in this repo. We'll have a link to this in the show notes, but it does go through each of these different iterations of how he's built this dashboard going from the completely static approach all the way to that fancy interactive, reactable version and interactive selection. So, yeah, there's gonna be a lot a lot to choose from here. Certainly, if you're a power user of things like CSS, there's a handy little SCSS snippet here too that style things a a bit more. So there's a lot to choose from in this space.

So as we mentioned a number of times highlighting the Quartle dashboard functionality, there was many, many in the community asking for the flex dashboard set up in Quartle. And now that is here. And I'm very excited to see to see where that goes. And just as exciting is the rest of the issue of our weekly itself because the highlights don't do enough justice to the great content in the rest of the issue, which, of course, is linked as always in the episode show notes. But Mike and I are gonna take a couple of minutes to talk about some additional fines that we've, found in this issue.

And going back to our first highlight, our our author, Mel Salmon, of that blog post, She's also been hard at work on another what can be a very thorny topic for both learning and teaching as well. Recently, I've been following this a bit on her Mastodon account. She has authored a new r package with a very unique name, which I'm probably gonna butcher it right here, called sacralopopet. Yeah. Send your send your feedback to me, I guess, for that one. But in any event, this is a package that is meant to help create in your R session the not so pleasant experiences that can happen with Git.

Looking at things like, you know, maybe messed up committed files, maybe a merge gone completely wrong. This is inspired by a very famous, site that you've probably bookmarked if you had a problem with Git called okay Git. You can fill in the blank on that. But in any event, in your R session, you can do some very fun things to learn how to resolve some of these issues in Git itself that you often will find yourself in one way or another, whoever willingly or not willingly, in your version control escapades.

So I'm gonna be looking at this because I'm gonna be likely teaching some form of get, you know, training or workshop at the day job. Maybe I even do that in the open source world. Who knows? But having a way to illustrate, you know, what just happens when things go wrong and let you practice how to fix it, I think that's extremely helpful because almost nothing in good goes exactly as planned the first time around. So knowing how to handle these thorny issues, especially on those merger requests, I think is very very helpful.

Now, Micah, what did you find?

[00:38:34] Mike Thomas:

No. That's a great find, Eric. I found that the with our package, has a new release, version new major release, version 3.0.0. There's a nice blog post from Leonel Henry, who's on the Pasa team, I believe, talking about, sort of, what the improvements are here. It looks like a lot of the improvements are around the performance and compatibility with BaseR's on dot exit function, which if you are a shiny developer, especially somebody who's authoring shiny apps that maybe connect to a database that you want to disconnect from, which you should be doing at the end of the users at the end of the users session, not the global session, the users session, you should be leveraging that. So the with our package, may be able to to help you do that and help you test, that functionality as well. We recently put out an open source, our package for working with some agricultural finance data, and that download some data from the web. And within my testing suite, the unit test would test that. I leverage the with our package to download that data into a temp file, ingest that data into a data frame, and run my test against that.

And everything sort of disappears at the end of those unit tests running and all the the checks pass with with, dev tools. And I don't have to worry about actual locations, within my own machine or with somebody else's machine or or within Crayon's machine when we finally send this off to Crayon about where, those those files are gonna be temporarily downloaded to with our just takes care of all that for me, and I can't speak highly enough about that package, as well as the blog post that Me'el Saman has authored around with our functionality that sort certainly helped me get set up there. So long story short, I am very much a new fan of of Withar. We'll be using it from here on out. And thanks to Leonel for, letting us know what's new in Withar 3 dot0.0.



[00:40:40] Eric Nantz:

Yeah. It's been, it's been a a very helpful package in my exploits as developing both apps and packages. And, frankly, your idea of building this into the testing of your package, I think, is extremely novel, especially as you have to deal with maybe other systems or resources from other systems, and you may have to download something, may have to temporarily write a config file to send to something. You don't want that left around because you're only doing it in a disposable way, maybe through a CICD pipeline. So with our very valuable in that space, especially for other thorny issues, like even just having a temporary change of directory that I'm working in just for some esoteric reason because of some other pipeline. I have to be somewhere else just for that function and get back to where I was.

Yeah. With r is is is very much appreciative in that kinda utility toolbox that I have for my day to day package and app, development needs. So, yeah, really enjoyed reading about version 3 that just got released. And, of course, as we mentioned, we love hearing from you and the community, and we're gonna tell you about the various ways you can get in touch with us. Of course, first, everything you wanna learn about our weekly is at rweekly.org. So if you haven't bookmarked that, please do. That's where you'll find every new issue and every every new every back catalog, so to speak. Every previous issue is right there at the taking.

And if you wanna join our curator team, we definitely have open slots. Please, get in touch with us. We have the details on the GitHub repository for our weekly. That's linked to, directly in the our weekly site itself. And, also, we love hearing from you directly for this very show. Whether it's me butchering another package name or whatnot or or our, speculation of our history, we'd love to hear it. So you can do that via the contact page that's linked in this episode show notes. It's always there.

And, also, if you wanna send us a fun little boost of your feedback using a podcast app such as Podverse, Fountain, Cast O Matic, or the podcast index itself, that's all right there. We have links to how to do that in the show notes as well. And thanks to all of our previous boosters for giving us some much needed encouragement, and we always welcome your feedback on that side too. And, also, we are sporadically on the social media spheres. I'm on that weapon x thing from time to time with at the at the r cast, but also more frequently, I'm Mastodon with atrpodcast@podcastindex.social, and I will cross post from time to time on LinkedIn. Just search for my name. You'll probably find me. And, Mike, where can the listeners find you? Likewise. Probably best on mastodon@mike_thomas@phostodon.org.



[00:43:27] Mike Thomas:

And if you wanna find me on LinkedIn, best way is to search Catchbrook Analytics, k e t c h b r o o k, to find out what I'm up to lately.

[00:43:36] Eric Nantz:

Awesome stuff. I enjoy seeing your post from time to time. Your your your hustle never stops either, so I hope you get some rest too when you can. But in any event, we're gonna stop hustling, so to speak, on this episode. We'll wrap things up here, and we'll be back with another edition of our weekly highlights next week."
"70","issue_2024_w_03_highlights",2024-01-17,44M 41S,"A tour of how the httr2 package streamlines API processing in R, five must-have ggplot2 extension packages for your next visualization, and the Appsilon Shiny Conf 2024 is shaping up to be the biggest yet for all things Shiny. Episode Links This week's curator: Colin Fay - [@ColinFay]](https://twitter.com/ColinFay) (Twitter) How to work with APIs…","[00:00:03] Eric Nantz:

Hello, friends. We are back at episode a 148 of the R Weekly highlights podcast. And coming to you from our respective, frigid areas here in the US, where we hope we can heat you up a little bit with some fun, our content, as always, coming straight from the latest issue on ourweekly.org. My name is Eric Nantz, and I'm delighted that you join us wherever you are around the world. And keeping warm in his abode there is my awesome cohost, Mike Thomas. Mike, how are you doing this morning?

[00:00:29] Mike Thomas:

Doing great, Eric. Go Lions. How about that? Go

[00:00:33] Eric Nantz:

Lions. For those that aren't aware, the the Detroit Lions have had a, bit of a struggle in really doing anything successful for many, many, and I do mean many years in the football landscape, but they won their 1st playoff game in 31 years, folks. That's a long time. I was, I'm not gonna date myself too much. Well, let's just say I was quite younger back then. And it was, yeah. Who knew that it would take that long to get back there? But the the Ford field was rocking. And as a Michigander all my life, it was, I could see why fans are crying in the sands. That was a moment, folks, and they're not stopping. They got another game this weekend, so we'll see what happens.



[00:01:13] Mike Thomas:

Keep it rolling.

[00:01:15] Eric Nantz:

Keep it rolling, baby. And we're gonna keep this podcast rolling as well because we got another awesome issue to talk about that's curated by the esteemed Colin Fay. By this point, you know who he is if you listen to the show just a little bit. For those who aren't aware, he is at ThinkR, and he does awesome art packages in the shiny space such as golem, which we use quite a bit in our day to day. But as always, he had tremendous help from our fellow our weekly team members and contributors like all of you around the world with your awesome poll requests and suggestions.

And first, on our highlights today, as we told you many times before, it is a new age, so to speak, in computing, and gone are the days where you have to rely on just getting your data from CSVs in a random place all the time or having to deal with other proprietary platforms or services. Now there is the wonder of APIs for you to access these services programmatically. And our first highlight is a great tutorial on a recent refresh, if you will, of a very important package in this space of using R to call these APIs very quickly and efficiently.

And in particular, Melissa Van Basso, who is an associate statistician at Statistics Canada, and she makes a lot of our videos about our programming and statistics. And that's literally one of her taglines in her channel called g g not. And if you haven't bookmarked it, you should because she pumps out a lot of terrific content on a very regular basis. Very inspiring. Her latest video is an aforementioned tutorial, quick but straight to the point, practical tutorial on working with APIs using h t t r 2. And she sets the stage by putting on some nice analogies about how APIs with respect to security works because most of the time, these APIs, yeah, they may technically be free to use and some are not, but one way or another, you need to authenticate to them in some way, shape, or form.

This can be very confusing to people, but she walks through 2 examples. 1, using OpenAI's API for, you guessed it, an image generation, request, but also something that I hope gets more spotlight in, in the our ecosystem these days is the API for GitLab. For those that aren't aware, GitLab is somewhat analogous to GitHub, only it is free and open source, at least open core. And a lot of people are turning to that to self host their version control repositories. And in fact, many organizations use that as well within their firewalls that give them that robust Git, like, hosting platform but under their control. But they have an API as well. So she walks through 2 examples, again, using those respective APIs and how to authenticate to them with h t t r 2.

And this is where you will have to inject some authorization magic, but there is a function in h t t r 2 to do just that. But it is interesting that she selected 2 examples because not every company behind these APIs is gonna have the exact same syntax for authentication. And, in fact, these two services do require slightly different request language or request structure in that authorization header, if you will. And the good news is this video does have an associated GitHub repo where she walks through the, example very clearly, and you can quickly get a get a glance at it. It's only about 50 lines or so, but you can see the different headers that she has to inject in these various APIs.

Now the best situation, of course, is if there is an r package that actually wraps this API that you're interested in. She's very upfront with the fact that in real world usage, you might wanna leverage that particular package instead of building something up yourself. But, again, this is a tutorial for how h t t r two works. So it's really great to see these, this example here. And, honestly, once you get the authentication down, that's kinda like more than half the battle, so to speak. And trust me, I've been there. I've had some wicked wars of authentication in the past.

But at that point, the other interesting nuance between these examples is the type of request that you're about to send. With respect to the GitLab API, she's interested in grabbing issue summaries, and that is facilitated by what we call a Git request. You're not really putting anything else in your request kind of transmit to the API service other than a various parameter or set of parameters in the URL of that actual request. Whereas, for the OpenAI example, she actually has to do what's called a post request, which is saying that you, as a submitter of that request, have to tell the API something about what you want or supply some important information.

And in this case, it's the prompt of what kind of image she wants to generate. And, of course, we love animals here on this podcast. So she has a prompt of acute baby sea otter, insert penguins or whatever your favorite animal is. You can probably do that too. But that has to be a post request. That's not through the URL itself. So, again, most of these APIs will have documentation, some may be better than others, on how to handle these requests and whether it's a get request or a post request. But oftentimes, when you're kinda like doing a query of what's out there, most of the time, that's gonna be a post request of some sort.

But in the end, once you get the results back, you're gonna get some nice, maybe not so nice, list of the results. Often, it's being compiled from JSON from the API itself. An h t r two will have a friendly wrapper to say, okay. This is a re a request coming back in JSON format, but I'm gonna translate that back to a list. And then it's up to you to wrangle that list afterwards. So that's more of an exercise for you to play with at the end here, but she does show a couple of examples of how you can subset this nested list for the various parameters like the title of that issue or milestone or other information such as the URL of that image that's being generated from OpenAI.

That's where there are other packages in the our ecosystem that can definitely help with wrangling lists. Mike and I love the per package for a lot of these situations. There are other interesting ones where it turns a nested list into a nested data frame. I've seen that mentioned in the past. There's lots of ways to post process this. But, again, half the battle is just getting authenticated to this and then leveraging h t t r two's new pipeable syntax to kind of build this step by step, the request process as a whole. So if you're new to it, definitely start with this package.

HTTR itself came before it, and I used it extensively back in the day. But if you're comfortable with the tidyverse, you're comfortable with pipeline processing, this is gonna be a very nice fit for you as you get involved in this landscape. And I can speak as a package author, albeit a very, low key package, that h t t r 2 was very easy to work with. My package called pod index r, which is interfacing the podcast 2.0 API from r, leverages h t t r 2 under the hood. So I did authentication via the mechanisms that she talks about here. I do want to mention one thing, though.

Never ever put your authentication keys in your script itself. And this is obviously a very brief example, but in real practice, environment variables are your friends here. So definitely take a look at that. That's also documented in the h t t r two's package Vignettes if you're curious about how that lingo works. But one thing that trips a lot of people up at the day job, if you're updating your environment variables, maybe you generated a fresh token for that service or whatnot. You've gotta restart that hour session first because it will not take effect until you do, and I've had more than a few calls, a few messages on our teams at work of, I changed my GitHub API, PA 2, and it's not working. Help. And I'm like, did you restart your session?

Oh, so don't just just it it happens, folks. It happens to the best of us. And, yes, it's happened to me even at this, experience level of APIs that just trips you up, especially the AWS ones. I'm gonna be starting on that. But, nonetheless, this was an awesome straight to the point tutorial. It's an easy watch, maybe 15 minutes at the most. And Melissa has a bunch of other terrific tutorials on his on her channel, so definitely have a look at that.

[00:10:16] Mike Thomas:

Yeah. This was probably my first, that I can remember introduction to Melissa, and she does a phenomenal job, like you said, Eric, in this quick nice 15 minute YouTube post about how to work with the new h t t r two package and and I do really like the the beauty of the pipeable syntax as you sort of build up either this get or post request and then finally execute that. And, yeah, I just can't speak highly enough about how well, Melissa really articulates every step of the process. You know, why she's writing the code that she is, particular syntax, things that you can get hung up on such as having to to quote, her back tick essentially different pieces of the, the the request that you are sending to make sure that, you know, our isn't, unable to to find certain objects because you're not quoting those and things like that. She she does a really nice job better than I could ever, explaining exactly how to build up these get and post requests using h t t r 2. I'm really thrilled that we have the h t two h t t r 2 package, now out and available and battle tested.

So I I would highly recommend folks who have been using h t t r, to to take a look and see if it makes sense to switch over to h t t r 2. I think you'll find some gains in efficiencies, some gains in, just sort of the the syntax style and things like that that might make your life a little bit easier and I think you know a big key to these packages is they help developers as well create our packages that serve as wrappers to API's right that make everybody's life easier so if you're somebody that, you know, has an API out there that you really wanna create an R package around to make other people's life easier interacting with that that data source or whatever that API serves, maybe the h t t r two package will help you do that more easily.

I believe in the past with h t t r, there have been vignettes and articles and things like that on how to develop in our package that wraps an API. So I'd be curious to see. I haven't haven't checked, but maybe we can put it in the show notes to see if there's sort of an updated version of that documentation using the h t t r two package, for wrapping your favorite API. But, again, a great walk through. Really nice examples here. 1 with a get request and one with a post request that sort of run the gamut of all the different types of of APIs that you may be working with. So can't speak highly enough about Melissa. If you're interested in taking a look at some of her other content, she has her own website atmelissavambusil.com.

So check it out.

[00:13:09] Eric Nantz:

Yeah. And, yeah. Follow-up to your quick question there. Yes. H t t r two does have a vignette exactly tailored to wrapping API. So definitely wonderful reading. I've read it literally 4 or 5 times as I was writing my little pod index r package, to to handle this. So it's very thorough, very, straightforward. And again, touches on those those nitty gritties like authentication and managing managing those effectively. But it's got two examples using the New York Times API for books as well as GitHub just API. So again, it's great to have 2 different examples because it's almost like no way no 2 APIs do things similarly. There's always some little minor differences here and there. And we're definitely seeing that now with HTTR 2 recently, celebrating its 1 dot o release, another, I think, reason why Melissa made this video. This is kind of the unofficial or somewhat official signal that this, like you said, is is tested by many people. It's ready to go and to augment your API workflows.

We're seeing some other packages in this kinda ecosystem area of API, you know, linking, wrangling, and now testing that are basing their work on HTTR 2 as well. So it's definitely becoming your go to, I think, for a lot of the API, operations in r itself. And I'd imagine we're gonna see more packages in the ecosystem start to wrap this, more fully as they, you know, find more APIs that just haven't been helped by the community just yet. So, yeah, wonderful tutorial, and it was a thrill to meet Melissa in person at deposit conf, back in in the fall last year. Yeah. She very personable and very humble and, yeah, really great to to see her turn through this terrific content. And there we say, we we hope you keep it up. And, there was even I remember looking at her LinkedIn post announcing this video.

Someone had the request. Hey. Have you ever thought about doing a live stream of your live coding? She sounds like she's not saying no to it. But, Melissa, if you're listening, if you want a buddy to help you along that, wink wink, I'll just say that.

[00:15:20] Mike Thomas:

Who knows? Who knows? And to double down on your best practices, Eric, put your API keys as environment variables in a dotrenvironment file and git ignore that thing immediately.

[00:15:32] Eric Nantz:

Immediately, folks. Yep. Because, you heard my rant maybe a year and a half ago. Somebody did not do that, and we were, dinged pretty badly at the the tape shop security group when they discovered that. So, yeah, you gotta be careful there, folks. Gotta be careful. Well, Mike, we were just saying how things are becoming more normal for us as we're getting, you know, the the 2024 season of our weekly highlights up and running. Well, one thing we didn't really have as much last time around was a great post around visualizations, and sure enough, that that's been normalized to this week, if you will, because friend of the show, Albert Rapp, is back to the highlights and for his first contribution to 2024 where he has both a blog post and an accompanying video talking about 5 of his favorite ggplot 2 extensions.

As we mentioned many times in highlights before, ggplot 2 is not just about gg ggplot 2, the package itself. Because of the extension system that it introduced a few years ago, now the community can take ggplot 2's functionality and supercharge it in many, and I do mean many different ways. And this roundup of extensions is a great tour de force to just see what it exactly is capable of. And the blog post is quite visual as you would expect, so we'll do our best to summarize it here. But right off the bat, one easy win for you as you think about annotations in your plots, as you think about formatting maybe your subtitles or your titles or access labels or whatnot, have a look at the g g text package.

This is one that has been mentioned in previous highlights before. This is authored by Klaus Wilk. We'll have a link to all these packages in the in the show notes as well. But this is a very quick way for you to augment that existing text that you want to put on your plot with HTML syntax or markdown syntax. So you're living the quartile lifestyle, the Rmarkdown lifestyle, or maybe your Shiny app styling. You can do that same kind of thing in ggplot2 as well. Ggtext just does this very well, very elegantly.

And if you're if you're comfortable with markdown, which many people are these days, it is a terrific fit for you on on your ggplot2 builds as well. And with a low HTML knowledge, the sky's the limit for what you can do in these annotations, in these textual representations. So must have, I would say, as you think about getting those plots in a more publication ready state. Speaking of multiple plots, one thorn in my side many, many years early in my g g file 2 exploits was the fact that, you know, I had these two plots. Maybe they were they were talking about the same domain, the same type of data, but they were completely different.

But yet I wanted to compose them in such a way to arrange them so they fit on a single page or whatnot. I've had a lot of battles with grid dot draw and other situations like this, but Patchwork, the r package by Thomas Lynn Peterson, software engineer at posit, is a terrific way to take almost any type of ggplot and compose multiple ones together using a very intuitive, you know, syntax that's native to ggplot2 itself with the plus operator. Just kind of combining the 2, you can stack them up with the the the slice or the fraction operator if you want, but there's a lot more customizations to say take one facet and make it big wide, maybe 2 plots below that that are sharing the space equally between each other.

This has been asked for me at the day job to help bring recommendations to this because oftentimes we'll have, like I said, these plots that are doing kind of different things, but yet using the same data, and they tell kind of different stories. So having in patchwork is so helpful to just customize a layout without a lot of fuss around it. So very, very helpful if you're in that situation like I've been many years ago. I'm having a lot of flashbacks to parmfrow,

[00:19:54] Mike Thomas:

which I Dude.

[00:19:55] Eric Nantz:

Oh, dude. Yes. Many, many. My dissertation was full of that. Yes. It was a nightmare to debug. Lot of college flashbacks for me. Yep. Yep. We didn't we didn't expect that. But every time we do this show, we always have flashbacks of some sort. Just didn't expect that one. But, moving on to more pleasant memories or things I wish I had known about before, Sometimes you wanna really get, you know, customizable with the types of patterns that you wanna put in the geomes that you surface in these plots. Maybe a gradient or maybe a little fun like symbol or or an image that you wanna fasten into a bar chart like setup.

The next package that Albert highlights is gg pattern. Gg pattern is authored by Mike FC. You probably know him as cool but useless on the various social spheres. And this is a very excellent package that lets you simply figure, specify either geometric or image type patterns to fill in your various geomes such as bar charts, such as, you know, line charts or things like that. And then Albert, in his example here on The Post, has a fun little representation of coffee bean, quantities, for for different countries in South America by using a little coffee icon in this waffle chart type setup.

That was really interesting, the things you can do with ggpatterns. So, again, this might be great for those of you who are trying to build infographic type setups or ggplot. This might be a great fit for you. And, again, combine that with things like ggtext, the sky's the limit once again. And then the next one we have here is kind of a potpourri or a collection of kind of utility type extensions, and this is off this is called ggforce, offered by the aforementioned Thomas Lynn Peterson. He has been in the ggplot2 game for many years.

And this package, I believe, came out not too long after the extension kind of paradigm or system was introduced in ggplot2. So ggforce lets you do a whole bunch of random type geomes that maybe are not quite the same kind of design choices as someone like Hadley has thought about for ggplot2. But if you wanna create bars like a bar chart with an arc or a circle geom, which is great for obviously things like pie charts or donut charts and the like, there's lots of interesting, geoms you can augment in your existing visualizations.

And ggforce has a whole bunch more. But again, to Albert's credit, he's got a fun little tutorial linked in the same blog post. And, he created donut charts and pie charts with gg4s. Again, lots of opportunities to combine this and really tailored, again, maybe for that infographic type setup with the alternative geoms that it offers. And last but certainly not least, we've got bump charts, which are becoming a lot more common these days as I look at visualizations, and that is offered by the gg bump package that is authored by David Sjoberg, and we'll have a link again to all these packages in the show notes. This is another great utility type ggplot2 extension.

Does one thing, does it well. And his example that Albert puts in here is looking at flight tracking for European flights across various countries and the trends that you see over time. And it's very easy to see kind of the performance or you might say the launch demo profile of these. And, again, I don't do a lot of these in my day to day job, but I think this is still something that you might be able to augment pretty nicely with another package like gghighlight to really fasten in on one particular country's, trend in flight patterns or flight rankings as well.

That again, that's just 5 extensions in this massive ecosystem of ggplot2. There's there's just so much to choose from. Albert does a great job of giving you 5 quick hits that I think can augment your visualizations very well in your day to day work. And he's got links to other areas where he's put these in practice. So he's he's been doing a lot in this space. We we met Albert, at at PasaConf, and he has turned around the visualization content quite a bit. He's very excited about it, and I believe he even has a short course that he's authored as well. So definitely be on the lookout for that as well.



[00:24:47] Mike Thomas:

Yeah. I really enjoy these, you know, sort of digestible visualization posts from Albert. And I guess 2 things that really stood out to me, you know, as as someone who has to provide clients with a lot of deliverables, you know, especially non technical folks at the end of the day, is the text customization in in gg text, you know, allowing you to write HTML essentially inside your string and to be able to to bold text, to to change the color of text, things like that on your your charts, I think, goes a long way towards communicating really the central idea around your data and the visualization you've created and what you are trying to, convey to the end audience. And then the other the other one that really stands out to me for very similar reasons is gonna be the g g highlight package as well, which, admittedly, I haven't used enough, but it it's incredibly simple in terms of the syntax that you can use to highlight a particular element of your chart. On this bump chart that Albert has created, you know, the the first argument in that is a dplyr filter kind of like syntax, where he's he's saying state, double equal sign, United Kingdom, because the line for the United Kingdom's flights is the one that he wants to highlight in green, and all of the other ones are going to be sort of in this in this soft gray in the background.

And it's really 1 or 2 lines of code to be able to do that and highlight this particular line and set all the others to this background gray color. And it stands out so beautifully on the chart, and it it really demonstrates, you know, the the key concept that Albert's trying to display, which is is show the end user, the UK's trend from 2016 to 22 of incoming and outgoing flights. And if the end audience wants to also look at the data for the other 5 countries in this chart, they can because it's there. It's just sort of abstracted a little bit into the background, to make the point and make the readers focus, on the United Kingdom.

So really cool, blog post by Albert. He always does a great job including not only the the charts and his rationale behind them, but also the code behind everything. I recently had a use case where I wanted to essentially take the I had a, flipped axis chart, and I wanted to take the values on the x axis and put them within the bars. And the first place that I looked was a on Albert's website. And, of course, I came across a blog post where he was doing exactly that. And it made my plot a little bit bigger because I was able to sort of exclude the x axis itself and put those values right on the bars, so they really stand out to the end user, and they don't have to sort of line up, the bar width to the corresponding x axis value. They can just see it right at the end of the bar.

And, it it was all thanks to Albert who made my life really easy in order to be able to accomplish that. So thanks to him again for continuing to churn out this great visualization content and expecting more of the same in in 2024.

[00:28:01] Eric Nantz:

Yeah. I think the a lot of these visuals you see here would fit really nicely if you're making any kind of business intelligence dashboard setup or other content where you gotta push the envelope a little bit, after that standard type of visualization. I can see this, this set being quite valuable in in this space. And, yeah, the syntax to to wrangle these packages are all doing a terrific job of if you're familiar with jgplot2, you're gonna feel right at home in putting these geoms in practice. So that user experience from a, you know, a content, you know, creator standpoint of graphs, you wanna make sure you're not, like, content shifting so much as you use these alternative geoms. So I think this is this is a great showcase of if you're comfortable with this language, you're gonna you're gonna feel right at home with the various extensions that you can push here. And, man, that g g text package, though, that is such an underrated gem in this space. I can't believe I didn't use this sooner, but better late than never.



[00:29:01] Mike Thomas:

Same here.

[00:29:04] Eric Nantz:

Yep. And one thing we don't want you to be weighed about, our last highlight is is more of a friendly call to action here because you all know Mike and I are fans of the shiny ecosystem. We talk about quite a bit on this very show and our other endeavors. And, yes, the shiny conference for 2024 is coming up this April, April 17th through 19th. And like years before, it is all virtual, but it is definitely expanding the footprint, so to speak, with not 1, not 2, not 3, but 4 exciting tracks of talk. And, of course, first off is that the early bird registration is open, so we'll have links to the registration page in the show notes to get you get your early bird tickets available there.

But in the aforementioned tracks, we have 4 tracks that you're gonna be learning about. One of them is the shiny innovation hub, and this is really looking at what are the latest developments, best practices, and creative uses of Shiny within the ecosystem itself using very novel approaches for problem solving. And that is being, chaired by Jacob Nowinski, who is the lead lab lead at Epsilon. He is chairing that track. The next track we have is Shiny and enterprise. I dare say, Mike, you might know a little bit about that as do I a little bit.

That is how Shiny is being used to transform and shape outcomes in the business sector. So going beyond analytics, looking at enhancing efficiency, helping decision making, and elevating these data analytic processes. Lots of great material there, I'm sure. And that is being chaired by Maria Gruschick who is senior delivery manager at Absalon. And this one hits a little close to home. This is the Shiny Life Sciences track because Shiny has been big in pharma and and health in general for quite a few years now.

This track is being chaired by yours truly here, and we're looking for application or talks that highlight ways that Shiny has been driving insights, enhancing drug development, enhancing collaboration within life sciences. And I dare say there's been a lot discussed in this field for quite a few years, but there's no shortage of new innovations happening here. And then lastly, we got the shiny for good track looking at ways as shiny as being used to help make positive impacts to the communities around the world for society, really looking at how they are helping these initiatives out there. And this is being chaired by good friend, John Harmon.

This is really important to see where Shiny can be used to help social impact, driving positive change, really helping the diverse communities all around the world supercharge their efforts. And I'm being more detailed about these tracks because we have calls for talks ongoing. We have until February 4th to send your submission for our talk, and the submission form will be linked in the show notes as well. So I know I speak probably on behalf of the other leaders of these tracks. We're looking for really exciting talks in this space, and don't ever feel like you have to be this huge pro at shiny to give an awesome talk. Really, I've seen some amazing talks from people just getting new to Shiny, but using it in an innovative way that is totally unexpected, but yet it's really helping their efforts. So really encourage you. If you've been using Shiny to help innovate your workflows.

Definitely send a proposal. We'd love to see it. And definitely get registered for early bird registration because this conference has been quite popular over the years, and I don't see that changing anytime soon.

[00:32:55] Mike Thomas:

Well, I'm super excited for this conference again this year, Eric. I think I've been attending it ever since they hosted the first one, maybe 3 years ago now at this point, something like that. And it's it's one of my favorites, obviously. We don't hide it very well in this podcast that we love Shiny, and I am thrilled to see what's put together thus far across the different tracks. Thrilled to see you up there, Eric. I think the Shiny in life sciences will get a ton of buzz. We're seeing that space grow and grow and grow across our own client base, and I don't think, across the rest of the world is any exception as well. So I've purchased early bird tickets for our team, and I finally just began the submission process for a shiny for good app showcase so I'm going to finish that up today see if I can push that through see if see if maybe it makes the cut But mostly, I'm excited to see what everybody else has to offer for this conference. So no reason not to tune in. Check it out. It's free for students. It's it's only a few dollars, for everybody else, and it's absolutely worthwhile if you are somebody interested in Shiny, doing Shiny, beginner, intermediate, advanced. Doesn't make a difference. You will get something out of this conference.



[00:34:14] Eric Nantz:

You sure will. And that's a really exciting point, Mike, because, now on top of talks themselves, we love these app showcases. You've done that before. We have a great app before, and it's it's really great ways to demonstrate where Shiny is being used in practice. So, yeah, we highly encourage you to sign up for it. And, also, like I said, if you wanna share your knowledge, share ways that Shiny has been brought innovation in your work. Yeah. We love to hear about it. And with these four tracks, we think we have something for everybody here. So there no matter which industry you're representing, no matter which part of academia you're a part of or any other research or community effort, we'd love to hear about it. And I'm really looking forward to reviewing these talk proposals in a in a few weeks. And judging by what happened last year, both John and I had a very hard time narrowing down, you know, which ones made the cut because there was a lot of great submissions last year. And I expect this to be exactly the same and really looking forward to working with the Appsilon team on helping organize this as we get closer to the conference days. And, yeah, should be a lot of fun.

Speaking of a lot of fun, folks, the rest of the issue does have a lot of fun for you to to look at with leveling up your our knowledge and innovations, what's happening in the world of data science and respect the package development, you know, tutorials and whatnot. And we'll take a couple minutes to talk about our additional finds here. I'm going pretty niche on this, but it's a very important niche, especially for someone I work with quite closely. You've heard me, speak the praises of what Will Landau has been doing with the targets ecosystem and more lately, the crew package for batch processing, asynchronous processing in your ecosystem.

And that is standing on the shoulders of 2 very important packages in your ecosystem, MirrorEye and NanoNext, which have just been updated on CRAN with some very important updates in this space with respect to where Will wants to take the crew package down the road. And in particular, the latest update for Mirai is now handling much more gracefully situations when someone needs to terminate a running job for whatever reason. There are some enhancements to the daemon behind the scenes to help make that a lot more elegant, a lot more graceful than in times past.

And this is very helpful as you think about the ways that batch processing can be launched on other back ends. I'm looking at you, AWS. That's one of them. We got our eyes on quite a bit. And the associated package, nano next, is also kind of the system library that Mirai is based on. That also has some important updates as well. But, again, these were very important updates for what Will is trying to do with the crew package down the road. So he was he was telling me in my ear last week that, yeah, he's really looking forward to these updates, and he's got some big plans now that now these have hit. So really, big thanks to Charlie Gao, who's the maintainer of these packages.

Really, really excited to see these updates here. Micah, what did you find?

[00:37:27] Mike Thomas:

I found a pretty cool, also niche blog post on something that I hadn't really seen discussed very much before, and it's called Security Headers for Shiny Applications. It's a jumping rivers post, I think, specifically authored by Colin Gillespie. And it's all about essentially profiling the security on the server or or your posit setup, wherever your Shiny apps are deployed. There are some functions from a package called server headers that allow you to sort of check the security around that server, such as SSL status, redirects, referrer policies, all sorts of things like that. So if you are someone interested in, information security around your Shiny apps, and may perhaps you're working with your IT team to host Shiny apps or deploy those Shiny apps on some, local or cloud based server, I think it might be of interest for you to to take a look at this blog post, because there's probably not a lot of other content out there that might be beneficial, you know, for your IT team to sort of understand, how Shiny apps are are hosted and deployed on a server and maybe the different security things that go along with securing that server in particular. So really cool one from Jumping Rivers, nice and short and sweet, and check it out.



[00:38:48] Eric Nantz:

Yeah. Excellent find. And, you know, as Shiny, it obviously takes more, you know, to has some more uptake in the industry as a whole, a lot of times, IT groups are are getting more familiar, especially on the the teams of security, on making sure that authors of Shiny apps are following best practices and whatnot. This might very well help those that are deploying Shiny space server products in their organizations to have a little have a little preview of what's being exposed under the hood. So, yeah, certainly great great to see that space. It's great to see developments from jumping rivers in that space as well.

And certainly speaking of, really loving to hear back, we'd love to hear back from all of you and your feedback. And we got a couple somewhat indirect feedback pieces to share. One direct that literally just came as they were recording this. But, you if you listened to last week's episode, Mike and I had some, I would say quite candid thoughts on the situation that Posit's found itself in and particularly Eway Cia's recent situation. We're gonna put a quick plug to and a shout out to Matt Dre in his recent blog post.

He has basically a huge appreciation for what he was done for the work he does in in his day job at the UK government and public sector. But he gave a little shout out to to me and Mike here about where we about our discussion about heat waves, for it. So thanks, Matt, for for, putting that in your blog post. We'll link to that in the show notes as well. And literally, Mike, late breaking feedback. It's almost as if he's psychic. But, John Harmon, we just mentioned, just shot me a note that with respect to APIs, which you touched on the first highlight here, he is writing a book about using APIs from r.

And we'll have a link to that in the show notes. It's, wapit. Io. We will link to that in the show notes. So thank you, John, for reminding me. I knew you're you're hard at work at that. I didn't realize how far along it was. So if you're leveling up your API now, it's, there's your chance.

[00:40:53] Mike Thomas:

That's awesome. I totally forgot. I think John had been working on some sort of like one size fits all package or project for, you know, one particular r package that could wrap any API essentially and and do it in a fairly agnostic way. I I don't know where that stands or or if I'm remembering that incorrectly and it's it's more about the book, but I am very excited to read that.

[00:41:20] Eric Nantz:

Yeah. You're exactly right. He is definitely working in that space. That package you're referring to is called beekeeper. Really fun package in this space because, hey, if anything can give you that, you know, what we like to call the use this or dev tools like setup to wrap these API APIs in the packages, I think beekeeper is gonna be a great, great way to do that. So I'm I'll double check how far along it is. We'll link to it in the show notes regardless. But he's been hard at work on that, I believe, ever since he received a grant from the r consortium on this API development work recently. So, yeah, John, thanks for that, real time feedback. You're you're lucky I had that that page open as we were recording. I didn't realize that was coming through. So, yep, APIs. It's a great time being APIs with our, I'll just say that, with what John's doing and and great tutorials from Alyssa and whatnot.

So, of course, we love hearing from you as well. Wherever you are in the world, we have various ways of doing that. You can get in touch with us on our contact page, which is linked directly in the show notes for each and every episode. Also, we love to get your contributions to the rweekly project via a poll request of, say, a new blog post, new package, a new tutorial, and whatnot. All the details are at rweekly.org. We we love to hear that. We love to see that. All the curators really appreciate it when the community helps step up as well.

And so we're looking for curators to join our team. Again, information on that is at rweekly.org and our GitHub repository. We're trying to make things easier for us, but the best thing we could have is more people to help us with it. So if you're interested at all and you're passionate about this space, it would be a great time to reach out to us. And, also, if you wanna reach out to us on one of those fancy new podcast apps that you've downloaded, like Podverse, Fountain, CurioCast, or whatnot, they're all supporting the boost technology, and that's an easy way for you to send us a message directly in the show and have a little fun along the way.

You can find complete details on that in the show notes, and we are proudly linked on the podcast index page where you can boost directly from there as well. Again, I wrote a little package around its API, and I got more stuff in store for that stuff coming up soon. So stay tuned. Maybe some more our content for this very show. Talk about meta. Right? Our content about a podcast to talk about on a podcast. That doesn't make a head spin. I don't know what those.

[00:43:49] Mike Thomas:

Inception.

[00:43:50] Eric Nantz:

Inception overload. Yep. And, but, yeah, we'd love to hear from you. And, definitely, you can get in touch with us on social medias as well. You'll find me mostly on Mastodon these days where I'm at our podcast, at podcast NSF social. Very sporadically on the x thingy, but also on LinkedIn from time to time. And, Mike, where can the listeners find you?

[00:44:12] Mike Thomas:

Yeah. Probably on LinkedIn primarily. I think my handle there is Michael j Thomas 2, or you can search Catch Brook Analytics, k e t c h b r o o k, to find me there. Or if you wanna find me on Mastodon, I am at mike_thomas@fostodon.org.

[00:44:32] Eric Nantz:

Very nice, Mike. Well, anyway, we will close-up shop here for episode 148, and we will be back with another new episode next week."
"71","issue_2024_w_02_highlights",2024-01-10,59M 10S,"We kick off 2024 with a jam-packed episode! Learn four ways to streamline your R workflows, a proposal for a new pipe assignment operator in base R, and our raw responses to a surprising turn of events affecting one of the most influential members of the R community. Episode Links This week's curator: Eric Nantz - @theRcast…","[00:00:03] Eric Nantz:

Hello, friends. Did you miss us? Yes. The R Weekly Highlights podcast is back and it is 2024. We are kicking off the new year in style. We have a supersized issue to talk about. But if you're new to the show, this is the show where we talk about the latest our weekly issue that's curating the latest and greatest in our content, whether it's, adventures in data science, new packages, tutorials, and much more. My name is Eric Nantz, and I'm always delighted that you joined us from wherever you are around the world. And I certainly hope you and your loved ones, family, friends, all had a safe and relaxing holiday season, whichever ones you celebrate.

Yeah. It's hard to believe it's 2024 already.

[00:00:45] Mike Thomas:

But just like last year, I don't do this alone. I got my awesome co host, Mike Thomas, joining me once again for another year of our weekly highlights. Mike, how are you doing today? I'm doing well. Eric, I'll be honest with you. I missed this. I hope the audience missed it as well. I missed it just as much as you folks. So I am so so excited that our weekly is back, that we had a great curator this week, and, the 2024 is hopefully now off and running for us here on our weekly.

[00:01:16] Eric Nantz:

Yes. It is amazing how this makes me feel a little more normal again going back on the mic with you, so to speak. It was definitely a busy break for me, but, yeah, it's great to great to get things back in motion on the our side of it. And, yeah, that curator, that wacky individual, yeah, that was actually yours truly this time. And I really went above and beyond in a different sense, and this is totally unrelated to the issue itself. But I figured I've seen some cool stuff built with Quarto lately. I saw the new dashboards. We've actually talked about on this very show recently.

I thought, you know what? For our curator team, we often have to share this rather cryptically formed URL of our public kind of curation calendar. I hosted on Nextcloud, which is great, by the way. Nothing against Nextcloud. But the URL they give me for sharing, yeah, that's just not gonna be easy to remember. So I thought, well, wait a minute here. Guess what? Because of my streaming adventures years ago in making what I call the streamers calendar, I know there are some HTML widgets out there that could play nicely with Cortl to render this same calendar, but, hopefully, a more friendly dashboard and give us both a calendar view and kind of a more tabular view of who's on which week for the curation.

So, yeah, I took a few days. I now have a portal dashboard of our public curation calendar, which I'll link to in the show notes. It's hosted on GitHub Pages, but it takes heavy inspiration from Garrigate and Buoy's Norfolk data portal dashboard. And I figured, you know what? If he can do it, so can I? So that is now publicly available. I have a link to it in the supplements of the show notes here in case you wanna have a look, but it was my first adventure with portal dashboard. So after I got that squared away, yeah, it became down the business and curating this week's issue.

And also, this is, like I mentioned, a supersized issue because we were off for a few weeks as a whole team. So there are a collection of stories here from previous weeks that would have been released if we weren't on break. So either way, buckle up. We got a lot to talk about, and there's a lot to read after the show too. But I can never do this alone. I have tremendous help from our fellow Rwicky team members and contributors like you around the world with your awesome poll requests to make all this happen.

And we lead off with kind of a nice kind of retrospective like post of of areas that we've talked about last year, especially with respect to how you can streamline your R workflows. And our our author this week is the esteemed Nicole Rennie who has been a frequent contributor to our weekly highlights in the past. And she ends up giving us this nice end of year blog post talking about 4 ways that she's learned how she can streamline her our workflows. And each of these hit home with me, and I'll be curious, Mike, how much you've been using these in practice as well.

The first area that she talks about, it may sound basic, but, boy, is it effective using template files. Because, you know, everyone does this. We have that set of scripts that we created. And then there just might be a new data type, but it's mostly the same. Maybe just a couple changes in variables or maybe a a similar service you're pulling from. And, yeah, you do the copypasta, as they say. Well, why not make a more templated structure like Nicola did with her tidy Tuesday analysis script? So now she has a set of scripts that she can create dynamically based on templates for that current week of TidyTuesday and having all of her reusable functions for visualization and other aesthetics and data processing all set to go. And, yes, this blog post has links to the individual blog posts where Nicola talks about this in more detail, that you can check out. So that is a great, great way to do it. Low hanging fruit, as they say, to get you some much needed savings and time.

And speaking of templates, if you find yourself making multiple GitHub repos that look really similar in structure, especially beginning the first time, yes, I feel seen. I do this almost every day at the day job. You can leverage GitHub repository templates. This is huge because instead of, like, doing the blank repo, maybe getting your r env library in there, getting like your helper scripts for that data processing or whatnot, Why not start yourself on the right foot? Have a template structure in place.

Nicole has been making great use of this in her workshop materials because she was quite busy in 2023 teaching some workshops. I'm still very privileged that we had her at the our pharma series of workshops talking about machine learning, and she had a tremendous job with the material on the GitHub repository that she created. But, yeah, that has now started in her workflow from a GitHub repository template. I have also made use of this with my fancy schmancy Docker container setup. So now every time I know I'm going to do an R related project, I want to leverage Versus Code with the Docker container and the custom RStudio IDE server edition in the Docker container.

Instead of like repopulating all those Docker files 1 by 1, I just have a repository template that I do for my new repo. Makes a heck of a lot of time savings. I only have to change a couple environment variables, and then literally, I can just bootstrap that thing in less than 5 minutes. It is, again, a huge time saver for me as well. Now 3rd up in this list is one I need to be much better at. So I'm coming confession time with Eric here. I do not do a great job of linting and styling my code. Well, guess what? There are r packages that help you with this. In particular, the lint r package and the styler package.

These can take away all those spacing issues, syntax errors that you might not detect until you actually run this. If you author Shiny apps, you know what I'm talking about. You're missing that bracket, you're missing that variable? You're in trouble kind of for that reactive. Don't get me started on that. But, yeah, it's not just finding those errors. It's also making sure that your code has a consistent style. And so Styler can be customized to meet your needs, but it's gonna get you off on the right foot very quickly.

And boy, is that helpful for multi team, multi person projects where you wanna make sure your whole team is having a unified approach on how you style the code. So I would definitely make use of Styler and also Linter to find all those nagging issues that you would manually have to detect yourself in the old days. And lastly, yeah, at the top of the show, I talked about quarto. Right? Well, quarto comes with a lot out of the box, and I do mean a lot, but there may be things that you wanna do to build on top of it. And so Nicole had an adventure in 2023 in their early part of the year on building a quarto extension to help extend the styling that she did for PDF reports.

This is really useful, especially if you wanna kinda get that similar flavor as you might have in the past with some of these custom R Markdown templates, but wanna have a similar thing with Quartle. These, ways that you can build extensions might be a path forward. It definitely does take a little getting used to. I mean, certainly, if you're comfortable with LaTeX, you're gonna be right at home with some of the styling for PDF output. But in general, I believe it's using Lua scripting, so you might have to do a little level up on that as you go along.

But the linked resources on her more detailed blog posts can get you up and running quickly. And also for a plug, I mentioned the R pharma workshops earlier. We had an R pharma workshop from Devin Pastore about building portal extensions. So I have a link to that in the supplements of the show notes as well if you want to really get into the weeds of building a complex extension. It's definitely something you might have to get used to. But again, if Kordle is not doing everything you want out of the box, you can definitely customize it. And I am a very big consumer of extensions, especially for the web based presentations of Reveal JS.

Big shout shout out to Emil Hunchfeld who has built immense extensions such as, like, the code window, even that fun little confetti, extension. I'm not sure if he created that or is that someone else. But I'm I I make use of quite a few in the reveal JS space. So, again, these are all great things that are attainable in your R journeys with making things a little more automated, a little more repeatable. And, yeah, Nikola, concludes the blog post of a little intriguing notes on what she's interested in. And, boy, she's also gonna be pursuing the Knicks package train as well. So Bruno is not the only one. I'm also trying to pursue this as well. So we might be seeing some blog posts from her about that. And, also, it looks like she's looking at Rust as well, another framework that's been getting a lot of attention in the R community these days. So fantastic way to inspire you at the start of this new year to maybe, supercharge your workflows a little bit.

So, Mike, what did you think of Nicola's,

[00:10:45] Mike Thomas:

blog post here? I couldn't agree more. I think there's a lot of tips in Nicola's blog post that would help us get off to a fresh start in the new year and start employing some of these best practices for ensuring that your workflows are consistent across projects and make you as efficient of a data scientist as you can possibly be. I think using template files is a great use case. I am so guilty of sometimes going from project to project and copying the majority of a read me file, for example, you know, especially where we talk about, you know, handing our projects off to clients and and how they can use the, you know, the RN package, that we have, you know, in our deliverable to essentially reproduce the environment that we created it with.

And, really, I think Readmes for us could be a template file and a great use case that a simple script like the one that Nicola actually has screenshotted here could help with that. You know, I know that there are packages like use this and dev tools and column and things like that that will actually create scripts for you. And the and the code is is pretty lightweight, so don't be afraid to to do it yourself. I think it's a really awesome trick that can be really underrated as well. Using GitHub repository templates, I will say, is one that that I actually do and that we actually do internally. And that is is hugely helpful. And I know, Eric, you do this, quite heavily, at least within your own, personal development work because you have, GitHub repository templates that spin up your your whole environment that make use of dev containers.

Right? So you can go essentially from project to project and have your your isolated environment and your development environment sort of consistent from project to project. And that is and that's really helpful as you go from project to project to ensure that all of the dependencies that you need to have, you know, within your projects that you're sort of consistently applying the same workflow to, can get installed quickly and consistently. I've seen a lot of articles lately as well from Rami Crispin, who is trying to, I think, accomplish a lot of the same things that you've talked about as well, Eric. And he has some great content online about how to set up your own, GitHub repository templates, I believe, for both our development and Python development using dev containers that have, you know, those those, dev container dot JSON files to sort of specify how you want your v s code environment to look, and then a Docker file to manage all of those dependencies.

And one of the cool things about that as well is if you're interested in leveraging, GitHub code spaces, you can essentially do that, apply that immediately just by clicking a button within GitHub, and it will leverage those dev container assets to spin up that environment in Versus Code in the cloud for you. You don't even need to have Versus Code installed on your own laptop. I know there's a cost associated with that, but I know it's it's pretty cool. And when we think about collaborating across teams, you know, these things become really important. And for us, that's not only collaborating across teams, but it's also handing off our deliverable to to the client at the end of the day and ensuring that they can reproduce our work, and and whatever we've created for them in terms of that deliverable exactly in within the same way that we intended it to be and the way that we created it. So that's really helpful.

And then the section on on linting and styling code, you know, it must be something in the water on this podcast, Eric, because I am also guilty of not utilizing the the linter or the styler r packages. I am very strict. We have an internal handbook about, you know, how to style, our our code and our our Python code, and we try to adhere to that pretty well. And obviously within our our code review process, you know, that that's a component of it to make sure that the code is styled, you know, in line with, sort of our expectations internally.

But I was talking to a client recently who were trying to help, set up sort of a good team data science workflow and implement some best practices within their organization and really trying to convince them that code styling is is important for collaboration and streamlining code review, and making all of those processes more efficient. And one thing that I, I think I failed to mention to them that I need to mention to them is how the lint are in the styler package can expedite that process and maybe also provide, safeguards to ensure that your code is styled in line with the guidelines that you've set up for your organization.

And maybe that's not necessarily the default styling that these packages have, but I know at least within 1 or or both of these packages, you can actually apply, you can apply some settings to how you want that code to be styled. And you can make some changes to to how you want code to be styled internally at your organization or or maybe just across your personal development projects. And I think that that is incredibly powerful and incredibly underrated, which I think is a theme of this blog post as well.

I love the section on building quarto extensions. You know, quarto is super hot in the streets right now. We are moving everything to quarto. All new projects are starting with quarto. One thing that that I love as well that I think goes, very aligns very well with, Nicola's section here on building quarto extensions is we unfortunately deliver a lot of PDF reports. And that means that we, use LaTeX quite a bit. In the quarto YAML file, you can actually set up that YAML file and set up, your your latex assets along with that, your dottex files, such that you can have parameters within these latex, assets.

For example, like the title of your report, a background image in your report, subtitle, things like that, that may change on a project to project basis. And you can specify those parameter values within your quarto yml file. And those will get passed to those Latex files. We do that on every single project, you know, because the project title is gonna change. Maybe the image that we'll have on the cover page is going to change. Things like that. But it's it's amazing how well those things, play nicely with each other. So if you're someone out there who finds yourself, you know, delivering a lot of PDF reports through quarto, I would recommend checking out some of the links that Nicola has in this blog post. If you you really wanna spruce up the look and feel of that PDF report, then I think learning a little bit of Lua, can go a long way as well. But there are ample links here, as well as on the quarto website. I think that can help you really make that quarto PDF Latec report, look, you know, nicely branded to your organization or look and feel however you want it to look and feel and customize it nicely. And Nicola gives us a little bit of a preview into some of the the work that she's, posted for herself in in 2024, and I can't wait to see, what comes next from Nicola because it's it's always super relevant, always super helpful, and and I learn a lot.



[00:18:03] Eric Nantz:

Me as well. And there are just so many threads here that I wanna implement both in my open source work and in my day job work. In fact, we're even thinking of, for the internal user group, I maintain, of having some kind of, maybe quarterly newsletter or bimonthly newsletter that can go out highlighting the current events and highlighting in the company initiatives. Maybe I could build that with Chordle, and then I saw a PDF setup like she's done in her earlier efforts. And, yeah, the linting thing, I really gotta get better with it. What's interesting is that in my dev container setup, I do have a hook to do linting automatically in Versus code with a dotlinter, kinda config file. So I'm I'm getting there on the right track. I just have to actually listen to the advice. But I will say the Versus Code is very obvious when it thinks you did a line longer than 80 characters. It will have a big old squiggly next to it, and it will annoy you enough that you probably will wanna change it. But certainly, I wanna make that more automated too and just get better habits. But, yeah, though those two areas and and, yeah, beefing up my templates in general.

Even just today, I have a project where I want to enhance my shiny bookmarking state hack that I've done at the day job. Instead of having to build that up scratch in each app 1 by 1, I want to do a template structure. I can just fold that in because I can't quite make it a package yet, but I just want those functions there every time. So I don't have to just manually copy paste it from that existing repo into the new one, change the path, change whatever. No. Ain't nobody got time for that in 2024.

So lots of things here I wanna make use of and really appreciate, Nikola's efforts on this. Couldn't agree more. And we're gonna shift gears a little bit to a very interesting proposal to maybe a future enhancement to the r language itself that is in the spirit of a recent enhancement that came a couple years ago. And that is making a case for a new pipe assignment operator in R itself. This blog post comes from David Hugh Jones. He's been a very active blogger in the R community, and he makes this case by talking about just kind of the status quo of how R does assignments.

You can either have, you know, passing by the value itself, you know, very much saying object, whatever, assign it to, you know, the result of another function or give it a constant or whatever have you. Sometimes it gets really wieldy when you have a variable that, of course, is being built upon a chain of functions. Right? This is one of the motivations for the pipe operator itself, which, of course, was first brought to light by the Magritter package. And now since r4.1 now has the base r pipe operator built in, which functions largely the same way. So you can construct your, say, data processing steps in a pretty nice kind of streamlined workflow using packages such as dplyr and many others that are pipe friendly.

Well, David wants to take this a little up a notch a bit more. What was in the Magritter package? It didn't get a lot of press at the time. Was it had its own version of a pipe assignment operator where it looks like you're going to do processing on a variable and then print it out kind of interactively. No. It would actually look like you're doing this interactively but actually change the value of the variable you had on the left side. So David is proposing why not do this in our proper. And he proposes this operator as like the less than sign pipe and then greater than sign. So it looks like it's a pipe surrounded by the two signs and then having that translate to a bunch of functions potentially to do something with that operation.

And his rationale is that it could make the code much simpler to read and more expressive and being able to kind of be more succinct with the syntax that you're typing. Now, this is where it gets interesting here. It's one thing to kind of conjecture this, but he decided to take matters in his own hands and actually look at what is being used in the general, you know, set of code that's being shared in open source of where this pipe assignment could be useful. So he actually has authored his own new package called code samples, which apparently has scraped open source code that has been shared on GitHub and Stack Overflow, as well as just the R package examples themselves that come from packages.

And he's done a nice little summary here in the blog post about how many times he has detected code that is pipeable but also pipeable and complex, having like more than a few lines in the pipeline operation. And so the post says there's about between 4 10% of operations could be simplified by an assignment pipe here, and that could be a potential gain. So you might want to look at this for yourself to kind of see in your code, do you have examples that match some of David's examples here, such as what he has from the Stack Overflow questions.

Looks like a pretty unwieldy sub setting of a w variable and then trying to translate that into a more friendly, pipeable assignment operator, where it basically streams it down to 4 lines of code, technically, instead of what looks like about 16 or 18 in the previous example. I will admit I've had a little bit of, gotchas when I've tried this in the Magrita approach before, and I admittedly moved away from it. But I can see where David's coming from here. So it is an interesting proposal. Now for those that are new to the r, you know, project itself, it does take a bit of time for things like this to land in. And, of course, of course, it has to be agreed upon by the r core team and whatnot. So there would be obviously a lot of discussion in play and that this would not happen anytime soon if it was to actually take shape.

But with that said, it is interesting to see what this could look like. So I credit Dave for taking the time to not only put this out into the public domain, but do some interesting kind of data scraping analysis of existing code to see where this might benefit. So definitely something to ponder. I'm kind of in between on this one. I don't really wanna say no to it, but I'm just not sure if I would be the best customer for it. So with that said, definitely food for thought, as I say. Eric, you know, this is one of those things where at first glance, it kinda gives me the heebie jeebies,

[00:25:03] Mike Thomas:

but then it might be, you know, this gets implemented, you know, a couple years from now, and then I finally adopt it, and then I feel maybe someday that, you know, like, don't know how I lived without it in the 1st place. Yeah. I think that happens occasionally. You know, I will give David a lot of credit because he he absolutely makes the case for it in this blog post. Doesn't just make the case for it. He actually writes the code to implement it in, Rsource. So he has, forked the Rsource repository, and then he has has made a commit that, you know, is 1200 lines of code, c code, I think, mostly, essentially, that that implements it and, kudos to him. Hats off. He's putting his money where his mouth is, and I think, absolutely, if this does get adopted, if it's something that the community does eventually want, then he will be the person essentially that that spearheaded this and that did a lot of the work I think to get it in to ours base code. Yeah. I think that probably that the base pipe itself for now is maybe still getting adopted. I would love to see some statistics on how widely used it is. We do still see a lot of code, you know, at our clients that contains the the Maggard or pipe, even new projects that they're working on.

I think maybe people are just just slow to adopt new things so I would imagine that it probably wouldn't be any different if this new pipe, did get implemented. And I can't help thinking that it looks like a person standing there with their hands on their hips. That's the best way that I would that's the best way that I would describe it. Always have to sort of see something, you know, it's like seeing something in a potato chip, but, when something new comes out. But I think it's super interesting. I have a little bit of trouble, you know, the explicit assignment with the arrow is something that I think helps me for code review to be able to really clearly see when an object is getting created.

I know that there are some folks out there, probably more in the in the Tidyverse community, that that, warn a little bit against the use of the assign function, for that that same reason because it's not sort of explicitly showing you where new objects are getting created in your environment as as you read some code. It's that's a function that I've tried to get away from, in in more recent years. So it's gonna be teach their own. I I would say if this is something that that you really feel could help you, then then get in touch with David and and support the cause. And I guess the great thing about open source is is you can use what you wanna use, and you don't have to use what you don't wanna use. Right? I'm sure the assignment arrow is not going away anytime soon. So I'm not very worried, but the, you know, the only place that I think it goes back to maybe our our discussion on Nicola's blog post is is adopting some style guides and sticking to them. Right? So if you want to adopt, this new pipe, if it ever gets merged, as sort of the way that you are going to go about assigning objects internally within your organization, then by all means, you know, merge that into your own style guides and into your own consistent practices. But I think consistency is important. I think ensuring that you are writing code that sets up the reviewers or the collaborators on that code, for success and to make their life easy and efficient, is really important. So that's that's really all I have on this blog post, my 2¢.

I don't feel super strongly one way or the other. I think it's it's really, really interesting. And again, kudos to David for not only making the case, but for going through, the entire process of actually implementing it and what that might look like.

[00:28:57] Eric Nantz:

Yeah. Definitely credit to him for, you know, following through on what this actually could look like. And, you know, I'm always I will never turn down having choice in this space. I mean, there may be others that would have such great benefit of a complex pipe assignment. Have at it. Right? I mean, I don't have to use everything that's in our core or these packages that I use. I just use what's best for the project and what's best for me. So I I will admit it would make me probably have a little harder time doing code review of it as of now. But, heck, maybe a practice makes perfect. Who knows? But, yeah, the assignment operator, you're gonna have to pry that out of my cold dead hands. I use that every single time, and that helps me reason out kind of where the key variables in this pipeline.

And, also, to me, it helps make debugging a little easier at the sake of being a little more verbose. For me, debugging, reviewing are probably the one criteria I'm gonna use as I think about whether I would implement this. But, again, I'll be very interested to see what the community has to say. So I'll be keeping an eye on, say, Mastodon and other, social areas to see what kind of, discussion this, spurs on, but we may be hearing more about this way earlier. Who knows?

[00:30:10] Mike Thomas:

Keep your eye on it. You never know.

[00:30:12] Eric Nantz:

Yes. Yes. And, one as we get to our our last, highlight here, let me preface this by saying when we talk about highlights, we're really more that's a general term for the the areas that we think are most newsworthy and that will probably spur the most discussion in a particular issue because I will admit on the surface, when you when we talk about this last post, I would not call it a highlight per se because we're about to get a little heavy here. But I think there are some thoughts that Mike and I definitely wanna share about this. So first, if you have been using r and in particular frameworks like r markdown for any amount of time, I think if you're not new to the community, you probably know who is most directly responsible for this.

We have to thank for all these amazing innovations in Rmarkdown alongside knitter itself, which honestly made R Markdown possible. Knitter, for those that aren't aware, is kind of the engine behind frameworks like R Markdown in the spirit of sweave or swive that comes built into R itself. But, admittedly, knitter is, in my humble opinion, much easier to build upon, much easier to customize. And once r once the r markdown format came to be, there are just so many lives that have been transformed in professional development, personal development, sharing your data science, you know, blogging with R Markdown and hence, blog down. In essence, what we call the down verse, you might say.

EWAY is directly responsible for this. EWAY had a blog post early this year, just a week ago. And I got wind of this from a, somewhat random post on Mastodon, and I almost didn't believe it. What has happened is that Eway was, unfortunately, given notice. And, unfortunately, posit has, decided to basically, take away his full time role, albeit to pay for the support on a contracting basis of the packages that the R Markdown ecosystem depends on. So, like I said, aforementioned like Knitter, R Markdown itself, and the like.

This has come to a shock to many of us in the community, and frankly, it sounds like a surprise to Eway himself in this post. But to Eway's credit, he has been very gracious in his response to this. He has been very cordial on acknowledging, posit for for all the years that he's been able to work on Knitter, R Markdown, and some of the newer team members that have really stepped up to help him. He he names a few names, but there certainly, there are many people that he has collaborated with, that have made the R Markdown, you know, this EwayDowns ecosystem so powerful in the in this space. So he also mentions that, as I mentioned earlier, the packages that he's been directly responsible for are not gonna be orphaned because if they have anything to do with our markdown in knitter, he is being paid to support that. It's just obviously not a full time job pay anymore.

So the one exception to this would be DT. He is looking. It sounds like Paz is gonna find someone new to that package, but, again, that this is just another, you know, consequence of this. The other interesting part of Eway's post is something I've been kind of observing a little bit. And as someone who has been a fan of, say, Linux and Unix for how many years, it definitely rings true to me a bit. Eway has been exploring kind of a more minimalist approach to some of the software development. I've seen some of the newer packages or newer utilities he's been spinning up. In fact, someone called TinyTek, a way to get latex installed in a very streamlined way onto your system without going through the full bloated like latex system, things like that. And he's also experimenting with other, areas in this, and it sounds like it's more of a philosophical shift.

He's not saying that either approach of, like, a minimalist approach versus an approach like quartile or even Shiny itself that tries to encompass a lot of things. I think they each have their value. Right? It just depends on your philosophy and where you want to take your development. Now, really, the part that hits home here is that it does sound like this was a bit of a surprise. And EWAY, for those that don't know, is a father. He's married. He's got his own house. He is definitely supporting his family.

So this is now, to be perfectly frank, an uncertain time for him. So he has put at the end of the blog post he doesn't do this, in fact, hardly ever. I don't remember last time he's mentioned this, but he did say that until he's able to land on his feet with, say, a new role in whatever industry he chooses to be, He has asked for a bit of help because of the little concern he has with this big change, and he does have a GitHub Sponsors page. Mike and I, full disclosure, are both sponsors of his work. I'm gladly so because we value so much of what he's done for us.

But I will say after this post made the rounds, he has received just an earth shattering amount of acknowledgments in the comments of this post. I think we have over 300 or so, at least over 100. It has gone triple digits since I commented on it. And this is just really if you ever had any doubt of what transformative effect Eway has had with his efforts in the R community, you just read the comments on this. There are so many that have said, we owe Eway so much because of our markdown, what it's done for reproducible research, what it's done for their reporting, what it's done for their being able to connect with the community. Like I said, LogDown.

I use LogDown. That's how my podcast site was built, for goodness sakes. Like, there are so many ways that we have been leveraging his utilities. So I will admit I am frankly disappointed at the way this came about. I am, again, giving full credit to Eway for being so gracious in this post. But, certainly, if you've gotten any value out of Eway's open source work, I think everybody makes their own decisions. But to me, the effects he's had on my development, our journey, my adventures in the community. He was my first interview on the our podcast, for goodness sakes. He actually said yes. I put him into a room at this local conference called MBSW with my half answer mic set up, and he talked with me.

And he was so gracious. I felt like I was, you know, meeting Yoda's Luke Skywalker kind of thing and I'll never be at Yoda's level, but I'm just saying that's how I felt. And I just and then subsequent interviews, he has given me some of the most candid thoughts I have ever had on that show is from Eway. So I feel personally very, you know, I consider him a good friend. Obviously, we live far apart. We only get to interact briefly, but he has done so much for me personally. But if you've had any benefit from me, Haysworth, I would just at least consider helping him out in this current time.

But I share what everybody said in in the responses of this, both in the post and on X and Mastodon. So many people's lives have changed because of what Ehue has done. So I sincerely hope he lands on his feet soon. We're thinking of Ehue. If there's anything we can do, obviously, we are we are here to help. But, best of luck to you. But again, the post has so many thoughts that came to mind after reading it. It definitely, like I said, not a highlight in the traditional sense, but the the impact the UA has had on the community cannot be overstated.

And I think this blog post clearly shows that. So that that's a lot for me. Mike, what do you got?

[00:38:40] Mike Thomas:

Yeah. I guess I'll start with maybe a couple of calls to action. And the first call to action I would say is is if you have benefited at all, especially financially, you know, if your salary, what you do for a living, includes, you know, leveraging our markdown or ever included leveraging our markdown, and you essentially were paid to to use our markdown, which is obviously free, I would have a hard time, you know, not justifying sponsoring e way, you know, in some way. I know a lot of people have, you know, including you and I, Eric, but the the value that and I I don't think it's unfair to say, you know, monetary value that the community has has probably gained in terms of a lot of employed data scientists, getting a ton of value from our markdown.

I'm not sure that's even quantifiable. So if you can, I don't think there's there's any better use case for making that charitable contribution, to Eway at this time and, you know, his journey? He is, so so that I guess that's my first call to action. And my second call to action would be if you have an opportunity within your organization, whether it be a contract opportunity or or an opportunity to where where you think, you need someone with his skill set as a as an incredible software engineer, reach out to him. Go to rweekly.org.

Check out this blog post, that's called by rstudio/posit. And I'm sure his contact information is at the bottom, of this blog post. I know his GitHub is. He has an about page in that blog post with a contact me link where you can get a hold of him. So if you have opportunities for Eway, reach out to him and see if they may be interested, you know, because he is, he is not necessarily just this miss mythical Yoda. He is a person, you know, with a family as well. And I know at this point in time, especially within tech, there's a lot of, folks that are probably experiencing similar things depending on, you know, where you work.

So it's, it can be a tricky time and unfortunately, you always been been bitten by it as well. So let's, let's try to lift him up, you know, because he has given so much to us, and those of us who are data scientists that use the R Markdown ecosystem and PageDown and BookDown and all these all these different utilities. So I guess those are that's what I wanted to start out with, is a couple calls to action. If you haven't yet really considered sponsoring him or reaching out to him with opportunities if you you see them.

Second one, it's it's hard, Eric, not to be emotional, about this. You know? And layoffs in in business are a reality. You know, one thing that I think is is disappointing to me, and I I don't think I'm I'm being too frank here, is is that it seemed like it was it was quite a quick surprise to Eway after someone who had had worked there for for 10 years and really given so much to, what our studio has been able to create. I have to imagine that, you know, while I understand that a lot of folks worked on on quarto, I have to imagine that a lot of it stands on the shoulders of what, Eway built within the R Markdown ecosystem for many years. So that's that's really disappointing that it it sort of came as a surprise to Eway. I'm very glad that they are at least employing him as a contractor to to be able to work on some things. You know, I guess guess the reality of business is that, you know, if you don't have enough work for someone, then you then you probably only want to pay them for the amount of work that you have for them. So I I don't know if the transition to quarto means that there's there's less work for e way to do on some of the software that he was maintaining and working on. That's just, I guess a guess, and we don't necessarily have that information at this time. But it's it's emotional, I think, more so because of the way that posit's structured as well as a public benefit corporation and not just, not just the regular corporation.

Right. And it's that creates sort of this dynamic between posit and the, our community. That's unique. Right. So I think when, when something takes, when changes take place at posit, we feel the effects of that sort of personally in a way. And that's, that that's very unique. And I think. Open source is really rooted in transparency, right? When we're doing open source work, you know, the, one of the benefits and the really cornerstones of open source software development is that others can see exactly what we're doing and and what we're working on, and they can contribute to it and and try to help and things like that.

So I I guess I would like like to some of these changes, for lack of a better word, to be a little bit more transparent within the community. It it feels like maybe there are some walls being built up, between the community and and posit at this point that may not have have previously existed. And I think things that come out like this, you know, I think the loss of a lot of our, posit academy folks, you know, as well as e way now. I think there's not a lot of, acknowledgement necessarily of it, except maybe by the folks who are directly affected by it. I think it's, it's unusually quiet, from, from their perspective. So I guess I would like to get a better understanding of, of sort of the directions that, that things are going, a better sense of transparency, just because of this, this really unique relationship that the art community has with, you know, what used to be our studio and what's now posit. It's it's something that, you know, is emotional, I think, to a lot of us. I don't think I'm just speaking for myself. So, you know, this was a tough blog post to read. I think EWAY may be taking it better than some of us reading it. He he's taking the high road to every extent here in this blog post. He's extremely grateful, to the folks that he worked with and who employed him for the past 10 years.

He he talks about in the the comments of the post that the in Chinese, the word crisis consists of the words, danger and opportunity. And he he's optimistic about the opportunity part, and he's not very concerned about the danger, and and he really believes it's going to be a blessing in disguise. So I guess if nothing else, I really hope that Eway, you know, finds a new role for him that he's excited about and loves, and and that he he really lands on his feet and and finds, you know, a next great opportunity for himself. So I think we we have a lot of thoughts on this blog post. I'm glad that we we we talked about it. It is, you know, emotional to me. That's that's sort of the best word that that I can use here to try to characterize, how I feel about this blog post. And I I think what Y Hue's work means to us and and what him as a member of the community, means to us.



[00:46:05] Eric Nantz:

Yeah. I echo a lot of that. And you are not alone. And I've seen posts on Mastodon of others kind of a little concerned about, I would say, like you said, the silence on some of these maneuvers because as those aren't aware, we did talk last year. POSIT did do some layoffs that affected some of their open source division as well as other divisions. And, yes, you might be able to hear from other people affected by it directly, but there wasn't a lot said on the, you know, the public facing kind of communication on that.

And as of now, I'm not seeing any new response to this either. I do think this may be a wake up call in a couple senses. Yes. There is an interesting dichotomy where unlike most of the tech industry, posit is a PBC. That puts you into a different lens, in my opinion. Whether you consider that right or wrong, they chose it. Right? They chose to be a PBC. That was JJ's vision that we heard in the keynote a couple years ago at at our studio conf. With that, I think there is and again, Eric's going on his soapbox here. I think there we are owed a little bit more transparency on this as those that are not just fans of open source, fans of data science, but this tooling is immensely important to the work we do, and especially in the open source side of it.

Yes, of course, the commercial projects help, too. But you know, for those that don't know where, Quarto, when it compiles, anything to do with our execution chunks, that's using Knitter under the hood. Guess who wrote Knitter? EWAY. So, like, Quarto is not possible on the r side of it without what EWAY did. So whoever paused it is now funneling more resources into that development, Fair Play, it's their company. Obviously, the interoperability is a big focus for them now. But at the same time, they are indeed standing on shoulders of absolutely giant efforts that EWAY has built here.

So, again, I do think a little more transparency is warranted here. And, you know, it did it has spurred on some concerns about future directions. Again, that could be a whole another podcast in and of itself. I think we'll just leave it as we're interested to see what 2024 holds. But, again, full marks, full credit to EWAY for taking the high road on everything here. And, again, the response from the community has been eye opening to say the least. I misspoke earlier. I mean, ever since the blog post, he has been hearing from many, many people, familiar names, new names, those that maybe are what I call dark matter developers and may not comment very much when they see somebody that has that impact on their daily work or their daily data science journey. They're coming out to say their thanks. So I do think just as open source in general, we need to do a better job of thanking contributors, not just in times like this, but regularly throughout the year because it can pick you up. If if you're an open source developer, you're having a rough time trying to maintain this, just having that pick me up really helps too. We don't just have to wait for an event like this. But with that said, Folker at the Evway for being gracious in this, and I would imagine he is going to be hearing from a lot of people about future opportunities.

I hope he takes what's best for him and his family and certainly will be very curious what the future holds for him. Well said, Eric. Well, yeah. It's hard to transition from that, but that was a jam packed, summary of our, our highlight stories here. But we're gonna close out with a couple of additional fines that we found in this issue. And, yeah. Well, I heard you when you heard me mention in in Nicola's, segment that, you know, she's gonna be pursuing Knicks as a as a tie in with r. Well, of course, our good friend Bruno Rodriguez has been continuing his journey with the NYX, package system and r, and he has a part 8 of his reproducible data science with NYX blog series that gets more into the kind of the the into the weeds of just how open source plays a critical role in Nick's itself.

And in fact, every package in Nick's is simply I'd say simply. I should not say simply. But it is a set of scripts that will take the upstream utilities, bundle it up into a way the next can understand, and then we can install that via the next package manager right then and there on the spot. And, of course, through, Bruno's work on the Rix package, he is trying to make that easier from within R itself. But acknowledging that many R users are very comfortable with the RStudio IDE, he's been trying to make that part of the Rix packaging process too, the Git, version of RStudio in that reproducible project.

So that then is tying into that custom R installation, that custom set of packages that that next package system is exposing. Well, RStudio is a bit of a finicky piece of software to compile and run and install manually, especially around that little thing called macOS. It's a bit wieldy. So in his post, he actually talked about setting up a GoFundMe to help pay for a specialized vendor to assist with that part of the process for macOS users. Unfortunately, it did not hit the funding. But to Bruno's credit, he has actually donated those funds that were, contributed back to the R Foundation itself. So fair play to Bruno for at least helping the R Project benefit from that request.

But I'm gonna conclude this additional fine summary back to Pazit for a second. Pause it? If you don't know now, you know that nicks is becoming a thing in data science. It's not just Bruno. Many others are pursuing this too. It's your ID folks. Maybe you could help out a bit on this too. Just saying. I'll leave it at that. Mike, what do you got?

[00:52:31] Mike Thomas:

There's a call to action. So that's a that's a great find. I still gotta get my hands dirty with nicks at some point these days, and I know Bruno has ample resources available to help me do that. One blog post I found that I think caught a lot of fire this past week and was really interesting was from Emily Riederer on Python Argo nomics. And it talks about essentially if you are an R developer needing to switch to Python for a particular project or, just trying to learn a little bit of Python, to to keep up.

Some it talks about how to, map some of the concepts from R into Python to get you up and running. And just sort of as a quick highlight, some of the the tooling that Emily recommends you use to get started with Py, with Python. For installation, she recommends pyenv package, which allows you to switch back and forth between different versions of Python really easily. So from on one project, you need to use Python 39, and the next project you need to use Python 310. Py n allows you to really easily, switch back and forth from the command line, I believe, between those 2, or multiple different versions of Python. You can set one to be sort of your your global version of Python. I believe that you want to use, as a default.

For data analysis, we have heard a lot about the Pandas package, sort of being the equivalent maybe to dplyr. Emily argues that the polars package actually has a syntax that is more similar to dplyr. I agree as well. It's also more performant. Polars is is a really, really cool package, and it's it's really, really efficient for working with data, large data in particular, but small datasets as well. I think it it works great for, and I think you'll find that the syntax looks very similar to, what you would find in DeepBlider verbs such as select, filter, group by, that they're all mapped sort of between DeepBlider and polar, so it should be pretty familiar to look at the code there.

For communication purposes, she references the great tables package, in Python, which is the port of the GT package, by Rich Iannone, I believe, at at posit, from r to Python. It's very new. So if you're a Python user looking to author, really nicely formatted tables within your reports, I would check that one out. And then notebooks, obviously, quarto. And lastly, in terms of environment management, I think she references the PDM package in Python as maybe being the one that would be equivalent to the r end package, or or the one that she prefers the most. So it's a great sort of list of the different tools that Emily uses to get up and running across all of her Python projects, as well as an explanation of of why she uses them and how they may, be similar or differ from your experience, using similar tooling in r.



[00:55:34] Eric Nantz:

Yeah. Emily does a fantastic job here. This is something that I've struggled with immensely. Even just knowing where to start, I I have a couple of projects that because I'm extending a package in Python for some RSS feed stuff, I'm probably gonna stay in that ecosystem or some of my, you know, very, you know, very in-depth kind of podcasting 2 point o expirations. So having kind of this familiarity of knowing where to go to maybe summarize that podcast database effectively in my portal notebook, go with these packages that give me that kind of our flavor a bit. For somebody that's new to Python, I think that is that is extremely helpful. So this should be your go to post if you're like me and you're just dabbling your toes in the Python for some interoperability work without it being your full time focus. Just knowing where to go first is immensely helpful because there's a wealth of choices out there. And you can go down rabbit holes on environment management and, frankly, bang your head against the wall. I can't tell you how many times I've bought projects at the day job because of the Python VM setup gone horribly wrong. Don't give me a start. Anaconda.

There'd be dragons on my HPC system with that. So, yeah, I'm sure I'm gonna be playing with a lot of what, Emily is recommending here quite a bit.

[00:56:55] Mike Thomas:

Dev containers.

[00:56:57] Eric Nantz:

To have containers. Yes. Exactly. I do have hooks in that in mind, if only the rest of my projects could benefit from that. Well, as you can tell, we've had a we've had a fun banter here. You can tell it's been a few weeks with Mike and I, so we came with our we came with our opinions as they say. But, of course, there is much more to rweekly itself than us just bantering. The full issue has so many more great content, lots of new packages and updated packages, some that really caught my attention on the visualization side of things, especially. So definitely have a check at that at rwig.org.

Also, we love to hear from all of you in the community. And going back to what we mentioned with VeeWay, I'm gonna make this call out now. And I will admit we don't get the whole boost thing very often on this show. I hope that changes from time to time. But for the month of January, if any of you are gracious enough to boost this show with your favorite podcast app or on the podcast index itself, which I have linked to in the show notes, I will funnel that directly to Eway for the month of January. So if you're interested in supporting Eway in a different way, the boost would be a way to I will personally make sure that happens.

So definitely keep that in mind, but, also, we just love hearing from you in general. We have a contact page on the episode show notes, and, also, we are, somewhat active on the social media spheres. I am more, active on Mastodon. My handle is at our podcast at podcast index dot social, sporadically on the x thing of at the r cast, and I'm also on LinkedIn from time to time. Mike, where can they find you?

[00:58:32] Mike Thomas:

LinkedIn is probably the best place to find me. I think my sort of tag there is Michael j Thomas 2. You can also find me on mastodon@mike_thomas@fostodon.org.

[00:58:46] Eric Nantz:

Very nice. Very nice. And like I said, it's great to be back in the swing of things with you. It feels more normal again as we kick off the month of January with this, supersized episode that we just gave you all here. Well, that will do it for us. Like I said, we came with our opinions. Hopefully, you enjoyed it. We'd love to hear from you, but we will be back with another episode of our weekly highlights next week."
"72","issue_2023_w_50_highlights",2023-12-13,43M 41S,"A data-driven investigation to the association of early birthdays and hockey players, one of the most-requested feature requests is coming to the next version of Quarto, and just why in the world does the View() function start with V? Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) (Twitter) Are Birth…",NA
"73","issue_2023_w_49_highlights",2023-12-06,36M 36S,"A timely collection of tips and tricks in adopting the cli package for your R package interfaces, how the deposits package addresses an all-to-familiar problem of sharing research data, and an encore of creating your own RStats-wrapped of your most used R functions. Episode Links This week's curator: Batool Almarzouq - @batool664…",NA
"74","issue_2023_w_48_highlights",2023-11-30,50M 34S,"A glimpse of refactoring functional R code to object-oriented programming with R6, using benchmarking as another input to adopting package dependencies, and building a high-performance CSV reader by combining R and Rust. Episode Links This week's curator: Tony Elhabr - @TonyElHabr (https://twitter.com/TonyElHabr) (Twitter) &…",NA
"75","issue_2023_w_45_highlights",2023-11-11,43M 6S,"From the ""is there anything R cannot do"" department comes QR code scanning, a tidy time series analysis on a major problem in the roads of Pittsburgh, and rolling up your sleeves with custom ggplot2 tricks to enhance a spatial visualization. Episode Links This week's curator: Colin Fay - [@ColinFay]](https://twitter.com/ColinFay) (Twitter) Scanning…",NA
"76","issue_2023_w_44_highlights",2023-11-01,44M 7S,"A collection of post-workshop answers for the R/Pharma introduction to tidymodels workshop, the Shiny UI Editor takes a huge step out of the alpha stage, and a unique approach to Shiny modules with the new component package. Episode Links This week's curator: Eric Nantz - @theRcast (https://twitter.com/theRcast) (Twitter) &…",NA
"77","issue_2023_w_42_highlights",2023-10-18,50M 14S,"Another collection of package testing workflow nuggets you can make great use of today, the definitive guide to effective use of logging in Shiny applications from the recent Shiny in Production conference, and a cautionary tale of the potential impact of default function arguments in your downstream analytical pipelines. Episode Links This week's…",NA
"78","issue_2023_w_41_highlights",2023-10-11,53M 53S,"How the {potools} package jump-starts your R package translations, the most-upvoted feature request lands in the RStudio IDE with GitHub Copilot integration, and a reflective post on the multiple paths to reproducible data science workflows in R. Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder))…",NA
"79","issue_2023_w_40_highlights",2023-10-05,42M 53S,"A new contender for speedy fuzzy joins of data frames enough to make Sonic jealous, a novel use of ggplot2 for creating a map that could have come from a vintage typewriter, and the immense progress of detecting R package system dependencies. Episode Links This week's curator: Ryo Nakagawara - @RbyRyo (https://twitter.com/R_by_Ryo)) (Twitter) &…",NA
"80","issue_2023_w_39_highlights",2023-09-27,39M 21S,"Reflections on the amazing posit::conf(2023), a new framework that'll have you snap into HTML slides, the Nix reproducible data science train powers forward into CI/CD territory, and leveraging parallel processing in spatial data prediction. Episode Links This week's curator: Batool Almarzouq - @batool664 (https://twitter.com/batool664)…",NA
"81","issue_2023_w_37_highlights",2023-09-13,37M 43S,"Another adventure with incorporating R packages into a WebR application, annotating your fancy equations in a Quarto PDF document, and unleasing a Bayesian model on UFO sightings data. Episode Links This week's curator: Jon Carroll - @carroll_jono (https://twitter.com/carroll_jono) (Twitter) & @jonocarroll@fosstodon.org…",NA
"82","issue_2023_w_36_highlights",2023-09-07,38M 35S,"A batch of R functions to level-up your development tasks, revisting a classic R inferno on object allocation, and a call for proposals to take R's infrastructure to new heights. Episode Links This week's curator: Colin Fay - @_ColinFay (https://twitter.com/_ColinFay) (Twitter) Three (four?) R functions I enjoyed this week…",NA
"83","issue_2023_w_35_highlights",2023-09-01,40M 0S,"The next generation of object-oriented programming in R arrives on CRAN, a novel use of R to automate R scripts and documents for Tidy Tuesday analyses, and a terrific presentation de-mystifying the world of web APIs in R. Episode Links This week's curator: Eric Nantz - @theRcast (https://twitter.com/theRcast) (Twitter) &…",NA
"84","issue_2023_w_34_highlights",2023-08-23,42M 29S,"A few key practices for data preprocessing leveraging the tidyverse, more amazing wins with open source to process high-dimensional USDA geospatial data sets, and an infinitely fascinating look at how recursion and infinite data structures can be used in your R adventures. Episode Links This week's curator: Jon Carroll - @carroll_jono…",NA
"85","issue_2023_w_33_highlights",2023-08-16,40M 20S,"Another excellent use case of Nix for solving R package installation woes, a practical dev journey of wrapping C code in an R package, and a guide for using the new refugees R package from UNHCR. Episode Links This week's curator: Ryo Nakagawara - @RbyRyo (https://twitter.com/R_by_Ryo)) (Twitter) & @RbyRyo@mstdn.social…",NA
"86","issue_2023_w_32_highlights",2023-08-09,43M 29S,"How a novel blend of automation and the YouTube API formed a new R-Ladies meetup recording dashboard built entirely with R, the momentum of webR continues with a fantastic guide to create a serverless Shiny app, and a new challenger in the world of high-performance data manipulation libraries arrives. Episode Links This week's curator: Jon Calder…",NA
"87","issue_2023_w_31_highlights",2023-08-03,38M 36S,"Reducing usage of for loops with the reduce function from purrr, filling spatial maps with density gradients to account for overplotting, and a fun way to add attribution to your fancy ggplots. Episode Links This week's curator: Tony Elhabr - @TonyElHabr (https://twitter.com/TonyElHabr) (Twitter) & @tonyelhabr@skrimmage.com…",NA
"88","issue_2023_w_30_highlights",2023-07-26,37M 31S,"How consistent formatting and styling is valuable technique for debugging, a visual tour-de-force of jazzing up your ggplots with the amazing ecosystem of extension packages, and why a little investment in learning HTML and CSS is worth your time as an R programmer. Episode Links This week's curator: Batool Almarzouq - @batool664…",NA
"89","issue_2023_w_29_highlights",2023-07-19,46M 37S,"The second edition of the highly-regarded R for Data Science arrives with substantial updates, an adventure with ""A Programming Language"" that brings new perspectives to functional programming approaches, and a new take on reproducibility in data science combining R with the Nix packaging system. Episode Links This week's curator: Eric Nantz -…",NA
"90","issue_2023_w_26_highlights",2023-06-28,33M 57S,"Releasing an Word document table into the land of markdown, a practical overview of sharing your machine learning model with others, and taking local control of checking the builds of your package across computing architectures. Episode Links This week's curator: Colin Fay - [@ColinFay]](https://twitter.com/ColinFay) (Twitter) Convert a Word table…",NA
"91","issue_2023_w_25_highlights",2023-06-21,36M 10S,"Uncovering powerful use cases of the slice() function in the tidyverse, a batch of new features and fixes for column labeling in gt 0.9.0, and a fun journey with tidymodels and visualizations on just how much the Reverend Thomas Bayes may have earned from his own home (probably). Episode Links This week's curator: Jon Carroll - @carroll_jono…",NA
"92","issue_2023_w_24_highlights",2023-06-14,43M 22S,"A batch of useful patterns for your next R project, the highly-anticipated dashboard components of {bslib} have arrived, and creating circle-based charts with customization using {ggtricks}. Episode Links This week's curator: Tony Elhabr - @TonyElHabr (https://twitter.com/TonyElHabr) (Twitter) & @tonyelhabr@skrimmage.com…",NA
"93","issue_2023_w_23_highlights",2023-06-06,37M 53S,"Another terrific illustration of open-source collaboration in the latest updates to gptstudio, and a comprehensive journey of web scraping in R to bring much-needed automation to a practical research problem. Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) (Twitter) On updating a chat assistant app for…",NA
"94","issue_2023_w_22_highlights",2023-06-01,38M 38S,"Another gem in the functional programming toolkit with partial functions, simplifying R package creation using fusen, and a creative visualization of worldwide parliament representation. Episode Links This week's curator: Ryo Nakagawara - @RbyRyo (https://twitter.com/R_by_Ryo)) (Twitter) & @RbyRyo@mstdn.social (https://mstdn.social/@R_by_Ryo)…",NA
"95","issue_2023_w_21_highlights",2023-05-25,39M 6S,"A must-have resource to get you primed for testing R packages interfacing with the web, how ggblend taps into new compositing functionality for clearer plots, and how R stacks up with Excel in handling dates. Episode Links This week's curator: Batool Almarzouq - @batool664 (https://twitter.com/batool664) (Twitter) Better Understanding Your Tools…",NA
"96","issue_2023_w_20_highlights",2023-05-17,41M 4S,"Introducing the new ggflowchart package, how a dockerized development environment is another win for reproducibility, and our take on Colin Fay's keynote from the Appsilon Shiny Conference. Episode Links This week's curator: Sam Parmar - @parmsam_ (https://twitter.com/parmsam_) (Twitter) & @parmsam@fosstodon.org (https://fosstodon.org/@parmsam)…",NA
"97","issue_2023_w_18_highlights",2023-05-03,39M 40S,"Why effective code reviews can bring many benefits to data science teams, the origin story of the sketch package to transpile R code to JavaScript, and a primer on error handling in both R and Python. Episode Links This week's curator: Colin Fay - @_ColinFay (https://twitter.com/_ColinFay) (Twitter) Pull Requests, Code Review, and The Art of…",NA
"98","issue_2023_w_17_highlights",2023-04-26,42M 34S,"A few strict checks offered in R 4.3.0, measuring and writing performant code in the Tidyverse, and a please for indenting your code with (more) spaces. Episode Links This week's curator: Eric Nantz - @theRcast (https://twitter.com/theRcast) (Twitter) & @rpodcast@podcastindex.social (https://podcastindex.social/@rpodcast) (Mastodon) What's new in R…",NA
"99","issue_2023_w_16_highlights",2023-04-19,42M 25S,"Using development containers to bootstrap a reproducible R and Quarto environment, a comprehensive approach to extending the data frame class, and plotting your own universe of labels with ggsolar. Episode Links This week's curator: Jon Carroll - @carroll_jono (https://twitter.com/carroll_jono) (Twitter) & @jonocarroll@fosstodon.org…",NA
"100","issue_2023_w_15_highlights",2023-04-12,47M 2S,"A data-driven look at package loading annotations in R scripts, a fit-for-purpose package that makes a large contribution to the global R ecosystem, and a collection of amazing showcases of webR in action that is paving the way for continued innovation. Episode Links This week's curator: Tony Elhabr - @TonyElHabr (https://twitter.com/TonyElHabr)…",NA
"101","issue_2023_w_14_highlights",2023-04-07,28M 37S,"Ten unique ways to create your own Web APIs in R, and how you can import local and remote data files in CSV and (yes) Excel formats with a selection of innovative R packages. Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) (Twitter) Hello world examples with 10 different R web API frameworks…","Hello friends, we are back with episode 117 of the RM Weekly Highlights podcast.
I am personally fresh from quite an adventure out in the Great Smoky Mountains, so I'm unwinding
a bit, but it's great to be back in the swing of things and to help me do that, of course,
is my great co-host, Mike Thomas.
Mike, how are you doing this morning?
I'm doing well.
Not quite as traveled as you lately, but looking forward to the warm weather this summer getting
me out there a little bit more, and as you know, we have some travel to do together later
this year.
You bet.
You bet.
Luckily, the travel I'll do to the conference is much less than what I just did.
It'll be a three-hour drive from me.
You'll have a short flight, but yeah, we are very much looking forward to Chicago in September
for the POSIT conference, and certainly, while we're on that subject, it's not too
late to register for our Shiny in Production workshop, so if you want more info on that,
you can just search for the POSITConf 2023.
You'll find the registration right there, and we'd be happy to have you.
Shameless plug.
You know, that's what we do here, but we don't just plug our stuff, of course.
We're plugging our weekly because our weekly is the reason we're here, and our issue this
week was curated by John Calder, another fantastic release, and as always, he had tremendous
help from our RWeekly team members and contributors like you all around the world, and we're going
to get really geeky with this one because our first highlight is a GitHub gist, but
it's got a lot packed in it, and it is from a good friend of ours who's been very active
in the Shiny in our community, Dr. Peter Salamos, who I was very fortunate to have join me in
the recent ShinyConf Shiny Dev Series live recording, which hopefully we'll have the
recording out very soon, but he was amazing with this post, which talks about how we can
do web API calls in R, which we have talked a lot about the Plumber package for good reason.
It is becoming a huge presence in the capabilities for R to have, in essence, equal footing of
all these other web technologies to put very sophisticated or simple or maybe somewhere
in between processes for use as a web API, but Plumber was not the first to do this,
folks.
In fact, this gist is a great stroll down memory lane for me because there is a lot.
In fact, Peter has 10 examples of producing web APIs from R, all with different frameworks,
and the first one that caught my eye, this isn't quite in chronological order of the
gist, but as I scroll through this, there is a package called RServe.
This is authored by Simon Urbanek, who, if that name sounds familiar, is a member of
the R core team, so there's some great cache right there.
He released RServe, get this, 2006, that is, if my math is correct, 17 years ago, R had
this capability.
Can you believe it?
It's just crazy, isn't it?
I remember experimenting a little bit of RServe back in my grad school days, and we were trying
to put a simple mix of PHP, MySQL, and a little bit of R analysis for some local parks to
serve up their data and give them some insights.
I admit I was way over my head, I didn't know how any of this stuff works, but it's great
to see just knowing what I know now, doesn't seem too bad.
And then there are others that struck my fancy a bit.
Rook is another package that was released in 2011.
It's actually been forked more recently, it was originally authored by Jeffrey Horner,
and it is based on the use of reference classes, or RC.
You don't hear about this as much as the R6 and S4 paradigms, but if you ever want to
know how you could do web APIs with this, Rook is an interesting thing to explore as
well.
And then I caught OpenCPU.
This is authored by Jeroen Ums, who we just talked about a few weeks ago, as the architect
behind the R universe infrastructure.
So I always thought to myself, OpenCPU was really kind of ahead of its time, it can do
all sorts of things from a web API standpoint.
I think that may have been a precursor to his adventures of our universe, of course,
don't quote me on that.
Jeroen, if you ever want to write back to us, let us know, but I think you definitely
got some inspiration from your adventures from OpenCPU because it is amazingly intricate
and very innovative.
And then, of course, there are some other nice ones here too, such as Thomas Lynn Peterson's
exploration of web APIs with the Fiery package.
That's an awesome name, Fiery, I love it.
And right off the bat, Thomas is quick to say that this is different than Shiny in his
very general, you have to do more of the heavy lifting, but with that you get more flexibility
in his opinion.
So it is an interesting take on how you could use Fiery from both a Shiny context, but also
in this context, just a simple web API.
You can do either one.
It's up to you to stitch it all together.
And so the whole gist is basically the same hello world type example, but again, coded
in 10 different packages.
And then in the end, you get to see a little comment in the gist that talks about the release
dates of each of these packages.
And that's where I saw that as if I didn't feel old enough, like I said, with our survey
being out in 2006, all the way to more recently, where we've seen adventures from John Kuhn
with the Amburix package, which I believe has been archived actually, but it was an
adventure nonetheless.
So a great timeline there at the end.
And he even has a nice little ggplot that shows the download stats that you can run
in your art console as well if you want to get some download numbers historically of
what's been happening here.
But really awesome, I'd say just run with it, have a nice comparison to see where we
come as a community in this technology.
And I really enjoyed seeing all the technical bits here and comparing and contrasting how
you can do web APIs in R with many of these packages.
Well, you covered quite a bit of it.
And yeah, I couldn't agree more.
And there are some big, big names behind these different API framework R packages, as you
had mentioned, and I thought it was really, really cool that Peter took the time to sort
of put this all together.
Because again, it's, I think, a great demonstration of just all the different options that we
have and different ways of sort of doing the same thing, right?
And one might be a little bit more tailored to your use case than another, depending on
what you're trying to accomplish.
So it's nice to have the different options out in front of us.
And for sure, it includes some of maybe the better known R packages such as Plumber and
HTTP UV that I'd seen before, but so many others that I did not know about, including
that Beaker, Rook, and OpenCPU, as you mentioned, R-Serve, REST R-Serve as well, which maybe
is a more recent incarnation of R-Serve.
So really, really interesting.
And APIs at a higher level are just really a phenomenal way to wrap your model's logic
into something that folks across your organization can use from their other applications that
don't need to know anything about R. And some people have strong opinions against microservices,
but I am not one of them.
And it doesn't even have to be a model, right?
It can be any R logic that you can wrap up into an API to allow others to leverage.
So again, it's just really nice to be able to see all these different options in front
of us and know that there are so many different ways to stand up an API from R and not everything
needs to be deployed with Python.
Oh, hot take number one of many, perhaps, I don't know, but no, I've lived that life
as well in the recent years where sometimes I'll do a simple API in front of a complicated
database and that way the user doesn't have to care about database credentials.
I take care of all that from the back end.
They just have to call a little get request, I'm able to get the info they need and protect
the other stuff.
And it's just that the art is stitching it all together.
I mean, I think separating business logic into fit for purpose things, it's kind of
like the Linux philosophy.
One package that does one thing and does it well, you could have microservices, they each
do one thing and do it well.
But again, the art is how do you orchestrate it together where it's easy for your users
of your overall app to get in line with it and be able to use it effectively, but also
as user developer or the team of developers to be able to maintain that and to be able
to develop them in sync.
Those are separate issues, of course, but you can have as much flexibility as you want
with these frameworks and it's very important in my day-to-day job right now.
Yes, and shout out to actually a project that I worked on with Peter, which was a Shiny
app that included the ability to download, I think TIFF files or raster images or something
like that from within the Shiny app, which those links in the app called out an API that
he had developed to sort of do exactly that, be able to query a database essentially and
download just the data that you wanted.
Oh, yeah.
I'm a little jealous.
I'd love to be able to collaborate with Peter on a project Sunday.
That sounds like a lot of fun.
Yeah.
I guess I'm kind of curious now that I got to figure out what framework he used for standing
up that API.
Yeah.
You got 10 possibilities to choose from, I guess, you're guessing.
Now you mentioned, Mike, trying to wrestle some data files maybe from a remote source
or another location.
Well, that's a good transition to our last highlight of today, which is, of course, you've
got your data.
They could be either locally stored or on a web server somewhere or somewhere in between.
What are some interesting and hopefully easier ways for you to get these into R for your
post-processing so that you can actually get to your data analysis?
Our next highlight is a blog post authored by Kieran Healy, who is a professor of sociology
at Duke University all about reading, in this case, remote data files or local data files.
In this case, we're talking about some congressional type data.
But he starts with case one, which is, you have a bunch of CSV files for whatever reason,
they're stored in separate files.
Well, this might be the easiest case to deal with because in functions from the readR package,
like read underscore CSV, note that's different than the base R, read.csv.
You can have a vector of files to read from.
Well, geez, all you have to do then is make sure you can get the path to your files.
Maybe it's a directory somewhere, feed it into read underscore CSV.
Bob's your uncle, as they say, and you've got it.
You've got all the data tidied up and bundled together.
Now, this is where we take a turn.
We're about to drive into another curve here, Mike.
It's time for Excel.
You have your seatbelts on.
Yeah, I might have to put two on.
Yeah, me too.
This is giving me flashbacks to my road trip recently, we were driving up this mountain
area, and then we had high winds, and I'm like, oh, it's our car going to tip over.
Well, Excel sometimes makes you feel that way too, because you never know what you're
going to get with this.
Unfortunately, with Excel, not everything is as easy as it was with the CSV version
because yes, there are packages to read Excel files into R. Well, yeah, you almost have
to at this point.
So one way or another, you have to deal with Excel, but read underscore xosx from the read
Excel package, unfortunately, does not bind multiple files together at once.
So what can you do when you still have a directory of Excel files stored somewhere and you want
to minimize the amount of times you call read underscore xosx?
Well, that's where our friend from the per package comes for per with map underscore
CFR, that is the easiest way to feed in that direct that pass the file names, and then
bind them all together by row.
Now that's all local.
This was talking the big picture here is about what happens when they're remote, right?
Well, in some websites, even today, you might have a website that looks just like an index
of links.
This is actually called a directory listing much like you might do on your computer, but
in a web form.
And so the example goes to having a whole bunch of Excel files that are stored on a
FTP front end site from the CDC.
Now how can you get these?
Well, this is where you have to get a little creative sometimes because you notice that
all these links are going to have a pattern to it.
They're going to have the same base address, but they might change the file name prefix,
but then they might have the same suffix.
One of the ways that Kieran tackles this issue is you could actually scrape this web page
using our vest to grab all the hyperlinks, which are of course the a tag in web lingo,
and then grab the source of each of those links and then use that and do a little massaging
to get the file names, get the output like extension.
And then now you've got that remote set of file names.
Now had these been CSVs, you'd be done because you could just read remote files directly
with read underscore CSV.
It doesn't care if it's local or remote as long as you get the path right.
But this is where it gets tricky with Excel, you can't quite do that easily.
So Kieran takes a novel trick, which admittedly I use in a production setting for a much different
context is you have to temporarily download that Excel file to a temp area somewhere and
then make sure that the file name, no matter what cryptic thing it puts in front of it,
has the right extension at the end.
That's important because read Excel will not be happy with you if it's not an XLSX or XLS
file.
So once he does that little magic again, in a nice little utility function that he calls
get underscore life table, he's able to simply then call read Excel's read XLSX on that temp
file and import it in.
These are tricks that those of us who have been in the trenches will have to do at some
point.
But luckily the example is very easy to follow in this blog post, it's very well narrated
all the different steps and why Kieran chose the frameworks he did.
But once you've got it, then you've got your tidy data once again, you're ready to do some
analysis.
But yes, Excel strikes again, but hey, it doesn't have to be too painful.
You don't have to feel like you're driving through a 2000 high mountain to get through
it.
This blog post is a great way to see just what's capable of.
Even your files are not exactly stored in a convenient place.
But Mike, what'd you think about this adventure here?
Yeah, I think it's a great demonstration about as data scientists, we have to employ a few
different tools usually to get to the end of what we need to accomplish here.
And that's very well demonstrated in this blog post with the use of R of S to try to
scrape those links, the use of curl to create a connection to the FTP site, to retrieve
the file names and sort of control what Kieran gets back.
I was going to say, you know, leave it to the life sciences industry not to provide
an API to their data, but instead an FTP with a bunch of Excel files.
Am I right, Eric?
Oh, yeah.
You couldn't have said it better.
Oof.
That hurts.
Sorry.
I'm just kidding.
No, but it's really beautifully concise code that Kieran puts together and that helper
function that get underscore life table is really nice.
There's assignment inside of a function, which is something that you don't see out in the
wild too often, but I can definitely see what he's going after there.
Employing the HGTR package, temp files as well, like you said, to download its data
temporarily, then read it into a data frame with read underscore XLSX from the read Excel
package.
And then just like you said, at the very end, what sound does a cat make?
We are employing per this one liner, essentially a map data frame across that, that get life
table function that he custom created.
And it couldn't be easier at the end to just concatenate all that data together into a
single table.
So really well articulated blog post.
And I think it's a use case probably that all of us have encountered at one time or
another.
It's going to be a great reference for me to have in my back pocket for the next time
that I inevitably run into a bunch of Excel files.
Yeah.
You can't avoid it.
It's like having clouds in the sky.
They're going to happen someday, but I, what I really like though, is that the post is
kind of like, if you're encountering this situation, you start to see a pattern of how
things are similar in this case, the links, right?
If you see that they're similar, stop yourself from being like, okay, I'll just copy paste,
copy paste, just, just put the brakes on for a second because you never know in those,
in this case, it would have been like 50 or so of these copypages or there was a file
for each state.
You could just have one typo somewhere and then it's all going to go crashing down.
So if this is part of a process that you want to automate and hopefully be hands off once
you initially develop it, you want to automate as much as you possibly can and avoid the
manual effort.
I don't think you need to hear us tell you that twice, but take it from someone who's
been down that trap of trying to get something done quickly and ended up, you know, hurting
myself a bit in the foot, so to speak, because I had one error and one typo and it was so
difficult to track down.
So if you can scrape or find some logic to piece the paths together, like, like Karen
does here, you're going to be much better off.
So again, just take it from somebody who's been through it the hard way.
Absolutely.
Couldn't agree more.
And one note, just because I think I got burned by this in the past recently, if you are going
to, if you, you absolutely need to download a file first before you read it into a data
frame, please use the temp file function from, from Basar.
Don't create a new directory in whatever working directory your user is going to be working
in at that point in time.
Also.
Oh, you're not the only one who's been down that trap before I had one of my most well
known internal packages was doing that for everything when I didn't know any better,
but that's been, that's been changed since because you know, you live and learn.
There's a lot of learning to be had folks and the rest of the issue of our weekly this
week and John, like I said, has done a tremendous job.
So we'll take a few minutes here to call out some additional fines.
And for me, it's more of a call to action.
You might say Oscar Barufa, if that name sounds familiar, he is the one who's authored the
big book of our site, which has been very much featured in previous episodes and a wonderful
resource for people to get their learning on across different subjects.
Well, he's looking for a community input on a possible proposal that he wants to send
to the art consortium, which if it's accepted could be a sanctioned project with funding
for some additional development to basically supercharge much of the infrastructure and
overall quality of the big book of our, he's got a pretty hefty wish list, but if this
gets selected, he would like to use some community input that he gets in this call to action
as part of his proposal.
So we'll have a link in the show notes where if you want to get in touch and send your
feedback to Oscar, he would greatly appreciate it.
But I did have a look at some of the things he's asking for to work on in this art consortium
proposal.
There are very important things in my opinion, like having a robust database structure of
the content that he could do a lot more with.
And also he wants to put it in the quarter.
Why not?
That's a big ticket item.
And then making, he has a big wish list here, but I think the last item might be the one
I most got my attention on is that each book, when you click on that, imagine if you're
able to get its table of contents right in the big book of our resource.
That could be a huge win for usability and user experience, but that's going to take
some infrastructure enhancements and engineering development to make happen.
So you know, I wish Oscar the best of luck.
Hopefully this little plug here, we'll get some more eyes on this with some more community
input.
But yeah, we're, we look forward to seeing hopefully some great improvements to the big
book of our in the near future.
I feel like there may potentially could be some overlap between what's happened in the
our universe from a technological standpoint in terms of, I don't know, doing a great
job at representing a ton of information, um, as well as the big book of our kind of
from a high level is sort of doing the same thing, right?
Trying to do a great job at encapsulating a ton of content resources.
So I, you know, that I think we've got some steam under our belt on this topic from, from
what's gone on in the our universe and hopefully Oscar can get some of those tailwinds as well
for this project.
Very cool.
Yep.
Well, Mike, what did you find this week?
I found two that I wanted to call out.
One highlight is something that I think is, is talked about quite often.
And I think folks have quite varying opinions on it as well from time to time.
It's balancing classes in classification problems.
It's a blog by Matt K. Um, and sort of the subtitle is why that's generally a bad idea.
So he, he looks at it from a few different use cases, uh, using all of your favorite
tidy verse packages as well to sort of explore differences in prediction and accuracy as
well as different resampling approaches like, like smoke, um, for balancing and rebalancing
training data and to take a look at sort of how that impacts the actual accuracy of the
predictions at the end of the day.
And some really great discussion as well on this blog post about when maybe it makes sense
to rebalance and when it might not make sense to rebalance.
So that was one highlight that I found.
And then a second one is, uh, an interview with Woojoon Jung, who is the founder of the
R Korea group and he was interviewed by the R Consortium to discuss, uh, his efforts
on using R in finance and accounting in Korea.
And I feel like, you know, we, we do hear a lot about life sciences and other sciences
where R is, is used heavily in, um, and in finance and accounting, maybe don't get quite
as much recognition.
And this could be biased a little bit on my end because I do a lot of work in that field.
Um, but I thought it was nice to see an interview and Woojoon Jung's perspective on how R is
being leveraged currently and, and hopefully in the future as well, um, in this particular
industry.
Yeah, that's something I'm going to be looking at as well.
I've greatly admired many of the prominent members in the finance community.
And in fact, um, one little fun fact, I believe the author or at least one of the main authors
of the VS code R extension, Koon Ren is heavily involved in finance as well.
So yeah, I've been greatly admired by the work.
So I'll be, I'll be reading that interview.
Um, after I record this one, I guess I'm downtime again.
Very, very nice.
And what else is nice?
Well, again, the rest of the issue.
And of course we will have links to everything we talked about in the show notes.
And we always love hearing from you.
If you want to get in touch with us on the podcast itself, we have a little contact page
directly linked in the show notes.
And as well, if you have one of those fancy new modern podcast apps like fountain or pod
verse, he could send us a little booster Graham inside the app itself to give us a little
bit of a nice message right in your podcast player.
So very easy to connect with a service called Albie.
And I have links to how you can do all that in the show notes as well.
And of course, our weekly loves your contributions.
We are just a pull request away from our markdown draft of the current issue.
If you find a great link to a package, a great blog post, an interesting fine, a great community
resource.
We love to hear about it.
So get it, go to our wiki.org.
You're going to see a link to the draft right there and a way to quickly send a poll request.
We'd love to hear what you have to share with us on the art community.
And you can also get in touch with us directly.
I am still sporadically on Twitter with that the art cast, but I'm also on Macedon with
at our podcast at podcast index on social causing all sorts of mischief over there.
Mike, where can people find you?
Yes.
Likewise, I find myself using Twitter less and less these days.
The content just seems to be less and less data science in terms of what I'm seeing on
my timeline for whatever reason.
So I am trying to be more active on Macedon.
And you can find me over there at Mike underscore Thomas at fossil.org.
And by the way, we do want to give a great shout out to James Wade, who very much surprised
us last night as we record this with an enhancement to his GPT tools package that we just talked
about recently, where apparently now we can transcribe our very own podcast with GPT tools.
Can you believe that?
You don't have to listen to us anymore.
Now, wait a minute, please still download our episodes.
But yes, I may have to put this in my production.
This is awesome stuff.
So that space is evolving quite rapidly.
So he's got links to that and most other examples that have come in GPT tools.
So that's worth checking out as well.
Yes.
Thank you, James.
Yeah.
Thanks for the shout out.
We appreciate it.
And it's great.
Great to see the innovations in this space.
Well, now we're going to that's going to wrap up episode 117.
My little one is frantically pulling my chair away.
So that means I got to go.
But we'll be back with episode 118 of our weekly highlights next week.
"
"102","issue_2023_w_13_highlights",2023-03-29,34M 31S,"A new perspective on the value of base R functions, enhancing the capabilities of gpttools with vector databases, and three ways you can add alt text to your R-based visualizations. Episode Links This week's curator: Ryo Nakagawara - @RbyRyo (https://twitter.com/R_by_Ryo) (Twitter) & @RbyRyo@mstdn.social (https://mstdn.social/@R_by_Ryo)…",NA
"103","issue_2023_w_11_highlights",2023-03-15,36M 45S,"The future of running R in your web browser is here with webR 0.1, a demonstration of integrating Quarto and webR with immense potential in the space of reproducible analysis, and two fundamental techniques from the world of software development tailored to non-programmers. Episode Links This week's curator: Sam Parmar - @parmsam_…",NA
"104","issue_2023_w_10_highlights",2023-03-11,34M 43S,"A episode full of discovery in this week's edition of R-Weekly Highlights! How you can parse your own R code with parse and getParseData, a closer look at the search capabilities in R-Universe, and a look back at the key milestones in the history of the R language. Episode Links This week's curators: Kellly Bodwin - @KellyBodwin…",NA
"105","issue_2023_w_09_highlights",2023-03-01,36M 11S,"How to easily create interactive versions of your favorite ggplots with ggiraph, bringing AutoML to R with forester, and a head-to-head comparison of R and Excel for common data wrangling and summaries. Episode Links This week's curator: Colin Fay - [@ColinFay]](https://twitter.com/ColinFay) (Twitter) Creating interactive visualizations with…",NA
"106","issue_2023_w_08_highlights",2023-02-22,37M 50S,"The current state and future of {rtweet}, bringing the best of testing and CI/CD in a statistical package, and navigating through a Shiny maze (literally). Episode Links This week's curator: Eric Nantz - @theRcast (https://twitter.com/theRcast) (Twitter) & @rpodcast@podcastindex.social (https://podcastindex.social/@rpodcast) (Mastodon) rtweet…",NA
"107","issue_2023_w_07_highlights",2023-02-15,38M 29S,"A glimpse into the day-to-day of maintaining an R package, exploring gender effects in art history data with the power of resampling, and a huge win for accessible SVG plots with R-Markdown. Episode Links This week's curator: Jon Carroll - @carroll_jono (https://twitter.com/carroll_jono) (Twitter) & @jonocarroll@fosstodon.org…",NA
"108","issue_2023_w_06_highlights",2023-02-08,34M 57S,"Just how far back can we turn back time with an R installation, many enhancements to joining data sets in dplyr 1.1.0, and a retrospective on the 2022 thirty-day map challenge. Episode Links This week's curator: Tony Elhabr - @TonyElHabr (https://twitter.com/TonyElHabr) (Twitter) & @tonyelhabr@skrimmage.com…",NA
"109","issue_2023_w_05_highlights",2023-02-01,37M 27S,"How you can make R package testing a little easier with switches, how the combination of group processing and compute resources can level up geospatial data processing, and a few quick wins to improve the responsiveness of your Shiny apps that got your podcast hosts to think hard about previous design choices! Episode Links This week's curator: Jon…",NA
"110","issue_2023_w_03_highlights",2023-01-25,31M 12S,"A large helping of football data for your analytics with the englishfootball package, building a Shiny application with both R and python, and a first look at upcoming conferences this year. Episode Links This week's curator: Ryo Nakagawara - @RbyRyo (https://twitter.com/R_by_Ryo)) (Twitter) & @RbyRyo@mstdn.social (https://mstdn.social/@R_by_Ryo)…",NA
"111","issue_2023_w_02_highlights",2023-01-19,34M 5S,"A glimpse into the world of end-to-end Shiny app testing with shinytest2, and an important look at the spectrum of reproducibility within R using container technology and services. Episode Links This week's curator: Batool Almarzouq - @batool664 (https://twitter.com/batool664) (Twitter) End-to-end testing with shinytest2 Part 2…",NA
"112","issue_2023_w_01_highlights",2023-01-11,29M 3S,"A big progress updates in the latest {knitr} development release (literally), how the {litr} package enables literate programming development for R packages, and how you can translate Morse code directly in R with the {remorse} package! Episode Links This week's curator: Sam Parmar - @parmsam_ (https://twitter.com/parmsam_) (Twitter) &…",NA
"113","issue_2022_w_52_highlights",2023-01-06,49M 32S,"Our first episode of 2023 covers the brand-new gpttools package to called chatGPT directly in R, a wholistic look at MLOps with the latest tidymodels tooling, and a spotlight on the lesser-known quantile regression. Plus listener feedback and much more! Episode Links This week's curator: Kelly Bodwin (@KellyBodwin…",NA
"114","issue_2022_w_50_highlights",2022-12-14,35M 11S,"Data munging and visualization of your Twitter archive with R, a successful Shiny app submission to the FDA, and scoring Rock Paper Scissors. Episode Links This week's curator: Eric Nantz - @theRcast (https://twitter.com/theRcast) (Twitter) & @rpodcast@podcastindex.social (https://podcastindex.social/@rpodcast) (Mastodon) Read and Visualize your…","Hello friends, it is episode 104 of the Our Weekly Highlights podcast.
It is right in the thick of the holiday season, so I hope all of you are enjoying your either
time off from office or just enjoying time with family, but hey, we're here to give you
a little our flavored entertainment for the next few minutes.
My name is Eric Nance and I am as always joined by my awesome co-host, Mike Thomas.
Mike, how are you doing this fine day?
I'm doing great, Eric.
I'm doing better than you by the looks of it, but we don't have any video for the audience
and it's a function of having young kids during flu and cold season, so just for everybody
out there.
I was sick a couple of weeks ago and Eric's sick now this week, but we still power through
because that's what we do for you.
Yes, it is all for you, the listeners, and yes, it is the season of giving, but I didn't
want to have this be given in my house, but yes, you can't control all that, but hey,
my voice is still working for the next few minutes and we're going to make it work here.
Oh yeah, let me check my notes here real quick, Mike, who's the curator?
Him again?
He's still here?
Yes, it is me.
It is me.
I was a curator this week before all this flu stuff set in, so at least I got the issue
out in one piece, but of course, I can't do any of my curation duties without the tremendous
help of our weekly team who make it so much easier to automate a few of the things at
least and grab the awesome links that you all send via PRs and the RSS feeds and it's
always a lot of fun to put all this together, so my big thanks to our curator team and contributors
like you all around the world for making issue 2022 week 50 happen.
So let's get right into it.
So when we utilize a major web service that doesn't have an upfront monetary cost, well,
I think it's become more common now, especially in today's world that the currency in this
case may not be so much fiscal dollars, it's our data and the hope is that the service
will be good stewards of our data and the future directions of the platform are aligned
with our principles.
At the very least, having the ability to grab the data from these walls of the service for
our own archival and perhaps even analysis purposes can always give us a little peace
of mind.
So if you've been following the tech sector, you likely know now that Elon Musk has recently
purchased Twitter and my goodness, the discourse around this has been strong to say the very
least, hence it is a great timing here that in our first highlight, Garret Gayden Bowie,
who is a data science educator and developer at Posit, has authored a fantastic blog post
on how you can unleash your R skills onto your own Twitter data archive for our first
highlight here.
Now there's a lot to this data wrangling and visualization adventure.
So we're going to cover like the major points here, but we won't be able to do the post
justice, you definitely want to look at it.
And we're going to start with what can be a perplexing issue with file data formats.
Now the good news is the data are JSON format.
Now anybody that's developed web stuff like the APIs or even done things with Shiny, JSON
is a pretty comfortable format to deal with.
Now these files on the other hand have a rather strange declaration at the top for assigning
objects in a namespace that's browser related.
That makes no sense even when I say it.
But this is the case where data is not always perfect, right?
So Garret writes some nice utility functions that clean all this stuff up and then makes
it reusable so that he can use a very nice, you know, pipe like workflow that's been made
famous by the tidy verse family of packages.
And those handy processing functions are included throughout the blog post.
So you can just take those and run with it for your own Twitter archive.
Of course, that's just half the battle, so to speak, right?
Once you're able to get the data into R, then he has some additional functions to get it
into a tidy, you know, tabular format.
We're dealing with things like nested list again, another, you know, minor hierarchical
complications.
But again, great use of the per package and others in the family of the tidy verse to
make the processing once you figure it out, quite elegant to get through.
Now, after the data are assembled, it's time for another visit to the ggplot2 corner.
And not only do these plots utilize a fun custom theme for the blog post gives me a
kind of like a, I don't know, like a sci-fi techie vibe.
It's a really cool theme here.
But these plots are interactive folks using custom tool tips that are powered by David
Gohell's ggiraff package.
This is one of those cases where Mike and I are not sure how to pronounce a package
name.
So David, if you are listening and you want to make corrections, you know how to find
us.
We'll tell you later in the episode.
You say ggiraff, I'll say ggiraff and one of us will be right.
That's right.
All the bases.
Let's cover our bases here.
Shall we?
Yeah.
No one makes it easy for us to pronounce these things here.
But anyway, the first display that Garrick makes is tweets per month, which reveal a
few insights into Garrick's usage based on key events in his life, which I can definitely
resonate with.
And then in another plot, we see what looks to be a pretty neat log linear relationship
between his popular tweets based on retweets and likes, which was kind of fascinating.
He personally is thankful that the tweets he has that get a lot of likes are the ones
that kind of make him feel good on the inside, so to speak, that he's sharing something useful
out there.
But this blog post has so much more.
There's other little nuggets here that are definitely insightful because this data archive
from Twitter gives you a lot more than just things about retweets and likes, Mike.
So why don't you take us through some of the things you found in the post here?
Absolutely.
One thing that I really loved is Garrick's use of polar area diagrams.
They're a chart type that I very rarely use because I feel like it's not often that I
have data that lends itself well to that type of chart.
However, in the blog post, Garrick creates absolutely stunning polar area diagrams to
show sort of a histogram of the time of day that he tweets, faceted by the day of the
week.
There's a separate plot for each day of the week, and the length of sort of the slice
of pie in these polar area diagrams represents how many tweets he typically tweeted out during
that particular time of day.
Another thing that really stood out to me are his use of tool tips along all of these
charts, and I think that's something that the ggiraffe package does very well with just
a little bit of HTML formatting that you can see in his code that he puts alongside the
plots in this blog, which is really nice because if you're like me, some of these other interactive
visualization packages really only allow you to create custom tool tips via JavaScript,
and you're not super comfortable with JavaScript yet or as comfortable as you would like to
be, and you're much more comfortable with HTML.
It's nice to have that option to create these beautiful tool tips and include HTML that
he sort of just glues together in these particular ggiraffe plots, which is really nice.
One place that these really nice HTML tool tips shine is in Garrick's analysis of the
Twitter advertising information.
This is data that I didn't even know we could get from the Twitter API, data around promoted
tweets or ads that have been shown to us on our timeline, and Garrick puts this information
together in this really nice horizontal bar plot where he shows the ad interactions by
advertisers.
In his case, Prime Video was the advertiser that showed up on his timeline the most.
It had 91 promoted tweet impressions, and then Prime Video had 92 promoted tweet engagements.
He has these bulleted lists under each tool tip that show exactly the different engagements
or the top five engagements related to that particular advertiser.
For each bar on the chart, which include Apple TV Plus, Action Network, it looks like a lot
of TV advertisers, but he's also got Microsoft, PNC Bank.
I thought that that was really interesting that that is data that we actually have access
to, which I think is nice because it's sort of, for us data scientists, a way to get paid
back, not financially, but paid back for the fact that we have to deal with these promoted
tweets and these ads in the middle of our timeline.
Obviously, it's a beautiful blog post in the fact that we have all of the code snippets,
which just reading the code underneath all of these charts, I learned some stuff from,
so I think everybody can potentially get something out of here between the data prep and the
visualization code and the HTML that he includes.
A really, really nicely done blog post that I certainly encourage everybody to take a
look at.
It's great to start off the highlights this week with a strong database post.
The visuals are amazing here, and I think both of you and I were remarking in the little
bit of a pre-show here that we both want to take a bigger, strong look at Gigi IRAF, that's
my version of it, to supercharge our plots that it can work well for both Shiny and outside
Shiny like in this R Markdown or other, perhaps Corridor as well.
Lots of interesting nuggets here in both the visualization and the processing because I
know Garrick's had to deal with some really unwieldy data in the past from, in this case
it's Twitter, but I've always had some fun conversation on some of the wrangling adventures
he's had in the past, so it's great to learn through his example and yeah, you'll want
to read this through a couple of times because there's a lot to unpack here, but it's all
very engaging and yeah, his bit of a call to action at the beginning is if you are concerned
about the future direction of that said Twitter platform, get your archive now, you never
know when you need it.
Yes the blog post, I think first sentences, Twitter finds itself in an dot dot dot interesting
dot dot dot transition period and I couldn't have said it better myself.
Very diplomatic way of saying it, so credit to Garrick for that great tone there, yes.
And now we're going to transition to another story that admittedly on the surface may not
look like a lot, but it could be a very fundamental shift for especially some of us in the life
sciences space.
The journey I'm about to summarize hopefully will resonate with any of you in our audience
trying to bring innovation to legacy processes via open source and in particular R itself.
As part of my external collaborations, it was about a year and a half ago that I joined
a cross pharma industry working group under the R consortium umbrella.
For those of you that aren't aware, R consortium is based in the Linux Foundation to help provide
financial support for R projects that are enhancing the R project itself or enhancing
the industry use of R. And it was about a year and a half ago that a work stream for
what we call clinical submissions in clinical trials using R was sanctioned.
And so I joined that and we've developed multiple pilots to prove out that an R base analysis
result could be delivered to regulators like the FDA using their submission transfer guidelines
and system, which I'm about to touch on a little bit here.
Now speaking of regulators like the FDA, but a huge part of this working group has been
their participation to now sharing and real time feedback on our ideas without their efforts.
We would not have achieved what I'm about to tell you.
So that's a real success story here is that this was full cooperation amongst those of
us in the pharma space, but also those of us on the other side of the proverbial wall
here, the regulator side, which is, I think, kind of like a new trend that we're going
to see in this industry going forward.
So last year, the working group was successful in transferring R based statistical outputs
are commonly referred to as tables, listings, and graphs to the FDA and a mock clinical
submission package using sanctioned data that was ready for mock use.
Now moving to this year, we up the ante, so to speak, for pilot two.
Now Mike, what truly game changing domain in the R landscape and help us interact and
share dynamic summaries on a web based format?
It's got to be shiny, Eric.
Yes, it is.
And hence we set out to create what amounts to a relatively simple shiny app that surfaces
the outputs of the first pilot in a completely self-contained reproducible bundle that the
FDA reviewers could install and execute on their respective systems.
This typically Windows laptops.
Now when you think of a self-contained reproducible bundle in R, I don't know about you, Mike,
but when I think of that, immediately what comes to mind are packages.
Naturally, I set out to bundle this application into, wait for it, Golem, because once you
go Golem, you can never go back.
I'm sorry I couldn't resist, but that's not the full story here.
Specifically, not just having the app as a package as a way to kind of set perhaps precedence
in the future, we needed a way to convert the source of the app package into literally
text files so it could fit within the transfer protocol.
This is where it can get really hairy, but this is again, at least a bit of a success
story in terms of collaboration.
Some very innovative colleagues in pharma have authored what's called the package light
package that's been authored by colleagues at Merck, which takes care of taking these
R scripts, whether it's in a package or just a folder or whatever have you, transfer them
into these text files, we feed it through the transfer protocol, and then this package
can then reassemble into real R scripts in a real package structure.
Yes, this is what I would consider jumping through a major loophole, but you got to start
somewhere.
This was quite a learning experience for me, but I'm happy to say that after all of our
efforts throughout the year, we had a successful submission of the app to the FDA just recently
as November.
But I am going to link to the GitHub repository where the app lives, and which you also find
the app itself, which again, nothing fancy, but it was successfully transferred to FDA,
hopefully setting precedence that Shiny can be a very valuable part of future submission
packages.
Shiny already has a major presence for many of us in life sciences, because we're ways
to interpret analysis results or we're surfacing novel algorithms.
And this pilot was a critical first step to making a clinical submission in Shiny technically
possible in the current landscape and sending the seeds, if you will, to even bigger innovations
hopefully next year.
So gratifying effort.
I will admit it's been a lot of work to get to this point, but we all have to start somewhere
and I'm hoping that the fruits of this labor will be realized especially next year as this
becomes hopefully more routine.
Well Eric, I know this is your baby.
When you go to the GitHub link and go to the repository, your name is all over it.
So just a gigantic congratulations to you.
It's a great step in the right direction for open source being used in life sciences and
highly regulated industries such as life sciences where open sources had trouble gaining footing.
And I think this not only is a great accomplishment in the context of what exactly you were trying
to accomplish with this submission, but I think it's a great accomplishment and testimony
to the progress that open source has made in highly regulated industries.
So this is really, really exciting for me to see.
All of our life sciences clients that we have at Ketchbrook, almost all of those projects
have been Shiny related.
So that's another really cool thing to see that the life sciences space is really strong
in their adoption and use and belief in Shiny.
So I really just hope to see this continue.
I hope that in the future the so to speak hoops that you had to jump through to get
this submitted successfully are less than the number of hoops that you had to go through
for this particular submission and that we can start to see maybe more of a linear path
in providing our packages, Docker containers, whatever it may be, to regulators such as
the FDA.
But it's really encouraging to hear as well that it was such a collaborative effort between
you and the FDA.
And obviously that's what made it successful, but that sort of tells the story that there
is not only buy-in from your side, but there's buy-in on the other side of the table as well.
And that's super important, I think, to the success of open source.
So kudos, hats off to you, Eric, something to be really, really proud of and hats off
to everyone involved and everyone that really believes in this effort, I think, of open
source and life sciences and highly regulated industries, because it's going to change outcomes.
It will.
The power that we have in the open source ecosystem between R, Python, Julia, whatever
you want to call it, you have the entire open source community working on these problems,
which is something that did not exist before.
And that's going to change lives and it's going to change outcomes.
I really do believe that.
Well, that's a very kind words, Mike, and this was certainly a team effort across the
board with my colleagues at the various other pharmas like Rosh and Merck and everyone else.
And then, yeah, FDA had a big seat to this table.
And yes, next year, we have additional pilots in store.
And one of them that I'm very interested in, I mentioned it a little bit earlier.
Wouldn't it be nice if we could just send these apps as a Docker container or easy way
just to run it with one command?
That's what we're going to try and do next year.
Can't make promises yet, but we already have like the little bit of seeds planted to explore
this next year.
So stay tuned in this space, it could get even more exciting, technically speaking too.
Now I don't know about you, but after that, after summarizing that saga, I need a little
breather, so to speak.
So we're going to, we're going to have fun with this last highlight.
And before I, before we get there, yes, the streak is over.
Albert did not make the highlights this week, but you know what?
I'm sure we'll see him again soon.
But in any event, Mike, you're going to take me back to an old game when I was a kid that
I played in the playground.
How does rock paper scissors fit into all this?
Well, I mean, the fact that Albert didn't make the highlights this week might have something
to do with the curator.
I don't know, Albert.
We'll have to talk about that.
I'm just kidding.
He will be back soon.
I have no doubt, but onto our last highlight of scoring rock paper scissors.
So I shamefully, very shamefully have not participated in Advent of Code yet, but I
do follow along and I love seeing everyone's solutions to these puzzles.
And if you don't know about Advent of Code, it's a daily coding puzzle, kind of like Wordle
for data scientists, I would say, and software engineers.
And again, Wordle shamefully is another one that I don't do.
So maybe that's why I don't do Advent of Code either.
But Advent of Code happens during the month of December.
And the other day, the Advent of Code puzzle was to build some sort of a function that
scores or outputs a win, lose, or draw result given two inputs that represent the two players'
moves in a game of rock paper scissors.
So the inputs would be Eric gave rock and Mike gave scissors.
And they're asking you to provide some sort of function that tells you who wins, either
Eric or Mike, given those two inputs.
So obviously, there's only a few different combinations of what could happen during a
game of rock paper scissors, given that you have two participants and each one has three
different options of what they could provide.
So I think it's nine total if I'm doing nine different combinations total that you need
to provide the outcomes for.
So Advent of Code gives you the input, and they tell you what the output should be.
And your job is to build, again, this thing in the middle that turns the input into the
output.
So I'm not sure if this is part of the Advent of Code unwritten rules or not, but I feel
like most of the solutions I've ever seen to Advent of Code always use base R. They
don't import any packages.
Maybe that's just what I've seen in the past through a very small sample size.
But this is the case in Tristan Maher's blog post that showcases his solution to this rock
paper scissors Advent of Code puzzle from day two, I think December 2nd.
He provides a really elegant solution involving nested lists.
So the first level of the list is what user one would throw.
So say rock.
And then the second element of the list would be the possible responses from the other participant,
from the other player.
So it could be either rock, which would result in a draw, scissors, which would result in
a win, right, because rock beats scissors, and then paper, which would result in a loss,
right?
So you do that for each of the combinations, and you have sort of this nested list that
has three tiers at the top.
And then underneath each of those individual tiers would be three tiers of a second list.
So my gut sort of really expected to see per package used, but it wasn't the case, right?
This is all base R that Tristan used.
And it led me to seeing a few base R functions, which I actually hadn't been familiar with.
There's a function called get element, which seems somewhat equivalent to the pluck function
from the per package for extracting an element from a list instead of having to build up
that bracketed one, one to extract the first element of the first list.
So really interesting, kind of lighthearted post and really nicely, concisely done answer
by Tristan to this day two advent of code puzzle that they had.
And I don't know, it's just fun for me to kind of do a little brain teaser with R and
maybe to get away from your daily grind a little bit and exercise some of those other
muscles that I think will help you be a better R coder in the long run.
So I would love to, I'm not making any promises here on the podcast that I can hold myself
to, but maybe I'll try to do at least one advent of code puzzle.
I do have some time freeing up and I'll let us know on the next podcast episode how that
went.
All right.
Way to hold yourself accountable, buddy.
One thing that I noticed, and again, really interesting read of the power of lists, like
lists are kind of like my MVP of objects and they are a language.
But the elegant thing here is that especially even now to some degree, I find myself catching
into an if else trap or I do a lot of if else if statements and that can be pretty unwieldy.
So taking advantage of how you can frame the problem differently, like what TJ does here,
I think is just a great kind of like meta principle that sometimes if you take a step back before
you do like your quote unquote tried and true solution, you might arrive at insights that
you didn't think were going to be possible.
Now TJ is already a master of lists.
He even references some of the way he uses them in his R Markdown compilations with Knitter.
He's a power user of that.
So I definitely lean some insights how I can make lists even more of an MVP in my daily
work too.
So lots of interesting ideas and yeah, I can't say that I've had a lot of time for advent
of code, but maybe I will pick up on that and there are definitely some really influential
members of the R community that, you know, it's kind of like their Superbowl almost they
wait for this time of year and they crunch away.
You often see David Robinson get on top of the leaderboard for submitting his solution
quite quickly on the R side of things and yeah, it's interesting to see the dichotomy
or I should say the trend in most of the time, like you said, Mike, these are base R solutions
and maybe that's just because they want to make it as easy as possible for others in
the community to run this on their own setups, which again is an admirable thing.
But I think also using base R, you know, kind of forces you to really up your game, so to
speak on programming logic too, because there are certain things that other packages take
away in a good way, I should say some of the pain points of coding these up in base R.
So knowing kind of what's on the inside of those solutions is another kind of interesting
way to add in a code can challenge us to explore that.
So yeah, I had a lot of fun reading TJ's posts really nicely done.
Absolutely.
Yeah, I think it's something that I've said once or twice before, but you know, we love
the package ecosystem in R especially.
That's what makes it so strong.
But I think spending some time with what we have available in base R might surprise you.
Some of the utilities that we do have available in there that you may take for granted because
we use, you know, the tidyverse every day that leverages a lot of these base R packages.
So I think it's base R utilities, I should say.
So I think it's good to have some perspective on both sides of the aisle in terms of the
package ecosystem and what comes out of the box within the R install.
Yep.
And I'll tell you what's not surprising.
Well, it's another fantastic issue of R Weekly, of course, we've got lots of good stuff in
here.
And I'm not just saying that because I was a curator this week.
We always have good stuff here.
So it'll take a couple of minutes here to share our additional finds here.
And this is a bit of a plug going back to some of the life signs trend here.
But I'm happy to say that after all the editing and crunching and saving video files that
I did a few weeks ago, the R Pharma Conference 2022 recordings are now available on YouTube.
So you can catch any of the presentations you missed and also highly regarded workshops.
To me, you're going to learn so much by watching those workshops.
And I'm not just saying that because I did want to.
These are really innovative stuff, all from quarto to using observable, putting automated
testing in your shiny apps, maybe building production apps to lots of interesting things
here to watch.
So definitely check out the playlist that we'll have in the supplements.
But in particular in this issue, we link to Robert Gentleman's keynote, that name should
sound familiar.
He's a co-founder of R itself.
That was a huge achievement for us to have him speak at the conference.
So definitely watch his keynote on some of the technical challenges that he's been facing
in his research group at Harvard with utilizing R on high dimensional complicated genetic
and bioinformatic data and some of the calls to action that he has for R can really be
taken to whole new levels.
So even a little bit of the history of R, which I never get tired of hearing, it's always
great to hear from the source.
So that was an amazing talk.
And again, every talk is amazing.
So definitely check those videos out.
Yes, absolutely.
And I will say that I was on hand for the shiny in production building production grade
shiny apps workshop done by Eric.
And it is absolutely worth rewatching if you were not there as well.
It was phenomenal.
But one other video I guess that I will call out here is Jacqueline Nolis is lightning
talk I think it's called from norm comp 2022, which the actual live talks start tomorrow.
So very, very exciting.
Check out norm comp if you haven't already.
But the lightning talks were pre recorded up on YouTube now.
And Jacqueline just does a hilarious, fantastic, relatable video that details her analysis
of really taking a look into sunset, I believe and sunrise data in Alaska specifically and
building trying to build a really nice Gigi plot but just seeing all sorts of crazy, wonky
things across time zones.
And she uses props in the video.
It's fantastically done.
It's only a few minutes long.
So I would I would if you're looking for something lighthearted, relatable, from a data wrangling
perspective, I would definitely check out that particular video.
A couple others, maybe that I'll just highlight really, really fast our blog post that says
please avoid detect cores in your R packages.
I guess the recommendation is to move away from the detect cores function in the parallel
package to the available cores function in the parallel Lee package.
So that's a blog post if you're doing a lot of parallel computing across multiple cores,
I would recommend checking out just in case you did not know that that blog post existed.
And then I'll throw one more out there that the base serve package, which is Bayesian
survival regression, got an update, I believe, and that is something that I use quite a bit
in my modeling.
So it sort of spoke to me.
Very good.
Yeah, I have a lot of Bayesian statisticians in my group at the day job.
And I know they love those kind of packages.
But yes, Jacqueline's talk, I was rolling after watching that the first time and I had to
I was watching it while my kid was trying to sleep and I meant I had to leave the room
because I was laughing out loud and I was like, oh, no, I'm going to wake him up.
It was hilarious.
So Jacqueline and her patented style absolutely nails that talk.
And as someone who dealt with messy daytime stuff a year and a half ago in my Twitch stream
calendar, I can definitely understand some of the pain points that she had to deal with
on that.
So kudos to her for finally figuring it out because that would have taken me probably
a year to figure all that stuff out.
Yes.
Yes.
Daytime's not fun.
Nope, they're not.
And time zone conversions are even worse.
But you didn't hear that from me.
What you do are going to hear from me is that we always welcome your feedback to the show.
So you can definitely get in touch with us and get in touch with the art weekly project
in multiple ways.
Of course, go to our weekly dot org, you're going to find the current issue and all the
archive of previous issues linked to all the podcast episodes.
And if you want to contribute a poll request, we have a GitHub repo already linked with
the upcoming issue draft ready for a little markdown magic coming from you.
If you want to share a great blog post, new package, video, anything that is supercharging
the art community, we'd love to see it.
And if you want to get in touch with us directly, well, our weekly is on mastodon now.
We are at our weekly at fastidon.org if you want to get in touch with us there and also
your friendly host here on mastodon and Twitter as well.
Although maybe I'll be downloading my Twitter archive, just in case.
But I'm still on there with at the our cast and also I'm on mastodon with at our podcast
at podcast index on social Mike, where can they find you online?
I am still hanging around Twitter at Mike underscore Ketchbrook K-E-T-C-H-B-R-O-O-K
and I'm also on mastodon at Mike underscore Thomas at fastidon dot social.
You got it.
Did I get it this time?
You got it.
Yes.
It's coming natural for both of us that will eventually happen.
Yes.
But yes, please get in touch with us.
We love hearing from you.
All feedback is welcome.
And if you want to have a little fun in your little podcast playing device, you can get
yourself a new podcast app at newpodcastapps.com and give us a little boost to show your love
for the show.
More details on that in the supplements of the show notes.
Well with that, my voice finally lasted this long.
I better quit while I'm ahead, so to speak.
We're going to close up shop of our weekly highlights episode 104 and we'll be back with
another episode either next week or soon.
Stay tuned.
"
"115","issue_2022_w_49_highlights",2022-12-08,33M 37S,"Big new features coming in {dplyr} 1.1.0, how you can make your own #rstats wrapped, and enhancing your Shiny apps with JavaScript (without knowing much JS). Plus your feedback and more! Episode Links This week's curator: Jon Carroll - @carroll_jono (https://twitter.com/carroll_jono) (Twitter) & @jonocarroll@fosstodon.org…","Hello friends, we are here with episode 103 of the Our Weekly Highlights podcast.
My name is Eric Nance and as always I am delighted for you to join us from wherever you are listening
around the world for some more great art content for your listening pleasure.
And it's always a pleasure to be joined by my awesome co-host who's rocking a bit of
new hardware here, Mike Thomas.
Mike, how are you doing today?
Doing very well, Eric.
I do have a new podcasting mic set up, so I have gotten pretty serious over here and
maybe listeners will notice the difference.
Maybe they won't, but I'm excited.
Yep.
You know it gets serious when you get the dedicated mic.
Yes, that is very nicely done and for those of you who are listening to audio, you can't
see it, but Mike's microphone is really solid, so I'm really impressed.
Mike's mic.
Exactly.
I should make a meme out of that perhaps.
Yes.
Yeah, so we're here to discuss Issue for this episode 103 that's been curated by Jonathan
Carroll, another longtime Our Weekly member of our curation team.
He's been a huge help with getting a lot of the infrastructure stuff back up and running
and getting access to various things and he and I have some ideas on how we can implement
some more automation with some of our new social media exploits with Macedon, but in
any event, he did a great job in this issue and as always, he had tremendous help from
our fellow Our Weekly team members and contributors like you all around the world.
And we begin our episode today with big news on arguably the foundational pillar of the
Tidyverse, specifically the Tidyverse team at Posit is preparing a grand release of dplyr
1.1.0 for January and Posit software engineer Davis Vaughn has authored a new blog post
to put the spotlight on some major new features and updates to response to community feedback.
And I'll start this off with an improvement I'm particularly excited about and that's
a new way to perform flexible joins of datasets using the new join underscore buy function.
Now what does this really mean?
What does this allow you to do?
Well, it's not just doing those typical case of joins where you have the variable in common
and it's more of a quote unquote equality fashion.
But now with this new join underscore buy, you have the ability to introduce custom expressions
for different types of joins, such as those dealing with inequality, maybe a rolling join
or an overlap join, which are nicely defined in the blog post.
And Davis also does a terrific job with including a realistic example of assigning employees
to a company party and how you can use a hybrid of these new join capabilities to make it
happen, which would not have been possible without a lot of custom manipulation in previous
dplyr functions.
And the write up in the blog post is really comprehensive, going through each step of
the join process and incrementally improving on it to get to that final stage.
And this example particularly hit home for me because a few years ago, I was tasked with
creating an app to let someone planning a company event at the day job, assign attendees
to tables at this company event and ensuring that at least one higher level manager or
executive was present at each table and that the attendees at a given table were representing
multiple functions or teams.
Now of course I use Shiny for that because I use Shiny for all the things, right?
But the onus was on the app user to manually create those assignments, even in the app
or within a spreadsheet that I could upload.
Well, think of it like this, if I could go back in time or if I have to add, if I have
to revise this app in the future, I could use the new version of dplyr to give them
a little button that says, hey, you do the assignment for me and I'll proofread it later
with a combination of all these joins perhaps or these flexible joins.
So I'm really excited to try that out if I get that opportunity again.
And I even have another opportunity perhaps to use this where I've seen some resources
and what's called the Odyssey project for harnessing real world health data.
And they have some really complicated SQL joins, a lot of inequality joins, a lot of
custom expressions inside.
So now with dplyr 1.1.0, I might be able to implement or translate some of those joins
into a dplyr syntax.
So I'm thinking of giving that a shot as well.
Now it is important to note that the highly regarded data.table package, which is very
immensely popular among many in the R community, has supported these quote, non-equi joins
for many years, but that was part of the inspiration for the tidyverse team to implement the new
join by function.
And it has been one of the most highly requested features in dplyr going all the way back
to 2016.
So it is really cool to see this new feature.
Those join improvements are potentially huge.
And I think I know a few places in my own workflow where I'm going to try to implement
those as soon as this package hits CRAN.
Another place that is a huge update in 1.1.0 to dplyr, stop me, Eric, have you ever done
a group by?
Again, some other dplyr function and then an ungroup.
How many times in your life do you think you've done that?
Too many to count, buddy, too many to count, right?
So what's coming in 1.1.0 is temporary grouping with an additional argument in verbs that
work by group, such as mutate, summarize, filter, and slice.
You've gained a new experimental argument,.by, which allows for inline and temporary
grouping.
So it's pretty powerful.
It's going to save you at least one line of code.
So if in your previous scripts you had written something like empty cars piped to group by
cylinder, and then summarize eight miles per gallon equals some mpg, and then you had to
do an ungroup after that, all you'll have to do now is empty cars, pipe it in to summarize,
mpg equals some mpg, comma, second argument is.by, and then cylinder.
So it can just be two lines of code.
The output is not grouped, so again that.by just creates a temporary grouping to perform
the calculation that summarize, mutate, filter, slice calculation and returns you an output
that is not grouped.
So you do not have to worry about tacking on your ungroup verb at the end of your pipeline.
And I mean, most of the time, probably for 90% of the use cases I have, that's exactly
what I'm doing.
I'm having to ungroup after I am doing that group by operation.
So this is going to save me a lot of code, save me a lot of time as well.
They're calling it an experimental argument.
I hope that it sticks around and we can expect to see it in 1.1.0, but I think it is going
to be a huge game changer in the tidyverse for all of us in all of our ETL and data manipulation
pipelines.
Yeah, I definitely see massive time savings with this operation.
I am very thankful that Davis was upfront about the ordering piece of it.
Now there are some in the community that are kind of concerned about this, that some may
have had pipelines in the past where they took advantage of the ordering that happened
in group by in other post-summarization or post-mutate operations.
What I'll have link in the show notes is a toot from deposit Macedon account where there
was some interesting discussion from some familiar faces actually to R Weekly on maybe
some of the caveats that might need to be thought about.
But I think as long as people are aware that the.by is not going to change the ordering
that was done by default when the data set was imported, then I think as long as you
know what to expect, I think it's definitely manageable.
But again, credit to POS for putting this out there now instead of waiting until the
CRAN release of 1.1.0 at all and then surprising people.
I think that's a very important thing in software development, especially in open source software
development for a package like dplyr that is used so widely across many different data
science workflows to be upfront about this and not surprise people.
So again, credit to Davis and the tidyverse team that put this together.
But it is a very exciting release nonetheless.
Yes.
And maybe just one or two more things I will note about this blog post, specifically starting
with the.by argument.
Just like group by, you can group by multiple columns.
You don't necessarily just have to provide one column to this.by argument.
You can use multiple columns, which is great.
And group by won't ever disappear.
That verb will never disappear.
So you don't necessarily have to worry about this impacting any of your production work.
And if you don't necessarily want to switch right now, you do not have to switch right
now.
Two other updates coming in 1.1.0, the arrange function is getting some improvements with
respect to character vectors.
And there is a new function I believe called reframe, which is a generalization of summarize.
So check out the blog post for more info on those other two improvements.
And certainly there are more features than what are summarized in the post.
So there are definitely links in the post to additional features from the GitHub repo.
And certainly if you have concerns about some of the new changes, that's what issue boards
are for.
And I've already seen a few issues posted after the release of this blog post to clarify
a few things.
So if you do have concerns and you see maybe a gap in testing the dev release, hey, that's
what feedback's for, right?
So I highly encourage people to check it out, especially if you're writing a package, an
app or whatever important pipeline you have that could make use of these new features.
So really nicely summarized by Davis and certainly, like I said, I could relate to that example
in the SQL joins because I was thinking, oh, you came up with that?
Where was this a few years ago?
Could have made my life easier.
But yeah, really exciting to see here.
Well, it is that time of year, Mike, we're approaching that we're in the holiday season
basically now it's the end of the year.
And you're probably being inundated like I am with various countdowns or top 10 lists
or whatever have you.
And apparently, I'm not one of them.
But if you use Spotify, they'll listen to your music streaming, you probably received
your own personalized list of your most listened to songs this year.
And yes, many, many people are tweeting that out in the various social media platforms.
In fact, I have a link to Travis Gertz humorous LinkedIn post about apparently a little bit
of arranging and summarization is like the new data science hotness and in these summaries,
I guess.
No, I'm kidding.
I know this can be a lot of fun.
Hey, we're in the our weekly highlights podcast, right?
How can we put a little our magic on this?
Well, the very talented Nicola Rennie, a data scientist at jumping rivers, enters the highlights
podcast once again, with how she pivoted from her listening habits to driving a distinctly
our stats flavored brapt of her most used functions in the year.
Now, this was a very fun exercise in the blog post on both code and introspection.
And of course, a little bit of data munging and visualization at the end.
So Nicola starts of importing all of her file paths and our scripts related to her tidy
Tuesday submissions, a great way to have kind of a calendar like chronological order of
how she's been using our this year.
And she also utilize Nicholas Cooper's NCMISCR package, easy for me to say, with a handy
function called list dot functions dot to that file.
That's a mouthful, isn't it?
But it does what it says, right?
It's going to take a set of file paths, look at them and and literally give you a list
of the functions and packages that were called in that script.
So Nicola combined that with some per iteration to assemble a tidy tipple of the function
frequencies or you might say the number of times it was used in her scripts.
Now of course, this wouldn't be complete without a top notch visualization, right?
Well of course, ggplot2 enters the game here and Nicola proceeds to assemble an infographic
of the top five functions that were called in these scripts.
Now this is quite meta in and of itself because three of the top five functions are indeed,
wait for it, from ggplot2 with AES being used 47 times across her Tidy Tuesday scripts.
So there you go, usually Tidy Tuesday has some kind of visualization, right?
So that's not very surprising, but hey, now you got quantifiable evidence in her case
that ggplot2 is an MVP of her Tidy Tuesday adventures.
This is a really entertaining read and again, very easy to follow too.
So there's ample opportunities wherever you want to do this for say your Tidy Tuesday
submissions or some I'm thinking about.
Say I have a directory of all my Shiny app code, what are the most common input widgets
I use or what are the most common reactive constructs I use?
I could see lots of fun doing that.
I'm a huge fan of Spotify Wrapped.
It's like one of the easiest data products ever made.
It's literally just a count and an order by.
I think I saw some people last year tweeting that Spotify Wrapped is the coolest AI I've
ever seen, which is just hilarious because it's like a two line sequel probably.
Yeah.
ChadGBT at AIN, but hey, you know what?
You got to start somewhere.
I put out a tweet today that I thought was great, hasn't gotten a whole lot of love,
but for the 90s babies out there like me, we know that the original ChadGBT was Smarter
Child if you're ever on AIM.
So I'm just going to leave that out there.
ChadGBT isn't exciting me that much.
I've seen this before.
Anywho, people absolutely love Spotify Wrapped.
So it was really cool to see Nicola implement this in an R spin.
And that pipeline of functions that she uses that you mentioned, Eric, to find the most
used functions across all your R scripts in a directory, that's really useful.
I can actually see myself using that maybe to try to write an internal package and understand
what are some of these functions that I'm just using all of the time.
So I don't know.
I feel like there are a couple of different interesting ways that I might be able to leverage
that particular logic that she's put together, which is really nice.
I'm looking forward to creating my own Spotify Wrapped using Nicola's code this afternoon.
I would love to see how my most used functions have changed over time.
Ooh, I love that idea.
That might be scary to look at.
And probably, you know, if I looked in a year from now, it'd be a lot of movement from group
buys and ungroups away into the.py argument.
So we'll see.
One of my favorite parts of the viz that she puts together is the color change at the top
that makes it look like someone took a bite out of the visual.
And she does it with some cool sort of random number generation with a particular seed,
as well as the cumulative sum function, just really sort of brilliant code to create what
looks like someone taking a bite out of the corner of the visual.
So I highly recommend you checking it out, not a ton of ggplot code, like a surprisingly
small amount of ggplot code to create this beautiful visual.
So I am absolutely excited to test it out myself, and I would encourage everybody else
to test it out themselves and tweet their results.
Yeah.
And the key part is that there were no custom other programs to help with that visual, right?
That was all in ggplot2, all in R itself.
So yes, another win in the ggplot2 notch, if you will, for creating infographics that
you would never guess were produced by R. Another fantastic visual.
And yes, even his top lists are also fair game here.
Really, really great read.
And yeah, if I turn loose on that set of code for the R scripts I made for my dissertation
compared to now, I don't even want to know.
Oh, gosh, no.
I've not looked at that code for many, many years, but it's on my hard drive here in the
basement somewhere.
I don't have the guts to look at that, but it would be a fun exercise, nonetheless.
Keep it tucked far, far away.
Yes, yes.
No one's going to hack that little mess over there, so thank goodness.
Well, speaking of little hacking, if you will, our last highlight talks about one of the
Mike's and I favorite topics, and that's Shiny development, of course, and how you might
be able to do a slight bit of hacking, but yet make a huge improvement to your app quality.
And what we're talking about here is that out of the tin, Shiny and its related package
ecosystem comes with so many features out of the box.
Of course, we have the huge selection of input widgets.
We have reactivity.
We have these great wrapper packages to give you new UIs, new ways of interaction.
And case in point, one of those being BS4-, one of Mike's and I's favorite packages that
create dashboards that look so professional, so polished.
So of course, shout out to David Grange and for making BS4- and the R interface suite.
But what if you're using that and you're doing what we may call a client-side interaction,
but you're still losing a bit of what happened in that interaction to the server side.
That's where you might be able to plug that hole with just a little bit of custom JavaScript.
And that's what for the third episode in a row, returning to the R Wiki Highlights, Albert
Rapp has another awesome blog post on how he enhanced one of his Shiny apps that was
serving a dashboard with JavaScript while fully admitting he is not a JavaScript expert.
And if I had known this, well, I kind of knew this was always possible in my early days
of Shiny, but I felt scared about it.
So if you've ever been intimidated about the idea of custom JavaScript in your apps, you
definitely need to read Albert's post here.
This is a terrific example of one of the features of BS4- when you have these little cards or
boxes in your app and then letting the user determine the order of them, you just click
and drag it around like you would anything else on a computer interface.
But the issue was the text that was inside these boxes was not being preserved in that
new order that the user did through this rearranging.
How do we get that out?
And that's where he took the moment to play around with a little bit of JavaScript in
the developer console, to me, for all the 80s geeks out there, going into the JavaScript
debugger console and like Chrome or your browser of interest is like my favorite movie of Tron
when Flynn goes into the game to hack in the MCP.
Yes, yes.
There's a 68.71% chance you're right.
But it's not so intimidating once you know what to look for, what element you need to
get, then the rest of the post goes through just a little bit of JavaScript code to grab
the contents of that text input from that card and then even do some more iterative
programming to get those values and a map like framework.
And then the hook, of course, is to make that manipulation available to your Shiny app on
the server side.
So in essence, he's made a custom input that he can observe upon or put in any reactive
or other construct to get that new ordering of the text that's available in those boxes.
Obviously, this is a post you want to read probably a couple of times if you're new to
this.
But the way he outlines this from the investigation, harnessing on the inputs needed or the text
inputs needed, and then bringing that back into Shiny, it's a great use case for just
how easy it is to get started in this, but it gives you that little seed of, I can take
this much further in other situations where I don't have an R package.
I don't have a built-in Shiny app function that will do this for me.
It's a great way to know kind of the inside of how Shiny works.
So again, great, great post by Albert here, and I really enjoyed reading it.
Yes, Albert just keeps coming back with fantastic content, a lot of data viz, but this time
it's around Shiny and JavaScript, which is just incredible.
If anybody knows what kind of coffee he drinks or energy drink he likes to drink to be this
productive and pump out this incredible RStats content, please let me know because I am going
to buy as much of it as I can.
You covered a lot of sort of the problem statement of what Albert was trying to accomplish here
with reordering these text area input boxes using the sortable function in bs for dash,
which is really, really nice that allows you to drag and drop different elements, but wanting
to get out the user's input into those boxes in the order that the boxes were dragged around
in, and this required some JavaScript.
One thing that Albert highlights and shows is how to play around with your web browser's
console by essentially right clicking, clicking on the developer or the inspect button in your
browser and then navigating to that console area and being able to actually have a blinking
cursor that allows you to enter in some particular command and get a response from the browser.
Really, really cool, something that I have not done enough of to be honest, so this was
a great introductory post for me and for anyone else who is looking to maybe get their hands
dirty with a little bit of JavaScript and getting into the developer portal in your
web browser, which was a really nice introductory way that Albert went about explaining this
in his blog post.
He talks about a few different ways to incorporate JavaScript code in your Shiny app, and two
of them involve the ShinyJS package, which is I believe a Dean Attali special, did I
get that wrong?
No, you are exactly right, Dean Attali has been one of my MVPs of my early Shiny career
and still to this day I use all of his packages in one way, shape, or form.
Absolutely, I do as well, so there's a couple ways to go about doing that.
You can just define your JavaScript code in a long text string and include your JavaScript
code via text variables with the ShinyJS package.
You can read your JavaScript code from a particular file using the extend ShinyJS function, or
you can incorporate JavaScript code actually without ShinyJS in a couple different ways.
If you have a particular button, you can set the onclick attribute of that, you can sneak
the JavaScript code into your app by placing the tags, dollar sign, script into the UI,
that function to the UI, and you don't necessarily need ShinyJS to do that, so there's a bunch
of different ways to go about accomplishing incorporating some JavaScript into your Shiny
app.
Typically, the way that I have in the past used JavaScript is really with visualization
libraries like Echarts for R or Reactable, they allow you to write a little bit of custom
JavaScript to do something beyond what those packages offer in just their R functions,
which is really nice, but I think going the next step will be a big deal for me here to
actually write some custom JavaScript outside of one of those packages and include it in
my app, and what a really nice use case that Albert had to showcase how to do this in a
few different ways.
Yeah, and if you are inspired by this exploration, like I certainly was, and you're wondering,
hey, where can I go to kind of get more ideas of what I can do?
What's the potential here for me?
We'll have linked in the show notes two excellent freely available resources.
We have David Grangin's outstanding Shiny user interfaces package, which is also available
online for free.
And great chapters on JavaScript interaction with Shiny apps, and then John Kuhn, who of
course is the author of Echarts for R, shout out to John.
We have a great link to his JavaScript for R book.
That's another great way to learn about the potential here.
So certainly you can go down quite a few rabbit holes here, but I think they're more than
worth it, especially when you get into situations like me where it's not just this app I'm making
for a handful of people, it's going to be for executives or it's our key leaders that
want the best they can get out of a user experience.
So really awesome, awesome posts, Albert.
And yeah, as Mike said, whatever you're, whatever you're consuming or in your routine, they'll
crank all this out.
Send it our way, please.
We'd love to have it.
Yes, please.
It's awesome.
And what else is awesome?
Well, of course it's the rest of the issue of R Weekly.
There's a ton of awesome content that Jonathan has put together for us and we'll mention
our additional finds here.
Now of course, for me, in this episode, you've heard a lot about the tidy verse for good
reason of course, but that's not the only verse in this episode.
I want to give a well-deserved shout out to my fellow Life Sciences R enthusiasts who
have spearheaded the Pharma verse, which is a true testament to the power of collaboration
and open source that's putting the power of R into generating clinical results and data
processing.
And what we have linked is a great post from the Posit blog, which is a summary of how
the Pharma verse started, which has some great times at the R Pharma Conference and how it's
just become a huge and important part of the full story of how we're getting together in
our industry to make things easier to cooperate together instead of in silos trying to do our
own version of these clinical reporting or processing needs.
Certainly ways to go, but it's really exciting to see the Pharma verse really take a lot
of foothold into what we're doing in Life Sciences and also a teaser for an upcoming
hackathon for one of the key packages called Admiral that's happening in January.
We'll have a link to that in the supplements as well.
Micah, what did you find?
There's a great post called Setting Up and Exploring a Larger Than Memory Aero Table.
And it is all about handling larger than memory data with Aero and DuckDB, which are two of
my new favorite tools for essentially handling anything related to ETL and large data, if
you will.
There's been a ton of buzz lately about DuckDB as well.
I think some of the Aero, Buzz Aero has been around a little bit longer, so maybe it's
died off a little bit, but I think it's still a fantastic project that I use all the time.
And we are now starting to see that there are intersections of both of these packages
and both of these technologies that we can leverage to make our queries even faster and
make our data prep even faster, which is really incredible.
And the authors really do a great job of showcasing the different advantages of using Aero, DuckDB,
or both in your ETL pipelines.
They run 100 trials of some code, some simulation code, and plot in a really nice ggplot the
time that it took to run that code with different combinations of Aero and DuckDB.
So I highly recommend checking out this post if, like me, you are interested in the most
cutting edge data manipulation libraries and technologies that we have available today.
Yeah.
Aero and that ecosystem is something I want to pay a lot more attention to in my upcoming
efforts in 2023 for some really complicated and voluminous data processing.
So I'm really excited to see that tutorial as well.
Yeah.
Excellent.
Excellent find there.
And for feedback to the show, I want to offer a little correction from last week.
And when we were talking about that great tutorial about using Rselenium for web scraping,
I admit I was under the impression that the Rselenium package had found a new maintainer
because I had seen a grand release very recently in the fall.
That's actually not the case.
Rselenium is still looking for an active maintainer.
And hopefully we will have somebody in the community step up and take reins of that very
important package.
So I want to give a great thank you to Cohen Huffkins on Macedon for letting us know about
that.
So, yep.
So if you're interested in Rselenium and taking reins of a very important package, certainly
get in touch with the GitHub repository for the package.
There's an issue that is looking for an active maintainer.
So again, thank you, Cohen, for that feedback.
We don't have any boosts this week, but if you're interested in sending a little love
to the show, if you're getting any value back from listening to this, you can send value
back to us in any way you like.
But you can do that easily with a new podcast app, which you can get at newpodcastapps.com
and send us a little boost and have a little fun with us.
As for where you can find us, well, we have R Weekly available on Macedon with at R Weekly
at Fossedon.org.
And also you can find me still somewhat on Twitter with at the Rcast and also on Macedon
with at our podcast at podcastintex.social.
And Mike, where can they find you?
Yes, you can find me on Twitter, still hanging around at Mike underscore Ketchbrook, or you
can find me on Mastodon at Mike underscore Thomas at Fossedon.org.
Thank you.
Yes.
One of these days it'll become natural for us to say this with repetition at will, of
course.
But yeah, please get in touch with us and again, we're always happy to hear feedback
or corrections or suggestions.
Nothing is off the table for us who want to make this podcast the best for all of you
out there listening.
Well, it's been a lot of fun as always, Mike.
And again, really enjoy seeing the new hardware.
You're a serious podcaster now.
That's awesome to see.
Absolutely.
Absolutely.
You know, it only took me 50 episodes or whatever it's been so far, but looking forward to what's
to come.
You bet.
Yep.
We've got a fantastic year, I'm sure coming up in 2023, but we still got to see how to
finish up 2022.
So that means that we will be back with episode 104 of the R Weekly Holidays podcast next
week.
"
"116","issue_2022_w_48_highlights",2022-12-01,28M 25S,"A new approach to adding package tests with {doctest}, scraping data from dynamic web pages with {RSelenium}, and a simple checklist to power up your next bar chart. This week's curator: Tony Elhabr - @TonyElHabr (https://twitter.com/TonyElHabr) (Twitter) & @tonyelhabr@skrimmage.com (https://mastodon.skrimmage.com/@tonyelhabr) (Mastodon) {doctest}…","Hello friends, welcome to episode 102 of the R Weekly Highlights podcast.
My name is Eric Nance and we are closing up the year rapidly approaching, but we're not
closing up our awesome looks at the R Weekly Highlights for the week.
And you know, can never, ever, ever do this anymore without my awesome co-host, Mike Thomas.
Mike, how have you been today?
I've been great, Eric, trying to balance working on one screen while watching the World Cup
on the other screen and so far going pretty well.
Yes, I was watching some World Cup action at my kid's practice yesterday for the USA
game when you're listening to this and a lot of people are glued to the TVs on that one
while kids are randomly skating in figure eights around the ice.
So yeah, interesting atmosphere nonetheless.
Good stuff and yeah, if you're enjoying the World Cup out there or you're more enjoying
the R content, we have a little bit of everything today as we always do.
And our issue this week has been curated by Tony Elhabar, one of my great colleagues
on the R Weekly team that I met in person at RStudioCon earlier this year.
Lots of fun conversations there.
And as always, thank you to Tony and also thanks to all of you around the world for
your poll requests and contributions to R Weekly and for our fellow curators.
So let's dive right into it.
So I've always felt spoiled enough as thinking about developing packages just by the existence
of reliable and time-saving packages such as Use This and DevTools that eliminate so
much of the manual effort that I've had in pain points when I created our package from
scratch.
Now, even with those powerful helpers, we are starting to see, I would say, some trends
in the last year, especially as we've been doing this podcast, more examples of additional
packages in the R ecosystem that can even put a little bit of extra on top, little extra
bit of help that takes this experience to another level and give you less excuses to
potentially put off that extra bit of development that maybe you were dreading.
Case in point, test.
Yes, I said it, test.
Sometimes in my past experience, the thing I wait till the very end to bolt onto my packages,
I admit I'm being honest here and I'm trying to be more prospective on it.
Now, of course, we're very familiar with test that.
That's certainly not going away.
But there's another interesting aspect to how you could get those tests generated while
you are also following a good practice of developing your examples of how your functions
work in a package.
And that's where our first highlight comes in with the doc test package version 0.1 that
was just released by David Hugh Jones, who's associate professor in the School of Economics
at the University of East Anglia.
And what's interesting about this is that if you've been following best practices with
the dev tools and the like, you've likely been writing your documentation of a function
with our oxygen tags, where you put the little at param and the description of the parameter
and all that.
Well, what doc test gives you is another oxygen tag to start building in your test cases.
That seems kind of mind blowing to me, but it actually works.
I don't know.
Are you as amazed about this as I am?
There's a lot of magic at play here, and I was absolutely blown away by this package.
I guess I didn't realize that other languages like Python and Rust already have this built
in.
But as you said, doc test enables you to write code and tags in your oxygen documentation
that automatically generate unit tests.
It's incredible.
This package doc test that isn't on Korean yet.
So install the dev version from GitHub and give it a whirl, but the GitHub page within
the read me as well as the vignettes and the package down site have some great examples
that make it pretty clear how exactly you would do this.
And it's pretty straightforward.
These tags that you would include look a lot like the tests that you would write in a test
app.
I see myself using doc test as a huge efficiency gain tool for functions that have a couple
simple tests, perhaps.
I'm not sure I'm entirely sold on using it for functions that have more and especially
more complex testing logic where I'm having to do some sort of random sampling and ensuring
distributions or things like that in my test.
But it seems like that could be difficult to do entirely with just these oxygen tags.
But if I'm not mistaken, I think I could use kind of a hybrid approach and only use doc
test on the functions where I choose to use it while going maybe the traditional or manual
testing route on other functions and not including the oxygen tag doc test, but just writing
up the full testing logic within my own test that file.
So pretty incredible.
Nice to see that this is covered in the highlights, again, version 1.0, 98% code coverage.
So in a testing related package, it's great to see such good code coverage.
And I'm very excited to watch this and see where it goes.
And I will absolutely be one of the users pulling down the dev version here from GitHub
and playing around with it and seeing how much efficiency I can gain here.
And it definitely encouraged our listeners to do the same.
Yeah.
And one great point you made is that this is not an all or nothing thing, right?
I mean, you can definitely, the way I view it is similar to you, my package will likely
have some utility functions that are definitely more nimble or a little less big in scope
where your tests are going to be written pretty concisely.
And that's where doc testing can be a huge help there.
But then if you have a more sophisticated function that maybe is doing an API call or
anything like that, you can still opt to do it the quote unquote old fashioned way with
test that you're not confined to doing all of one approach or all of the other.
And that's a microcosm of a lot of the way these package helpers work.
You can take the bits you want.
Like I don't have to use the wrappers to generate data for my package, but I want to maybe from
use this and the like, but this, I think there's going to be cases where you will get overwhelmed
pretty quickly for a more sophisticated code around your test case.
But I think again, having another easier entry point to a best practice is stuff that really
helps you as a developer, especially new to package development, make that leap a little
bit easier.
And then as you get more experienced and you can choose to stick with it, you can choose
to go full bore test that or do the hybrid like I'm talking about.
So I think for my next internal package I make, I'm going to take a hybrid approach
with doc tests as it matures and hopefully gets a grand release or our universe release
and in their future and then I'll start using that in bigger projects, but certainly great
time to play with it.
Great time to get feedback as always.
And that's of course the virtue of open source with having all these available at our disposal
for lots of use cases.
So really well done.
Hopefully this will get some good reception and certainly happy to see it.
Absolutely.
There's a couple of nuances.
Maybe I'll throw out there that if you want to use doc test in your package, should alter
your package's description file to add the DT underscore Rocklet to Roxygen as well as
add doc tests to the suggests as a dependency in your package.
That's what's recommended here.
So as you get started with this, maybe keep that in mind as well.
Very good.
And that's always seen magic to kind of the things you can put in that description file
to just kind of instruct how a package imports these resources.
That's something I'm going to try and learn about next year because I've seen this many
approaches with just our oxygen in general, but even other things like even RM has some
flies you can put in there too.
So that's another rabbit hole that I'm going to go down one way or another next year for
sure.
Yes.
Absolutely love the reminds me of the attachment package that is used in Gollum to amend the
description files with the dependencies that just get read from parsing the code within
your package saves me an outrageous amount of time.
Oh yeah.
You can, you can never quantify the, the awesome savings you get for, you know, taking away
a lot of this manual effort.
And another way you might be able to take away manual effort is we mentioned many times
on this podcast before, but you know, it's 2022 those, that data you want is probably
not going to be on the CSV file all the time.
We're seeing a rich amount of resources of data available on the web.
And one way to get that is for some good old fashioned scraping.
Scraping is the art of using either programming or tools or a combo of both to take what's
being served on a website.
Maybe it's a table of data.
Maybe it's a list of things, massaging a little bit, importing it into your favorite language
and crunching out your data science skills with it.
And that's where our next highlight comes in, where you might have a case where the
data is not just being served in a quote unquote static way.
You might think of a page on Wikipedia, for example, that has like a little table there
of maybe, I don't know, country flags or whatever have you.
And you can use traditional packages like HTTR or Rvest to interrogate that.
Or if it has an API, that's definitely a great approach there too.
But you don't always get that sometimes.
Sometimes those pages have hidden away behind that glitzy little table they serve up on
the webpage, some JavaScript magic to pull that data in.
That's where you got to go a little bit outside the box.
And that's where a package called our selenium comes in.
I've had a lot of experience with this, so I'll be sharing my thoughts shortly.
But what we're talking about here is a great slide deck made by Etienne Bakker.
Hopefully I'm saying that right.
A PhD student from Lisser-Lunksenberg with a really comprehensive tutorial about not
just what our selenium is, but why you would want to use it and maybe a couple pitfalls
along the way.
So maybe you could dive through this, Mike, what did you learn from Etienne's presentation
here?
This is a really holistic blog post covering just about every aspect of web scraping in
R that I can think of.
It's actually not a blog post.
It's a slide deck that appears to be made with Kordo.
It's beautiful.
There you go.
Kordo all the time now.
Yes.
So I always appreciate any sort of educational content that doesn't assume you already have
everything installed.
And Etienne calls out the fact that you may have installation issues that you have to
hurdle before you can get going with our selenium.
And he does a really nice job of explaining the different workarounds.
You can try to troubleshoot any of the issues that you run into around ensuring that you
have JavaScript installed on your machine, ensuring that you have administrative access
to Firefox, if that's the browser that you decide to use for automated scraping purposes.
Like you said, Eric, if the site is static, our vest might be all you need in terms of
packages.
But if the site is dynamic, where it does have those JavaScript bells and whistles,
you are likely going to need our selenium to interact with the page.
And you can do just about anything with our selenium that you could do manually in terms
of scrolling down a page, going back or forward on a site, clicking on a button or a link,
editing a text box, adding text or navigating, interacting with a particular widget on that
page.
So it is pretty incredible.
It does feel like magic if you've never done any web scraping before with our selenium.
It does feel like magic that you are programmatically interacting with the web page.
So this slide deck is loaded with excellent examples and use cases involving both static
and dynamic sites, scraping both static and dynamic sites.
There are even links to additional blog posts around web scraping with our selenium, particularly
some around parallel scraping that I saw, like utilizing multiple browsers simultaneously,
something that I've never tried before, but I do have some long running scraping jobs
that might be interesting use cases for taking a look at parallel scraping.
And one of those, I will shout out one of those links is to Ivan Mullane's blog post
from 2020 on web scraping with our selenium and our vest.
Ivan is a very good friend of mine and a brilliant art developer.
And we will link to that blog post as well in the show notes.
But it's really nice to see Eddie and not only including a ton of fantastic content
that he developed himself, but also referencing some of the other folks in this space who
have written educational content on web scraping with our as well.
So it's definitely a resource that I think belongs maybe in the big book of R or belongs
wherever you store important R resources that you come across beyond just your mastodon
or Twitter favorites button.
You bet.
And also with our selenium itself, it was kind of recent, maybe a year ago or so that's
actually been folded into our OpenSci as well.
So it's being actively maintained, there was a period where it was a bit stagnant, but
now it looks like active development is back.
And yeah, I'll embellish a little bit on what I teased earlier.
I have definitely explored selenium quite a bit as a precursor to like what we know
now with shiny test, a way to test shiny apps and the tests of our web things I was creating.
I admit getting selenium working in an enterprise environment is painful, at least it was for
me.
Now, one thing that could help though, you may remember I've been harping on a certain
technology that can abstract away a lot of these niche pain points you might have.
You can put selenium in a Docker container folks and not have to deal with the Java nightmares
that I had to deal with years ago.
So I will also in addition to what Mike you asked to link in the show notes, I'm also
going to link a great tutorial from Callum Taylor on putting selenium in Docker and then
using our selenium to call that instead of a local install.
That could be a big help to those of you out there that have shared the pain I've had with
getting Java stuff working, especially on Windows machines.
Not that I don't know anything about that, but you can use containers to your advantage
at least for my case and that made selenium a lot more accessible for me.
I'm going to have to read that post as well because my weekly GitHub action that has been
running with selenium flawlessly for the last six to nine months broke this week because
there's a GitHub actions update from Node.js from 12 to version 16 and I am struggling
with figuring out how to fix it if you can't tell, but I'll leave it at that.
There are so many rabbit holes where you go on GitHub action failures, especially when
dependencies are updated without you knowing it.
You know me, I like to have these, you might say these mental checklists of things I have
to build up before I launch these efforts.
Well, our last high is going to give us some checklists to think about.
We have a little visit to our visualization corner as always and mostly on these episodes
and in particular, very regular contributor these days, Albert Rapp is back at it again
with the visualization roots, so to speak, about a great blog post that he's put together
on little, you know, might say checklist items that you can have as you're creating that
next fancy little bar chart that summarized in your great data insight.
And so this is a very practical post.
He literally starts the bar chart from literally the vanilla ggplot to aesthetic and then starts
to gradually build both things that make it easier to interpret the result, but also ways
to present the results in a cleaner way.
So what kind of items did you resonate with the most here on this checklist, Mike?
Yeah, well, first things first, this looks like another great quarto blog, shout out
quarto if I haven't done that enough times yet today.
So for everyone counting out there, there's another quarto shout out for you.
Fantastic blog post again, great walkthrough by Albert Rapp on some awesome data viz content.
Horizontal bars are greater than vertical bars.
I said what I said, but especially important when you have more than I'd say five categories,
which I often have more than five categories, invert that access, flip it and let's see
those categories on the y axis, please.
But data viz is all about communicating the data in the most efficient and effective way
possible to the audience and Albert highlights in this blog post, just some great tips for
doing that for bar plots specifically, including bar ordering, taking sort of what I would
call a minimalistic approach and dropping your access labels in favor of data labels
within each bar.
The blog post concludes with a pretty interesting bar plot where the labels for both x and y
values are above each individual bar, something that you're probably going to have to check
out yourself, so check out the issue in this particular blog post because it's tricky to
articulate on a podcast again.
But as always, Albert includes all of the fantastic code that produces each of the charts
that he shows as steps along the way towards the final chart in this blog post.
And I'll note that this is all contained to the tidy verse.
There's no additional plotting packages beyond ggplot2 at play here and that's just another
example of how far ggplot2 can get us in creating beautiful data visualizations.
So kudos to Albert for continuing to just pump out fantastic data viz and our content
for us and this is a really nicely done blog post.
Yeah, really enjoyed reading it and I'm happy to say that even in an unlikely situation,
I followed at least some of this advice when I last year created this completely over the
top shiny dashboard that visualized virtual racing league results.
It started as a bar chart and ended up being an animated chart of little racing cars.
But it was horizontal.
I got that one right.
Yes, appreciate that.
The cars weren't driving against gravity.
Not quite, although they were going backwards.
That's another story for another day.
But yes, but it was fun to build that.
But again, with respect to this post, everything that you said, Mike, is all achievable.
No hacks, no additional packages needed over in ggplot2.
It's a great companion to some of the advice we also see from other thought leaders like
Cedric Schurer and his many ggplot resources.
I think what Albert does here is that this is probably what you're going to start with
as you get into the exercise of, okay, I know how to make a chart, but how do I start adding
these nice features to make interpretation easier, to make presentation easier?
Because again, you never know where these lead to.
Maybe this chart is going to go to an executive somewhere.
So you got to make sure they can quickly see that main point on the plot.
So the little touches here do add up, folks, it really does.
Even when I made an app last year for work where I was doing ggplots of clinical results
to get to an eventual PowerPoint presentation, I would let the user kind of do these little
checklist items if opt into certain things.
But yeah, having awareness of these items and be able to mix and match to your needs,
great flexibility that ggplot2 offers and great job by Albert to summarize all that
up for us.
Yeah, really excellent blog post here as always.
And I don't know how he cranks all this stuff out.
If he's got some magic pull for time management, I need to take it because I'm not able to
crank in nearly the amount of stuff out that he is, but it's really, really well done here.
You're not kidding.
Yep.
And what else we're not kidding about, another really awesome issue of our weekly.
That's a broken record at this point, but that's why we do this podcast, right?
So really great insights here and a little find that I had as I was coming through this,
one that's not on the issue, but I want to call out as well, but I'll get to what's on
the issue here is that when I was preparing for the good old days of graduate or university
school, I would have these tests, right?
I have to memorize certain concepts or it was in stats or computer science and the like.
Does you know that now in R you can create flashcards?
Yes you can with the flasher package created by Jeffrey Stevens.
I was poking around this a little bit, but it is actually built on top of reveal JS to
give you what are kind of under the hood slides, but a great way to test your knowledge, you
know, a little nice thing you might want to do next time you're preparing for that really
hard exam if you're out there in the school work.
I feel you.
I remember to graduate school both fondly and not so fondly, especially for those exams.
So these are the help you're studying.
Why not?
That's absolutely something that I would do instead of just writing out 10 flashcards
in 15 minutes.
I would program flashcards using the flasher package for like eight hours trying to figure
out how to get these beautiful flashcards to work and maybe learn some of the content
along the way.
Yeah.
That that's never the fun part, right?
It's getting there.
Yeah.
And the other thing I want to quickly mention before I turn back over to Mike is our friends
at jumping rivers have released videos of their recent shiny in production conference
and there's about like seven out there at the time of our recording here.
So I highly recommend checking those out if you're kind of wanting to see how others in
the enterprise are using shiny and very important situations, some great knowledge shared by
Colin Faye and others in the community on widget building and comparing different frameworks
or web apps.
Lots of lots of good content there.
But yeah, Mike, what did you find this week?
Yes.
So our weeklies own Tony Elhavar created a fantastic gist on GitHub using HETR package
and some tidyverse packages to leverage FIFA's API and show you how to download player specific
or team specific statistics from the World Cup.
So this was an exciting one as a soccer nerd and data nerd.
This was an exciting gist to come across and I thought it was kind of an interesting unique
entry into our weekly this week where you can get your hands dirty with some code that's
laid out right in front of you.
Very timely, too.
We were talking about scraping earlier.
So another great way to get your scraping hats on.
And luckily for this one, you don't have to worry about custom JavaScript serving those
results.
So your life's a little easier then.
And I wish all these sites would make it easier to access all this.
But hey, at least this one does.
So great, great gist as always.
And also want to give a quick plug getting to our kind of feedback segment.
We don't have any new feedback per se.
But our previous contributor, Rasta Calavera, has actually been boosting the show again,
but automatically.
You might be wondering how the heck is that pulled off?
That's for one of these new podcast apps called Fountain, where they kind of give you a little
budget of these stats to play with.
And just by listening to a show, you can contribute any little bit every few minutes or so.
So thank you, Rasta, for the extra, I believe, 100 sats from that little automatic contribution.
And however way you want to contribute, all you have to do is find yourself on those fancy
new podcast apps at newpodcastapps.com.
And we'll be glad to hopefully hear from you in the future.
But also we want to hear from you for contributing to R Weekly itself.
You know how you can get your content on there?
Well, you go to rweekly.org, first of all, if you don't have a bookmark.
Bookmark it today so that you can get access to the upcoming draft and send us a little
pull requests of any great blog posts, new package, tutorial out there, whatever have
you, any great novel use of data science in R. We're always happy to put that into the
next issue.
So we're always just a pull request away, folks.
And also if you want to get in touch with us at R Weekly, we also have a new Mastodon
account that's at rweekly at fostodon.org.
And also you can get in touch with each of us.
You're friendly hosts here individually.
I am still somewhat on Twitter with at the Rcast, but also I am on Mastodon as well with
at our podcast at podcastindex.social.
And Mike, where can they find you?
I'm still hanging around on Twitter for the time being as well at mike underscore Ketchbrook.
And then I am on Mastodon as at Mike underscore Thomas, let me check my profile, what's the
at fostodon.org.
At Mike underscore Thomas at fostodon.org.
Yeah.
So you can tell we're still getting used to calling these out, but it's going to become
second nature sooner or later.
But we've been hearing from some of you already on these new avenues.
So thank you for the shout outs and certainly stay tuned for more updates in the future.
Speaking of the future, well, we're going to have to close up episode 102, but we will
be back with episode 103 of the R Weekly Highlights podcast next week.
"
"117","issue_2022_w_47_highlights",2022-11-23,31M 14S,"Reshaping your R function syntax with {codegrip}, ways you can apply DRY principles to R package development, and a new online book teaching you how to create beautiful tables in R with {gt} Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) {codegrip} 0.0.0.9000 (https://github.com/lionel-/codegrip):…","Hello friends, we are listening to episode 101 of the R Weekly Highlights podcast.
Insert your joke about your favorite 101 class or lecture, but here we are.
It's a new era with a new series of episodes and I'm very happy to be joined by my awesome
co-host Mike Thomas.
Mike, how are you doing today?
I'm doing great, Eric.
As you said, 101, I guess it's somewhat of a new era.
We can feel like we're starting fresh on episode 1.
Yeah, yeah, all the feels, right?
All the newfound feels and apparently you've got some newfound equipment we may be testing
out next week as well.
So that's another perk for our listeners to stay tuned.
Yes, hopefully my audio quality will be orders of magnitude greater than it was.
So everybody's going to have to listen in, obligated to listen in next week.
Of course, teaser has been unleashed.
And what else is unleashed, another fantastic issue of R Weekly.
We are talking about issue for week 47, you know, end of year is approaching, so that
number 52 is drawn soon.
But this week our issue was curated by John Calder, another longtime member of our R Weekly
team.
And as always, he had tremendous help from our R Weekly team members and contributors
like you around the world.
So as usual, if you're new to the show, we do a little roundup of the highlights mentioned,
we'll give some additional finds and yes, we're going to have our first ever feedback
segment at the end of the show.
So stick around folks, you'll like it.
We've been there.
You finally solved those last bugs in your code.
The deadline is fast approaching for that report or maybe that shiny app deployment.
You quickly save those scripts, you upload them, you commit them to get your Git repo,
whatever have you.
And yeah, you solved the problem.
No more errors in your code, right?
But you didn't really have time to help out future you and future collaborators with your
code readability.
No judgment here.
I've definitely been there, such as maybe having a function call where you have a lot
of parameters and you've expanded that whole line of calls to over 200 columns and you've
got to look at a little scroll bar to keep up with everything.
That's just one example.
Perhaps you have a mix of multiple function parameters in a single line.
Or worse yet, closing brackets that aren't on their own line.
Okay, okay.
I'm being quite opinionated here based on past experience.
But it sure would be nice, especially if you're new to adhering to various style guides for
your R programming to interactively apply quick tidying up of your R code style.
That's where a very new package released by Posit Software Engineer and Leon O. Henry
aims to fit this bill with a package called CodeGrip.
And in its first dev release on GitHub, CodeGrip comes with both RStudio add-ins as well as
Emacs commands, hey, hey, respect to all the Emacs power users, especially those org mode
ninjas, you know who you are, to reshape your function calls.
I'm very much in the camp of the long format style, where you have every function parameter
as a new line and CodeGrip lets you easily break that wide call into a long one.
Now, like I said, don't worry, it's a safe space here, I don't judge.
If you do like that wide format call, you can go from long to wide the other way too.
Again, all your preference.
Now the readme of CodeGrip gives you a few recommendations for keyboard shortcuts, again,
another enhancement to your productivity flow, to quickly move the cursor to the function
parameters even if you have nested ones.
Maybe one of your parameters is a list or a vector of other things, you can quickly
navigate around that.
Now as I said, this is still quite new, so Leon O. shares a roadmap at the end of the
readme on the GitHub repo for additional features, such as reshaping expressions inside those
curly brackets, React isn't shiny, say hello, and reshaping repeated function calls, which
you might see in packages like DT or some of your dplyr pipelines.
So very intriguing, and you may be wondering, wait, aren't there packages to help you style
your code already?
Well, yes, there are, certainly there's a styler package amongst others, but what I'm
intrigued about CodeGrip is it's kind of a way for you to see how this is applied by
opting into it, but in an interactive way.
I think that's a really neat way to learn how different styles of our function writing
could work.
So again, I think there's always a space for everything here, so I'm really intrigued by
where CodeGrip goes in its future development.
So Mike, we're going to have, I'm going to get your opinion on here.
Are you a long or wide function writer?
Well, I think I'm thinking about, if it's any indication, I'm thinking about submitting
a pull request to CodeGrip to only allow from conversion from wide to long.
Nobody needs to go from long to wide.
Just kidding.
We absolutely do not judge, and it's, if you have a style guide that you're adhering to
by all means, but ease of code interpretation in grokking the code that you wrote or someone
else wrote is something that I've seen taken for granted too often, unfortunately.
And not only does this apply to commenting your code or structuring your repository,
but it also applies to styling your code and styling your code consistently is an important
aspect of any project and any team, especially for a collaborative project and avoiding technical
debt in your organization, but even just for future you as well, to be able to step back
into the project quickly when a new issue or feature request arises down the road.
Take a look at the code that you wrote six months ago, a year ago, and grok it really
quickly.
I highly recommend adopting a code style guide within your team.
There's a whole entire tidy verse style guide ebook, which we will link to in the show notes
already linked to in the show notes for Eric, as well as other resources like Google's our
style guide, which is just an adaptation of the tidy verse style guide with some minor
adjustments.
I might even imagine that there are some folks out there who like me a few years ago have
never even heard or thought about the concept of styling your code consistently.
So this blog might be a great place to start and this repository might be a great place
to get you up and running quickly, especially considering the fact that there's just a nice
add-in that you can use within the RStudio IDE.
This is going to be maybe a hot take, but I think as good data scientists, a ton of
our job is not just about writing code, but it's about thoughtfully designing all of the
pieces of the solution that we're creating.
And one of those pieces is the code that we write.
It's a thing.
It's heavy.
It can be really hard to style your code consistently during the development process when you're
just trying to get your fingers on the keyboard as fast as possible and trying to connect
what's in your brain to your script.
But it requires you to stop and spend some time with each piece of code you write before
moving on to the next piece of code if you are really doing this thoughtfully and correctly.
But tools and packages, fortunately for us, like CodeGrip, can be monumentally helpful
in making the code styling process more efficient as well as keeping our styling consistent
across scripts in our repository or even across different repositories that we have within
our organization.
So I absolutely can't wait to try it.
I think it's a great complement to the other styling tools that we have in our ecosystem.
I cannot claim to be an Emacs aficionado or expert, so I will be using the add-in for
now.
But if you can't wrap your head around exactly how it works just based upon us talking on
a podcast audio, the repository has a bunch of great information as well as GIFs, animated
pictures, which to me, in terms of documentation, are just chef's kiss.
Can't get any better than that, showing you exactly what the package can do.
Yeah, well said.
And one of the biggest challenges I've had is not only dealing with me personally, but
when you get collaborators on a project, getting them to buy into a style guide early on can
save you a lot of time in the future.
But again, it's great to see CodeGrip and the others in the ecosystem can help you rescue
that maybe before it goes too far.
But it's a disciplined thing, trust me.
I know I had a very big project a year or so ago where multiple collaborators jumped
on and I wasn't on the pulse with the style guide right away.
And yeah, reading that after various code reviews, I was like, yeah, I should have done
that a little better.
But hey, better late than never.
So really great package in this.
And yeah, I love the idea of showing off this interactively.
And by the way, Mike, there's a little side comment here, but there's always been a debate
in the tech sector about how to pronounce, is it JIF or GIF?
I just learned from one of the podcasts I listened to on the Linux community that it
is JIF because the programmers said choosy programmers choose JIF.
There's a shout out for some of you that grew up when I did.
It's good to know because I said GIF up until literally today.
And I was very resistant to adopt the soft G JIF for whatever reason.
But I feel like at this point, I have just heard it way more than I've heard the hard
G. So I'm going to convert today's the day.
You know what?
Yeah.
If we can't provide any other service on this podcast, we have put that debate to rest.
So you can thank us later.
We'll tell you how to thank us later.
Moving right along.
You know, while we're on the topic of being nice to future you and future collaborators,
now one of the most productive techniques for assembling a set of related R functions
and over-processing is to bundle them into a new R package.
We've had many highlights in the past that provide a really practical introduction to
taking that next step in your dev journey, especially if you're new to that world.
But it is easy, again, to speak from experience here, to fall into a few habits that can prolong
your development time or debugging efforts, such as that dreaded copy and pasting of a
function or other processing code many, many times.
Here to keep your R package development nice and dry is Indajit Patil, a software engineer
with the Syncra R consulting firm with a jam-packed presentation on how you can apply dry or DRY
principles, however have you, which means don't repeat yourself, in your next R package.
There is a bunch of great recommendations.
So Mike, why don't you take us through some of the ones you read about in this highlight
here?
Absolutely.
There is a wealth of knowledge within this slide deck.
I have immediate takeaways that I am going to literally use in my work today.
The slide deck kicks off with a quote that says, copy and paste is a design error.
It stopped me in my tracks, really spoke to me in my pre-data science days.
I used to copy and paste so much, and nowadays I think I audibly fuss any time that I have
to copy and paste anything.
Fortunately, I don't do that very often, so shout out to Functional Programming and R
and Shiny Modules.
But for whatever reason, whenever I do find myself copying and pasting, it's a last resort
for me.
It might sound silly, but just the way that I've wired myself up to this point and tried
to ingrain DRY, dry principles, in everything that I do.
For whatever reason, I had never thought about using child RMD documents or markdown documents
in a package.
I've used them many, many times in building a report, and that's one of the pieces of
advice that Indrajeet gives in this slide deck, so that you can use the same markdown
document across your readme or your vignette, or at least the same piece of markdown that
you can use, because maybe there are some things that are different that you might want
to represent in a vignette and you might not want to represent in your readme or vice versa,
but probably there is going to be some overlaps and things that are important enough to document
in multiple places, and a child RMD document is a fantastic way to do that, so that when
you do sort of stitch your package together, knit your package together, you only have
to make changes to this overlapping markdown in one place, and that's really the theme
of this slide deck.
I love the links to packages that use these DRY principles in their structure.
I see the DM package referenced quite a bit in this slide deck.
I think it makes sense because Sincra is the author of that DM package, so it looks like
maybe Indrajeet had a hand in working on that DM package and applying a lot of these principles.
I'm not sure, but it's a great reference.
If you haven't used DM before, even just to check out the package down site that it has,
it's phenomenal in terms of the material that's available there for documentation around that
package.
Repeating yourself is not only inefficient in terms of the time it takes to make the
same change in multiple places, but it's actually dangerous due to the increased likelihood
that you'll forget to make a change in one of those many places required.
Modularizing your code, functional programming, and taking a DRY mindset are key to any production
grade code base.
There are fantastic examples across unit testing, including data in your package within this
slide deck.
I don't know what other... There's a lot to pull out here, Eric.
I don't know if there are any other pieces that you want to specifically highlight from
this slide deck.
Oh, yeah, there's a bunch, but the one that got this image in my mind of why the heck
did I know about this sooner is a package that was new to me called Patrick to actually
help parameterize the unit testing calls that you make with test that.
What do we mean by this?
Well, imagine you're building a unit test.
You've opted into that great practice, but you find yourself doing the same kind of either
expect equal or expect whatever expectation you have.
You're doing it over and over again, but all your changes may be like one function parameter
or one other result.
Well, this Patrick package will let you do a tibble of these different, you might say,
invocations going into that expect and what you want out of it so that you can have this
very elegant little metadata frame or tibble that is called then to apply one of these
expectation functions to.
So again, a great way to save you both keystrokes, but also again, potential errors you might
have and copy and pasting that over and over again.
So next time I build a pretty sophisticated unit test suite, I'm going to be looking at
this Patrick package quite a bit.
And then the other aha moment for me is that if you find yourself doing a lot of conditional
messages, if there are errors or warnings that you want the user to use and you find yourself
again repeating that general syntax of it from time to time and maybe varying like what
function it comes from or what parameter triggered it, you can actually create a list in your
package of these more utility kind of functions to call that message and only vary the things
that you need to vary.
So a nice hybrid of things like perhaps the glue package to help you stitch this together
and everything like that.
So that again, you're saving keystrokes, but you have one source of truth for how you're
sending these messages, which to me is the real nugget here, having one source of truth
for a lot of these things so that the repeated calls can build from that instead of multiple
sources of truths that just happen to be the same thing.
So that to me really, really enlightening, especially when you think you've solved a
lot of these issues, but these are the issues that don't really manifest themselves in any
adverse way other than costing you time.
And if you can find some time savings so you can concentrate on the harder problems that
you're trying to solve in your package, opting into these dry principles I think is a big
help.
So yeah, lots of great tips here and definitely a must read, I think if you're building any
kind of sophisticated R package, whether it's for your organization or you're going to release
that out to the CRAN ecosystem, really nice job here.
Absolutely, no, and I just had on the topic of conditions, especially, I just had sort
of a, I don't know, enlightenment idea of, especially if you're, you have maybe a lot
of functions, they use the example in the slide deck, where you're expecting that X
is positive, for example, and you have four or five different functions that you need
to meet that condition in the function.
Instead of writing that separately within each function, you sort of write that condition
within its own function.
You only have to write one test against that condition function, that logic, instead of
having to write four or five tests for that same condition across the four or five functions.
So it's actually going to make your code base a lot smaller.
And that's something I think that I'm going to use this afternoon.
Very good.
Hey, if one thing we've learned over the hundred episodes we've done prior to this is that
a lot of these highlights are things that you and I both say, you know what, I need
to start doing that today, or I do that for my next project.
I just love learning about this stuff, so love it, love it.
Now I will admit the first couple of highlights are definitely a slightly more developer-focused,
let's put it that way.
Now no episode of our week of highlights is complete without bringing the A game of producing
a visually pleasing product all created within R and perhaps a little help from other open
source utilities.
And so one of the mainstays in our first 100 episodes is back again.
And I was so excited about their contribution.
I shouted him out a week early actually, but I'll be shouting it out properly this time.
Albert Rapp, friend of the show and frequent contributor to the highlights is back at it
again with not just a blog post.
Albert has released an early version of a fully online book on creating beautiful tables
with one that gets a lot of attention in the community these days, GT.
And oh, by the way, the book is produced with Quartle.
There's another callback for you.
And I've been on the record that GT is a package that's authored by Richie Young from Posit
that has some of the best documentation and vignettes I've seen.
So this book is surely not trying to replace those, but what I really like about what Albert's
book is doing here is the approaches to introducing GT through realistic scenarios that you might
find yourself doing, especially if you're new to this world.
And in the case of Albert's book here, it's building a simple summary table, the penguins
data set from the ground up while introducing some nice logical and clean guidelines that
creating tables so that we aren't overwhelmed right away.
Again, we're taking a stepwise approach here.
And then he gets to the fancy stuff.
And what I mean by fancy, imagine being able with GT, able to put in some nice attractive
spark lines, little widgets, icons, whatever have you tapping into packages like GT extras
to really blend all this together.
So again, a very nice case approach to building these tables is a fantastic read.
And I dare say Albert has big plans for this book.
I haven't inkling what they might be, but I'll let the reader find out later on.
But this is a great resource.
So if you want to have a great introduction to GT, this is your spot to do it.
What'd you think about this book, Mike?
I don't know how many times we've said it, but Albert Rapid is at it again with some
more very timely RStats content and seen a lot of ggplot data viz stuff from him.
And this time we're doing tabular data viz instead, which is really cool.
The book starts out, like you said, Eric, with a really nice walkthrough of creating
just a beautiful GT table from start to finish with Palmer penguins.
That table includes grouped headers and group rows, summary statistics underneath the data,
formatting, introducing background, color and text color.
Really, really nice sort of case study from start to finish.
Then he employs the GT extras package in that section that I think is literally called fancy
stuff, as you said.
One of the highlights of that section for me is actually the very beginning.
The GT extras package continues to surprise me because I did not know there was this simple
gtplot summary function that you can just throw at any data frame and it'll return a
nice GT table with summary statistics like your mean, median, standard deviation, missingness
statistics for each column in that table and even histograms for numeric columns in that
table.
And it's all in this really beautiful format.
And like you said, in that fancy stuff section, it showcases how GT extras brings in the ability
to embed spark lines, images, ggplots that you can create, you can put right in a row
in your table and more.
The rest of the book walks through formatting, styling and more case studies.
And I think although it is still a work in progress, overall, this book is a fantastic
new resource that we have at our fingertips.
So I'm all for the more the merrier in terms of what we have around GT and documentation
and vignettes and case studies and examples.
So I think this is going to be a really nice resource and excited to see how it grows.
Absolutely, and speaking of tables, just a quick little mini plug is that if you're thinking
about entering POSIT's 2022 table contest, the deadline has been extended.
So I believe it's been extended to December 6 or so.
It'll be in the show notes regardless if I get that wrong, but that's definitely a great
time to use GT or any of the other wealth of amazing table packages in R.
It's December something, I know that, so you have a few more days.
Yeah, maybe after you eat all that turkey, if you're here in the States and you need
something fun to do besides watch football, maybe make a table.
I'm just saying, I'm just saying, you know, what else would we like to say?
Another fantastic issue of our weekly, there's way more than what we talked about here.
So as always, Mike and I take a little time to cause some cool little finds that may be
in the rest of the issue.
And actually for me, one of the topics that came up during my recent appearance on Rachel
Dempsey's data science hangout last week was advice for those trying to get their foot
in the door, so to speak, with a career in data science.
Now what I'm about to say, I can't guarantee that it will automatically land you that dream
role, but one really neat way to potentially impress an organization in preparation for
say an interview or other screening is to build, guess what?
A custom shiny app as an accessory for your cover letter or other introduction material
in a very neat tutorial authored by Javier Araka-Ditku, hopefully I got that right.
But a great little tutorial from start to finish on how he branded a simple little shiny
app in the style of the organization that he was applying to.
So again, really quick wins, completely with how he put that on his GitHub repo and got
it deployed very seamlessly.
So again, great little way to stand out and as a big fan of shiny, I would never turn
that opportunity down if I was new to this.
What did you find, Mike?
So I found two, I guess that I'll call out in this week's highlights.
One, it's not a tool that I am familiar with, but I'm thinking that maybe some others out
there in the data viz space would be interested.
There is a brand new R package and I believe the package itself is just called Figma and
it is for interacting with the Figma API, which I think Figma is one of sort of the
most popular graphic design data illustrator applications out there, if I'm not mistaken.
So I was thinking of some of my folks in the online data science community, like Tanya
Shapiro or folks like that who do a lot of data viz work at the intersection of R and
other tools might be interested in this brand new Figma R package.
I think the data that you get out of it from this API for any particular document that
you have up in Figma really gives you a wealth of information about each shape on the page,
the type of shape, the location of that shape.
So could be interesting to some folks out there, maybe, maybe not worth checking out.
Then the other one is a repository on GitHub called World Cup 2022, which is some modeling
and simulation of the 2022 FIFA World Cup, which is in full swing as we speak.
And the author of this repository has Brazil as the most likely to take home the World
Cup.
So it's interesting to see how the Cup plays out compared to the modeling and simulation
that was done in this repository.
So great work here.
Yeah.
A lot of eyes are on that tournament right now and I think there have been some major
upsets even already.
So it's going to be an interesting ride to get there and we'll see if Brazil is going
to take it home as these simulations show.
But as a fan of the hockey playoffs, I know that predictions can go out the window quite
quickly.
That's the way sports are for you.
But fantastic read there, great, great finds, Mike.
And before we close up shop here, we're going to bring a brand new segment to the show,
which was kind of teased last week.
We got ourselves some feedback, everybody.
And the feedback is coming as way of a podcast boost.
And this boost comes from, hopefully I'm saying this right, Rasta Calavera via the Fountain
podcast app, which is one of the new podcast apps you could download on your phone or computer,
whatever have you.
They sent us 99 sats with the message, first-time listener, not really in an R user, but I enjoyed
the conversation.
Well, there you go.
Maybe, maybe listening to this podcast will convince you to use R. But yes, if you're
interested in sending your support for the show, all you have to do is get yourself a
new podcast app and newpodcastapps.com and then each of these apps can have a quick and
easy way for you to hit a little boost button to give us a little positive encouragement.
Maybe you like a particular highlight or find that we shout out, you know, any feedback
is welcome.
So thank you again, Rasta Calavera for that kind boost.
And also thanks to all of you for listening around the world.
We've had a great reception to episode 100 last week, and certainly Mike and I are going
to continue this train forward as long as it's on the rails here, so to speak.
And you might be wondering, well, where can you find us?
Well, first about the Rwiki project itself, if you'd like to get involved, it's easy enough.
Just go to rwiki.org, feel free to send us a poll request of a great find that you have.
You have a blog post, new package, great video tutorial, whatever have you.
We're always happy to put that into the upcoming issue and everything's written in Markdown.
Very easy to contribute for all of us.
And also if you want to find the Rwiki project on social media, we now have a brand new Mastodon
account that is at rwiki at fostodon.org.
And also feel free to give that a follow if you want to get to the latest updates of the
Rwiki project.
Maybe we'll put a little shout out for this issue coming out.
We're still working out the kinks of the automation piece here.
But also if you want to find me, I am both on Twitter, albeit who knows how long these
days with at the Rcast, but I'm also on Mastodon with at Rpodcast at podcastindex.social.
So feel free to give me a shout there if you'd like to get in touch.
And Mike, where can people find you?
As of this morning, you can find me at mikethomas at fostodon.org.
So I'm pretty excited about that.
I have zero followers and I am following zero people, but I am excited to grow on there.
So I guess that's the new spot.
I dare say in less than 24 hours, you're going to get some followers there.
Well, hashtag just saying.
We'll see.
We'll see.
But yeah, welcome to the Fetaverse, Mike.
But anyway, yeah, it's an exciting time.
We're going to see a lot of the R community be a part of this and R Weekly is going to
be there along the way.
And so if you ever want to hear the back catalog of our recent episodes, again, just head to
rweekly.org.
We got the podcast linked right at the top of the page.
And again, please give us a follow and send us your shout outs if you like on those social
media accounts.
But with that, we're going to close up episode 101 of R Weekly Highlights and we will be back
with another edition next week.
"
"118","issue_2022_w_46_highlights",2022-11-16,41M 10S,"A major achievement unlocked! In episode 100 of RWeekly Highlights: The new {rtoot} package for collecting and analyzing Mastodon data, using the {unheadr} package to fix broken and irregular column headers, a tour of the apply functions in base R, and creating posters of NBA rosters with R and ImageMagick. Plus a big announcement on a new way to…","Hello, friends. This is episode 100. Yes, you heard that right. 100 of the R Weekly
Highlights podcast. It is amazing that we got here and I'm feeling very happy to share
that with all of you around the world. And of course, this episode 100, I'm not going
to mince words here, would not have been possible without my supremely awesome co-host and joining
me many episodes along the way. Mike Thomas. Mike, how are you doing today, my friend?
I'm doing great. We had last week off because of the RFARMA conference. So it's nice to
have a little bi-week, get some rest and come back strong for episode 100 today. I don't
know if I've really tallied how many of the 100 I've been around for, but it's been fun.
So I appreciate you having me on for however many episodes it's been.
Yes. Well, it's been a true pleasure. And yeah, it turns out when regarding last week,
when you're involved in a very time consuming conference, I'll be it in a good way. Yeah,
it was hard to squeeze another episode, but we're here now. So hopefully we can be more
regular from this point forward. But again, yeah, really happy that we made this milestone
and we're going to have fun along the way like we always do because we got a really
jam-packed issue to talk about. And in fact, there are four, count them, four highlights
for episode 100. So that tells you how supersize it is. And then towards the end of the show,
I'm going to have a pretty big announcement about how all of you can even support the
show in a really fun way. But let's get to the meat of it, shall we? And our curator
this week is Rio Nakagorora, then another longtime contributor to the Our Weekly Project.
And of course, he had tremendous help as always from our fellow Our Weekly team members and
contributors like you all around the world. So I couldn't have scripted, frankly, a more
fitting first highlight in this episode is we're going to dive into a really powerful
new theme that I think brings community right to the forefront of a lot of the avenues of
technology that we're working with today. So many of these popular online services in
say social media, the DevOps sector and other parts of tech are brought to us by a single
entity or a single company. And certainly that can work really well and be very convenient
for us. But it's not always a bright outlook, especially when a huge change in ownership
or future direction takes place for a certain company. Now, I won't be around the bush
any longer. If you've been keeping up on the latest tech news recently, you've heard that
Elon Musk has acquired Twitter after a rather long saga, which had its own twists and turns
that we definitely don't have time to get into today. But the effects of this event
have been pretty widespread. And for a myriad of reasons, many in the our community and
other communities have been on the lookout for kind of a new social media messaging platform.
Well, to put our way back machine in motion here, in early 2016, a recent college graduate
named Eugene Roccho took advantage of a period between graduation and starting an actual
job to put into fruition his own take on what he felt Twitter should be. A decentralized
completely open source micro blogging platform called Mastodon, which is not governed by
a single company. It is a federation, if you will, across multiple members of a worldwide
community. Get to what all that means in a little bit. But there are a few key differences
to be aware of when you're going from the Twitter mindset to how Mastodon works. And
honestly, a terrific summary of this has been written by our weekly contributor, Danielle
Navarro. And we'll put a link to her great blog post in the episode show notes today.
And you know what else Mastodon has? An API, of course. And in almost the blink of an eye,
if you will, David Shoach, the team lead for transparent social analytics in Jesus in Germany,
has authored an R package, appropriately named Rtute, for all of us to interact with on Mastodon
directly in R. And I got to think the inspiration for that name probably came from another highly
acclaimed package that dealt with a Twitter API called Rtweet. And so what is Rtute all
about? Well, kind of the major things that you might expect out of a client that deals
with this kind of platform. Rtute lets you easily grab the various toots, which is analogous
to tweets on the other platform that you have made, perhaps toots associated with a hashtag
such as RStats, grabbing metadata around users, and getting trends that are seen in the various
servers out there. And again, I'm really impressed by this, because apparently from the idea
of this package to its release on CRAN was about a week. That's crazy to me to get something
like this done in one week. That just shows you that with motivation and the tooling we
have for package development, you can get an idea to the masses, if you will, very quickly.
There still sounds like some improvements that are to be had. But the reason this is
so important now is that much like how we're seeing this convergence of the R community
to the various Macedon servers out there, I think the community contributing to a package
like this is going to go a long way to how we can leverage Macedon and its services in
a very powerful data driven way to pave the way for future research or future over developments.
And I'm really excited to see what the future entails. The package is really cleanly written,
really concise code to get our repos out there if you want to contribute with the issues
that have already been identified. And honestly, we'll us on the R Weekly side, we'll be taking
a hard look at this package because alongside this post, I'm happy to announce that R Weekly
is now officially on Macedon. We now have an account called at R Weekly at Fostedon.org.
We'll have a link to that in the show notes, but we will be posting on that account each
new issue release and other fun tidbits or news related to the R Weekly project. So it's
a really exciting time for us and R2 is going to be a very important component of our revised
backend to take advantage of these services to the fullest. So yeah, Mike, when are you
going to get tootin' out with R2?
When you have an eight month old, you just can't take the word toots seriously, but I'm
going to try to do my best.
It'll be tough, I understand.
But I saw some, you know, I really appreciated this blog post and the speed to which this
package has hit CRAN and allowed us to get up and running with the Macedon API is incredible.
One of the ones that piqued my interest from the blog post is a function called get timeline
home, which allows you to download the most recent toots from your own timeline, which
I feel like could be easily spun into a shiny app or something like that, that you could
have, I don't know, running on a Raspberry Pi and a little monitor on your desk that
just sort of shows you the latest statuses of those that you're connected with on Macedon.
I haven't used a ton of the R2 functionality, but it seems like R2 has some really interesting
functions, you know, which means that Macedon has a lot of interesting APIs for doing things
that go well beyond just posting a status, like you talked about getting metadata, checking
out who your connections are, who others connections are.
There's some really great getting started with Macedon blogs that I've seen come out
in the last few days really for learning how to sign up, log in, post statuses, connect
with others on the platform and more, but I think maybe from a bigger picture color
commentary perspective, you know, for us having a place where the data science community can
come together is really, really important.
I know it's been important to me and you career wise and everything that I've seen, you know,
in terms of blog posts in the community, people are really saying the same thing that interestingly
Twitter has really shaped some of our careers, you know, and that's really not an understatement
in the data science community.
Twitter used to be this place, but I think it's going to be interesting how, you know,
the whole server landscape shapes out for the DS community in particular.
I know that there's a few folks out there who are looking at spinning up, I guess, their
own Macedon servers, so it'll be interesting to, I guess, see how all the chips fall there
and I am yet to sign up, but I think it's going to happen today, Eric.
I think it's, I think I'm too late.
Oh, I don't think there's such thing as too late.
You'd be surprised.
Fun fact, when I formally introduced the, or set the first toot from the R Weekly account,
the reception was very positive.
Like I couldn't believe it.
It was, I feel like the time there's a real big inflection point right now.
And again, you hate to see it coming from an event like what's happening as we're hearing
about Twitter having a pretty massive set of layoffs or are obviously our, our thoughts
are with anybody that's been affected by that adversely.
I just want to always look at positives out of this as well.
And the fact that now the data science communities, you know, our communities, you know, the intersection
of this can feel like coming to a platform that's not going to be uprooted by a single
person, a single process, decentralize is a good thing here.
And that's where Mastodon is one of the more quote unquote famous examples of something
called the Fediverse.
It's not just Mastodon folks, there are many tools that are taking advantage of this technology
that's underpinning ways of decentralizing a lot of what we used to think was somewhat
closed wall gardens, if you will, anything like PureTube, like other communication platforms.
It's a really exciting time and even podcasting is getting into this too.
So I'm more to say about that later.
So again, really exciting time and I'm just getting started with it as well.
So I haven't figured everything out yet, but I'm really excited to see what a journey has.
And I think the positive effects that you just said, Mike, that we've had, you know,
with the Twitter communities on with R and data science, I think we're going to see those
benefits and probably even more so in this new era of our social media communication
with Mastodon.
So I'm excited.
I do have friends that have, you know, said sometimes a bit of a difficult transition
trying to figure out how to keep up with the latest.
So there is going to be an adjustment.
It's not a one-to-one replacement, but I think with due time, we're seeing, like you said,
some great resources being written by members of our communities as well as pointers to
resources that are excellent for getting started.
So again, we'll have Daniel's blog posts and the show notes along with others that we think
would be really helpful in this journey.
And so like I said, a very exciting time indeed.
Absolutely. And I do have more faith in the decentralization model for this particular
brand of social media than I have for maybe currency at the moment.
Well said.
Well said.
Yes.
We could have a whole nother rant about that.
We're going to transition here to our next highlight where we always have our callbacks
on our weekly, right, and especially the episodes we've had previously.
And so it's quite appropriate that in episode 100, we have a callback to a very recent episode
where we saw our friends at the TidyX crew give their take on how to tidy up a somewhat
messy data format and illustrate their approaches to handle that.
Well, if you didn't think that was messy enough, our second highlight brings another tool available
for your importing and cleaning arsenal, especially with messy spreadsheet data.
So this was inspired by a recent Our Ladies Chile meetup that shared a somewhat, you might
say spooky Excel data import postdoctoral researcher Luis de Verde Arrigotia.
He shared a novel use case of his very own R package called Unheader.
That's a cool name, isn't it?
To tame the wild issue of extremely bizarre column headers in your data.
So what are we talking about here?
So you could tell this is definitely inspired by some real world situations where the examples
have headers that have a mix of like units of measurement, the variable name, and then
just blank cells somewhere to just delineate spaces between headings and columns.
And this would be extremely difficult to manage without the help of a few packages or in this
case his own package Unheader to tame this in a really concise way with basically one
or two function calls to translate that header into something that you can do something with,
so to speak, where instead of having like three or four levels, if you will, now you
have a header that's clean with like the variable name and then the measurement separated by
underscore.
But then you get back to your comfortable tidy syntax that you can deal with.
But again, for any of you in the trenches that are dealing with this data from like
raw instruments or collaborators, I think Excel and having the fanciest layouts in the
world is such a good thing.
It's not such a good thing for us data scientists, is it?
So Unheader, I think is a really great package to put in your toolbox to deal with these
situations.
And I would definitely have a look at this if I'm in the rather unfortunate position
of dealing with this raw data anytime soon.
But Mike, what did you think about Unheader and some of the great utilities that it offers?
I love this package and this blog post was my first introduction to the Unheader package.
So I'm very grateful for the package and that it hit our weekly this week.
If you're a fan of those like satisfying class of videos, like someone power washing away
dirt to get to the shine underneath, then you are going to absolutely love this blog
post in this package.
And Luis has these beautiful visuals in there.
They remind me of something that like Alice and Horse would put together showing sort
of the messy data at the start with some annotation on the side and some graphics with a little
dog that's pointing out the issues in the data and really, really well described problem
statement and then what he wants to get to through these visuals.
And not only is the Unheader package just a great name, but also some of these functions
within the package, MASH headers, I just love those function names as well.
And it's pretty incredible how little syntax it takes in this package to do quite a bit
if you're someone who has ever had to wrangle these multi-column headers, maybe the cells
in Excel were merged and centered and you have data sort of all over the place before
you actually hit, you have column header data all over the place before you actually hit
the observations in your dataset and the functions that he has written in this package to sort
of concatenate these column headers from different rows together to identify where the white
space and the null values are to make these clean headers at the end of the day.
It's just really beautiful syntax and code and I think it's incredibly useful.
It probably most of the time is going to be a great complement to the read Excel package
or open XLS packages if you are ingesting Excel data that has crazy headers this way
because we're always trying to automate these workflows when we can and part of that automation
is restructuring our column headers and not always just tidying up the actual observations
in the data, unfortunately.
So I think this is another tidy-ish tool in your toolbox to have and very grateful for
Luis to have not only put the package together but the blog post as well.
Yeah, this would have been really handy many years ago when I thought I was going to get
some really clean data from a lab vendor giving us some custom biomarker data but no, their
idea of tidy was way different than mine with some of the most cryptic headers known to
humankind and I had to figure out how do I make sense of it for one and then figure out
how to get it all tidied up.
So yeah, if you're in this situation, Unheader from Luis is going to be a great asset to
your toolbox as you said.
So yeah, tidying doesn't always have to be a chore.
These packages make it a heck of a lot easier and definitely highly recommended to check
that out.
It's a great transition.
You bet because sometimes it is pretty easy to take for granted that we were able to put
packages like Unheader or others on our particular setups for running R. Maybe you can, like
I said, install new packages like that, maybe you can swap out a compiler to get the most
speed out of your computations and much more.
But you may also find yourself in a situation where you are dealing with a bit of constraints
in your environment, maybe it's from an IT group or whatnot.
Never.
Never.
Oh, never.
Oh, yeah.
Yeah.
You should have heard the pre-show something constrains poor Mike's been dealing with.
Oh, that's a bonus content waiting to happen, but it's you might find yourself in a situation
where the IT admins give you an R install.
So that's good.
But that's about it.
You don't have R studio.
You don't have anything like that.
You got the flash and prompt at your disposal and you just better make the best of it.
Well, if that resonates with you or maybe the more appropriately, if you find yourself
in that situation unwillingly, then our next highway kind of brings us to the root of how
important concepts in R that you can do any time like vectorization and functional programming
can be achieved in this quote unquote vanilla install.
And that's where a cognitive neuroscientist, Anthanasia Monica Mowinckel walks us through
a common development process of creating code that loops through certain observations and
performs derivations on particular columns of a data set.
And starting with what many of us probably did when we first learned R, create your loops,
create some variables in the global environment.
But that can have some various pitfalls that can occur, especially when you start to troubleshoot
things.
And so that's where the post transitions to, hey, we got to take a functional approach
to this.
So that's where she starts building custom functions that will seem pretty familiar to
some of the concepts we've been talking about through, frankly, the life cycle of this very
podcast.
Have yourself fit for purpose functions.
You can reuse them in many different ways and it's going to make your environment cleaner
and make debugging a lot easier.
And then where can we fit those in?
That's where Anthanasia gives us a tour of how the various apply functions that are built
into your basic installation of R can work, going from the typical apply to s-apply, m-apply,
which is her favorite.
And honestly, you also get to see some of the little idiosyncrancies are somewhat different
calls that you have to make with parameters as you transition from these.
And while this wasn't necessarily the intent of her post, I also think that the blog post
serves as another use case of why the highly acclaimed per package exists.
Because with per, you do get a consistent API, if you will, of running these map reduced
like functions with clear inputs and clear expectations of what the output should be.
Now, again, in her use case, she may be doing a very limited system where hers just simply
isn't available.
So you have to make the best of what you have.
So it's important to have that perspective, especially in these situations.
But it's also showing you that, yes, in BaseR, you can accomplish all these things.
Just got to have a little more investment to learn how the functions work and get a
hang of it, hopefully building some examples for your repertoire to have as reference.
So again, maybe you don't find yourself in this situation routinely, but I have found
myself in this from here and there when I got a custom VM setup to do some HPC work.
And for whatever reason, the IT group couldn't get me that fancy R installation with thousands
of packages like in our default install.
So I had to make the best of it while I waited for certain things to complete.
So again, great illustration of the concepts.
Like I said, vectorization, functional programming can be hugely valuable no matter which environment
you're in.
So that was quite a reality check like post, Mike, what did you think about Anastasia's
post here?
One thing I really loved about Anastasia's blog post is the fact that she actually showcases
some of the errors that she runs into in the console along the way as she works through
the problem from start to finish.
And I think that this is so important.
It's not something that we see in blog posts very often, but I think when you're trying
to teach a new concept to showcase those errors that you ran into along the way, instead of
just saying, hey, here's the right way to do it, I think can really instruct people
a lot better sometimes and can really be a better learning experience for folks who are
trying to follow along with that blog post sometimes because it really provides a vulnerable
look into your thought process that most often will be similar to somebody else's thought
process along the way.
I always believe that it's great use of time to take a dive into Basar once in a while.
I know the tidyverse is amazing.
We know the tidyverse is amazing, but you might be surprised what utility there is in
some Basar functions.
If you do spend some time, just check out what there is in the Basar installation.
There was a tweet recently that we'll link to in the show notes, and it was like, what
is your favorite Basar function?
I can't remember who posted it out there.
Two of mine are the any and all functions from Basar that allow you to provide a vector
of different logicals to each of those functions, and if one of them is true in the any function,
then it'll return a true, and if one of them is false in the all function, it'll return
a false.
It's very useful for checks in my Shiny apps, particularly, or whenever you're doing any
sort of data engineering code where you're having to use some control flow, some if statements,
the any and the all functions, I find useful all the time.
I don't know, Eric, do you have any favorite Basar functions?
Oh, yeah, those are always in any app I make.
It's hard to avoid not benefiting from those great simple ways of invoking the conditional
logic, and one that I always have in my scripts is finding unique values of variables with
unique.
The unique function is literally in every program I make, because I'm always troubleshooting,
oh, wait, did I get all those treatment group levels, or did I get all those lab values,
or oh, jeez, this data set has 5,000 observations.
I don't want to just skim through that.
Just give me the uniques, baby, and then that's what I get, so unique is very valuable in
my Basar tool set.
Unique in sort.
Oh, yes.
Always in those widgets, yeah, in the choices for the widgets.
Absolutely.
Yeah.
Oh, that's a great call out.
Yeah.
There are tons of gems like that.
Set diff.
Set diff.
Oh, yeah.
Another huge one there, too.
I was just doing that recently to troubleshoot why I was not getting certain observations
and lab data for this FDA submission stuff I'm working on, so that being very quick to
figure out, oh, yeah, because I took it from that data set, and I didn't have that record.
Oh, okay.
Now I got it.
So yeah, lots of these things are quite valuable in your tool set.
Well, Mike, I couldn't have said it better myself.
A lot of that base functionality is hugely important to have knowledge of.
And the other part that I mentioned towards the outset was this idea of how R can really
cleanly interop with other tools in the open source world.
And that's where our last highlight in this supersized edition of R-Week Highlights comes
into play, because no R-Weekly episode would be complete without a little visit to our
data viz corner with a little API magic to boot.
And so we marveled in the past on how R itself, or like I said, in combination with other
open source visualization tools, you can make something that looks just as professional
as coming from that fancy high-cost product like Adobe Illustrator, just as an example.
And with his first post produced in that little engine called Quartle, the latest blog post
from Abdul Issa Bitta, who is a full stack developer and an actualist, brings us a comprehensive
guide to creating posters of current NBA player headshots for each team in the league with
both R and some great graphics software in the open source world.
Now first, ain't nobody got time to hard code all those names.
So the first step is to utilize the public-facing ESPN APIs to dynamically grab JSON representations
of both the team and individual player metadata.
That's a pretty clean win there.
And fortunately, he did some research to find where those APIs were, but you get some JSON
out of that.
So of course, that means some wrangling, right?
Data wrangling or JSON, who hasn't done that if you've dealt with APIs in the past?
But Abdul has some really clean and concise code with explanations on how he wrangled
that with the per package.
There's another callback to extract only the bits needed for his particular project here.
And one of the major goals of this post was not just to do some wrangling there.
It was also to look at how can we generate the actual poster.
And surely you can do this some with ggplot too, I'm sure.
And he even mentions that.
But the goal of this was to figure out how can we hook into another tool that's available
in the open source community to do it.
And that's where he hooks into what I call the Swiss army knife of graphics processing
called image magic.
And in particular, image magic contains a handy function called montage that allows
for a collection of image files to be concatenated together into one, well, montage.
See?
Good names, right?
The results look terrific.
But like any experience of real world data, there can be a couple little edge cases here,
such as dealing with long player names for certain teams and et cetera, figuring out
how to deal with that to print cleanly.
But when you look to the end of this post, it looks terrific.
Really polished visualization.
And in the end, it's a fantastic walkthrough and illustration to show how open source can
stand shoulder to shoulder with those proprietary behemoths in the visualization stack.
So really cool use case of grabbing your data, assaging it a little bit, and then bringing
that interoperability principle in mind with R to bring that novel visualization to fruition.
So Mike, you're going to make any sports posters out of all this?
I think I'm going to have to.
I mean, this is bringing me back to like my college dorm room days with some sports posters
on the wall.
I think I could, instead of going out and buying one for 25 bucks, I think with a little
R, a little image magic, some bash work on the command line, I think I could make one
up myself here.
These posters look absolutely incredible.
I feel like it's an opportunity to say in an end to end project, it's never just blank.
So in this case, it was never just R. We usually say it's never just shiny, but in this case,
it was never just R. With that image magic command line tool, it's a phenomenal way to
stitch these images together on a very fluid background that looks like, I don't know,
all of these individuals were really standing together against the same background.
It's pretty incredible.
He shows not only the bash commands for that, but also how to execute those bash commands
using the system functionality in base R, and to execute those commands using R. Love
the quarto blog.
The call outs maybe are like my favorite thing in the entire world, and he has a couple really
nice call outs.
I don't know.
They come out so, so well in a blog post.
They really make you stop and take time to read them.
So fantastic post from top to bottom.
I'm not sure if I was familiar with the R JSON package.
I've been familiar with the JSON Lite package.
It looks like there's some overlapping functionality there, perhaps.
Yeah, I'm fairly sure R JSON came before JSON Lite.
And certainly many, many people have used that in the past too.
So they both do a good job.
Yes, it absolutely looks like it.
And again, he's employing Per to not only create one poster, but to create multiple
posters over all 30 teams in the NBA to have this one final massive graphic that fits right
on this HTML page in your quarto doc.
So incredible read from top to bottom.
I think it's another great example of sort of accomplishing an entire project within
a blog post from start to finish.
And I think if you're the type who learns really well from use case project based type
learning, this would be a fantastic blog post for you to check out.
Well said.
Yeah.
And another, like I said, great, great example of how you can stitch a lot of these pipelines
together and be able to create something really aesthetically visually pleasing and not have
to shell out a whole boatload of money to that other software.
Not that I don't have opinions about that, but I digress, I digress, but what we can
all agree on is that this is a fantastic issue.
And as I mentioned, we were off last week.
So for my additional little fine here, I'm going to give a call back to last week's issue.
And in particular, an amazing blog post from another regular contributor to our week.
Basically, Shannon Peligi, who posted basically a narrative of her in-depth conversation with
Posit's own Jenny Bryan, affectionately titled Yak Shaving.
You're going to have to read the post to figure out why it's called Yak Shaving.
But at a high level, this is talking about Jenny's guide or Jenny's principles, how
she approaches learning a new technical paradigm, a new technical idea, and just the practical
side of how she goes about it.
That's a really entertaining read and also quite insightful, especially as I think about
what's on my docket next year to learn every year a new skill or a new technology or a
new way to orchestrate things together.
I'm definitely going to take some lessons learned from Shannon's post here.
So Mike, what did you want to call out today?
Sure.
No, that's a great pull up from last week's highlight.
I found one in this week's highlight, which is Julia Silge's post.
I thought it was pretty topical today on how to delete all of your tweets programmatically
in R in the R tweet package.
She does have a link to how to download your Twitter archive first, which is a great first
step.
Just that first paragraph, otherwise you will not have any of your tweets anymore.
But if you feel like you want to go through the process of downloading your Twitter archive
and then getting rid of your tweets as well as your account, I think you can follow along
with Julia's post to do that.
Excellent.
Fine.
And I think a lot of people are going to be taking advantage of this, not to this in future,
maybe myself included.
I'm not sure if I'll delete everything, but at the minimum having a backup never, ever
hurts for sure.
Absolutely.
And it looks like Julia has been hard at work because it looked like there were a few tidy
models ecosystem packages that had brand new releases this week as well, or very recently
as well, including parsnip and a couple others that I had seen.
Yeah.
That world never stops spinning, right?
Tidy models is an exciting place to watch, and I'm always eager to see Julia's blog posts
and as well as her screencasts, which are always entertaining as well.
And I want to thank Julia on behalf of the recent endeavor I was embarking on, the R
Pharma Conference, where Julia was a panelist on lessons and ideas for women developing
careers in life sciences and the analytics space.
So she did a great job with that.
So thank you, Julia.
We see here in the audio form here.
And yeah, so that's, we're about to wrap up episode 100, but I want to make an announcement
here that I'm really excited to share with all of you about the future direction of the
show.
What's not changing?
Me and Mike coming on here and bantering about our praise of the highlights and our community
in general.
You're always going to get that, but I'm happy to announce a way to make it even easier for
you, the listener, to share a little bit back with us as they sway or a concept that I've
been reading about quite a bit and I'm fully invested in called value for value.
What this really means that if you get any use out of this podcast, however big or small,
we're going to make it super easy for you to share your value back with us.
Is it a set amount?
No, this is what you choose.
And the easiest way to get started with this is to grab yourself a new podcast app.
And I have a URL just for you to find it, newpodcastapps.com that'll be linked in the
show notes.
And there will be a handy little button in many of these apps that you could download,
whether it's iOS, Android, the web or both.
In particular, I like the ones called fountain pod verse cast the Maddox.
Those are just the name of few where you can send us what's called a boost to give us a
little encouragement perhaps in our endeavors here.
And so I won't say too much more about it, I want to encourage all of you to do your
research and read up on this if you're really interested.
But I do want to mention that I would not be putting my voice or my support for this
if I didn't really believe in it.
And if you've ever listened to many of my podcasts in the past, whether it's our weekly
highlights or the old art podcast, which I hope to resurrect again someday, I have never
taken sponsors ever because I wanted to just be driven by the community for the community.
And this endeavor of value for value actually has some great synergies of what we just talked
about at the top with the mastodon situation, a decentralized way for you to give value
back to us and us to share value with you.
So again, have a look at newpodcastsapps.com and certainly reach out to me if you'd like
more details on how all this works.
But I want to thank some good friends in the community that might be listening to this.
Adam Curry, who actually is the originator of podcasts itself, has educated me on this
as well as my friends at Jupiter Broadcasting have been using this principle quite a bit
in their endeavors.
So again, check that out and just have a look and let me know what you think.
So with that, how can you reach us?
Well, as I mentioned at the outset, our weekly itself has a brand new mastodon account.
We are at our weekly at fostodon.org.
We'll have a link to that in the notes.
And also you can find me.
I am still on Twitter, as they say, with that the artcast, but I am also on mastodon at
our podcast at podcastindex.social, we'll have a link to that in the show notes.
But Mike, where can people find you?
I will be on mastodon today, I promise, but I do not know my handle yet, address or whatever
it's called.
So for now, I'm hanging on on Twitter, but at Mike underscore Ketchbrook k-e-t-c-h-b-r-o-o-k
and we'll hope to have some similar variation to that in my mastodon handle.
And Eric, I think I speak for the whole community where we certainly do not doubt and very much
appreciate all of the effort you have put in to make this content fully community driven.
So thank you very much for your efforts.
And I think we're excited about what the future holds.
And this episode is brought to you by MongoDB.
So you want to store your structured data in an unstructured way for no reason at all,
just to bring it back structured into a data frame later, MongoDB, it's perfect.
Well there you go.
Yeah, we've got a new direction, don't we?
That wasn't quite in the value for value script, but we'll let it aside, we'll let it aside
for now.
But it is an exciting adventure that we're about to embark in the next batch of episodes,
who knows how long this train will last.
We're excited to have you all along the journey to join us for this.
So with that, we're going to close up shop on episode 100.
Again, thank you to everybody around the world who's been listening to all the previous episodes
when I was foaming my way through the very beginning and then right at the ship when
Mike came along to join me for the ride.
So we will see you again in a week.
Until then, that's the end of episode 100 of our weekly highlights.
And again, we'll be back here next week.
"
"119","issue_2022_w_44_highlights",2022-11-04,29M 40S,"Embracing the dual role of data scientist and software developer with state-of-the-art tooling, illustrating the fundamentals of Shiny (literally), and the TidyX crew put their data wrangling skills to the test. Episode Links This week's curator: Sam Parmar (@parmsam_ (https://twitter.com/parmsam_)) Are you Data Scientists or Software Developers?!…",NA
"120","issue_2022_w_42_highlights",2022-10-19,38M 28S,"The power of Quarto's interoperability shines again with integrating R and JavaScript maps, as well as the grammar of table generation in both R and Python. Plus boost the launching of your R session with the startup package. Episode Links This week's curator: Kelly Bodwin Let’s make maps with bertin.js in Quarto…",NA
"121","issue_2022_w_41_highlights",2022-10-12,37M 7S,"The magic of automated Shiny app deployment and data aggregation using GitHub actions, 6 productivity hacks for Quarto, and valuable tips for managing large codebases in R. Episode Links This week's curator: Colin Faye (@_colinFay (https://twitter.com/_colinfay)) Automatically deploying a Shiny app for browsing #RStats tweets with GitHub Actions…",NA
"122","issue_2022_w_40_highlights",2022-10-05,36M 49S,"Design principles for data analysis, unraveling pipeline analyses with {Unravel}, and visualizing simulated environmental changes in western Canada with Shiny. Episode Links This week's curator: Eric Nantz Design Principles for Data Analysis (https://www.tandfonline.com/doi/full/10.1080/10618600.2022.2104290?journalCode=ucgs20) {Unravel} - A…",NA
"123","issue_2022_w_39_highlights",2022-09-28,33M 29S,"A collection of highlights to give your future developer self a helping hand: Deploying a Flexdashboard using GitHub Pages and Docker, an illustrated guide showcasing the perks of Git and GitHub for version control, and how the logger package integrates smoothly with plumber for an API package. Additional note: The recording of this episode was met…",NA
"124","issue_2022_w_38_highlights",2022-09-21,30M 45S,"A few major benefits of adopting variable labels for R data frames, wrapping a plumber API into a package with mariobox, and getting started with obtaining new data in R via APIs and web scraping. Episode Links This week's curator: Tony ElHabr (@TonyElHabr (https://twitter.com/TonyElHabr)) The case for variable labels in R…",NA
"125","issue_2022_w_37_highlights",2022-09-14,26M 41S,"A collection of highlights powered by mathematics, statistics, and a little bit of R magic: Mapping wind data with R, calculating the expected statistic in football, and how the vetiver package fits in an ML-Ops production flow using Docker and Plumber. Episode Links This week's curator: Jon Calder (@jonmcalder…",NA
"126","issue_2022_w_36_highlights",2022-09-08,29M 55S,"Using base R to decrypt an Australian coin's hidden messages, the Palmer Penguins data set achieves another milestone, and visualizing multiple statistcal properties with faded raincloud plots. Episode Links This week's curator: Miles McBain (@MilesMcBain (https://twitter.com/MilesMcBain)) Australian Signals Directorate 50c Coin Decryption…",NA
"127","issue_2022_w_35_highlights",2022-08-31,25M 17S,"Using hierarchical forecasting to explore subway fare recovery, and how you can learn more about {gtsummary} to create your next publication-quality table within R. Episode Links This week's curator: Batool Almarzouq (@batool664) (https://twitter.com/batool664) Exploring Types of Subway Fares with Hierarchical Forecasting…",NA
"128","issue_2022_w_34_highlights",2022-08-24,35M 26S,"The rstudio::conf(2022) presentation recordings are now available for viewing, and we learn about the unique development journey of the new CRAN release of the countdown package. Episode Links This week's curator: Sam Parmar (@parsam_ (https://twitter.com/parmsam_)) Talk recordings and workshop materials from rstudio::conf(2022)…",NA
"129","issue_2022_w_33_highlights",2022-08-17,35M 6S,"Avoid repeating yourself by using dplyr's across function, going inside the process of creating a custom theme in ggplot2, and a few keyboard-centric tricks to manage your RStudio pane viewing. Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) Using across() to create multiple columns…",NA
"130","issue_2022_w_32_highlights",2022-08-10,24M 51S,"A wealth of R content from the UseR! 2022 conference is now available, focusing on accessibility in the diffify tool, and a great recap of rstudio::conf(2022) from TidyX. Episode Links This week's curator: Batool Almarzouq (@batool664) (https://twitter.com/batool664) useR2022 recordings are now on the conference YouTube channel…",NA
"131","issue_2022_w_31_highlights",2022-08-04,30M 23S,"RStudio re-brands as Posit, the shinytest2 package continues to make waves in the Shiny community, and more Quarto tips to boost your workflow. Episode Links This week's curator: Kelly Bodwin (@KellyBodwin (https://twitter.com/KellyBodwin)) RStudio rebrands as Posit (https://www.rstudio.com/blog/rstudio-is-becoming-posit/) {shinytest2}: For…",NA
"132","issue_2022_w_28_highlights",2022-07-15,31M 19S,"Another great use case for Docker containers with interactive R-Markdown reports, a recap of RStudio's presence at the Appsilon Shiny conference, and building an interactive point-and-click game with Shiny. Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) Containerizing Interactive R Markdown…",NA
"133","issue_2022_w_27_highlights",2022-07-13,37M 2S,"Advice on building Docker containers for Shiny applications, an R-centric tutorial on fundamentals with shell, and tips on evaluating GitHub activity for contributors. Episode Links This issue's curator: Tony ElHabr (@TonyElHabr (https://twitter.com/TonyElHabr)) UseR!2022: Best Practices for Shiny Apps with Docker and More…",NA
"134","issue_2022_w_26_highlights",2022-06-29,32M 27S,"Flexing new table-creation capabilities in the latest flextable update, how less is more with Shiny application processing, and the RainbowR community shines once again. Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) {flextable} 0.7.2 (https://www.ardata.fr/en/post/2022/06/23/flextable-0-7-2-is-out/) -…",NA
"135","issue_2022_w_25_highlights",2022-06-22,25M 46S,"Previewing the upcoming rstudio::conf, why you should (or shouldn't) build an API client package, and monitoring Shiny application usage with Hotjar. Episode Links This week's curator: Colin Fay (@_colinFay (https://twitter.com/_colinfay)) rstudio::conf(2022) Conference Schedule (https://www.rstudio.com/blog/rstudio-2022-conf-schedule/) Why You…",NA
"136","issue_2022_w_24_highlights",2022-06-15,22M 37S,"Many improvements to {gt} version 0.6, and creating flow charts effeciently with {ggplot2}. Episode Links This week's curator: Batool Almarzouq (@batool664) (https://twitter.com/batool664) Changes (for the better) in {gt} 0.6.0 (https://www.rstudio.com/blog/changes-for-the-better-in-gt-0-6-0/) Creating flowcharts with {ggplot2}…",NA
"137","issue_2022_w_23_highlights",2022-06-08,30M 10S,"Creating user interfaces in R with a vintage toolkit, and a candid take on learning R with the right perspectives in mind. Episode Links This week's curator: Sam Parmar (@parmsam_ (https://twitter.com/parmsam_)) {tickle} (https://github.com/coolbutuseless/tickle): a package for creating UIs in base R R will always be arcane to those who do not make…",NA
"138","issue_2022_w_22_highlights",2022-06-01,21M 42S,"Annotated screencasts of David Robinson's Tidy Tuesday analyses, and the second edition of Deep Learning with R is on the way. Episode Links This week's curator: Batool Almarzouq (@batool664) (https://twitter.com/batool664) RScreencasts.com (https://www.rscreencasts.com/?ref=rweekly): A collection of 80+ hours of time-stamped, annotated…",NA
"139","issue_2022_w_21_highlights",2022-05-25,35M 11S,"Amazing software development resources for data scientists, community-contributed R markdown tips & tricks to save you time, and combining GitHub gists and Carbon screenshots with gistillery. Episode Links This week's curator: Eric Nantz (@theRcast (https://twitter.com/thercast) Software Development Resources for Data Scientists…",NA
"140","issue_2022_w_20_highlights",2022-05-19,28M 45S,"A preview of R for Data Science (2nd edition) with missing data, creating topography maps, and (yes) playing the drums directly in R. Episode Links This week's curator: Kelly Bodwin (@KellyBodwin (https://twitter.com/KellyBodwin)) New r4ds chapter: missing values (https://r4ds.hadley.nz/missing-values.html) Making a crisp topography map with R…",NA
"141","issue_2022_w_19_highlights",2022-05-11,26M 17S,"A dungeon-crawler for your R console, storytelling in ggplot2 with rounded rectangles, and updates to the tidymodels recipes suite of packages. Episode Links This week's curator: Colin Fay (@_colinFay (https://twitter.com/_colinfay)) Simple procedural dungeons in R (https://www.rostrum.blog/2022/05/01/dungeon/) Storytelling in ggplot using rounded…",NA
"142","issue_2022_w_18_highlights",2022-05-05,30M 43S,"A brand-new tables gallery powered by the R community, noteworthy items from the Appsilon Shiny conference, and R-Markdown is not going anywhere. Episode Links This week's curator: Eric Nantz (@theRcast (https://twitter.com/thercast)) RStudio Community Table Gallery (https://www.rstudio.com/blog/rstudio-community-table-gallery/) shinytest2, Rhino…",NA
"143","issue_2022_w_17_highlights",2022-04-27,32M 55S,"Lessons from teaching R to non-programmers, loading a large and messy CSV file with data.table and command-line tools, Bayesian analyses with the brms package, and getting a better understanding of the tidyeval framework. Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) 6 Lessons I learned from…",NA
"144","issue_2022_w_16_highlights",2022-04-20,29M 53S,"An important change coming to the R-spatial ecosystem, enhancing function error reporting with chaining, and traveling down the monad rabbit hole. Episode Links This week's curator: Tony ElHabr (@TonyElHabr (https://twitter.com/TonyElHabr)) R-spatial evolution: retirement of rgdal, rgeos and maptools…",NA
"145","issue_2022_w_15_highlights",2022-04-13,31M 28S,"The cone of silence is lifted for the Quarto publishing engine, re-creating a storytelling look with ggplot2, and programming a fun Dragon Realm game in Shiny. Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) We don't talk about Quarto…",NA
"146","issue_2022_w_14_highlights",2022-04-06,26M 56S,"A trifecta of new R packages for data validation, function logging, and a new ggplot2 extension. Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) Exemplar: a prototype R package for data validation (https://mdneuzerling.com/post/exemplar-a-prototype-r-package-for-data-validation/) {chronicler} 0.1…",NA
"147","issue_2022_w_13_highlights",2022-03-30,29M 15S,"After a brief hiatus, the R-Weekly Highlights podcast is back! In this episode we discuss: Understanding the native R pipe, and using RopenSci's pkgcheck within GitHub Actions. Episode Links This week's curators: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) and Batool Almarzouq (@batool664) (https://twitter.com/batool664) Understanding…",NA
"148","update_on_rweekly_and_2021_reflections",2022-01-14,49M 30S,"An update on the current state of RWeekly, plus Eric and Mike reflect on their journeys with data science from the industry and consulting perspectives in an eventful 2021! Episode Links The very first issue of RWeekly: https://rweekly.org/issue-0.html R for Data Science Learning Community: https://www.rfordatasci.com discoRd:…",NA
"149","issue_2021_w_50_highlights",2021-12-15,35M 34S,"A batch of Shiny tips from creating an application tailored to teaching statistics, and the Big Book of R gains nine new entries to the collection. Episode Links This week's curator: Batool Almarzouq (@batool664) (https://twitter.com/batool664) 6 simple Shiny things I have learned from creating a somewhat small app…",NA
"150","issue_2021_w_49_highlights",2021-12-09,32M 13S,"A fascinating journey to understand serialization of RDS files in R, and giving an old domain new life with R markdown while giving back to charity. Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) Data serialisation in R (https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/) Day 02:…",NA
"151","issue_2021_w_48_highlights",2021-12-01,33M 2S,"How GitHub Actions empowers the {cffr} package to perform automated testing with 2,000 packages, recap of the recent R-Ladies Philly workshop on automated testing in R, and introducing the new {filebin} package for easy file sharing. Episode Links This week's curator: Colin Fay (@_colinFay (https://twitter.com/_colinfay)) How I Test cffr on (about)…",NA
"152","issue_2021_w_46_highlights",2021-11-17,37M 9S,"A cautionary tale about ML interpretations with food, practical solutions for dealing with big data in R, and the adventures of installing R on the new Apple Silicon hardware. Episode Links This week's curator: Tony Elhabr (@TonyElHabr (https://twitter.com/TonyElHabr) Why machine learning hates vegetables (a dialogue about Zillow)…",NA
"153","issue_2021_w_45_highlights",2021-11-10,33M 59S,"A lesser-known R function drives finding coordinates on fictitious Pokemon maps, creating customized point shapes with ggplot2 and gggrid, and how the branchMover Shiny app can save you a load of time and effort with GitHub branch renaming. Episode Links This week's curator: Miles McBain (@MilesMcBain (https://twitter.com/MilesMcBain)) Get…",NA
"154","issue_2021_w_44_highlights",2021-11-03,24M 42S,"An analysis of dialogue from ""The Office"", and a package promoting accessibility for visually impaired R-Users. Episode Links This week's curator: Wolfram Qin Analyzing The Office's dialogues (https://daniloderosa.com/blog/theoffice/) {BrailleR} 0.32.1 (https://cran.r-project.org/package=BrailleR): Improved Access for Blind Users Entire issue…",NA
"155","issue_2021_w_43_highlights",2021-10-27,30M 23S,"A tutorial on getting started with aRtistry, simulating the Squid Game bridge scene, and a video demonstration of installing Shiny server on AWS. Episode Links This week's curator: Kelly Bodwin (@KellyBodwin (https://twitter.com/KellyBodwin)) Simulating the Squid Game Bridge Scene…",NA
"156","issue_2021_w_42_highlights",2021-10-20,30M 1S,"A major announcement for R developers interested in type safety, thoughts on using Visual Studio Code from the perspective of a long-time RStudio user, and the adventures of filling regions between lines with ggplot2. Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) Introducing rpp: The long-term goal of…",NA
"157","issue_2021_w_41_highlights",2021-10-13,27M 38S,"Using the helpers from usethis for pull request workflows, 2021 New York R conference videos now available, and the origins of the newly released ggalignment package for D&D inspired alignments. Plus, a new era of the podcast begins with our new co-host Mike Thomas! Episode Links This week's curator: Eric Nantz (@theRcast…",NA
"158","issue_2021_w_40_highlights",2021-10-06,22M 50S,"Parameterized reports in RMarkdown with Plumber, an updated history of the pipe operator in R, and creating data from an image with reticulate Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) The Power of Parameterized Reports With Plumber…",NA
"159","issue_2021_w_39_highlights",2021-09-30,25M 43S,"Data visualization accessibility, curating for R-Ladies, and a soccer data pipeline. Plus an annoucement on my goals for the future of the podcast. Episode Links This week's curator: Batool Almarzouq (@batool664) (https://twitter.com/batool664) Resources for Data Viz Accessibility…",NA
"160","issue_2021_w_38_highlights",2021-09-22,12M 35S,"Eras of MTV and system commands Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) Finding the Eras of MTV's The Challenge Through Clustering (https://jlaw.netlify.app/2021/09/15/finding-the-eras-of-mtv-s-the-challenge-through-clustering/) How to Use System Commands in your R Script or Package…",NA
"161","issue_2021_w_37_highlights",2021-09-15,11M 54S,"Techniques for creating generative art in R, time tracking with clockify, and the four pipes of magrittr Episode Links This week's curator: Colin Faye (@_colinFay (https://twitter.com/_colinfay)) Art, jasmines, and the water colours (https://blog.djnavarro.net/posts/2021-09-07_water-colours/) {clockify} Time Tracking from R…",NA
"162","issue_2021_w_36_highlights",2021-09-08,15M 52S,"Elegant maps with tmap, a data validation ecosystem, and a major release for gitlabr. Episode Links This week's curator: Wolfram Qin Elegant and informative maps with tmap (https://r-tmap.github.io/tmap-book/visual-variables.html) A lightweight data validation ecosystem with R, GitHub, and Slack…",NA
"163","issue_2021_w_35_highlights",2021-09-01,15M 55S,"The July top 40 R packages, R Markdown advanced tips video, and colored world maps Episode Links This week's curator: Tony ElHabr (@TonyElHabr (https://twitter.com/TonyElHabr)) July 2021: ""Top 40"" New CRAN Packages (https://rviews.rstudio.com/2021/08/26/july-2021-top-40-new-cran-packages/) R Markdown Advanced Tips to Become a Better Data…",NA
"164","issue_2021_w_34_highlights",2021-08-24,16M 20S,"Debugging lessons with source, illustrating the coefficient of variation, and the exciting conclusion to SLICED season one Episode Links This week's curator: Miles McBain (@MilesMcBain (https://twitter.com/MilesMcBain)) Keep your R scripts locally sourced: A lesson from debugging (https://www.tjmahr.com/keep-it-locally-sourced/) Exploring R² and…",NA
"165","issue_2021_w_33_highlights",2021-08-17,9M 49S,"The new {flow} package for visualization R code, and producing donut charts of COVID-19 cases using R. Episode Links This week's curator: Wolfram Qin (@RbyRyo (https://twitter.com/R_by_Ryo)) {flow} 0.0.2 (https://cran.r-project.org/package=flow): View and Browse Code Using Flow Diagrams. Some Covid Donuts To End The Week…",NA
"166","issue_2021_w_32_highlights",2021-08-10,17M 23S,"Creating mobile-friendly Shiny apps, {gitlabr} 2.0.0, and achieving reproducible workflows with R and Docker Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) Making Shiny apps mobile friendly (https://jnolis.com/blog/shiny_mobile/) {gitlabr} 2.0.0 (https://github.com/statnmap/gitlabr): Access to the…",NA
"167","issue_2021_w_31_highlights",2021-08-04,15M 46S,"Up and running with officedown, testing with a reprex to solve your problems, and a tidy take on performing hypothesis testing with statsExpressions. Episode Links This week's curator: Kelly Bodwin (@KellyBodwin (https://twitter.com/KellyBodwin)) Up and running with Officedown (https://alison.rbind.io/blog/2021-07-officedown/) Reminder to test…",NA
"168","issue_2021_w_30_highlights",2021-07-27,19M 4S,"Top 3 coding best practices from the Shiny contest, improvements in Target Markdown for {targets} 0.6.0, and the new {facetious} package for alternative facets with {ggplot2} Episode Links This week's curator: Eric Nantz (@theRcast (https://twitter.com/thercast)) Top 3 Coding Best Practices from the Shiny Contest…",NA
"169","issue_2021_w_29_highlights",2021-07-20,12M 14S,"How to become a better R code detective, a practical introduction to custom fonts, and making error messages your own. Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) How to become a better R code detective? (https://masalmon.eu/2021/07/13/code-detective) Setting up and debugging custom fonts:…",NA
"170","issue_2021_w_28_highlights",2021-07-13,9M 38S,"Creating a package directly from R-Markdown with {fusen}, and a new milestone release of the {googledrive} package Episode Links This week's curator: Batool Almarzouq (@batool664) (https://twitter.com/batool664) {fusen} 0.2.2 (https://cran.r-project.org/package=fusen): Build a Package from R Markdown File googledrive 2.0.0…",NA
"171","issue_2021_w_27_highlights",2021-07-07,12M 50S,"Practical tips on starting new R projects, and improving a visualization of US streaming market share. Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) Draw me a project (https://masalmon.eu/2021/06/30/r-projects/) Improving a Visualization…",NA
"172","issue_2021_w_26_highlights",2021-06-29,11M 7S,"Creating your own CRAN-like repository with R-universe, results the third annual Shiny contest, and insights on why to use Shiny. Episode Links This week's curator: Colin Faye (@_colinFay (https://twitter.com/_colinfay)) How to create your personal CRAN-like repository on R-universe (https://ropensci.org/blog/2021/06/22/setup-runiverse/) Winners…",NA
"173","issue_2021_w_25_highlights",2021-06-23,12M 3S,"Projecting and tracking COVID-19 infection rates in England with R, leveraging Wikidata to tag scientific abstracts, and a new deep-learning workflow with the luz package Episode Links This week's curator: Robert Hickman (@robwhickman (https://twitter.com/robwhickman)) Tracking SARS-CoV-2 In England with {epidemia}…",NA
"174","issue_2021_w_24_highlights",2021-06-15,13M 9S,"Using Animal Crossing data with the Google Vision API and machine learning, the latest Shiny developer series with Nick Strayer, and ensuring robust database transactions in Shiny Episode Links This week's curator: Tony Elhabr ([@TonyElHabr] Everybody Loves Raymond: Running Animal Crossing Villagers through the Google Vision API…",NA
"175","issue_2021_w_23_highlights",2021-06-08,11M 57S,"Reusing knitr chunk options, the combo of VS-Code and R in 2021, and say hello to gggrid Episode Links This week's curator: Miles McBain (@MilesMcBain (https://twitter.com/MilesMcBain)) Reusing Code Chunks and Chunk Options with knitr (https://yihui.org/en/2021/05/knitr-reuse/) R in 2021 with VSCode…",NA
"176","issue_2021_w_22_highlights",2021-06-01,8M 10S,"Extracting and analyzing Apple health data, and the top 40 CRAN packages for April Episode Links This week's curator: Wolfram Qin (https://github.com/qinwf) Changes in Apple Health Export (https://www.johngoldin.com/blog/2021-05-changes-in-apple-health-export/) April 2021: ""Top 40"" New CRAN Packages…",NA
"177","issue_2021_w_21_highlights",2021-05-26,15M 53S,"The big new features in R 4.1.0, what makes a great function example, and evolution of a ggplot Episode Links This week's curator: Kelly Bodwin (@KellyBodwin (https://twitter.com/KellyBodwin)) New features in R 4.1.0 (https://www.jumpingrivers.com/blog/new-features-r410-pipe-anonymous-functions/) Package documentation: What makes a good example?…",NA
"178","issue_2021_w_20_highlights",2021-05-18,11M 9S,"A tidymodels approach to the Introduction to Statistical Learning learning labs, exploring class imbalance on the TidyX video series, and encrypting and hosting a R Markdown report. Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) ISLR: tidymodels labs…",NA
"179","issue_2021_w_19_highlights",2021-05-11,10M 59S,"Using the trifecta of map, walk, and pivot for data processing, obtaining quotes from the Friends characters in R, and putting the spolight on Shiny user interfaces in the Shiny Dev Series. Episode Links This week's curator: Hey, it's me, Eric! (@theRcast (https://twitter.com/thercast)) Map, Walk, Pivot…",NA
"180","issue_2021_w_18_highlights",2021-05-04,8M 50S,"A call to action for testing R 4.1, a practical guide to unit tests, and a tutorial on creating pizza charts with football data Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) R Can Use Your Help: Testing R Before Release…",NA
"181","issue_2021_w_17_highlights",2021-04-27,12M 37S,"Exploring wikidata with the {tidywikidatar} package, accessibility improvements in {knitr}, and the top 40 new CRAN packages for March. Episode Links This week's curator: Batool Almazrouq (@batool664) (https://twitter.com/batool664) What does Wikidata know about members of the European Parliament?…",NA
"182","issue_2021_w_16_highlights",2021-04-20,10M 8S,"The latest news from rOpenSci, creating ggplot2 postcards with ggirl, and an introduction to process mining in R Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) rOpenSci News Digest, April 2021 (https://ropensci.org/blog/2021/04/16/latest-ropensci-news-digest/) {ggirl} 1.0.1…",NA
"183","issue_2021_w_15_highlights",2021-04-13,13M 29S,"reprex 2.0, using Kubernetes and the future package, and SQL in RMarkdown Episode Links This week's curator: Colin Faye (@_colinFay (https://twitter.com/_colinfay)) reprex 2.0.0 (https://www.tidyverse.org/blog/2021/04/reprex-2-0-0/) Using Kubernetes and the Future Package to Easily Parallelize R in the Cloud…",NA
"184","issue_2021_w_14_highlights",2021-04-05,9M 56S,"{workflowsets} with tidy models, exploring other {ggplot2} goems, and top 10 R errors Episode Links This week's curator: Robert Hickman (@robwhickman (https://twitter.com/robwhickman)) workflowsets 0.0.1 (https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/) Exploring Other ggplot2 Geoms (https://ivelasq.rbind.io/blog/other-geoms/) The top 10…",NA
"185","issue_2021_w_13_highlights",2021-03-30,10M 58S,"The Minard System in R, ggplot2 wizardry, a slackbot created with plumber and googleCouldRunner Episode Links This week's curator: Tony Elhabr (@TonyElHabr (https://twitter.com/tonyelhabr)) ""The Minard System"" in R (http://minard.schochastics.net/) A guide to creating a Slackbot that sends weekly updates via plumber, googleCloudRunner and Cloud…",NA
"186","issue_2021_w_12_highlights",2021-03-23,12M 1S,"{gt} tables cookbook, best weather cities, and mapping over many files Episode Links This week's curator: Miles McBain (@MilesMcBain (https://twitter.com/MilesMcBain)) The GT Cookbook (https://themockup.blog/static/gt-cookbook.html) Cities with Best (and Worst) Weather, 2021 edition (https://taraskaduk.com/posts/2021-03-14-best-weather-2/) How to…",NA
"187","issue_2021_w_11_highlights",2021-03-16,11M 12S,"Time series forecasting with torch, automated scraping of stock metrics with GitHub Actions, and default knitr options and hooks. Episode Links This week's curator: Robert Hickman (@robwhickman (https://twitter.com/robwhickman)) Introductory time series forecasting with torch…",NA
"188","issue_2021_w_10_highlights",2021-03-09,10M 40S,"Serverless dashboards, learning tidy evaluation by re-implementing dplyr, and bootstrap confidence intervals with tidy modeling Episode Links This week's curator: Kelly Bodwin (@KellyBodwin (https://twitter.com/KellyBodwin)) Server(shiny)-less dashboards with R, {htmlwidgets} and {crosstalk}…",NA
"189","issue_2021_w_09_highlights",2021-03-02,10M 26S,"Random effects, GGanimate intro, and updates to dplyr backends Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) Random effects and penalized splines are the same thing (https://www.tjmahr.com/random-effects-penalized-splines-same-thing/) GGanimating a geographic introduction…",NA
"190","issue_2021_w_08_highlights",2021-02-23,12M 19S,"Multi-page Shiny apps, R package citation, and refactoring the squashinformr package Episode Links This week's curator: Eric Nantz (@theRcast (https://twitter.com/thercast)) Multi-page {shiny} Applications with {brochure} (https://colinfay.me/brochure-r-package/) Make Your R Package Easier to Cite…",NA
"191","issue_2021_w_07_highlights",2021-02-16,9M 15S,"Installing packages, {distill} for personal websites, and Shiny app stories Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) The Comprehensive Guide to Installing R Packages from CRAN, Bioconductor, GitHub and Co. (https://thomasadventure.blog/posts/install-r-packages/) Distill it down…",NA
"192","issue_2021_w_06_highlights",2021-02-09,10M 41S,"Episode Links This week's curator: Tony Elhabr (@TonyElHabr (https://twitter.com/tonyelhabr)) Shiny 1.6: Theming, Caching, Accessibility (https://blog.rstudio.com/2021/02/01/shiny-1-6-0/) Remote Pair Programming in R Using Visual Studio Code and Live Share (https://ivelasq.rbind.io/blog/vscode-live-share/) Lists are my secret weapon for reporting…",NA
"193","issue_2021_w_05_highlights",2021-02-02,13M 45S,"rstudio::global 2021 and UseR! 2021 call for abstracts Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) rstudio::global 2021 talks (https://rstudio.com/resources/rstudioglobal-2021/) rstudio::global(2021) %>% summarise() (https://clarewest.github.io/blog/post/rstudio-global-2021-summarise/) 15th Mar:…",NA
"194","issue_2021_w_04_highlights",2021-01-26,9M 57S,"{blogdown} v1.0, announcing {pagedreport}, and the rOpenSci Community Contributing Guide Episode Links This week's curator: Colin Faye (@_colinFay (https://twitter.com/_colinfay)) Announcing blogdown v1.0 (https://blog.rstudio.com/2021/01/18/blogdown-v1.0/) Announcing pagedreport…",NA
"195","issue_2021_w_03_highlights",2021-01-18,9M 36S,"Data Science as an atomic habit, Japan soccer league season review, Fantasy football scheduling, and UseR! 2021 call for tutorials Episode Links This week's curator: Wolfram King (https://github.com/qinwf) Data science as an atomic habit (https://malco.io/2021/01/04/data-science-as-an-atomic-habit/) J.League Soccer 2020 Season Review with R!…",NA
"196","issue_2021_w_02_highlights",2021-01-11,7M 45S,"Plots with GitHub Actions, and major updates to {renv} and {fastai} Episode Links This week's curator: Batool Almazrouq (https://twitter.com/batool664) Automatic Rendering of a Plot with GitHub Actions (https://amitlevinson.com/blog/automated-plot-with-github-actions) {renv} 0.12.5 (https://cran.r-project.org/package=renv): Project…",NA
"197","issue_2021_w_01_highlights",2021-01-05,10M 17S,"Data Science course in a box, up and running with blogdown, and voting visualization in Switzerland Episode Links This week's curator: Wolfram King (https://github.com/qinwf) Data Science Course in a Box (https://datasciencebox.org/) - The core content of the course focuses on data acquisition and wrangling, exploratory data analysis, data…",NA
"198","issue_2020_52_highlights",2020-12-28,5M 34S,"R Markdown family updates & table contest results Episode Links This week's curator: Wolfram King (https://github.com/qinwf) Winners of the 2020 RStudio Table Contest (https://blog.rstudio.com/2020/12/23/winners-of-the-2020-rstudio-table-contest/) Latest News from the R Markdown Family (https://blog.rstudio.com/2020/12/21/rmd-news/) Supplemental…",NA
"199","issue_2020_51_highlights",2020-12-21,7M 35S,"Targetopia, extracting JSON data, and ggrepel update Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) The targetopia: An R package ecosystem for democratized reproducible pipelines at scale (https://wlandau.github.io/posts/2020-12-14-targetopia/) Extracting JSON data from websites and public APIs with R…",NA
"200","issue_2020_50_highlights",2020-12-14,8M 26S,"Underrated tidyverse functions, bullet chart variants, and AWS Lambda with R Episode Links This week's curator: Eric Nantz (@theRcast (https://twitter.com/theRcast)) Underrated Tidyverse Functions (https://hugo-portfolio-example.netlify.app/projects/tidyverse_functions/) Bullet Chart Variants in R…",NA
"201","issue_2020_49_highlights",2020-12-07,7M 35S,"Extended ggplot2 tutorial, static code analysis, and a customized visual CV with ggplot2 Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) An extended version of ""A ggplot2 Tutorial for Beautiful Plotting""…",NA
"202","issue_2020_48_highlights",2020-12-01,8M 15S,"About this episode Your first R package, magrittr 2.0, and Engineering Shiny use case Episode Links This week's curator: Maelle Salmon (@ma_salmon (https://twitter.com/ma_salmon)) Your first R package in 1 hour (https://www.pipinghotdata.com/posts/2020-10-25-your-first-r-package-in-1-hour/) magrittr 2.0 is here!…",NA
"203","issue_2020_47_highlights",2020-11-24,7M 53S,"About this episode Testthat utility belt, NHS-R conference, and application of Bayesian networks to sports injury prediction Episode Links This week's curator: Jon Calder (@jonmcalder (https://twitter.com/jonmcalder)) NHS-R 2020 Week Long Conference -- so much great content, so little time to catch it all...…",NA
"204","issue_2020_46_highlights",2020-11-16,7M 23S,"About this episode Open-access tools to find conronaviruses, developing inside containers, and error handling Episode Links This week's curator: Colin Faye (@_colinFay (https://twitter.com/_colinfay)) Using Open-Access Tools (rentrez, taxize) to Find Coronaviruses, Their Genetic Sequences, and Their Hosts…",NA
"205","issue_2020_45_highlights",2020-11-10,7M 9S,"About this episode Single source publishing, rainbow parentheses, and VisiumExperiment Episode Links This week's curator: Robert Hickman (@robwhickman (https://twitter.com/robwhickman)) Single-source publishing for R users (https://masalmon.eu/2020/11/06/single-source-publishing-r/) RStudio 1.4 Preview: Rainbow Parentheses…",NA
"206","issue_2020_44_highlights",2020-11-02,6M 12S,"About this episode {emphatic} highlighting, analyzing open political data, and helping data-science learners Episode Links This week's curator: Tony Elhabr (@TonyElHabr (https://twitter.com/tonyelhabr)) {emphatic} (https://github.com/coolbutuseless/emphatic): Augments the output of data.frames and matrices in R by adding user-defined ANSI…",NA
"207","issue_2020_43_highlights",2020-10-26,6M 25S,"About this episode Rolling averages with {slider}, personal art map, and flood mapping Episode Links This week's curator: Miles McBain (@MilesMcBain (https://twitter.com/MilesMcBain)) Rolling Averages with {slider} and Covid Data (https://www.njtierney.com/post/2020/10/20/roll-avg-covid/) Personal Art Map with R…",NA
"208","issue_2020_42_highlights",2020-10-19,6M 33S,"About this episode Climate animation, decomposition and smoothing with R and python, and a Raspberry Pi dashboard Episode Links This week's curator: Ryo Nakagawara (@RbyRyo (https://twitter.com/R_by_Ryo)) Climate animation of maximum temperatures (https://dominicroye.github.io/en/2020/climate-animation-of-maximum-temperatures/) Decomposition and…",NA
"209","issue_2020_41_highlights",2020-10-12,8M 11S,"About this episode Topics in package development, contributing to ROpenSci, and shining a light on learnr Episode Links This week's curator: Eric Nantz (@theRcast (https://twitter.com/thercast)) Picking and researching blog topics about R package development (https://blog.r-hub.io/2020/10/09/topic-research/) Hacktober? Any Month is a Good Month to…",NA
"210","issue_2020_40_highlights",2020-10-05,6M 52S,"About this episode Visual markdown editing, stat layers in ggplot2, and learnr tutorials in a package Episode Links This week's curator: Jonathan Carroll (@carroll_jono (https://twitter.com/carroll_jono)) RStudio v1.4 Preview: Visual Markdown Editing (https://blog.rstudio.com/2020/09/30/rstudio-v1-4-preview-visual-markdown-editing/) How to deliver…",NA
"211","issue_2020_39_highlights",2020-09-28,6M 29S,"About this episode A calendar right in your R console, accessibility tooling for Shiny, and shinydashboardPlus v2.0 Episode Links This week's curator: Maelle Salmon (@ma_salmon (https://twitter.com/ma_salmon)) A Calendar in Your R Console (https://www.garrickadenbuie.com/blog/r-console-calendar/) accessibility (a11y) tooling for shiny…",NA
"212","issue_2020_38_highlights",2020-09-21,6M 15S,"About this episode Making learning to code friendlier with art, ggforce functions, and debugging in VSCode Episode Links It's a Bird, It's a Plane ... It's a ggforce function (https://ihaddadenfodil.com/post/it-s-a-bird-it-s-a-plane-it-s-a-ggforce-function/) Making Learning to Code Friendlier with Art — An Interview with Dr. Allison Horst…",NA
"213","issue_2020_37_highlights",2020-09-14,6M 28S,"About this episode Guidelines for creating better tables, a controlled vocabulary to name data frame columns, and exploring reactivity in Shiny applications. Episode Links 10+ Guidelines for Better Tables in R: Make tables people ACTUALLY want to read. (https://themockup.blog/posts/2020-09-04-10-table-rules-in-r/) Column Names as Contracts…",NA
"214","issue_2020_36_highlights",2020-09-07,5M 59S,"About this episode A stunning combination of physics and 3-D visualization, behind the curtain of package installation, and an alternative workflow for error handling in functions. Episode Links Plinko Statistics: Insights from the Bean Machine (https://www.tylermw.com/plinko-statistics-insights-from-the-bean-machine/) State of R packages in your…",NA
"215","issue_2020_35_highlights",2020-08-31,4M 30S,"About this episode A substantial update to the magrittr package coming soon, creating visualizations in D3 from an R user's perspective, and a big book of R. Episode Links magrittr 2.0 is coming soon (https://www.tidyverse.org/blog/2020/08/magrittr-2-0/) D3 to R to D3 (https://maya.rbind.io/post/2020/d3-to-r-to-d3/) Big Book of R…",NA
"216","issue_2020_34_highlights",2020-08-24,6M 2S,"About this episode Choosing an operating system for R users, examining regression techniques, and inside the development of dittodb / generating data from truncated distributions Episode Links Best OS for R users (https://www.jimhester.com/post/2020-08-20-best-os-for-r/) Lines of best fit (http://freerangestats.info/blog/2020/08/23/highered-ols)…",NA
"217","issue_2020_33_highlights",2020-08-17,4M 17S,"About this episode Exploring comic book creation with the tidyverse and ggplot2, and updates to the showtext and shinycssloader packages on CRAN. Episode Links A visualization exploring types of comic transitions as described in Scott McCloud's ""Understanding Comics"". (https://github.com/sharlagelfand/understanding-comics) {showtext} 0.9…",NA
"218","introduction",2020-08-13,3M 47S,"In this introduction episode, Eric Nantz shares an introduction to the RWeekly community project as well as the movitation for creating this brand new RWeekly Highlights podcast! Episode Links rweekly.org (https://rweekly.org) github.com/rweekly/rweekly.org (https://github.com/rweekly/rweekly.org)",NA
